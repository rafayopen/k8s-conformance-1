I1217 13:32:32.978921      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-643038348
I1217 13:32:32.979137      15 e2e.go:241] Starting e2e run "50ada80f-3a79-4a37-92e6-653481cf2063" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576589551 - Will randomize all specs
Will run 215 of 4413 specs

Dec 17 13:32:33.074: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 13:32:33.075: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 17 13:32:33.096: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 17 13:32:33.137: INFO: 23 / 23 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 17 13:32:33.137: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Dec 17 13:32:33.138: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 17 13:32:33.147: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 17 13:32:33.147: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 17 13:32:33.147: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Dec 17 13:32:33.147: INFO: e2e test version: v1.15.3
Dec 17 13:32:33.150: INFO: kube-apiserver version: v1.15.3
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:32:33.152: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename watch
Dec 17 13:32:33.198: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:32:38.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-315" for this suite.
Dec 17 13:32:44.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:32:45.063: INFO: namespace watch-315 deletion completed in 6.203931577s

• [SLOW TEST:11.911 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:32:45.064: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 17 13:32:45.112: INFO: Waiting up to 5m0s for pod "pod-b77c552d-0f4b-4c31-9907-7bb94a752401" in namespace "emptydir-917" to be "success or failure"
Dec 17 13:32:45.118: INFO: Pod "pod-b77c552d-0f4b-4c31-9907-7bb94a752401": Phase="Pending", Reason="", readiness=false. Elapsed: 5.408381ms
Dec 17 13:32:47.122: INFO: Pod "pod-b77c552d-0f4b-4c31-9907-7bb94a752401": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009177561s
Dec 17 13:32:49.127: INFO: Pod "pod-b77c552d-0f4b-4c31-9907-7bb94a752401": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014148482s
STEP: Saw pod success
Dec 17 13:32:49.127: INFO: Pod "pod-b77c552d-0f4b-4c31-9907-7bb94a752401" satisfied condition "success or failure"
Dec 17 13:32:49.129: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-b77c552d-0f4b-4c31-9907-7bb94a752401 container test-container: <nil>
STEP: delete the pod
Dec 17 13:32:49.171: INFO: Waiting for pod pod-b77c552d-0f4b-4c31-9907-7bb94a752401 to disappear
Dec 17 13:32:49.173: INFO: Pod pod-b77c552d-0f4b-4c31-9907-7bb94a752401 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:32:49.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-917" for this suite.
Dec 17 13:32:55.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:32:55.339: INFO: namespace emptydir-917 deletion completed in 6.161402333s

• [SLOW TEST:10.276 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:32:55.344: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-655bb8af-5dd2-4a87-9606-11fb6be6b192
STEP: Creating a pod to test consume configMaps
Dec 17 13:32:55.451: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9f5ab442-cacd-4d43-a746-12bb05836860" in namespace "projected-2195" to be "success or failure"
Dec 17 13:32:55.456: INFO: Pod "pod-projected-configmaps-9f5ab442-cacd-4d43-a746-12bb05836860": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452891ms
Dec 17 13:32:57.461: INFO: Pod "pod-projected-configmaps-9f5ab442-cacd-4d43-a746-12bb05836860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010180534s
Dec 17 13:32:59.465: INFO: Pod "pod-projected-configmaps-9f5ab442-cacd-4d43-a746-12bb05836860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01354985s
STEP: Saw pod success
Dec 17 13:32:59.465: INFO: Pod "pod-projected-configmaps-9f5ab442-cacd-4d43-a746-12bb05836860" satisfied condition "success or failure"
Dec 17 13:32:59.467: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-configmaps-9f5ab442-cacd-4d43-a746-12bb05836860 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 13:32:59.484: INFO: Waiting for pod pod-projected-configmaps-9f5ab442-cacd-4d43-a746-12bb05836860 to disappear
Dec 17 13:32:59.487: INFO: Pod pod-projected-configmaps-9f5ab442-cacd-4d43-a746-12bb05836860 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:32:59.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2195" for this suite.
Dec 17 13:33:05.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:33:05.605: INFO: namespace projected-2195 deletion completed in 6.116052078s

• [SLOW TEST:10.262 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:33:05.606: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-29a5cc3f-cd7a-48e9-9743-0039624e5189
Dec 17 13:33:05.657: INFO: Pod name my-hostname-basic-29a5cc3f-cd7a-48e9-9743-0039624e5189: Found 0 pods out of 1
Dec 17 13:33:10.666: INFO: Pod name my-hostname-basic-29a5cc3f-cd7a-48e9-9743-0039624e5189: Found 1 pods out of 1
Dec 17 13:33:10.667: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-29a5cc3f-cd7a-48e9-9743-0039624e5189" are running
Dec 17 13:33:10.671: INFO: Pod "my-hostname-basic-29a5cc3f-cd7a-48e9-9743-0039624e5189-n29w7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-17 13:33:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-17 13:33:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-17 13:33:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-17 13:33:05 +0000 UTC Reason: Message:}])
Dec 17 13:33:10.671: INFO: Trying to dial the pod
Dec 17 13:33:15.687: INFO: Controller my-hostname-basic-29a5cc3f-cd7a-48e9-9743-0039624e5189: Got expected result from replica 1 [my-hostname-basic-29a5cc3f-cd7a-48e9-9743-0039624e5189-n29w7]: "my-hostname-basic-29a5cc3f-cd7a-48e9-9743-0039624e5189-n29w7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:33:15.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-508" for this suite.
Dec 17 13:33:21.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:33:21.795: INFO: namespace replication-controller-508 deletion completed in 6.10274791s

• [SLOW TEST:16.189 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:33:21.795: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Dec 17 13:33:21.842: INFO: Waiting up to 5m0s for pod "client-containers-0d5d2f1e-f934-40fb-b484-faa3b9d83926" in namespace "containers-3100" to be "success or failure"
Dec 17 13:33:21.857: INFO: Pod "client-containers-0d5d2f1e-f934-40fb-b484-faa3b9d83926": Phase="Pending", Reason="", readiness=false. Elapsed: 15.702501ms
Dec 17 13:33:23.868: INFO: Pod "client-containers-0d5d2f1e-f934-40fb-b484-faa3b9d83926": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025983545s
Dec 17 13:33:25.872: INFO: Pod "client-containers-0d5d2f1e-f934-40fb-b484-faa3b9d83926": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030601678s
STEP: Saw pod success
Dec 17 13:33:25.872: INFO: Pod "client-containers-0d5d2f1e-f934-40fb-b484-faa3b9d83926" satisfied condition "success or failure"
Dec 17 13:33:25.876: INFO: Trying to get logs from node conformance-k8s-node-2 pod client-containers-0d5d2f1e-f934-40fb-b484-faa3b9d83926 container test-container: <nil>
STEP: delete the pod
Dec 17 13:33:25.897: INFO: Waiting for pod client-containers-0d5d2f1e-f934-40fb-b484-faa3b9d83926 to disappear
Dec 17 13:33:25.899: INFO: Pod client-containers-0d5d2f1e-f934-40fb-b484-faa3b9d83926 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:33:25.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3100" for this suite.
Dec 17 13:33:31.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:33:32.038: INFO: namespace containers-3100 deletion completed in 6.136203625s

• [SLOW TEST:10.244 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:33:32.040: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-3a92fc0c-1502-4da8-89b5-6f1f2a2cfd91
STEP: Creating a pod to test consume secrets
Dec 17 13:33:32.096: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e1cdfd69-240a-4152-8687-3cbac5ddec77" in namespace "projected-4819" to be "success or failure"
Dec 17 13:33:32.103: INFO: Pod "pod-projected-secrets-e1cdfd69-240a-4152-8687-3cbac5ddec77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.530171ms
Dec 17 13:33:34.107: INFO: Pod "pod-projected-secrets-e1cdfd69-240a-4152-8687-3cbac5ddec77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010658369s
STEP: Saw pod success
Dec 17 13:33:34.107: INFO: Pod "pod-projected-secrets-e1cdfd69-240a-4152-8687-3cbac5ddec77" satisfied condition "success or failure"
Dec 17 13:33:34.110: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-secrets-e1cdfd69-240a-4152-8687-3cbac5ddec77 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 13:33:34.144: INFO: Waiting for pod pod-projected-secrets-e1cdfd69-240a-4152-8687-3cbac5ddec77 to disappear
Dec 17 13:33:34.152: INFO: Pod pod-projected-secrets-e1cdfd69-240a-4152-8687-3cbac5ddec77 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:33:34.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4819" for this suite.
Dec 17 13:33:40.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:33:40.307: INFO: namespace projected-4819 deletion completed in 6.149893002s

• [SLOW TEST:8.267 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:33:40.308: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 17 13:33:46.907: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-437 pod-service-account-627d2339-44e7-40aa-aef1-bf0091715dca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 17 13:33:47.606: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-437 pod-service-account-627d2339-44e7-40aa-aef1-bf0091715dca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 17 13:33:47.850: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-437 pod-service-account-627d2339-44e7-40aa-aef1-bf0091715dca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:33:48.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-437" for this suite.
Dec 17 13:33:54.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:33:54.252: INFO: namespace svcaccounts-437 deletion completed in 6.145410636s

• [SLOW TEST:13.943 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:33:54.252: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:33:54.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2332" for this suite.
Dec 17 13:34:00.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:34:00.404: INFO: namespace services-2332 deletion completed in 6.107529494s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.151 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:34:00.404: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 17 13:34:00.473: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:34:01.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2473" for this suite.
Dec 17 13:34:07.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:34:07.671: INFO: namespace replication-controller-2473 deletion completed in 6.140071898s

• [SLOW TEST:7.268 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:34:07.676: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-69975fc8-6ee8-4c0d-b32e-be68ca03e1a0
STEP: Creating a pod to test consume configMaps
Dec 17 13:34:07.732: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6183f5b2-13c2-4478-a2de-b2e8dd846635" in namespace "projected-3910" to be "success or failure"
Dec 17 13:34:07.745: INFO: Pod "pod-projected-configmaps-6183f5b2-13c2-4478-a2de-b2e8dd846635": Phase="Pending", Reason="", readiness=false. Elapsed: 12.683106ms
Dec 17 13:34:09.751: INFO: Pod "pod-projected-configmaps-6183f5b2-13c2-4478-a2de-b2e8dd846635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018028385s
Dec 17 13:34:11.758: INFO: Pod "pod-projected-configmaps-6183f5b2-13c2-4478-a2de-b2e8dd846635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025077249s
STEP: Saw pod success
Dec 17 13:34:11.758: INFO: Pod "pod-projected-configmaps-6183f5b2-13c2-4478-a2de-b2e8dd846635" satisfied condition "success or failure"
Dec 17 13:34:11.762: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-configmaps-6183f5b2-13c2-4478-a2de-b2e8dd846635 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 13:34:11.789: INFO: Waiting for pod pod-projected-configmaps-6183f5b2-13c2-4478-a2de-b2e8dd846635 to disappear
Dec 17 13:34:11.792: INFO: Pod pod-projected-configmaps-6183f5b2-13c2-4478-a2de-b2e8dd846635 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:34:11.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3910" for this suite.
Dec 17 13:34:17.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:34:17.907: INFO: namespace projected-3910 deletion completed in 6.111816265s

• [SLOW TEST:10.232 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:34:17.912: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3068
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec 17 13:34:17.990: INFO: Found 0 stateful pods, waiting for 3
Dec 17 13:34:28.000: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 13:34:28.000: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 13:34:28.000: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 17 13:34:37.998: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 13:34:37.998: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 13:34:37.998: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 17 13:34:38.029: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 17 13:34:48.086: INFO: Updating stateful set ss2
Dec 17 13:34:48.093: INFO: Waiting for Pod statefulset-3068/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec 17 13:34:58.201: INFO: Found 2 stateful pods, waiting for 3
Dec 17 13:35:08.206: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 13:35:08.207: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 13:35:08.207: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 17 13:35:08.229: INFO: Updating stateful set ss2
Dec 17 13:35:08.246: INFO: Waiting for Pod statefulset-3068/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 17 13:35:18.256: INFO: Waiting for Pod statefulset-3068/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 17 13:35:28.275: INFO: Updating stateful set ss2
Dec 17 13:35:28.352: INFO: Waiting for StatefulSet statefulset-3068/ss2 to complete update
Dec 17 13:35:28.352: INFO: Waiting for Pod statefulset-3068/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 17 13:35:38.361: INFO: Waiting for StatefulSet statefulset-3068/ss2 to complete update
Dec 17 13:35:38.361: INFO: Waiting for Pod statefulset-3068/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 17 13:35:48.361: INFO: Deleting all statefulset in ns statefulset-3068
Dec 17 13:35:48.364: INFO: Scaling statefulset ss2 to 0
Dec 17 13:36:28.386: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 13:36:28.391: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:36:28.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3068" for this suite.
Dec 17 13:36:34.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:36:34.536: INFO: namespace statefulset-3068 deletion completed in 6.127025783s

• [SLOW TEST:136.624 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:36:34.541: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-a4315558-71bf-4a56-980e-18304da43750
STEP: Creating a pod to test consume secrets
Dec 17 13:36:34.606: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4bf25aa3-7320-4b12-929f-6db3878ce249" in namespace "projected-310" to be "success or failure"
Dec 17 13:36:34.610: INFO: Pod "pod-projected-secrets-4bf25aa3-7320-4b12-929f-6db3878ce249": Phase="Pending", Reason="", readiness=false. Elapsed: 4.232058ms
Dec 17 13:36:36.616: INFO: Pod "pod-projected-secrets-4bf25aa3-7320-4b12-929f-6db3878ce249": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010010819s
STEP: Saw pod success
Dec 17 13:36:36.616: INFO: Pod "pod-projected-secrets-4bf25aa3-7320-4b12-929f-6db3878ce249" satisfied condition "success or failure"
Dec 17 13:36:36.619: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-secrets-4bf25aa3-7320-4b12-929f-6db3878ce249 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 13:36:36.644: INFO: Waiting for pod pod-projected-secrets-4bf25aa3-7320-4b12-929f-6db3878ce249 to disappear
Dec 17 13:36:36.648: INFO: Pod pod-projected-secrets-4bf25aa3-7320-4b12-929f-6db3878ce249 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:36:36.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-310" for this suite.
Dec 17 13:36:42.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:36:42.779: INFO: namespace projected-310 deletion completed in 6.125570997s

• [SLOW TEST:8.238 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:36:42.782: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:37:07.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4684" for this suite.
Dec 17 13:37:13.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:37:13.213: INFO: namespace container-runtime-4684 deletion completed in 6.109308118s

• [SLOW TEST:30.432 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:37:13.217: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 13:37:13.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4fe64f1-d88e-4a4e-ac1e-ad5392c65c70" in namespace "projected-1609" to be "success or failure"
Dec 17 13:37:13.270: INFO: Pod "downwardapi-volume-b4fe64f1-d88e-4a4e-ac1e-ad5392c65c70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468564ms
Dec 17 13:37:15.274: INFO: Pod "downwardapi-volume-b4fe64f1-d88e-4a4e-ac1e-ad5392c65c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007123257s
STEP: Saw pod success
Dec 17 13:37:15.274: INFO: Pod "downwardapi-volume-b4fe64f1-d88e-4a4e-ac1e-ad5392c65c70" satisfied condition "success or failure"
Dec 17 13:37:15.277: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-b4fe64f1-d88e-4a4e-ac1e-ad5392c65c70 container client-container: <nil>
STEP: delete the pod
Dec 17 13:37:15.309: INFO: Waiting for pod downwardapi-volume-b4fe64f1-d88e-4a4e-ac1e-ad5392c65c70 to disappear
Dec 17 13:37:15.311: INFO: Pod downwardapi-volume-b4fe64f1-d88e-4a4e-ac1e-ad5392c65c70 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:37:15.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1609" for this suite.
Dec 17 13:37:21.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:37:21.429: INFO: namespace projected-1609 deletion completed in 6.113737508s

• [SLOW TEST:8.212 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:37:21.429: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 13:37:21.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-3694'
Dec 17 13:37:21.839: INFO: stderr: ""
Dec 17 13:37:21.840: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 17 13:37:21.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-3694'
Dec 17 13:37:22.136: INFO: stderr: ""
Dec 17 13:37:22.137: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 17 13:37:23.142: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:37:23.142: INFO: Found 0 / 1
Dec 17 13:37:24.141: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:37:24.141: INFO: Found 0 / 1
Dec 17 13:37:25.141: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:37:25.141: INFO: Found 0 / 1
Dec 17 13:37:26.142: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:37:26.142: INFO: Found 1 / 1
Dec 17 13:37:26.142: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 17 13:37:26.145: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:37:26.145: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 17 13:37:26.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 describe pod redis-master-c85mh --namespace=kubectl-3694'
Dec 17 13:37:26.247: INFO: stderr: ""
Dec 17 13:37:26.247: INFO: stdout: "Name:           redis-master-c85mh\nNamespace:      kubectl-3694\nPriority:       0\nNode:           conformance-k8s-node-2/192.168.1.5\nStart Time:     Tue, 17 Dec 2019 13:37:21 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.233.100.19\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://86335bef083d395d168ef3ac3e5de62bd5b9ac5a98a3f4e8a12932776208ba9d\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 17 Dec 2019 13:37:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-v7w6v (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-v7w6v:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-v7w6v\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                             Message\n  ----    ------     ----  ----                             -------\n  Normal  Scheduled  5s    default-scheduler                Successfully assigned kubectl-3694/redis-master-c85mh to conformance-k8s-node-2\n  Normal  Pulling    4s    kubelet, conformance-k8s-node-2  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     2s    kubelet, conformance-k8s-node-2  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    2s    kubelet, conformance-k8s-node-2  Created container redis-master\n  Normal  Started    1s    kubelet, conformance-k8s-node-2  Started container redis-master\n"
Dec 17 13:37:26.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 describe rc redis-master --namespace=kubectl-3694'
Dec 17 13:37:26.355: INFO: stderr: ""
Dec 17 13:37:26.355: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-3694\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: redis-master-c85mh\n"
Dec 17 13:37:26.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 describe service redis-master --namespace=kubectl-3694'
Dec 17 13:37:26.458: INFO: stderr: ""
Dec 17 13:37:26.459: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-3694\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.17.126\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.100.19:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 17 13:37:26.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 describe node conformance-k8s-master-1'
Dec 17 13:37:26.587: INFO: stderr: ""
Dec 17 13:37:26.587: INFO: stdout: "Name:               conformance-k8s-master-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=conformance-k8s-master-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 17 Dec 2019 13:26:33 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 17 Dec 2019 13:28:04 +0000   Tue, 17 Dec 2019 13:28:04 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 17 Dec 2019 13:37:24 +0000   Tue, 17 Dec 2019 13:26:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 17 Dec 2019 13:37:24 +0000   Tue, 17 Dec 2019 13:26:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 17 Dec 2019 13:37:24 +0000   Tue, 17 Dec 2019 13:26:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 17 Dec 2019 13:37:24 +0000   Tue, 17 Dec 2019 13:27:53 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.1.6\n  Hostname:    conformance-k8s-master-1\nCapacity:\n cpu:                2\n ephemeral-storage:  40593708Ki\n hugepages-2Mi:      0\n memory:             4046596Ki\n pods:               110\nAllocatable:\n cpu:                1800m\n ephemeral-storage:  37411161231\n hugepages-2Mi:      0\n memory:             3444196Ki\n pods:               110\nSystem Info:\n Machine ID:                 e095f1460fda466da9e426a3e0944a15\n System UUID:                E095F146-0FDA-466D-A9E4-26A3E0944A15\n Boot ID:                    8217a4eb-fc21-49aa-b0b7-35f38e5e5b9b\n Kernel Version:             4.4.0-31-generic\n OS Image:                   Ubuntu 16.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.7\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nPodCIDR:                     10.233.64.0/24\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-node-p4hhg                                          150m (8%)     300m (16%)  64M (1%)         500M (14%)     9m41s\n  kube-system                coredns-74c9d4d795-ltz8d                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     8m37s\n  kube-system                dns-autoscaler-7d95989447-wfqv8                            20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         8m32s\n  kube-system                kube-apiserver-conformance-k8s-master-1                    250m (13%)    0 (0%)      0 (0%)           0 (0%)         10m\n  kube-system                kube-controller-manager-conformance-k8s-master-1           200m (11%)    0 (0%)      0 (0%)           0 (0%)         10m\n  kube-system                kube-proxy-mb7q6                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m\n  kube-system                kube-scheduler-conformance-k8s-master-1                    100m (5%)     0 (0%)      0 (0%)           0 (0%)         10m\n  kube-system                nodelocaldns-vnnrl                                         100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     8m30s\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-trj8h    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m21s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                920m (51%)      300m (16%)\n  memory             221286400 (6%)  856515840 (24%)\n  ephemeral-storage  0 (0%)          0 (0%)\nEvents:\n  Type    Reason                   Age                From                                  Message\n  ----    ------                   ----               ----                                  -------\n  Normal  NodeHasSufficientMemory  10m (x8 over 10m)  kubelet, conformance-k8s-master-1     Node conformance-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    10m (x8 over 10m)  kubelet, conformance-k8s-master-1     Node conformance-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     10m (x7 over 10m)  kubelet, conformance-k8s-master-1     Node conformance-k8s-master-1 status is now: NodeHasSufficientPID\n  Normal  Starting                 10m                kube-proxy, conformance-k8s-master-1  Starting kube-proxy.\n  Normal  Starting                 10m                kubelet, conformance-k8s-master-1     Starting kubelet.\n  Normal  NodeHasSufficientMemory  10m                kubelet, conformance-k8s-master-1     Node conformance-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    10m                kubelet, conformance-k8s-master-1     Node conformance-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     10m                kubelet, conformance-k8s-master-1     Node conformance-k8s-master-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  10m                kubelet, conformance-k8s-master-1     Updated Node Allocatable limit across pods\n  Normal  Starting                 10m                kube-proxy, conformance-k8s-master-1  Starting kube-proxy.\n  Normal  NodeReady                9m33s              kubelet, conformance-k8s-master-1     Node conformance-k8s-master-1 status is now: NodeReady\n"
Dec 17 13:37:26.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 describe namespace kubectl-3694'
Dec 17 13:37:26.682: INFO: stderr: ""
Dec 17 13:37:26.682: INFO: stdout: "Name:         kubectl-3694\nLabels:       e2e-framework=kubectl\n              e2e-run=50ada80f-3a79-4a37-92e6-653481cf2063\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:37:26.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3694" for this suite.
Dec 17 13:37:48.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:37:48.789: INFO: namespace kubectl-3694 deletion completed in 22.102670435s

• [SLOW TEST:27.360 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:37:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-69e682b0-b465-4d33-867b-6d269591d94a
STEP: Creating a pod to test consume configMaps
Dec 17 13:37:48.855: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9f95640a-f60d-49be-a477-7039245bd6b7" in namespace "projected-9625" to be "success or failure"
Dec 17 13:37:48.858: INFO: Pod "pod-projected-configmaps-9f95640a-f60d-49be-a477-7039245bd6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.178103ms
Dec 17 13:37:50.863: INFO: Pod "pod-projected-configmaps-9f95640a-f60d-49be-a477-7039245bd6b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008716043s
STEP: Saw pod success
Dec 17 13:37:50.863: INFO: Pod "pod-projected-configmaps-9f95640a-f60d-49be-a477-7039245bd6b7" satisfied condition "success or failure"
Dec 17 13:37:50.866: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-configmaps-9f95640a-f60d-49be-a477-7039245bd6b7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 13:37:50.902: INFO: Waiting for pod pod-projected-configmaps-9f95640a-f60d-49be-a477-7039245bd6b7 to disappear
Dec 17 13:37:50.904: INFO: Pod pod-projected-configmaps-9f95640a-f60d-49be-a477-7039245bd6b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:37:50.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9625" for this suite.
Dec 17 13:37:56.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:37:57.045: INFO: namespace projected-9625 deletion completed in 6.136931521s

• [SLOW TEST:8.252 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:37:57.046: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-h4sg
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 13:37:57.116: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h4sg" in namespace "subpath-1833" to be "success or failure"
Dec 17 13:37:57.119: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.939098ms
Dec 17 13:37:59.135: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019114609s
Dec 17 13:38:01.141: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Running", Reason="", readiness=true. Elapsed: 4.024621867s
Dec 17 13:38:03.145: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Running", Reason="", readiness=true. Elapsed: 6.029495654s
Dec 17 13:38:05.152: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Running", Reason="", readiness=true. Elapsed: 8.035887243s
Dec 17 13:38:07.156: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Running", Reason="", readiness=true. Elapsed: 10.039981495s
Dec 17 13:38:09.161: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Running", Reason="", readiness=true. Elapsed: 12.044803746s
Dec 17 13:38:11.166: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Running", Reason="", readiness=true. Elapsed: 14.049919086s
Dec 17 13:38:13.172: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Running", Reason="", readiness=true. Elapsed: 16.05564485s
Dec 17 13:38:15.181: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Running", Reason="", readiness=true. Elapsed: 18.065446551s
Dec 17 13:38:17.186: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Running", Reason="", readiness=true. Elapsed: 20.070005486s
Dec 17 13:38:19.194: INFO: Pod "pod-subpath-test-downwardapi-h4sg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.077699043s
STEP: Saw pod success
Dec 17 13:38:19.194: INFO: Pod "pod-subpath-test-downwardapi-h4sg" satisfied condition "success or failure"
Dec 17 13:38:19.198: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-subpath-test-downwardapi-h4sg container test-container-subpath-downwardapi-h4sg: <nil>
STEP: delete the pod
Dec 17 13:38:19.231: INFO: Waiting for pod pod-subpath-test-downwardapi-h4sg to disappear
Dec 17 13:38:19.237: INFO: Pod pod-subpath-test-downwardapi-h4sg no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-h4sg
Dec 17 13:38:19.237: INFO: Deleting pod "pod-subpath-test-downwardapi-h4sg" in namespace "subpath-1833"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:38:19.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1833" for this suite.
Dec 17 13:38:25.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:38:25.356: INFO: namespace subpath-1833 deletion completed in 6.111549483s

• [SLOW TEST:28.311 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:38:25.357: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 13:38:25.412: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a99c539-cc33-4b2b-b2f7-040049ee0080" in namespace "projected-4387" to be "success or failure"
Dec 17 13:38:25.420: INFO: Pod "downwardapi-volume-5a99c539-cc33-4b2b-b2f7-040049ee0080": Phase="Pending", Reason="", readiness=false. Elapsed: 7.545262ms
Dec 17 13:38:27.425: INFO: Pod "downwardapi-volume-5a99c539-cc33-4b2b-b2f7-040049ee0080": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013092686s
STEP: Saw pod success
Dec 17 13:38:27.425: INFO: Pod "downwardapi-volume-5a99c539-cc33-4b2b-b2f7-040049ee0080" satisfied condition "success or failure"
Dec 17 13:38:27.429: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-5a99c539-cc33-4b2b-b2f7-040049ee0080 container client-container: <nil>
STEP: delete the pod
Dec 17 13:38:27.457: INFO: Waiting for pod downwardapi-volume-5a99c539-cc33-4b2b-b2f7-040049ee0080 to disappear
Dec 17 13:38:27.471: INFO: Pod downwardapi-volume-5a99c539-cc33-4b2b-b2f7-040049ee0080 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:38:27.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4387" for this suite.
Dec 17 13:38:33.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:38:33.575: INFO: namespace projected-4387 deletion completed in 6.104023614s

• [SLOW TEST:8.218 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:38:33.576: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1217 13:38:43.653538      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 17 13:38:43.653: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:38:43.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9593" for this suite.
Dec 17 13:38:49.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:38:49.749: INFO: namespace gc-9593 deletion completed in 6.092694324s

• [SLOW TEST:16.174 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:38:49.753: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 13:38:49.820: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed1b0c99-bb96-453e-8445-c5d053f57dcb" in namespace "projected-1511" to be "success or failure"
Dec 17 13:38:49.823: INFO: Pod "downwardapi-volume-ed1b0c99-bb96-453e-8445-c5d053f57dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.966609ms
Dec 17 13:38:51.827: INFO: Pod "downwardapi-volume-ed1b0c99-bb96-453e-8445-c5d053f57dcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007164023s
STEP: Saw pod success
Dec 17 13:38:51.827: INFO: Pod "downwardapi-volume-ed1b0c99-bb96-453e-8445-c5d053f57dcb" satisfied condition "success or failure"
Dec 17 13:38:51.830: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-ed1b0c99-bb96-453e-8445-c5d053f57dcb container client-container: <nil>
STEP: delete the pod
Dec 17 13:38:51.855: INFO: Waiting for pod downwardapi-volume-ed1b0c99-bb96-453e-8445-c5d053f57dcb to disappear
Dec 17 13:38:51.857: INFO: Pod downwardapi-volume-ed1b0c99-bb96-453e-8445-c5d053f57dcb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:38:51.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1511" for this suite.
Dec 17 13:38:57.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:38:57.989: INFO: namespace projected-1511 deletion completed in 6.128477796s

• [SLOW TEST:8.237 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:38:57.993: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 13:38:58.061: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 17 13:39:03.067: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 17 13:39:03.067: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 17 13:39:03.099: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8691,SelfLink:/apis/apps/v1/namespaces/deployment-8691/deployments/test-cleanup-deployment,UID:d6982519-f56b-495c-a1a7-fe231d9e18ce,ResourceVersion:3692,Generation:1,CreationTimestamp:2019-12-17 13:39:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Dec 17 13:39:03.113: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-8691,SelfLink:/apis/apps/v1/namespaces/deployment-8691/replicasets/test-cleanup-deployment-55bbcbc84c,UID:78763b04-86b7-4e78-aa0b-5eb7f3316543,ResourceVersion:3694,Generation:1,CreationTimestamp:2019-12-17 13:39:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment d6982519-f56b-495c-a1a7-fe231d9e18ce 0xc00275ebb7 0xc00275ebb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 17 13:39:03.113: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 17 13:39:03.113: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-8691,SelfLink:/apis/apps/v1/namespaces/deployment-8691/replicasets/test-cleanup-controller,UID:dbdea0d9-329b-49f3-9f98-6fd71586e9e0,ResourceVersion:3693,Generation:1,CreationTimestamp:2019-12-17 13:38:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment d6982519-f56b-495c-a1a7-fe231d9e18ce 0xc00275eadf 0xc00275eaf0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 17 13:39:03.123: INFO: Pod "test-cleanup-controller-zcktg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-zcktg,GenerateName:test-cleanup-controller-,Namespace:deployment-8691,SelfLink:/api/v1/namespaces/deployment-8691/pods/test-cleanup-controller-zcktg,UID:9657eaa0-295a-4ec6-9881-44f863590674,ResourceVersion:3684,Generation:0,CreationTimestamp:2019-12-17 13:38:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller dbdea0d9-329b-49f3-9f98-6fd71586e9e0 0xc00275f45f 0xc00275f470}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8tpc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8tpc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8tpc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00275f4d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00275f4f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:38:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:39:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:39:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:38:58 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:10.233.100.25,StartTime:2019-12-17 13:38:58 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-17 13:38:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://69d62b252b433e9074c705535f51f548cd3e42ca95f910caca6eef232b05d6bf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:39:03.123: INFO: Pod "test-cleanup-deployment-55bbcbc84c-rf6dg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-rf6dg,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-8691,SelfLink:/api/v1/namespaces/deployment-8691/pods/test-cleanup-deployment-55bbcbc84c-rf6dg,UID:3be34412-66dc-4fc3-aad4-fc48ee6d7f5c,ResourceVersion:3700,Generation:0,CreationTimestamp:2019-12-17 13:39:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 78763b04-86b7-4e78-aa0b-5eb7f3316543 0xc00275f5c7 0xc00275f5c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8tpc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8tpc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8tpc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00275f630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00275f650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:39:03 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:39:03.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8691" for this suite.
Dec 17 13:39:09.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:39:09.249: INFO: namespace deployment-8691 deletion completed in 6.114265048s

• [SLOW TEST:11.256 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:39:09.254: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 17 13:39:09.287: INFO: PodSpec: initContainers in spec.initContainers
Dec 17 13:39:56.010: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-81f33fdf-0ef0-4abe-9b4a-9eaaec205f53", GenerateName:"", Namespace:"init-container-9316", SelfLink:"/api/v1/namespaces/init-container-9316/pods/pod-init-81f33fdf-0ef0-4abe-9b4a-9eaaec205f53", UID:"1d9566e5-de5d-4c9d-a651-c6a94aad65bc", ResourceVersion:"3878", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63712186749, loc:(*time.Location)(0x7ec7a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"287916503"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xg2jn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000dc8000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xg2jn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xg2jn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xg2jn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0025c4098), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"conformance-k8s-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002f2e0c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025c4130)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025c4150)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0025c4158), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0025c415c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712186749, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712186749, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712186749, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712186749, loc:(*time.Location)(0x7ec7a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.7", PodIP:"10.233.93.6", StartTime:(*v1.Time)(0xc0023d01e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0023ae070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0023ae0e0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://2eeb92b87b90d6d68910930079ce0bcdd028fc1ae05a92bf3d870b6dc78080de"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0023d0320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0023d0280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:39:56.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9316" for this suite.
Dec 17 13:40:18.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:40:18.129: INFO: namespace init-container-9316 deletion completed in 22.111029869s

• [SLOW TEST:68.875 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:40:18.132: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 17 13:40:18.393: INFO: Pod name wrapped-volume-race-da2e68b5-1d07-4d30-ac76-0414dbaee76e: Found 0 pods out of 5
Dec 17 13:40:23.407: INFO: Pod name wrapped-volume-race-da2e68b5-1d07-4d30-ac76-0414dbaee76e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-da2e68b5-1d07-4d30-ac76-0414dbaee76e in namespace emptydir-wrapper-362, will wait for the garbage collector to delete the pods
Dec 17 13:40:33.492: INFO: Deleting ReplicationController wrapped-volume-race-da2e68b5-1d07-4d30-ac76-0414dbaee76e took: 6.723449ms
Dec 17 13:40:33.792: INFO: Terminating ReplicationController wrapped-volume-race-da2e68b5-1d07-4d30-ac76-0414dbaee76e pods took: 300.382052ms
STEP: Creating RC which spawns configmap-volume pods
Dec 17 13:41:11.114: INFO: Pod name wrapped-volume-race-c94f93b2-959e-43ac-ae87-3cf344579921: Found 0 pods out of 5
Dec 17 13:41:16.122: INFO: Pod name wrapped-volume-race-c94f93b2-959e-43ac-ae87-3cf344579921: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c94f93b2-959e-43ac-ae87-3cf344579921 in namespace emptydir-wrapper-362, will wait for the garbage collector to delete the pods
Dec 17 13:41:28.218: INFO: Deleting ReplicationController wrapped-volume-race-c94f93b2-959e-43ac-ae87-3cf344579921 took: 11.634337ms
Dec 17 13:41:28.518: INFO: Terminating ReplicationController wrapped-volume-race-c94f93b2-959e-43ac-ae87-3cf344579921 pods took: 300.244998ms
STEP: Creating RC which spawns configmap-volume pods
Dec 17 13:42:09.251: INFO: Pod name wrapped-volume-race-f69e88dd-2c0f-4fdc-81a0-f8cb6caf7b16: Found 0 pods out of 5
Dec 17 13:42:14.261: INFO: Pod name wrapped-volume-race-f69e88dd-2c0f-4fdc-81a0-f8cb6caf7b16: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f69e88dd-2c0f-4fdc-81a0-f8cb6caf7b16 in namespace emptydir-wrapper-362, will wait for the garbage collector to delete the pods
Dec 17 13:42:24.352: INFO: Deleting ReplicationController wrapped-volume-race-f69e88dd-2c0f-4fdc-81a0-f8cb6caf7b16 took: 9.519005ms
Dec 17 13:42:24.652: INFO: Terminating ReplicationController wrapped-volume-race-f69e88dd-2c0f-4fdc-81a0-f8cb6caf7b16 pods took: 300.214534ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:43:10.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-362" for this suite.
Dec 17 13:43:18.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:43:18.761: INFO: namespace emptydir-wrapper-362 deletion completed in 8.115887122s

• [SLOW TEST:180.629 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:43:18.761: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 17 13:43:20.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec pod-sharedvolume-8b0bb533-f7d4-4f46-ab29-27bbb7183c5f -c busybox-main-container --namespace=emptydir-1911 -- cat /usr/share/volumeshare/shareddata.txt'
Dec 17 13:43:21.054: INFO: stderr: ""
Dec 17 13:43:21.054: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:43:21.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1911" for this suite.
Dec 17 13:43:27.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:43:27.166: INFO: namespace emptydir-1911 deletion completed in 6.105609464s

• [SLOW TEST:8.405 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:43:27.166: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Dec 17 13:43:27.223: INFO: Waiting up to 5m0s for pod "var-expansion-77ad9661-e14a-445e-988e-218152792155" in namespace "var-expansion-2965" to be "success or failure"
Dec 17 13:43:27.230: INFO: Pod "var-expansion-77ad9661-e14a-445e-988e-218152792155": Phase="Pending", Reason="", readiness=false. Elapsed: 6.642071ms
Dec 17 13:43:29.236: INFO: Pod "var-expansion-77ad9661-e14a-445e-988e-218152792155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012115093s
STEP: Saw pod success
Dec 17 13:43:29.236: INFO: Pod "var-expansion-77ad9661-e14a-445e-988e-218152792155" satisfied condition "success or failure"
Dec 17 13:43:29.238: INFO: Trying to get logs from node conformance-k8s-node-2 pod var-expansion-77ad9661-e14a-445e-988e-218152792155 container dapi-container: <nil>
STEP: delete the pod
Dec 17 13:43:29.261: INFO: Waiting for pod var-expansion-77ad9661-e14a-445e-988e-218152792155 to disappear
Dec 17 13:43:29.263: INFO: Pod var-expansion-77ad9661-e14a-445e-988e-218152792155 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:43:29.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2965" for this suite.
Dec 17 13:43:35.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:43:35.379: INFO: namespace var-expansion-2965 deletion completed in 6.10831054s

• [SLOW TEST:8.213 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:43:35.381: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 17 13:43:35.429: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:43:49.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5763" for this suite.
Dec 17 13:43:55.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:43:55.614: INFO: namespace pods-5763 deletion completed in 6.142031867s

• [SLOW TEST:20.234 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:43:55.617: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Dec 17 13:43:55.674: INFO: Waiting up to 5m0s for pod "var-expansion-e5c8aa0a-c388-45fc-960d-96a690b7653f" in namespace "var-expansion-7403" to be "success or failure"
Dec 17 13:43:55.684: INFO: Pod "var-expansion-e5c8aa0a-c388-45fc-960d-96a690b7653f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.616778ms
Dec 17 13:43:57.689: INFO: Pod "var-expansion-e5c8aa0a-c388-45fc-960d-96a690b7653f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014422943s
STEP: Saw pod success
Dec 17 13:43:57.689: INFO: Pod "var-expansion-e5c8aa0a-c388-45fc-960d-96a690b7653f" satisfied condition "success or failure"
Dec 17 13:43:57.694: INFO: Trying to get logs from node conformance-k8s-node-2 pod var-expansion-e5c8aa0a-c388-45fc-960d-96a690b7653f container dapi-container: <nil>
STEP: delete the pod
Dec 17 13:43:57.743: INFO: Waiting for pod var-expansion-e5c8aa0a-c388-45fc-960d-96a690b7653f to disappear
Dec 17 13:43:57.746: INFO: Pod var-expansion-e5c8aa0a-c388-45fc-960d-96a690b7653f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:43:57.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7403" for this suite.
Dec 17 13:44:03.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:44:03.874: INFO: namespace var-expansion-7403 deletion completed in 6.123993116s

• [SLOW TEST:8.258 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:44:03.877: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 13:44:06.016: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:44:06.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-315" for this suite.
Dec 17 13:44:12.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:44:12.168: INFO: namespace container-runtime-315 deletion completed in 6.115535864s

• [SLOW TEST:8.291 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:44:12.171: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 17 13:44:18.252: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1217 13:44:18.252201      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:44:18.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3251" for this suite.
Dec 17 13:44:24.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:44:24.366: INFO: namespace gc-3251 deletion completed in 6.110780451s

• [SLOW TEST:12.195 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:44:24.367: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 13:44:24.400: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 17 13:44:24.416: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 17 13:44:29.421: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 17 13:44:29.421: INFO: Creating deployment "test-rolling-update-deployment"
Dec 17 13:44:29.427: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 17 13:44:29.442: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 17 13:44:31.454: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 17 13:44:31.458: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 17 13:44:31.469: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-5674,SelfLink:/apis/apps/v1/namespaces/deployment-5674/deployments/test-rolling-update-deployment,UID:cc35e19f-b2a2-4cdc-8667-779718cd30f2,ResourceVersion:5883,Generation:1,CreationTimestamp:2019-12-17 13:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-17 13:44:29 +0000 UTC 2019-12-17 13:44:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-17 13:44:31 +0000 UTC 2019-12-17 13:44:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 17 13:44:31.472: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-5674,SelfLink:/apis/apps/v1/namespaces/deployment-5674/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:b75b86a9-15b5-4932-accc-7c7e34ce582a,ResourceVersion:5873,Generation:1,CreationTimestamp:2019-12-17 13:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment cc35e19f-b2a2-4cdc-8667-779718cd30f2 0xc002557717 0xc002557718}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 17 13:44:31.472: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 17 13:44:31.472: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-5674,SelfLink:/apis/apps/v1/namespaces/deployment-5674/replicasets/test-rolling-update-controller,UID:09f6fab0-8d99-4966-99c3-79604f8bbcc4,ResourceVersion:5882,Generation:2,CreationTimestamp:2019-12-17 13:44:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment cc35e19f-b2a2-4cdc-8667-779718cd30f2 0xc00255763f 0xc002557650}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 17 13:44:31.476: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-pswf4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-pswf4,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-5674,SelfLink:/api/v1/namespaces/deployment-5674/pods/test-rolling-update-deployment-79f6b9d75c-pswf4,UID:5ea98597-f8ff-463f-a9c3-e947088dbadf,ResourceVersion:5872,Generation:0,CreationTimestamp:2019-12-17 13:44:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c b75b86a9-15b5-4932-accc-7c7e34ce582a 0xc002557fe7 0xc002557fe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hlf6c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hlf6c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-hlf6c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00357c050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00357c070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:44:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:44:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:44:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:44:29 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:10.233.100.37,StartTime:2019-12-17 13:44:29 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-17 13:44:30 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://63c40f023fd0a12e9cdeaf53315c6f9516f5272cec10a980a8058f4227ca955f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:44:31.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5674" for this suite.
Dec 17 13:44:37.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:44:37.611: INFO: namespace deployment-5674 deletion completed in 6.130970605s

• [SLOW TEST:13.245 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:44:37.613: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:45:37.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2576" for this suite.
Dec 17 13:45:59.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:45:59.771: INFO: namespace container-probe-2576 deletion completed in 22.105498463s

• [SLOW TEST:82.158 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:45:59.772: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 17 13:45:59.811: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:46:03.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2406" for this suite.
Dec 17 13:46:09.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:46:09.879: INFO: namespace init-container-2406 deletion completed in 6.112813259s

• [SLOW TEST:10.108 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:46:09.882: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 13:46:09.941: INFO: Waiting up to 5m0s for pod "downwardapi-volume-184684e6-e3bc-426f-97c3-ffa8a31f2ae8" in namespace "downward-api-1818" to be "success or failure"
Dec 17 13:46:09.946: INFO: Pod "downwardapi-volume-184684e6-e3bc-426f-97c3-ffa8a31f2ae8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.85898ms
Dec 17 13:46:11.950: INFO: Pod "downwardapi-volume-184684e6-e3bc-426f-97c3-ffa8a31f2ae8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009303744s
STEP: Saw pod success
Dec 17 13:46:11.951: INFO: Pod "downwardapi-volume-184684e6-e3bc-426f-97c3-ffa8a31f2ae8" satisfied condition "success or failure"
Dec 17 13:46:11.961: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-184684e6-e3bc-426f-97c3-ffa8a31f2ae8 container client-container: <nil>
STEP: delete the pod
Dec 17 13:46:11.986: INFO: Waiting for pod downwardapi-volume-184684e6-e3bc-426f-97c3-ffa8a31f2ae8 to disappear
Dec 17 13:46:11.988: INFO: Pod downwardapi-volume-184684e6-e3bc-426f-97c3-ffa8a31f2ae8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:46:11.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1818" for this suite.
Dec 17 13:46:18.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:46:18.131: INFO: namespace downward-api-1818 deletion completed in 6.139521963s

• [SLOW TEST:8.250 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:46:18.132: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-6694b7cf-be5a-4b3a-864b-778bfd2194f5
STEP: Creating secret with name s-test-opt-upd-3a95ae6e-bb46-4953-9d10-18df1e09637e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6694b7cf-be5a-4b3a-864b-778bfd2194f5
STEP: Updating secret s-test-opt-upd-3a95ae6e-bb46-4953-9d10-18df1e09637e
STEP: Creating secret with name s-test-opt-create-e6cc9100-5a24-49ba-9e04-718d41d83f03
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:46:24.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3419" for this suite.
Dec 17 13:46:46.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:46:46.447: INFO: namespace secrets-3419 deletion completed in 22.108574137s

• [SLOW TEST:28.315 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:46:46.448: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-0d9cf81b-d6bb-4ebe-b512-a02f8b4c9da9
STEP: Creating a pod to test consume secrets
Dec 17 13:46:46.494: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4caf409c-f9fb-4231-895e-1346ec9a60b9" in namespace "projected-6319" to be "success or failure"
Dec 17 13:46:46.505: INFO: Pod "pod-projected-secrets-4caf409c-f9fb-4231-895e-1346ec9a60b9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.359987ms
Dec 17 13:46:48.508: INFO: Pod "pod-projected-secrets-4caf409c-f9fb-4231-895e-1346ec9a60b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013739108s
Dec 17 13:46:50.512: INFO: Pod "pod-projected-secrets-4caf409c-f9fb-4231-895e-1346ec9a60b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017283725s
STEP: Saw pod success
Dec 17 13:46:50.512: INFO: Pod "pod-projected-secrets-4caf409c-f9fb-4231-895e-1346ec9a60b9" satisfied condition "success or failure"
Dec 17 13:46:50.515: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-secrets-4caf409c-f9fb-4231-895e-1346ec9a60b9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 13:46:50.547: INFO: Waiting for pod pod-projected-secrets-4caf409c-f9fb-4231-895e-1346ec9a60b9 to disappear
Dec 17 13:46:50.552: INFO: Pod pod-projected-secrets-4caf409c-f9fb-4231-895e-1346ec9a60b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:46:50.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6319" for this suite.
Dec 17 13:46:56.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:46:56.676: INFO: namespace projected-6319 deletion completed in 6.12057409s

• [SLOW TEST:10.228 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:46:56.676: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 17 13:46:59.308: INFO: Successfully updated pod "labelsupdate86631385-e3d2-4179-a42e-ea84e9b74ffc"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:47:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4310" for this suite.
Dec 17 13:47:23.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:47:23.480: INFO: namespace downward-api-4310 deletion completed in 22.136618469s

• [SLOW TEST:26.805 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:47:23.483: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 13:47:23.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79b6d940-3cbb-4379-8a4f-051283d43bbd" in namespace "downward-api-6294" to be "success or failure"
Dec 17 13:47:23.555: INFO: Pod "downwardapi-volume-79b6d940-3cbb-4379-8a4f-051283d43bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.687933ms
Dec 17 13:47:25.564: INFO: Pod "downwardapi-volume-79b6d940-3cbb-4379-8a4f-051283d43bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027184589s
STEP: Saw pod success
Dec 17 13:47:25.564: INFO: Pod "downwardapi-volume-79b6d940-3cbb-4379-8a4f-051283d43bbd" satisfied condition "success or failure"
Dec 17 13:47:25.568: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-79b6d940-3cbb-4379-8a4f-051283d43bbd container client-container: <nil>
STEP: delete the pod
Dec 17 13:47:25.605: INFO: Waiting for pod downwardapi-volume-79b6d940-3cbb-4379-8a4f-051283d43bbd to disappear
Dec 17 13:47:25.609: INFO: Pod downwardapi-volume-79b6d940-3cbb-4379-8a4f-051283d43bbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:47:25.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6294" for this suite.
Dec 17 13:47:31.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:47:31.744: INFO: namespace downward-api-6294 deletion completed in 6.131016181s

• [SLOW TEST:8.261 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:47:31.744: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 17 13:47:31.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-7083'
Dec 17 13:47:32.421: INFO: stderr: ""
Dec 17 13:47:32.421: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec 17 13:47:37.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pod e2e-test-nginx-pod --namespace=kubectl-7083 -o json'
Dec 17 13:47:37.558: INFO: stderr: ""
Dec 17 13:47:37.558: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-12-17T13:47:32Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-7083\",\n        \"resourceVersion\": \"6561\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7083/pods/e2e-test-nginx-pod\",\n        \"uid\": \"6e25f0af-f0c7-4908-a331-09df6d87a4de\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-gzcmp\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"conformance-k8s-node-2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-gzcmp\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-gzcmp\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-17T13:47:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-17T13:47:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-17T13:47:34Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-17T13:47:32Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://0a3dfacb1ee74db6ed7c6b3e2f95c5023af161b00b8833f94669a72832c83c49\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-17T13:47:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.100.45\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-17T13:47:32Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 17 13:47:37.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 replace -f - --namespace=kubectl-7083'
Dec 17 13:47:37.823: INFO: stderr: ""
Dec 17 13:47:37.823: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Dec 17 13:47:37.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete pods e2e-test-nginx-pod --namespace=kubectl-7083'
Dec 17 13:47:39.775: INFO: stderr: ""
Dec 17 13:47:39.775: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:47:39.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7083" for this suite.
Dec 17 13:47:45.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:47:45.880: INFO: namespace kubectl-7083 deletion completed in 6.099723976s

• [SLOW TEST:14.136 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:47:45.881: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-27
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-27
STEP: Deleting pre-stop pod
Dec 17 13:47:56.987: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:47:56.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-27" for this suite.
Dec 17 13:48:35.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:48:35.111: INFO: namespace prestop-27 deletion completed in 38.107876558s

• [SLOW TEST:49.231 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:48:35.113: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 13:48:35.178: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75e10e09-5075-4022-b41d-dfa5708a42b1" in namespace "projected-7410" to be "success or failure"
Dec 17 13:48:35.184: INFO: Pod "downwardapi-volume-75e10e09-5075-4022-b41d-dfa5708a42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.587741ms
Dec 17 13:48:37.190: INFO: Pod "downwardapi-volume-75e10e09-5075-4022-b41d-dfa5708a42b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011242086s
STEP: Saw pod success
Dec 17 13:48:37.190: INFO: Pod "downwardapi-volume-75e10e09-5075-4022-b41d-dfa5708a42b1" satisfied condition "success or failure"
Dec 17 13:48:37.193: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-75e10e09-5075-4022-b41d-dfa5708a42b1 container client-container: <nil>
STEP: delete the pod
Dec 17 13:48:37.217: INFO: Waiting for pod downwardapi-volume-75e10e09-5075-4022-b41d-dfa5708a42b1 to disappear
Dec 17 13:48:37.221: INFO: Pod downwardapi-volume-75e10e09-5075-4022-b41d-dfa5708a42b1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:48:37.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7410" for this suite.
Dec 17 13:48:43.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:48:43.432: INFO: namespace projected-7410 deletion completed in 6.204995578s

• [SLOW TEST:8.319 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:48:43.433: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec 17 13:48:43.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-2654'
Dec 17 13:48:43.733: INFO: stderr: ""
Dec 17 13:48:43.733: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 17 13:48:44.747: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:48:44.747: INFO: Found 0 / 1
Dec 17 13:48:45.738: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:48:45.738: INFO: Found 0 / 1
Dec 17 13:48:46.738: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:48:46.738: INFO: Found 1 / 1
Dec 17 13:48:46.738: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 17 13:48:46.742: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:48:46.742: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 17 13:48:46.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 patch pod redis-master-4rv7x --namespace=kubectl-2654 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 17 13:48:46.832: INFO: stderr: ""
Dec 17 13:48:46.832: INFO: stdout: "pod/redis-master-4rv7x patched\n"
STEP: checking annotations
Dec 17 13:48:46.837: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 13:48:46.837: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:48:46.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2654" for this suite.
Dec 17 13:49:08.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:49:08.937: INFO: namespace kubectl-2654 deletion completed in 22.094607658s

• [SLOW TEST:25.504 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:49:08.940: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 13:49:08.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bef6d306-6252-47a2-a749-917237faec53" in namespace "projected-1850" to be "success or failure"
Dec 17 13:49:08.986: INFO: Pod "downwardapi-volume-bef6d306-6252-47a2-a749-917237faec53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.355029ms
Dec 17 13:49:10.995: INFO: Pod "downwardapi-volume-bef6d306-6252-47a2-a749-917237faec53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013366822s
Dec 17 13:49:13.000: INFO: Pod "downwardapi-volume-bef6d306-6252-47a2-a749-917237faec53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018415645s
STEP: Saw pod success
Dec 17 13:49:13.000: INFO: Pod "downwardapi-volume-bef6d306-6252-47a2-a749-917237faec53" satisfied condition "success or failure"
Dec 17 13:49:13.004: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-bef6d306-6252-47a2-a749-917237faec53 container client-container: <nil>
STEP: delete the pod
Dec 17 13:49:13.031: INFO: Waiting for pod downwardapi-volume-bef6d306-6252-47a2-a749-917237faec53 to disappear
Dec 17 13:49:13.034: INFO: Pod downwardapi-volume-bef6d306-6252-47a2-a749-917237faec53 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:49:13.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1850" for this suite.
Dec 17 13:49:19.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:49:19.203: INFO: namespace projected-1850 deletion completed in 6.162787998s

• [SLOW TEST:10.263 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:49:19.207: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-2bf0004f-cf4d-4ea1-9830-ece46bac2c9e in namespace container-probe-3361
Dec 17 13:49:25.267: INFO: Started pod liveness-2bf0004f-cf4d-4ea1-9830-ece46bac2c9e in namespace container-probe-3361
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 13:49:25.272: INFO: Initial restart count of pod liveness-2bf0004f-cf4d-4ea1-9830-ece46bac2c9e is 0
Dec 17 13:49:47.338: INFO: Restart count of pod container-probe-3361/liveness-2bf0004f-cf4d-4ea1-9830-ece46bac2c9e is now 1 (22.066050954s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:49:47.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3361" for this suite.
Dec 17 13:49:53.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:49:53.457: INFO: namespace container-probe-3361 deletion completed in 6.101779845s

• [SLOW TEST:34.251 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:49:53.458: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-2808/secret-test-1136655a-8258-471f-81fa-058ff47fe184
STEP: Creating a pod to test consume secrets
Dec 17 13:49:53.511: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ca619c4-bfb4-41ae-92f3-f5daa694d135" in namespace "secrets-2808" to be "success or failure"
Dec 17 13:49:53.521: INFO: Pod "pod-configmaps-6ca619c4-bfb4-41ae-92f3-f5daa694d135": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57421ms
Dec 17 13:49:55.524: INFO: Pod "pod-configmaps-6ca619c4-bfb4-41ae-92f3-f5daa694d135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0130567s
STEP: Saw pod success
Dec 17 13:49:55.525: INFO: Pod "pod-configmaps-6ca619c4-bfb4-41ae-92f3-f5daa694d135" satisfied condition "success or failure"
Dec 17 13:49:55.527: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-6ca619c4-bfb4-41ae-92f3-f5daa694d135 container env-test: <nil>
STEP: delete the pod
Dec 17 13:49:55.542: INFO: Waiting for pod pod-configmaps-6ca619c4-bfb4-41ae-92f3-f5daa694d135 to disappear
Dec 17 13:49:55.544: INFO: Pod pod-configmaps-6ca619c4-bfb4-41ae-92f3-f5daa694d135 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:49:55.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2808" for this suite.
Dec 17 13:50:01.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:50:01.684: INFO: namespace secrets-2808 deletion completed in 6.135001393s

• [SLOW TEST:8.226 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:50:01.684: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4079
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 17 13:50:01.717: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 17 13:50:25.814: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.100.53:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4079 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 13:50:25.814: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 13:50:25.990: INFO: Found all expected endpoints: [netserver-0]
Dec 17 13:50:25.994: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.93.25:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4079 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 13:50:25.994: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 13:50:26.145: INFO: Found all expected endpoints: [netserver-1]
Dec 17 13:50:26.149: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.95.8:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4079 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 13:50:26.149: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 13:50:26.307: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:50:26.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4079" for this suite.
Dec 17 13:50:48.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:50:48.432: INFO: namespace pod-network-test-4079 deletion completed in 22.1200245s

• [SLOW TEST:46.748 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:50:48.433: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-caa00fe9-5a43-4197-bf6c-f2ba3d96bfcb in namespace container-probe-4524
Dec 17 13:50:52.485: INFO: Started pod liveness-caa00fe9-5a43-4197-bf6c-f2ba3d96bfcb in namespace container-probe-4524
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 13:50:52.489: INFO: Initial restart count of pod liveness-caa00fe9-5a43-4197-bf6c-f2ba3d96bfcb is 0
Dec 17 13:51:10.538: INFO: Restart count of pod container-probe-4524/liveness-caa00fe9-5a43-4197-bf6c-f2ba3d96bfcb is now 1 (18.049201059s elapsed)
Dec 17 13:51:30.584: INFO: Restart count of pod container-probe-4524/liveness-caa00fe9-5a43-4197-bf6c-f2ba3d96bfcb is now 2 (38.095549905s elapsed)
Dec 17 13:51:50.641: INFO: Restart count of pod container-probe-4524/liveness-caa00fe9-5a43-4197-bf6c-f2ba3d96bfcb is now 3 (58.152408003s elapsed)
Dec 17 13:52:10.692: INFO: Restart count of pod container-probe-4524/liveness-caa00fe9-5a43-4197-bf6c-f2ba3d96bfcb is now 4 (1m18.20308555s elapsed)
Dec 17 13:53:12.854: INFO: Restart count of pod container-probe-4524/liveness-caa00fe9-5a43-4197-bf6c-f2ba3d96bfcb is now 5 (2m20.365097698s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:53:12.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4524" for this suite.
Dec 17 13:53:18.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:53:18.995: INFO: namespace container-probe-4524 deletion completed in 6.120048576s

• [SLOW TEST:150.562 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:53:18.996: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-faa42e5c-3baa-49e2-a2de-06a7562f9ee1
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:53:21.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4861" for this suite.
Dec 17 13:53:43.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:53:43.204: INFO: namespace configmap-4861 deletion completed in 22.117546961s

• [SLOW TEST:24.208 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:53:43.204: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-1900/configmap-test-9b02f947-57ea-465b-8b7f-b039669eaf98
STEP: Creating a pod to test consume configMaps
Dec 17 13:53:43.262: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5401786-9de7-49b7-95a9-a198d18f2b25" in namespace "configmap-1900" to be "success or failure"
Dec 17 13:53:43.271: INFO: Pod "pod-configmaps-d5401786-9de7-49b7-95a9-a198d18f2b25": Phase="Pending", Reason="", readiness=false. Elapsed: 9.157718ms
Dec 17 13:53:45.275: INFO: Pod "pod-configmaps-d5401786-9de7-49b7-95a9-a198d18f2b25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012848844s
Dec 17 13:53:47.279: INFO: Pod "pod-configmaps-d5401786-9de7-49b7-95a9-a198d18f2b25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017365328s
STEP: Saw pod success
Dec 17 13:53:47.279: INFO: Pod "pod-configmaps-d5401786-9de7-49b7-95a9-a198d18f2b25" satisfied condition "success or failure"
Dec 17 13:53:47.282: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-d5401786-9de7-49b7-95a9-a198d18f2b25 container env-test: <nil>
STEP: delete the pod
Dec 17 13:53:47.316: INFO: Waiting for pod pod-configmaps-d5401786-9de7-49b7-95a9-a198d18f2b25 to disappear
Dec 17 13:53:47.318: INFO: Pod pod-configmaps-d5401786-9de7-49b7-95a9-a198d18f2b25 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:53:47.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1900" for this suite.
Dec 17 13:53:53.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:53:53.428: INFO: namespace configmap-1900 deletion completed in 6.106618431s

• [SLOW TEST:10.224 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:53:53.431: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 17 13:53:53.488: INFO: Waiting up to 5m0s for pod "downward-api-1f44baeb-5ce3-42b7-bf69-84093c4b3332" in namespace "downward-api-7447" to be "success or failure"
Dec 17 13:53:53.494: INFO: Pod "downward-api-1f44baeb-5ce3-42b7-bf69-84093c4b3332": Phase="Pending", Reason="", readiness=false. Elapsed: 4.959392ms
Dec 17 13:53:55.502: INFO: Pod "downward-api-1f44baeb-5ce3-42b7-bf69-84093c4b3332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013728064s
STEP: Saw pod success
Dec 17 13:53:55.503: INFO: Pod "downward-api-1f44baeb-5ce3-42b7-bf69-84093c4b3332" satisfied condition "success or failure"
Dec 17 13:53:55.505: INFO: Trying to get logs from node conformance-k8s-node-2 pod downward-api-1f44baeb-5ce3-42b7-bf69-84093c4b3332 container dapi-container: <nil>
STEP: delete the pod
Dec 17 13:53:55.536: INFO: Waiting for pod downward-api-1f44baeb-5ce3-42b7-bf69-84093c4b3332 to disappear
Dec 17 13:53:55.539: INFO: Pod downward-api-1f44baeb-5ce3-42b7-bf69-84093c4b3332 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:53:55.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7447" for this suite.
Dec 17 13:54:01.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:54:01.656: INFO: namespace downward-api-7447 deletion completed in 6.112043653s

• [SLOW TEST:8.225 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:54:01.656: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 13:54:01.719: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 17 13:54:01.737: INFO: Number of nodes with available pods: 0
Dec 17 13:54:01.737: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 17 13:54:01.771: INFO: Number of nodes with available pods: 0
Dec 17 13:54:01.771: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:02.777: INFO: Number of nodes with available pods: 0
Dec 17 13:54:02.777: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:03.775: INFO: Number of nodes with available pods: 0
Dec 17 13:54:03.775: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:04.775: INFO: Number of nodes with available pods: 1
Dec 17 13:54:04.775: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 17 13:54:04.794: INFO: Number of nodes with available pods: 1
Dec 17 13:54:04.794: INFO: Number of running nodes: 0, number of available pods: 1
Dec 17 13:54:05.797: INFO: Number of nodes with available pods: 0
Dec 17 13:54:05.797: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 17 13:54:05.820: INFO: Number of nodes with available pods: 0
Dec 17 13:54:05.820: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:06.825: INFO: Number of nodes with available pods: 0
Dec 17 13:54:06.825: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:07.825: INFO: Number of nodes with available pods: 0
Dec 17 13:54:07.825: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:08.831: INFO: Number of nodes with available pods: 0
Dec 17 13:54:08.831: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:09.826: INFO: Number of nodes with available pods: 0
Dec 17 13:54:09.826: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:10.825: INFO: Number of nodes with available pods: 0
Dec 17 13:54:10.825: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:11.825: INFO: Number of nodes with available pods: 0
Dec 17 13:54:11.825: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:12.825: INFO: Number of nodes with available pods: 0
Dec 17 13:54:12.825: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:13.827: INFO: Number of nodes with available pods: 0
Dec 17 13:54:13.827: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:14.826: INFO: Number of nodes with available pods: 0
Dec 17 13:54:14.826: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:15.825: INFO: Number of nodes with available pods: 0
Dec 17 13:54:15.825: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:16.823: INFO: Number of nodes with available pods: 0
Dec 17 13:54:16.823: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:17.826: INFO: Number of nodes with available pods: 0
Dec 17 13:54:17.826: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:18.826: INFO: Number of nodes with available pods: 0
Dec 17 13:54:18.826: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:19.824: INFO: Number of nodes with available pods: 0
Dec 17 13:54:19.825: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:20.826: INFO: Number of nodes with available pods: 0
Dec 17 13:54:20.826: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:54:21.824: INFO: Number of nodes with available pods: 1
Dec 17 13:54:21.824: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8364, will wait for the garbage collector to delete the pods
Dec 17 13:54:21.891: INFO: Deleting DaemonSet.extensions daemon-set took: 6.756539ms
Dec 17 13:54:22.191: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.242849ms
Dec 17 13:54:25.594: INFO: Number of nodes with available pods: 0
Dec 17 13:54:25.594: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 13:54:25.601: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8364/daemonsets","resourceVersion":"7973"},"items":null}

Dec 17 13:54:25.604: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8364/pods","resourceVersion":"7973"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:54:25.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8364" for this suite.
Dec 17 13:54:31.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:54:31.755: INFO: namespace daemonsets-8364 deletion completed in 6.129324951s

• [SLOW TEST:30.100 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:54:31.758: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 17 13:54:31.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-693'
Dec 17 13:54:31.944: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 17 13:54:31.944: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Dec 17 13:54:31.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete deployment e2e-test-nginx-deployment --namespace=kubectl-693'
Dec 17 13:54:32.061: INFO: stderr: ""
Dec 17 13:54:32.061: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:54:32.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-693" for this suite.
Dec 17 13:54:54.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:54:54.176: INFO: namespace kubectl-693 deletion completed in 22.110765046s

• [SLOW TEST:22.419 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:54:54.177: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-k5h7f in namespace proxy-6151
I1217 13:54:54.249809      15 runners.go:180] Created replication controller with name: proxy-service-k5h7f, namespace: proxy-6151, replica count: 1
I1217 13:54:55.302756      15 runners.go:180] proxy-service-k5h7f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 13:54:56.303048      15 runners.go:180] proxy-service-k5h7f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 13:54:57.303532      15 runners.go:180] proxy-service-k5h7f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 13:54:58.304038      15 runners.go:180] proxy-service-k5h7f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 13:54:59.304280      15 runners.go:180] proxy-service-k5h7f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 13:55:00.304581      15 runners.go:180] proxy-service-k5h7f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 13:55:01.304854      15 runners.go:180] proxy-service-k5h7f Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 17 13:55:01.309: INFO: setup took 7.085871731s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 17 13:55:01.329: INFO: (0) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 18.413996ms)
Dec 17 13:55:01.329: INFO: (0) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 19.467537ms)
Dec 17 13:55:01.329: INFO: (0) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 19.213406ms)
Dec 17 13:55:01.331: INFO: (0) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 21.430455ms)
Dec 17 13:55:01.331: INFO: (0) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 19.744228ms)
Dec 17 13:55:01.331: INFO: (0) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 19.983491ms)
Dec 17 13:55:01.331: INFO: (0) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 21.215811ms)
Dec 17 13:55:01.332: INFO: (0) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 21.659535ms)
Dec 17 13:55:01.332: INFO: (0) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 20.678279ms)
Dec 17 13:55:01.332: INFO: (0) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 20.891784ms)
Dec 17 13:55:01.332: INFO: (0) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 21.935608ms)
Dec 17 13:55:01.333: INFO: (0) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 22.799278ms)
Dec 17 13:55:01.333: INFO: (0) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 22.81775ms)
Dec 17 13:55:01.334: INFO: (0) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 24.340938ms)
Dec 17 13:55:01.334: INFO: (0) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 24.418749ms)
Dec 17 13:55:01.335: INFO: (0) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 25.445749ms)
Dec 17 13:55:01.347: INFO: (1) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 11.919828ms)
Dec 17 13:55:01.349: INFO: (1) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 13.830194ms)
Dec 17 13:55:01.350: INFO: (1) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 13.61535ms)
Dec 17 13:55:01.350: INFO: (1) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 13.506577ms)
Dec 17 13:55:01.350: INFO: (1) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 13.52433ms)
Dec 17 13:55:01.351: INFO: (1) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 14.496367ms)
Dec 17 13:55:01.351: INFO: (1) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 16.087734ms)
Dec 17 13:55:01.352: INFO: (1) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 15.344294ms)
Dec 17 13:55:01.352: INFO: (1) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 16.062809ms)
Dec 17 13:55:01.352: INFO: (1) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 14.806441ms)
Dec 17 13:55:01.352: INFO: (1) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 15.612623ms)
Dec 17 13:55:01.352: INFO: (1) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 14.963095ms)
Dec 17 13:55:01.352: INFO: (1) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 16.017108ms)
Dec 17 13:55:01.352: INFO: (1) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 15.684263ms)
Dec 17 13:55:01.353: INFO: (1) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 16.253058ms)
Dec 17 13:55:01.353: INFO: (1) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 15.634985ms)
Dec 17 13:55:01.364: INFO: (2) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 11.04921ms)
Dec 17 13:55:01.365: INFO: (2) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 11.999934ms)
Dec 17 13:55:01.366: INFO: (2) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 13.013702ms)
Dec 17 13:55:01.366: INFO: (2) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 12.743884ms)
Dec 17 13:55:01.370: INFO: (2) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 17.016902ms)
Dec 17 13:55:01.370: INFO: (2) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 16.925975ms)
Dec 17 13:55:01.370: INFO: (2) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 15.705647ms)
Dec 17 13:55:01.370: INFO: (2) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 17.577625ms)
Dec 17 13:55:01.370: INFO: (2) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 16.356865ms)
Dec 17 13:55:01.371: INFO: (2) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 16.166922ms)
Dec 17 13:55:01.371: INFO: (2) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 16.111815ms)
Dec 17 13:55:01.371: INFO: (2) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 16.402072ms)
Dec 17 13:55:01.371: INFO: (2) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 16.790439ms)
Dec 17 13:55:01.371: INFO: (2) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 16.936195ms)
Dec 17 13:55:01.371: INFO: (2) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 17.332765ms)
Dec 17 13:55:01.371: INFO: (2) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 17.385657ms)
Dec 17 13:55:01.383: INFO: (3) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 12.077776ms)
Dec 17 13:55:01.384: INFO: (3) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 12.424914ms)
Dec 17 13:55:01.385: INFO: (3) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 13.555301ms)
Dec 17 13:55:01.385: INFO: (3) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 13.932594ms)
Dec 17 13:55:01.386: INFO: (3) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 13.391226ms)
Dec 17 13:55:01.386: INFO: (3) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 14.003146ms)
Dec 17 13:55:01.386: INFO: (3) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 13.746948ms)
Dec 17 13:55:01.386: INFO: (3) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 14.354371ms)
Dec 17 13:55:01.386: INFO: (3) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 14.454761ms)
Dec 17 13:55:01.386: INFO: (3) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 14.818702ms)
Dec 17 13:55:01.391: INFO: (3) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 19.073952ms)
Dec 17 13:55:01.392: INFO: (3) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 20.50772ms)
Dec 17 13:55:01.392: INFO: (3) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 20.449644ms)
Dec 17 13:55:01.392: INFO: (3) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 20.269508ms)
Dec 17 13:55:01.392: INFO: (3) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 20.172867ms)
Dec 17 13:55:01.393: INFO: (3) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 21.860227ms)
Dec 17 13:55:01.398: INFO: (4) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 4.348311ms)
Dec 17 13:55:01.408: INFO: (4) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 14.348308ms)
Dec 17 13:55:01.408: INFO: (4) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 14.653514ms)
Dec 17 13:55:01.409: INFO: (4) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 14.931561ms)
Dec 17 13:55:01.409: INFO: (4) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 15.432161ms)
Dec 17 13:55:01.409: INFO: (4) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 14.923137ms)
Dec 17 13:55:01.409: INFO: (4) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 15.461453ms)
Dec 17 13:55:01.410: INFO: (4) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 16.027167ms)
Dec 17 13:55:01.410: INFO: (4) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 16.481275ms)
Dec 17 13:55:01.412: INFO: (4) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 18.350248ms)
Dec 17 13:55:01.413: INFO: (4) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 18.800737ms)
Dec 17 13:55:01.413: INFO: (4) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 18.725927ms)
Dec 17 13:55:01.413: INFO: (4) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 19.131781ms)
Dec 17 13:55:01.413: INFO: (4) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 18.952093ms)
Dec 17 13:55:01.413: INFO: (4) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 19.390763ms)
Dec 17 13:55:01.413: INFO: (4) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 19.280331ms)
Dec 17 13:55:01.421: INFO: (5) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 6.97979ms)
Dec 17 13:55:01.421: INFO: (5) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 7.197077ms)
Dec 17 13:55:01.421: INFO: (5) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 6.848928ms)
Dec 17 13:55:01.426: INFO: (5) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 12.277048ms)
Dec 17 13:55:01.427: INFO: (5) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 12.467494ms)
Dec 17 13:55:01.427: INFO: (5) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 12.921391ms)
Dec 17 13:55:01.427: INFO: (5) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 13.084767ms)
Dec 17 13:55:01.428: INFO: (5) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 13.561546ms)
Dec 17 13:55:01.428: INFO: (5) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 13.82499ms)
Dec 17 13:55:01.428: INFO: (5) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 13.63271ms)
Dec 17 13:55:01.428: INFO: (5) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 14.089447ms)
Dec 17 13:55:01.428: INFO: (5) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 14.153886ms)
Dec 17 13:55:01.429: INFO: (5) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 14.577771ms)
Dec 17 13:55:01.429: INFO: (5) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 14.696592ms)
Dec 17 13:55:01.429: INFO: (5) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 15.140985ms)
Dec 17 13:55:01.429: INFO: (5) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 15.210739ms)
Dec 17 13:55:01.436: INFO: (6) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 6.378149ms)
Dec 17 13:55:01.445: INFO: (6) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 15.433634ms)
Dec 17 13:55:01.446: INFO: (6) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 15.557814ms)
Dec 17 13:55:01.447: INFO: (6) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 16.805589ms)
Dec 17 13:55:01.448: INFO: (6) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 17.35996ms)
Dec 17 13:55:01.448: INFO: (6) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 17.605388ms)
Dec 17 13:55:01.453: INFO: (6) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 22.578586ms)
Dec 17 13:55:01.453: INFO: (6) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 22.896101ms)
Dec 17 13:55:01.453: INFO: (6) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 22.739673ms)
Dec 17 13:55:01.453: INFO: (6) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 23.055426ms)
Dec 17 13:55:01.453: INFO: (6) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 22.567174ms)
Dec 17 13:55:01.453: INFO: (6) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 22.640224ms)
Dec 17 13:55:01.453: INFO: (6) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 23.541357ms)
Dec 17 13:55:01.453: INFO: (6) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 22.934908ms)
Dec 17 13:55:01.459: INFO: (6) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 29.595052ms)
Dec 17 13:55:01.460: INFO: (6) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 29.588853ms)
Dec 17 13:55:01.468: INFO: (7) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 8.508199ms)
Dec 17 13:55:01.471: INFO: (7) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 10.274667ms)
Dec 17 13:55:01.471: INFO: (7) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 11.311799ms)
Dec 17 13:55:01.475: INFO: (7) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 14.451334ms)
Dec 17 13:55:01.475: INFO: (7) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 14.66387ms)
Dec 17 13:55:01.475: INFO: (7) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 14.954106ms)
Dec 17 13:55:01.476: INFO: (7) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 15.290639ms)
Dec 17 13:55:01.476: INFO: (7) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 15.55568ms)
Dec 17 13:55:01.476: INFO: (7) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 15.602724ms)
Dec 17 13:55:01.476: INFO: (7) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 15.953851ms)
Dec 17 13:55:01.476: INFO: (7) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 15.971404ms)
Dec 17 13:55:01.477: INFO: (7) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 16.535995ms)
Dec 17 13:55:01.478: INFO: (7) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 17.733381ms)
Dec 17 13:55:01.479: INFO: (7) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 18.228132ms)
Dec 17 13:55:01.479: INFO: (7) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 18.446981ms)
Dec 17 13:55:01.479: INFO: (7) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 18.407731ms)
Dec 17 13:55:01.483: INFO: (8) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 4.333577ms)
Dec 17 13:55:01.484: INFO: (8) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 5.02873ms)
Dec 17 13:55:01.485: INFO: (8) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 5.923877ms)
Dec 17 13:55:01.499: INFO: (8) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 18.5007ms)
Dec 17 13:55:01.499: INFO: (8) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 18.67968ms)
Dec 17 13:55:01.499: INFO: (8) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 18.27068ms)
Dec 17 13:55:01.499: INFO: (8) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 20.014937ms)
Dec 17 13:55:01.499: INFO: (8) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 19.812576ms)
Dec 17 13:55:01.499: INFO: (8) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 19.588835ms)
Dec 17 13:55:01.500: INFO: (8) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 19.549151ms)
Dec 17 13:55:01.500: INFO: (8) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 20.525905ms)
Dec 17 13:55:01.500: INFO: (8) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 19.95896ms)
Dec 17 13:55:01.500: INFO: (8) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 20.328985ms)
Dec 17 13:55:01.503: INFO: (8) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 22.499665ms)
Dec 17 13:55:01.503: INFO: (8) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 22.616758ms)
Dec 17 13:55:01.503: INFO: (8) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 22.383365ms)
Dec 17 13:55:01.508: INFO: (9) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 3.854763ms)
Dec 17 13:55:01.508: INFO: (9) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 4.689779ms)
Dec 17 13:55:01.509: INFO: (9) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 4.80788ms)
Dec 17 13:55:01.510: INFO: (9) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 5.492911ms)
Dec 17 13:55:01.510: INFO: (9) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 6.129077ms)
Dec 17 13:55:01.513: INFO: (9) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 7.689666ms)
Dec 17 13:55:01.513: INFO: (9) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 7.859226ms)
Dec 17 13:55:01.513: INFO: (9) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 8.162548ms)
Dec 17 13:55:01.514: INFO: (9) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 9.31106ms)
Dec 17 13:55:01.514: INFO: (9) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 9.26302ms)
Dec 17 13:55:01.515: INFO: (9) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 10.770131ms)
Dec 17 13:55:01.517: INFO: (9) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 11.145808ms)
Dec 17 13:55:01.517: INFO: (9) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 11.634874ms)
Dec 17 13:55:01.517: INFO: (9) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 11.599333ms)
Dec 17 13:55:01.521: INFO: (9) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 15.124296ms)
Dec 17 13:55:01.521: INFO: (9) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 15.502211ms)
Dec 17 13:55:01.528: INFO: (10) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 6.370433ms)
Dec 17 13:55:01.529: INFO: (10) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 6.437381ms)
Dec 17 13:55:01.529: INFO: (10) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 7.091409ms)
Dec 17 13:55:01.529: INFO: (10) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 7.464317ms)
Dec 17 13:55:01.530: INFO: (10) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 7.860764ms)
Dec 17 13:55:01.531: INFO: (10) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 8.50048ms)
Dec 17 13:55:01.532: INFO: (10) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 10.154964ms)
Dec 17 13:55:01.533: INFO: (10) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 11.130111ms)
Dec 17 13:55:01.534: INFO: (10) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 11.75409ms)
Dec 17 13:55:01.534: INFO: (10) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 12.456256ms)
Dec 17 13:55:01.534: INFO: (10) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 12.069599ms)
Dec 17 13:55:01.534: INFO: (10) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 11.884648ms)
Dec 17 13:55:01.534: INFO: (10) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 12.513923ms)
Dec 17 13:55:01.535: INFO: (10) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 13.614968ms)
Dec 17 13:55:01.536: INFO: (10) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 13.874179ms)
Dec 17 13:55:01.537: INFO: (10) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 14.610506ms)
Dec 17 13:55:01.547: INFO: (11) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 8.463071ms)
Dec 17 13:55:01.547: INFO: (11) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 9.322959ms)
Dec 17 13:55:01.547: INFO: (11) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 9.187749ms)
Dec 17 13:55:01.548: INFO: (11) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 10.05937ms)
Dec 17 13:55:01.548: INFO: (11) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 10.306827ms)
Dec 17 13:55:01.548: INFO: (11) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 9.976177ms)
Dec 17 13:55:01.548: INFO: (11) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 10.114279ms)
Dec 17 13:55:01.549: INFO: (11) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 11.799796ms)
Dec 17 13:55:01.549: INFO: (11) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 12.227704ms)
Dec 17 13:55:01.550: INFO: (11) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 11.969741ms)
Dec 17 13:55:01.550: INFO: (11) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 11.833045ms)
Dec 17 13:55:01.549: INFO: (11) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 12.490147ms)
Dec 17 13:55:01.552: INFO: (11) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 13.528361ms)
Dec 17 13:55:01.552: INFO: (11) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 15.359429ms)
Dec 17 13:55:01.552: INFO: (11) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 15.156503ms)
Dec 17 13:55:01.553: INFO: (11) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 15.201746ms)
Dec 17 13:55:01.560: INFO: (12) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 6.489684ms)
Dec 17 13:55:01.560: INFO: (12) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 6.30288ms)
Dec 17 13:55:01.561: INFO: (12) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 5.61751ms)
Dec 17 13:55:01.561: INFO: (12) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 7.406288ms)
Dec 17 13:55:01.561: INFO: (12) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 6.405647ms)
Dec 17 13:55:01.562: INFO: (12) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 7.484251ms)
Dec 17 13:55:01.562: INFO: (12) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 8.237865ms)
Dec 17 13:55:01.563: INFO: (12) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 7.428992ms)
Dec 17 13:55:01.563: INFO: (12) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 7.301211ms)
Dec 17 13:55:01.563: INFO: (12) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 9.789826ms)
Dec 17 13:55:01.563: INFO: (12) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 7.831188ms)
Dec 17 13:55:01.564: INFO: (12) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 9.669333ms)
Dec 17 13:55:01.564: INFO: (12) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 11.570329ms)
Dec 17 13:55:01.565: INFO: (12) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 10.331366ms)
Dec 17 13:55:01.565: INFO: (12) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 10.497243ms)
Dec 17 13:55:01.565: INFO: (12) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 12.143758ms)
Dec 17 13:55:01.573: INFO: (13) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 7.0421ms)
Dec 17 13:55:01.573: INFO: (13) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 5.874661ms)
Dec 17 13:55:01.573: INFO: (13) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 6.25629ms)
Dec 17 13:55:01.573: INFO: (13) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 6.240322ms)
Dec 17 13:55:01.573: INFO: (13) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 6.939708ms)
Dec 17 13:55:01.573: INFO: (13) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 6.549572ms)
Dec 17 13:55:01.574: INFO: (13) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 7.611225ms)
Dec 17 13:55:01.574: INFO: (13) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 7.545675ms)
Dec 17 13:55:01.574: INFO: (13) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 7.818362ms)
Dec 17 13:55:01.574: INFO: (13) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 7.970318ms)
Dec 17 13:55:01.577: INFO: (13) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 11.159546ms)
Dec 17 13:55:01.577: INFO: (13) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 9.62524ms)
Dec 17 13:55:01.577: INFO: (13) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 11.761617ms)
Dec 17 13:55:01.577: INFO: (13) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 9.925293ms)
Dec 17 13:55:01.578: INFO: (13) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 11.631076ms)
Dec 17 13:55:01.578: INFO: (13) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 11.82285ms)
Dec 17 13:55:01.599: INFO: (14) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 19.999808ms)
Dec 17 13:55:01.599: INFO: (14) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 19.855777ms)
Dec 17 13:55:01.599: INFO: (14) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 19.804185ms)
Dec 17 13:55:01.599: INFO: (14) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 19.875768ms)
Dec 17 13:55:01.600: INFO: (14) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 21.431516ms)
Dec 17 13:55:01.600: INFO: (14) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 20.862447ms)
Dec 17 13:55:01.601: INFO: (14) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 21.099529ms)
Dec 17 13:55:01.601: INFO: (14) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 22.032693ms)
Dec 17 13:55:01.602: INFO: (14) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 22.288391ms)
Dec 17 13:55:01.602: INFO: (14) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 22.196373ms)
Dec 17 13:55:01.602: INFO: (14) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 22.385881ms)
Dec 17 13:55:01.603: INFO: (14) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 23.661022ms)
Dec 17 13:55:01.603: INFO: (14) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 24.379667ms)
Dec 17 13:55:01.605: INFO: (14) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 25.489317ms)
Dec 17 13:55:01.605: INFO: (14) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 25.472738ms)
Dec 17 13:55:01.605: INFO: (14) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 26.132079ms)
Dec 17 13:55:01.608: INFO: (15) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 3.293795ms)
Dec 17 13:55:01.611: INFO: (15) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 5.353773ms)
Dec 17 13:55:01.611: INFO: (15) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 5.495538ms)
Dec 17 13:55:01.611: INFO: (15) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 5.867146ms)
Dec 17 13:55:01.613: INFO: (15) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 8.024974ms)
Dec 17 13:55:01.613: INFO: (15) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 7.155194ms)
Dec 17 13:55:01.613: INFO: (15) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 7.115958ms)
Dec 17 13:55:01.614: INFO: (15) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 8.487855ms)
Dec 17 13:55:01.614: INFO: (15) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 7.815377ms)
Dec 17 13:55:01.615: INFO: (15) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 9.040999ms)
Dec 17 13:55:01.616: INFO: (15) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 9.85574ms)
Dec 17 13:55:01.616: INFO: (15) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 9.410137ms)
Dec 17 13:55:01.616: INFO: (15) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 10.262027ms)
Dec 17 13:55:01.616: INFO: (15) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 9.915571ms)
Dec 17 13:55:01.616: INFO: (15) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 10.89358ms)
Dec 17 13:55:01.617: INFO: (15) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 10.489155ms)
Dec 17 13:55:01.637: INFO: (16) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 20.394731ms)
Dec 17 13:55:01.637: INFO: (16) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 20.452615ms)
Dec 17 13:55:01.637: INFO: (16) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 20.077273ms)
Dec 17 13:55:01.637: INFO: (16) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 20.44193ms)
Dec 17 13:55:01.637: INFO: (16) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 20.394269ms)
Dec 17 13:55:01.637: INFO: (16) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 20.119441ms)
Dec 17 13:55:01.637: INFO: (16) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 20.17184ms)
Dec 17 13:55:01.638: INFO: (16) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 20.230878ms)
Dec 17 13:55:01.638: INFO: (16) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 20.74773ms)
Dec 17 13:55:01.638: INFO: (16) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 21.015771ms)
Dec 17 13:55:01.639: INFO: (16) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 21.304242ms)
Dec 17 13:55:01.639: INFO: (16) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 21.296131ms)
Dec 17 13:55:01.639: INFO: (16) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 21.716274ms)
Dec 17 13:55:01.639: INFO: (16) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 21.836204ms)
Dec 17 13:55:01.639: INFO: (16) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 21.72257ms)
Dec 17 13:55:01.639: INFO: (16) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 22.072753ms)
Dec 17 13:55:01.649: INFO: (17) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 9.217609ms)
Dec 17 13:55:01.649: INFO: (17) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 8.75783ms)
Dec 17 13:55:01.650: INFO: (17) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 9.626802ms)
Dec 17 13:55:01.650: INFO: (17) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 9.804904ms)
Dec 17 13:55:01.650: INFO: (17) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 9.870224ms)
Dec 17 13:55:01.651: INFO: (17) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 10.914ms)
Dec 17 13:55:01.651: INFO: (17) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 10.718468ms)
Dec 17 13:55:01.651: INFO: (17) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 10.876235ms)
Dec 17 13:55:01.652: INFO: (17) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 11.810428ms)
Dec 17 13:55:01.652: INFO: (17) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 11.910461ms)
Dec 17 13:55:01.652: INFO: (17) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 12.072734ms)
Dec 17 13:55:01.652: INFO: (17) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 12.306962ms)
Dec 17 13:55:01.652: INFO: (17) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 12.001372ms)
Dec 17 13:55:01.653: INFO: (17) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 12.547142ms)
Dec 17 13:55:01.653: INFO: (17) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 13.405243ms)
Dec 17 13:55:01.653: INFO: (17) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 12.947099ms)
Dec 17 13:55:01.667: INFO: (18) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 13.083835ms)
Dec 17 13:55:01.668: INFO: (18) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 14.500988ms)
Dec 17 13:55:01.668: INFO: (18) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 14.345913ms)
Dec 17 13:55:01.669: INFO: (18) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 14.421139ms)
Dec 17 13:55:01.669: INFO: (18) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 14.301733ms)
Dec 17 13:55:01.669: INFO: (18) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 14.033235ms)
Dec 17 13:55:01.669: INFO: (18) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 14.753487ms)
Dec 17 13:55:01.670: INFO: (18) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 14.612605ms)
Dec 17 13:55:01.670: INFO: (18) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 16.332985ms)
Dec 17 13:55:01.670: INFO: (18) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 15.427669ms)
Dec 17 13:55:01.671: INFO: (18) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 15.870017ms)
Dec 17 13:55:01.671: INFO: (18) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 15.748726ms)
Dec 17 13:55:01.671: INFO: (18) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 17.328241ms)
Dec 17 13:55:01.672: INFO: (18) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 16.651099ms)
Dec 17 13:55:01.673: INFO: (18) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 19.027445ms)
Dec 17 13:55:01.673: INFO: (18) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 18.905723ms)
Dec 17 13:55:01.677: INFO: (19) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 3.677161ms)
Dec 17 13:55:01.685: INFO: (19) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 11.135531ms)
Dec 17 13:55:01.685: INFO: (19) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname2/proxy/: bar (200; 10.678722ms)
Dec 17 13:55:01.686: INFO: (19) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">... (200; 10.359713ms)
Dec 17 13:55:01.686: INFO: (19) /api/v1/namespaces/proxy-6151/services/http:proxy-service-k5h7f:portname1/proxy/: foo (200; 12.359517ms)
Dec 17 13:55:01.686: INFO: (19) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:462/proxy/: tls qux (200; 11.41101ms)
Dec 17 13:55:01.687: INFO: (19) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname1/proxy/: tls baz (200; 12.163198ms)
Dec 17 13:55:01.687: INFO: (19) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:162/proxy/: bar (200; 12.523704ms)
Dec 17 13:55:01.687: INFO: (19) /api/v1/namespaces/proxy-6151/services/https:proxy-service-k5h7f:tlsportname2/proxy/: tls qux (200; 12.055695ms)
Dec 17 13:55:01.688: INFO: (19) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname1/proxy/: foo (200; 13.777232ms)
Dec 17 13:55:01.688: INFO: (19) /api/v1/namespaces/proxy-6151/services/proxy-service-k5h7f:portname2/proxy/: bar (200; 13.688341ms)
Dec 17 13:55:01.688: INFO: (19) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5/proxy/rewriteme">test</a> (200; 12.733799ms)
Dec 17 13:55:01.688: INFO: (19) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:443/proxy/tlsrewritem... (200; 13.530564ms)
Dec 17 13:55:01.688: INFO: (19) /api/v1/namespaces/proxy-6151/pods/http:proxy-service-k5h7f-n44n5:160/proxy/: foo (200; 14.177209ms)
Dec 17 13:55:01.689: INFO: (19) /api/v1/namespaces/proxy-6151/pods/https:proxy-service-k5h7f-n44n5:460/proxy/: tls baz (200; 13.765876ms)
Dec 17 13:55:01.689: INFO: (19) /api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/: <a href="/api/v1/namespaces/proxy-6151/pods/proxy-service-k5h7f-n44n5:1080/proxy/rewriteme">test<... (200; 14.695286ms)
STEP: deleting ReplicationController proxy-service-k5h7f in namespace proxy-6151, will wait for the garbage collector to delete the pods
Dec 17 13:55:01.748: INFO: Deleting ReplicationController proxy-service-k5h7f took: 6.120936ms
Dec 17 13:55:02.048: INFO: Terminating ReplicationController proxy-service-k5h7f pods took: 300.380136ms
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:55:03.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6151" for this suite.
Dec 17 13:55:09.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:55:09.781: INFO: namespace proxy-6151 deletion completed in 6.124569671s

• [SLOW TEST:15.604 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:55:09.781: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 13:55:09.822: INFO: Creating deployment "nginx-deployment"
Dec 17 13:55:09.829: INFO: Waiting for observed generation 1
Dec 17 13:55:11.860: INFO: Waiting for all required pods to come up
Dec 17 13:55:11.870: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 17 13:55:13.895: INFO: Waiting for deployment "nginx-deployment" to complete
Dec 17 13:55:13.901: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec 17 13:55:13.910: INFO: Updating deployment nginx-deployment
Dec 17 13:55:13.910: INFO: Waiting for observed generation 2
Dec 17 13:55:15.923: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 17 13:55:15.926: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 17 13:55:15.930: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 17 13:55:15.940: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 17 13:55:15.940: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 17 13:55:15.943: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 17 13:55:15.948: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec 17 13:55:15.948: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec 17 13:55:15.957: INFO: Updating deployment nginx-deployment
Dec 17 13:55:15.957: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec 17 13:55:15.973: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 17 13:55:18.012: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 17 13:55:18.025: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7340,SelfLink:/apis/apps/v1/namespaces/deployment-7340/deployments/nginx-deployment,UID:9b3d6e2a-ca98-4d46-b538-6d6af996e15f,ResourceVersion:8484,Generation:3,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-12-17 13:55:15 +0000 UTC 2019-12-17 13:55:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-17 13:55:16 +0000 UTC 2019-12-17 13:55:09 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec 17 13:55:18.045: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-7340,SelfLink:/apis/apps/v1/namespaces/deployment-7340/replicasets/nginx-deployment-55fb7cb77f,UID:2902f261-9158-4b84-9dfe-80f96ca1325c,ResourceVersion:8481,Generation:3,CreationTimestamp:2019-12-17 13:55:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 9b3d6e2a-ca98-4d46-b538-6d6af996e15f 0xc002d120a7 0xc002d120a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 17 13:55:18.045: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec 17 13:55:18.045: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-7340,SelfLink:/apis/apps/v1/namespaces/deployment-7340/replicasets/nginx-deployment-7b8c6f4498,UID:53ef8451-6ce5-4c8c-a39b-8501e785e636,ResourceVersion:8463,Generation:3,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 9b3d6e2a-ca98-4d46-b538-6d6af996e15f 0xc002d12177 0xc002d12178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec 17 13:55:18.071: INFO: Pod "nginx-deployment-55fb7cb77f-45vtt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-45vtt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-45vtt,UID:f1f380ca-26ce-4825-8927-dd426a6b0474,ResourceVersion:8486,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d12b00 0xc002d12b01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d12b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d12b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.078: INFO: Pod "nginx-deployment-55fb7cb77f-48j2r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-48j2r,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-48j2r,UID:57a7ca9d-8f17-4a99-b753-9607b7a8abd7,ResourceVersion:8387,Generation:0,CreationTimestamp:2019-12-17 13:55:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d12c60 0xc002d12c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d12cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d12cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.099: INFO: Pod "nginx-deployment-55fb7cb77f-5fhsc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5fhsc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-5fhsc,UID:5257547e-7b4e-47d0-a2cd-c6e93c22f912,ResourceVersion:8557,Generation:0,CreationTimestamp:2019-12-17 13:55:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d12dc0 0xc002d12dc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d12e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d12e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:10.233.95.12,StartTime:2019-12-17 13:55:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.105: INFO: Pod "nginx-deployment-55fb7cb77f-7xrg5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7xrg5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-7xrg5,UID:b5bf6a45-e637-4254-accc-0432d78f3ad3,ResourceVersion:8361,Generation:0,CreationTimestamp:2019-12-17 13:55:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d12f40 0xc002d12f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d12fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d12fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.115: INFO: Pod "nginx-deployment-55fb7cb77f-cc4bv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cc4bv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-cc4bv,UID:df27a751-d680-476c-b383-978dd27b825b,ResourceVersion:8539,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d130a0 0xc002d130a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d13110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.116: INFO: Pod "nginx-deployment-55fb7cb77f-gcb47" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gcb47,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-gcb47,UID:6175e3d2-2027-4623-bb32-6c6f4076f171,ResourceVersion:8561,Generation:0,CreationTimestamp:2019-12-17 13:55:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d13200 0xc002d13201}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d13270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:10.233.100.65,StartTime:2019-12-17 13:55:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.116: INFO: Pod "nginx-deployment-55fb7cb77f-mtjt7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mtjt7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-mtjt7,UID:df886c20-d817-4aae-8ea2-cdaf6d2a7424,ResourceVersion:8508,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d13380 0xc002d13381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d133f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.122: INFO: Pod "nginx-deployment-55fb7cb77f-p2x5d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-p2x5d,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-p2x5d,UID:60249ed5-c1d1-4538-8d47-3b061a557912,ResourceVersion:8506,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d134e0 0xc002d134e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d13550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.123: INFO: Pod "nginx-deployment-55fb7cb77f-rgn9z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rgn9z,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-rgn9z,UID:977f52fc-61ca-49ff-a5cd-bd555e3207c4,ResourceVersion:8522,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d13640 0xc002d13641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d136b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d136d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.125: INFO: Pod "nginx-deployment-55fb7cb77f-rkmjq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rkmjq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-rkmjq,UID:ac8a70c6-fa9f-4b07-8d75-b05c5e462698,ResourceVersion:8362,Generation:0,CreationTimestamp:2019-12-17 13:55:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d137a0 0xc002d137a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d13810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:13 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2019-12-17 13:55:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.128: INFO: Pod "nginx-deployment-55fb7cb77f-tjfpb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tjfpb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-tjfpb,UID:da655c7d-ba81-4c78-8f89-94dc2f760938,ResourceVersion:8528,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d13900 0xc002d13901}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d13970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.129: INFO: Pod "nginx-deployment-55fb7cb77f-tjj95" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tjj95,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-tjj95,UID:8c48e39e-5866-4784-83ed-72c972eb48cc,ResourceVersion:8497,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d13a60 0xc002d13a61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d13ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13af0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.129: INFO: Pod "nginx-deployment-55fb7cb77f-tk6wz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tk6wz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-55fb7cb77f-tk6wz,UID:c82e4ba0-decb-4fb7-bb8d-640f06000c73,ResourceVersion:8460,Generation:0,CreationTimestamp:2019-12-17 13:55:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 2902f261-9158-4b84-9dfe-80f96ca1325c 0xc002d13bc0 0xc002d13bc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d13c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.129: INFO: Pod "nginx-deployment-7b8c6f4498-2qg52" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2qg52,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-2qg52,UID:c346e3e0-b37d-4424-ae9a-54e00a191ff9,ResourceVersion:8496,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002d13d20 0xc002d13d21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d13d80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13da0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.130: INFO: Pod "nginx-deployment-7b8c6f4498-45wqw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-45wqw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-45wqw,UID:e3116101-0a41-4d9f-8c62-ea08235d7feb,ResourceVersion:8526,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002d13e60 0xc002d13e61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d13ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d13ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.130: INFO: Pod "nginx-deployment-7b8c6f4498-557sc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-557sc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-557sc,UID:b683f6c7-81e9-476c-b1f7-d52932c9fe89,ResourceVersion:8304,Generation:0,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002d13fa0 0xc002d13fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:10.233.95.11,StartTime:2019-12-17 13:55:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-17 13:55:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://908a1a9f8dcd76d7e24c7aab9f686c216b637a558221c0d6c51f2dbc33663270}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.130: INFO: Pod "nginx-deployment-7b8c6f4498-58lzt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-58lzt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-58lzt,UID:9264f415-f857-4f21-a990-9b6777b5e90c,ResourceVersion:8499,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c580f0 0xc002c580f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.130: INFO: Pod "nginx-deployment-7b8c6f4498-7xmwf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7xmwf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-7xmwf,UID:bb7cccfc-ac8b-40a5-9f48-a9f05dc402af,ResourceVersion:8319,Generation:0,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c58230 0xc002c58231}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c582b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:10.233.100.64,StartTime:2019-12-17 13:55:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-17 13:55:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://810a0f38458a9e50b15f692c44cfe6841b92043cd684f659c5219b401f70aa39}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.131: INFO: Pod "nginx-deployment-7b8c6f4498-7zkvn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7zkvn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-7zkvn,UID:431673d5-0838-476c-ad50-43a6da8d8780,ResourceVersion:8449,Generation:0,CreationTimestamp:2019-12-17 13:55:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c58380 0xc002c58381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c583e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:15 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.131: INFO: Pod "nginx-deployment-7b8c6f4498-8xthn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8xthn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-8xthn,UID:2a8c9e51-55d2-4f0a-9ea4-073639982ae6,ResourceVersion:8306,Generation:0,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c584c0 0xc002c584c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:10.233.95.9,StartTime:2019-12-17 13:55:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-17 13:55:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://265ebff3fa58e563bc31e58f442c9d45c8b1632fa1a6c4209650169cd1f2e54b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.131: INFO: Pod "nginx-deployment-7b8c6f4498-d496l" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d496l,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-d496l,UID:fa3c9f13-ed9b-426a-9ebf-f2243aee39ce,ResourceVersion:8314,Generation:0,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c58610 0xc002c58611}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:10.233.100.63,StartTime:2019-12-17 13:55:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-17 13:55:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c03d6c121644eb481fbe2d775cd14a2c0acf6637410d5d170c1e0b8cdf3002ff}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.131: INFO: Pod "nginx-deployment-7b8c6f4498-ddn8l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ddn8l,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-ddn8l,UID:c90a93d3-535f-4ddd-9942-a707fbdd6db2,ResourceVersion:8535,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c58760 0xc002c58761}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c587c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c587e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.131: INFO: Pod "nginx-deployment-7b8c6f4498-dnqss" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dnqss,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-dnqss,UID:fc152fad-70f6-4581-acf8-7ad48fb73fe5,ResourceVersion:8507,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c588a0 0xc002c588a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.131: INFO: Pod "nginx-deployment-7b8c6f4498-fq2gt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fq2gt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-fq2gt,UID:5a3d5282-be50-48a4-a742-6b2d352d28fa,ResourceVersion:8476,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c589e0 0xc002c589e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.132: INFO: Pod "nginx-deployment-7b8c6f4498-h6m5c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-h6m5c,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-h6m5c,UID:2de45c57-0153-4588-ab27-f0ddc08e8d94,ResourceVersion:8489,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c58b20 0xc002c58b21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.132: INFO: Pod "nginx-deployment-7b8c6f4498-km4m8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-km4m8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-km4m8,UID:10339982-f8fd-47b5-b1bd-93601112ada3,ResourceVersion:8317,Generation:0,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c58c60 0xc002c58c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:10.233.100.61,StartTime:2019-12-17 13:55:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-17 13:55:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e6766790ea84275070d21ec67760cfb7983b1f3b59480f5b557992ea2d20cd80}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.132: INFO: Pod "nginx-deployment-7b8c6f4498-lblfb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lblfb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-lblfb,UID:32422433-ae7e-411b-ac24-6635a2c5e2ff,ResourceVersion:8328,Generation:0,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c58db0 0xc002c58db1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:10.233.93.30,StartTime:2019-12-17 13:55:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-17 13:55:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://dd7d9fc42a64d3828c377aa0bf03e05ab498529c13fdfc5c5f73eeb4faa3de43}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.132: INFO: Pod "nginx-deployment-7b8c6f4498-pqldg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pqldg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-pqldg,UID:ce1b3da5-17ed-4ba4-95b6-70562ea53802,ResourceVersion:8477,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c58f00 0xc002c58f01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c58f60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c58f80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.132: INFO: Pod "nginx-deployment-7b8c6f4498-qghh4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qghh4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-qghh4,UID:3845c408-c20f-498f-9c14-762f72ed3beb,ResourceVersion:8466,Generation:0,CreationTimestamp:2019-12-17 13:55:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c59040 0xc002c59041}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c590a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c590c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.132: INFO: Pod "nginx-deployment-7b8c6f4498-sft4t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sft4t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-sft4t,UID:1faf3e1b-34ff-4a66-b595-bb086b752b65,ResourceVersion:8492,Generation:0,CreationTimestamp:2019-12-17 13:55:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c59180 0xc002c59181}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c591e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c59200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.132: INFO: Pod "nginx-deployment-7b8c6f4498-sp527" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sp527,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-sp527,UID:ecdd8934-611b-4f84-b57a-2e69d866eaab,ResourceVersion:8323,Generation:0,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c592c0 0xc002c592c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c59320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c59340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:10.233.100.62,StartTime:2019-12-17 13:55:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-17 13:55:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0f2e84bba2236352dfea5357d22ca8b5b351c86240cd1791b86ed84fe15f1239}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.133: INFO: Pod "nginx-deployment-7b8c6f4498-vgpfz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vgpfz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-vgpfz,UID:1fdd99d7-5fff-46a2-9ba9-2096bcacbb67,ResourceVersion:8488,Generation:0,CreationTimestamp:2019-12-17 13:55:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c59410 0xc002c59411}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c59470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c59490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2019-12-17 13:55:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 17 13:55:18.133: INFO: Pod "nginx-deployment-7b8c6f4498-x9xsk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x9xsk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7340,SelfLink:/api/v1/namespaces/deployment-7340/pods/nginx-deployment-7b8c6f4498-x9xsk,UID:c48f851e-2014-4dba-b602-fee5e1f082f5,ResourceVersion:8310,Generation:0,CreationTimestamp:2019-12-17 13:55:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 53ef8451-6ce5-4c8c-a39b-8501e785e636 0xc002c59550 0xc002c59551}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5zlsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5zlsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5zlsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c595b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c595d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:55:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.4,PodIP:10.233.95.10,StartTime:2019-12-17 13:55:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-17 13:55:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2120c5806b251315c5606f41c30d4ca07ebfa81a3e7a1db0ebadf44600cd8c39}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:55:18.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7340" for this suite.
Dec 17 13:55:26.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:55:26.282: INFO: namespace deployment-7340 deletion completed in 8.144206523s

• [SLOW TEST:16.501 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:55:26.288: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 13:55:26.379: INFO: Create a RollingUpdate DaemonSet
Dec 17 13:55:26.386: INFO: Check that daemon pods launch on every node of the cluster
Dec 17 13:55:26.397: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:26.407: INFO: Number of nodes with available pods: 0
Dec 17 13:55:26.407: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:55:27.422: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:27.435: INFO: Number of nodes with available pods: 0
Dec 17 13:55:27.435: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:55:28.412: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:28.416: INFO: Number of nodes with available pods: 0
Dec 17 13:55:28.416: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:55:29.413: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:29.417: INFO: Number of nodes with available pods: 3
Dec 17 13:55:29.417: INFO: Number of running nodes: 3, number of available pods: 3
Dec 17 13:55:29.417: INFO: Update the DaemonSet to trigger a rollout
Dec 17 13:55:29.425: INFO: Updating DaemonSet daemon-set
Dec 17 13:55:32.458: INFO: Roll back the DaemonSet before rollout is complete
Dec 17 13:55:32.465: INFO: Updating DaemonSet daemon-set
Dec 17 13:55:32.465: INFO: Make sure DaemonSet rollback is complete
Dec 17 13:55:32.469: INFO: Wrong image for pod: daemon-set-8h4c5. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 17 13:55:32.469: INFO: Pod daemon-set-8h4c5 is not available
Dec 17 13:55:32.480: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:33.488: INFO: Wrong image for pod: daemon-set-8h4c5. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 17 13:55:33.488: INFO: Pod daemon-set-8h4c5 is not available
Dec 17 13:55:33.495: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:34.487: INFO: Wrong image for pod: daemon-set-8h4c5. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 17 13:55:34.487: INFO: Pod daemon-set-8h4c5 is not available
Dec 17 13:55:34.492: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:35.488: INFO: Wrong image for pod: daemon-set-8h4c5. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 17 13:55:35.488: INFO: Pod daemon-set-8h4c5 is not available
Dec 17 13:55:35.493: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:36.488: INFO: Wrong image for pod: daemon-set-8h4c5. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 17 13:55:36.488: INFO: Pod daemon-set-8h4c5 is not available
Dec 17 13:55:36.492: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:37.487: INFO: Wrong image for pod: daemon-set-8h4c5. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 17 13:55:37.488: INFO: Pod daemon-set-8h4c5 is not available
Dec 17 13:55:37.493: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:38.486: INFO: Wrong image for pod: daemon-set-8h4c5. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 17 13:55:38.486: INFO: Pod daemon-set-8h4c5 is not available
Dec 17 13:55:38.491: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:55:39.485: INFO: Pod daemon-set-7hmb4 is not available
Dec 17 13:55:39.491: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9047, will wait for the garbage collector to delete the pods
Dec 17 13:55:39.567: INFO: Deleting DaemonSet.extensions daemon-set took: 16.071116ms
Dec 17 13:55:39.868: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.564521ms
Dec 17 13:55:49.472: INFO: Number of nodes with available pods: 0
Dec 17 13:55:49.472: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 13:55:49.474: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9047/daemonsets","resourceVersion":"9150"},"items":null}

Dec 17 13:55:49.483: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9047/pods","resourceVersion":"9150"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:55:49.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9047" for this suite.
Dec 17 13:55:55.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:55:55.694: INFO: namespace daemonsets-9047 deletion completed in 6.193448108s

• [SLOW TEST:29.407 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:55:55.695: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:55:55.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3137" for this suite.
Dec 17 13:56:01.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:56:01.955: INFO: namespace kubelet-test-3137 deletion completed in 6.153064262s

• [SLOW TEST:6.260 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:56:01.955: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-d3bdbf2b-45a0-4ff3-95a4-ef21bc75efbe
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:56:02.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4655" for this suite.
Dec 17 13:56:08.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:56:08.151: INFO: namespace secrets-4655 deletion completed in 6.13326335s

• [SLOW TEST:6.196 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:56:08.154: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4693
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-4693
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4693
Dec 17 13:56:08.229: INFO: Found 0 stateful pods, waiting for 1
Dec 17 13:56:18.247: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 17 13:56:18.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4693 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 13:56:18.499: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 13:56:18.499: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 13:56:18.499: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 17 13:56:18.510: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 17 13:56:28.520: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 13:56:28.521: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 13:56:28.541: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:56:28.542: INFO: ss-0  conformance-k8s-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:56:28.542: INFO: 
Dec 17 13:56:28.542: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 17 13:56:29.553: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989547796s
Dec 17 13:56:30.563: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978573171s
Dec 17 13:56:31.569: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968521282s
Dec 17 13:56:32.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.96267501s
Dec 17 13:56:33.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.955547626s
Dec 17 13:56:34.587: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.950855008s
Dec 17 13:56:35.594: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.943906235s
Dec 17 13:56:36.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.937783844s
Dec 17 13:56:37.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 931.910248ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4693
Dec 17 13:56:38.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4693 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 17 13:56:38.846: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 17 13:56:38.846: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 17 13:56:38.846: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 17 13:56:38.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4693 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 17 13:56:39.098: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 17 13:56:39.098: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 17 13:56:39.098: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 17 13:56:39.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4693 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 17 13:56:39.328: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 17 13:56:39.328: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 17 13:56:39.328: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 17 13:56:39.332: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 17 13:56:49.337: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 13:56:49.337: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 13:56:49.337: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 17 13:56:49.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4693 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 13:56:49.590: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 13:56:49.590: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 13:56:49.590: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 17 13:56:49.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4693 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 13:56:49.839: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 13:56:49.839: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 13:56:49.839: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 17 13:56:49.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4693 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 13:56:50.080: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 13:56:50.080: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 13:56:50.080: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 17 13:56:50.080: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 13:56:50.083: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 17 13:57:00.101: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 13:57:00.101: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 13:57:00.101: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 13:57:00.119: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:00.120: INFO: ss-0  conformance-k8s-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:00.120: INFO: ss-1  conformance-k8s-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:00.120: INFO: ss-2  conformance-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:00.120: INFO: 
Dec 17 13:57:00.120: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 13:57:01.129: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:01.130: INFO: ss-0  conformance-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:01.130: INFO: ss-1  conformance-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:01.130: INFO: ss-2  conformance-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:01.130: INFO: 
Dec 17 13:57:01.130: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 13:57:02.144: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:02.144: INFO: ss-0  conformance-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:02.144: INFO: ss-1  conformance-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:02.144: INFO: ss-2  conformance-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:02.144: INFO: 
Dec 17 13:57:02.144: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 13:57:03.149: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:03.149: INFO: ss-0  conformance-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:03.149: INFO: ss-1  conformance-k8s-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:03.149: INFO: ss-2  conformance-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:03.149: INFO: 
Dec 17 13:57:03.149: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 13:57:04.154: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:04.155: INFO: ss-0  conformance-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:04.155: INFO: ss-2  conformance-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:04.155: INFO: 
Dec 17 13:57:04.155: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 17 13:57:05.159: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:05.159: INFO: ss-0  conformance-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:05.159: INFO: ss-2  conformance-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:05.159: INFO: 
Dec 17 13:57:05.159: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 17 13:57:06.165: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:06.165: INFO: ss-0  conformance-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:06.165: INFO: ss-2  conformance-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:06.165: INFO: 
Dec 17 13:57:06.165: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 17 13:57:07.169: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:07.169: INFO: ss-0  conformance-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:07.169: INFO: ss-2  conformance-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:07.169: INFO: 
Dec 17 13:57:07.169: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 17 13:57:08.191: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:08.191: INFO: ss-0  conformance-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:08.191: INFO: ss-2  conformance-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:28 +0000 UTC  }]
Dec 17 13:57:08.191: INFO: 
Dec 17 13:57:08.191: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 17 13:57:09.195: INFO: POD   NODE                    PHASE    GRACE  CONDITIONS
Dec 17 13:57:09.195: INFO: ss-0  conformance-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 13:56:08 +0000 UTC  }]
Dec 17 13:57:09.195: INFO: 
Dec 17 13:57:09.195: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4693
Dec 17 13:57:10.200: INFO: Scaling statefulset ss to 0
Dec 17 13:57:10.210: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 17 13:57:10.212: INFO: Deleting all statefulset in ns statefulset-4693
Dec 17 13:57:10.215: INFO: Scaling statefulset ss to 0
Dec 17 13:57:10.225: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 13:57:10.228: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:57:10.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4693" for this suite.
Dec 17 13:57:16.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:57:16.381: INFO: namespace statefulset-4693 deletion completed in 6.124299102s

• [SLOW TEST:68.227 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:57:16.384: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 17 13:57:16.468: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6116,SelfLink:/api/v1/namespaces/watch-6116/configmaps/e2e-watch-test-label-changed,UID:9df38710-2cd6-477e-8535-e5f829ddd583,ResourceVersion:9580,Generation:0,CreationTimestamp:2019-12-17 13:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 17 13:57:16.468: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6116,SelfLink:/api/v1/namespaces/watch-6116/configmaps/e2e-watch-test-label-changed,UID:9df38710-2cd6-477e-8535-e5f829ddd583,ResourceVersion:9581,Generation:0,CreationTimestamp:2019-12-17 13:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 17 13:57:16.468: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6116,SelfLink:/api/v1/namespaces/watch-6116/configmaps/e2e-watch-test-label-changed,UID:9df38710-2cd6-477e-8535-e5f829ddd583,ResourceVersion:9583,Generation:0,CreationTimestamp:2019-12-17 13:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 17 13:57:26.507: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6116,SelfLink:/api/v1/namespaces/watch-6116/configmaps/e2e-watch-test-label-changed,UID:9df38710-2cd6-477e-8535-e5f829ddd583,ResourceVersion:9603,Generation:0,CreationTimestamp:2019-12-17 13:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 17 13:57:26.508: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6116,SelfLink:/api/v1/namespaces/watch-6116/configmaps/e2e-watch-test-label-changed,UID:9df38710-2cd6-477e-8535-e5f829ddd583,ResourceVersion:9604,Generation:0,CreationTimestamp:2019-12-17 13:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 17 13:57:26.508: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6116,SelfLink:/api/v1/namespaces/watch-6116/configmaps/e2e-watch-test-label-changed,UID:9df38710-2cd6-477e-8535-e5f829ddd583,ResourceVersion:9605,Generation:0,CreationTimestamp:2019-12-17 13:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:57:26.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6116" for this suite.
Dec 17 13:57:32.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:57:32.663: INFO: namespace watch-6116 deletion completed in 6.146367792s

• [SLOW TEST:16.279 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:57:32.667: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2378
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2378
STEP: Creating statefulset with conflicting port in namespace statefulset-2378
STEP: Waiting until pod test-pod will start running in namespace statefulset-2378
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2378
Dec 17 13:57:36.749: INFO: Observed stateful pod in namespace: statefulset-2378, name: ss-0, uid: 8ac5adff-6150-4d0c-bfa8-67fe57e6a915, status phase: Pending. Waiting for statefulset controller to delete.
Dec 17 13:57:39.096: INFO: Observed stateful pod in namespace: statefulset-2378, name: ss-0, uid: 8ac5adff-6150-4d0c-bfa8-67fe57e6a915, status phase: Failed. Waiting for statefulset controller to delete.
Dec 17 13:57:39.104: INFO: Observed stateful pod in namespace: statefulset-2378, name: ss-0, uid: 8ac5adff-6150-4d0c-bfa8-67fe57e6a915, status phase: Failed. Waiting for statefulset controller to delete.
Dec 17 13:57:39.119: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2378
STEP: Removing pod with conflicting port in namespace statefulset-2378
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2378 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 17 13:57:43.156: INFO: Deleting all statefulset in ns statefulset-2378
Dec 17 13:57:43.159: INFO: Scaling statefulset ss to 0
Dec 17 13:57:53.195: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 13:57:53.198: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:57:53.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2378" for this suite.
Dec 17 13:57:59.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:57:59.348: INFO: namespace statefulset-2378 deletion completed in 6.131865545s

• [SLOW TEST:26.681 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:57:59.349: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 17 13:57:59.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-4440'
Dec 17 13:58:00.005: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 17 13:58:00.005: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Dec 17 13:58:04.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4440'
Dec 17 13:58:04.132: INFO: stderr: ""
Dec 17 13:58:04.132: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:58:04.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4440" for this suite.
Dec 17 13:58:26.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:58:26.257: INFO: namespace kubectl-4440 deletion completed in 22.118435785s

• [SLOW TEST:26.909 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:58:26.260: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 17 13:58:26.350: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4113,SelfLink:/api/v1/namespaces/watch-4113/configmaps/e2e-watch-test-watch-closed,UID:5ff00c1c-7885-49f0-8d27-42996ed6fc8d,ResourceVersion:9888,Generation:0,CreationTimestamp:2019-12-17 13:58:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 17 13:58:26.350: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4113,SelfLink:/api/v1/namespaces/watch-4113/configmaps/e2e-watch-test-watch-closed,UID:5ff00c1c-7885-49f0-8d27-42996ed6fc8d,ResourceVersion:9889,Generation:0,CreationTimestamp:2019-12-17 13:58:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 17 13:58:26.382: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4113,SelfLink:/api/v1/namespaces/watch-4113/configmaps/e2e-watch-test-watch-closed,UID:5ff00c1c-7885-49f0-8d27-42996ed6fc8d,ResourceVersion:9890,Generation:0,CreationTimestamp:2019-12-17 13:58:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 17 13:58:26.382: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4113,SelfLink:/api/v1/namespaces/watch-4113/configmaps/e2e-watch-test-watch-closed,UID:5ff00c1c-7885-49f0-8d27-42996ed6fc8d,ResourceVersion:9891,Generation:0,CreationTimestamp:2019-12-17 13:58:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:58:26.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4113" for this suite.
Dec 17 13:58:32.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:58:32.488: INFO: namespace watch-4113 deletion completed in 6.100907513s

• [SLOW TEST:6.228 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:58:32.488: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-94289f97-0aa2-4da2-a7ba-f238e7f77860
STEP: Creating a pod to test consume configMaps
Dec 17 13:58:32.541: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e2b3eed3-eba1-4ea5-a5d8-44f515d1d8c4" in namespace "projected-5850" to be "success or failure"
Dec 17 13:58:32.544: INFO: Pod "pod-projected-configmaps-e2b3eed3-eba1-4ea5-a5d8-44f515d1d8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305732ms
Dec 17 13:58:34.549: INFO: Pod "pod-projected-configmaps-e2b3eed3-eba1-4ea5-a5d8-44f515d1d8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008193711s
Dec 17 13:58:36.555: INFO: Pod "pod-projected-configmaps-e2b3eed3-eba1-4ea5-a5d8-44f515d1d8c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013767473s
STEP: Saw pod success
Dec 17 13:58:36.555: INFO: Pod "pod-projected-configmaps-e2b3eed3-eba1-4ea5-a5d8-44f515d1d8c4" satisfied condition "success or failure"
Dec 17 13:58:36.558: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-configmaps-e2b3eed3-eba1-4ea5-a5d8-44f515d1d8c4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 13:58:36.586: INFO: Waiting for pod pod-projected-configmaps-e2b3eed3-eba1-4ea5-a5d8-44f515d1d8c4 to disappear
Dec 17 13:58:36.590: INFO: Pod pod-projected-configmaps-e2b3eed3-eba1-4ea5-a5d8-44f515d1d8c4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:58:36.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5850" for this suite.
Dec 17 13:58:42.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:58:42.706: INFO: namespace projected-5850 deletion completed in 6.112032769s

• [SLOW TEST:10.218 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:58:42.709: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 17 13:58:42.866: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:42.870: INFO: Number of nodes with available pods: 0
Dec 17 13:58:42.870: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:58:43.883: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:43.888: INFO: Number of nodes with available pods: 0
Dec 17 13:58:43.888: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:58:44.876: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:44.881: INFO: Number of nodes with available pods: 1
Dec 17 13:58:44.881: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 13:58:45.876: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:45.881: INFO: Number of nodes with available pods: 3
Dec 17 13:58:45.881: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 17 13:58:45.899: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:45.903: INFO: Number of nodes with available pods: 2
Dec 17 13:58:45.903: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:46.915: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:46.926: INFO: Number of nodes with available pods: 2
Dec 17 13:58:46.926: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:47.912: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:47.916: INFO: Number of nodes with available pods: 2
Dec 17 13:58:47.916: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:48.920: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:48.924: INFO: Number of nodes with available pods: 2
Dec 17 13:58:48.924: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:49.909: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:49.913: INFO: Number of nodes with available pods: 2
Dec 17 13:58:49.913: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:50.909: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:50.913: INFO: Number of nodes with available pods: 2
Dec 17 13:58:50.913: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:51.909: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:51.920: INFO: Number of nodes with available pods: 2
Dec 17 13:58:51.920: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:52.912: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:52.917: INFO: Number of nodes with available pods: 2
Dec 17 13:58:52.917: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:53.909: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:53.914: INFO: Number of nodes with available pods: 2
Dec 17 13:58:53.914: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:54.909: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:54.914: INFO: Number of nodes with available pods: 2
Dec 17 13:58:54.914: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:55.910: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:55.914: INFO: Number of nodes with available pods: 2
Dec 17 13:58:55.914: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:56.913: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:56.918: INFO: Number of nodes with available pods: 2
Dec 17 13:58:56.918: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:57.911: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:57.915: INFO: Number of nodes with available pods: 2
Dec 17 13:58:57.915: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:58.909: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:58.913: INFO: Number of nodes with available pods: 2
Dec 17 13:58:58.913: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:58:59.908: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:58:59.920: INFO: Number of nodes with available pods: 2
Dec 17 13:58:59.920: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:59:00.909: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:59:00.913: INFO: Number of nodes with available pods: 2
Dec 17 13:59:00.913: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 13:59:01.915: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 13:59:01.928: INFO: Number of nodes with available pods: 3
Dec 17 13:59:01.929: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7331, will wait for the garbage collector to delete the pods
Dec 17 13:59:02.000: INFO: Deleting DaemonSet.extensions daemon-set took: 11.620368ms
Dec 17 13:59:02.300: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.321133ms
Dec 17 13:59:09.506: INFO: Number of nodes with available pods: 0
Dec 17 13:59:09.506: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 13:59:09.510: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7331/daemonsets","resourceVersion":"10112"},"items":null}

Dec 17 13:59:09.513: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7331/pods","resourceVersion":"10112"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:59:09.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7331" for this suite.
Dec 17 13:59:15.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:59:15.660: INFO: namespace daemonsets-7331 deletion completed in 6.130834101s

• [SLOW TEST:32.951 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:59:15.662: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Dec 17 13:59:15.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-4300'
Dec 17 13:59:15.971: INFO: stderr: ""
Dec 17 13:59:15.971: INFO: stdout: "pod/pause created\n"
Dec 17 13:59:15.971: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 17 13:59:15.971: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4300" to be "running and ready"
Dec 17 13:59:15.980: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.036681ms
Dec 17 13:59:17.985: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013375688s
Dec 17 13:59:19.989: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.018095279s
Dec 17 13:59:19.989: INFO: Pod "pause" satisfied condition "running and ready"
Dec 17 13:59:19.989: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 17 13:59:19.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 label pods pause testing-label=testing-label-value --namespace=kubectl-4300'
Dec 17 13:59:20.085: INFO: stderr: ""
Dec 17 13:59:20.085: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 17 13:59:20.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pod pause -L testing-label --namespace=kubectl-4300'
Dec 17 13:59:20.160: INFO: stderr: ""
Dec 17 13:59:20.160: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 17 13:59:20.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 label pods pause testing-label- --namespace=kubectl-4300'
Dec 17 13:59:20.244: INFO: stderr: ""
Dec 17 13:59:20.244: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 17 13:59:20.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pod pause -L testing-label --namespace=kubectl-4300'
Dec 17 13:59:20.318: INFO: stderr: ""
Dec 17 13:59:20.318: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Dec 17 13:59:20.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-4300'
Dec 17 13:59:20.392: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 13:59:20.392: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 17 13:59:20.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get rc,svc -l name=pause --no-headers --namespace=kubectl-4300'
Dec 17 13:59:20.488: INFO: stderr: "No resources found.\n"
Dec 17 13:59:20.488: INFO: stdout: ""
Dec 17 13:59:20.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -l name=pause --namespace=kubectl-4300 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 13:59:20.565: INFO: stderr: ""
Dec 17 13:59:20.565: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:59:20.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4300" for this suite.
Dec 17 13:59:26.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:59:26.672: INFO: namespace kubectl-4300 deletion completed in 6.102001286s

• [SLOW TEST:11.010 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:59:26.672: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Dec 17 13:59:26.737: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8687" to be "success or failure"
Dec 17 13:59:26.744: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05186ms
Dec 17 13:59:28.751: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013753726s
Dec 17 13:59:30.755: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017412292s
STEP: Saw pod success
Dec 17 13:59:30.755: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 17 13:59:30.759: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 17 13:59:30.783: INFO: Waiting for pod pod-host-path-test to disappear
Dec 17 13:59:30.786: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:59:30.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8687" for this suite.
Dec 17 13:59:36.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:59:36.905: INFO: namespace hostpath-8687 deletion completed in 6.115526583s

• [SLOW TEST:10.233 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:59:36.909: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 13:59:36.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8a7bc7c-0283-437e-80a8-18ec3a970326" in namespace "downward-api-2278" to be "success or failure"
Dec 17 13:59:36.960: INFO: Pod "downwardapi-volume-d8a7bc7c-0283-437e-80a8-18ec3a970326": Phase="Pending", Reason="", readiness=false. Elapsed: 5.158104ms
Dec 17 13:59:38.965: INFO: Pod "downwardapi-volume-d8a7bc7c-0283-437e-80a8-18ec3a970326": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010064313s
STEP: Saw pod success
Dec 17 13:59:38.966: INFO: Pod "downwardapi-volume-d8a7bc7c-0283-437e-80a8-18ec3a970326" satisfied condition "success or failure"
Dec 17 13:59:38.977: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-d8a7bc7c-0283-437e-80a8-18ec3a970326 container client-container: <nil>
STEP: delete the pod
Dec 17 13:59:39.015: INFO: Waiting for pod downwardapi-volume-d8a7bc7c-0283-437e-80a8-18ec3a970326 to disappear
Dec 17 13:59:39.018: INFO: Pod downwardapi-volume-d8a7bc7c-0283-437e-80a8-18ec3a970326 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:59:39.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2278" for this suite.
Dec 17 13:59:45.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:59:45.131: INFO: namespace downward-api-2278 deletion completed in 6.109001945s

• [SLOW TEST:8.223 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:59:45.133: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 17 13:59:45.197: INFO: Waiting up to 5m0s for pod "pod-a1498222-1f13-4463-865c-e1c2f81d99dd" in namespace "emptydir-3031" to be "success or failure"
Dec 17 13:59:45.200: INFO: Pod "pod-a1498222-1f13-4463-865c-e1c2f81d99dd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.754656ms
Dec 17 13:59:47.205: INFO: Pod "pod-a1498222-1f13-4463-865c-e1c2f81d99dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008260938s
STEP: Saw pod success
Dec 17 13:59:47.205: INFO: Pod "pod-a1498222-1f13-4463-865c-e1c2f81d99dd" satisfied condition "success or failure"
Dec 17 13:59:47.209: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-a1498222-1f13-4463-865c-e1c2f81d99dd container test-container: <nil>
STEP: delete the pod
Dec 17 13:59:47.234: INFO: Waiting for pod pod-a1498222-1f13-4463-865c-e1c2f81d99dd to disappear
Dec 17 13:59:47.238: INFO: Pod pod-a1498222-1f13-4463-865c-e1c2f81d99dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:59:47.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3031" for this suite.
Dec 17 13:59:53.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 13:59:53.360: INFO: namespace emptydir-3031 deletion completed in 6.117431154s

• [SLOW TEST:8.226 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 13:59:53.360: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-674ee919-bd2c-40b4-a1b7-c334ba49c6c4
STEP: Creating a pod to test consume secrets
Dec 17 13:59:53.484: INFO: Waiting up to 5m0s for pod "pod-secrets-1ca39223-ba5a-4df0-a850-921e9b3ea7ef" in namespace "secrets-9615" to be "success or failure"
Dec 17 13:59:53.494: INFO: Pod "pod-secrets-1ca39223-ba5a-4df0-a850-921e9b3ea7ef": Phase="Pending", Reason="", readiness=false. Elapsed: 9.103807ms
Dec 17 13:59:55.504: INFO: Pod "pod-secrets-1ca39223-ba5a-4df0-a850-921e9b3ea7ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019043803s
STEP: Saw pod success
Dec 17 13:59:55.504: INFO: Pod "pod-secrets-1ca39223-ba5a-4df0-a850-921e9b3ea7ef" satisfied condition "success or failure"
Dec 17 13:59:55.508: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-secrets-1ca39223-ba5a-4df0-a850-921e9b3ea7ef container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 13:59:55.537: INFO: Waiting for pod pod-secrets-1ca39223-ba5a-4df0-a850-921e9b3ea7ef to disappear
Dec 17 13:59:55.540: INFO: Pod pod-secrets-1ca39223-ba5a-4df0-a850-921e9b3ea7ef no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 13:59:55.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9615" for this suite.
Dec 17 14:00:01.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:00:01.653: INFO: namespace secrets-9615 deletion completed in 6.106548549s
STEP: Destroying namespace "secret-namespace-4046" for this suite.
Dec 17 14:00:07.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:00:07.787: INFO: namespace secret-namespace-4046 deletion completed in 6.133582074s

• [SLOW TEST:14.427 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:00:07.787: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:00:07.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84cf0829-7fa6-432e-aad1-06d4ed80aca5" in namespace "downward-api-173" to be "success or failure"
Dec 17 14:00:07.851: INFO: Pod "downwardapi-volume-84cf0829-7fa6-432e-aad1-06d4ed80aca5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.48494ms
Dec 17 14:00:09.856: INFO: Pod "downwardapi-volume-84cf0829-7fa6-432e-aad1-06d4ed80aca5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016414387s
STEP: Saw pod success
Dec 17 14:00:09.856: INFO: Pod "downwardapi-volume-84cf0829-7fa6-432e-aad1-06d4ed80aca5" satisfied condition "success or failure"
Dec 17 14:00:09.859: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-84cf0829-7fa6-432e-aad1-06d4ed80aca5 container client-container: <nil>
STEP: delete the pod
Dec 17 14:00:09.893: INFO: Waiting for pod downwardapi-volume-84cf0829-7fa6-432e-aad1-06d4ed80aca5 to disappear
Dec 17 14:00:09.898: INFO: Pod downwardapi-volume-84cf0829-7fa6-432e-aad1-06d4ed80aca5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:00:09.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-173" for this suite.
Dec 17 14:00:15.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:00:16.027: INFO: namespace downward-api-173 deletion completed in 6.12369369s

• [SLOW TEST:8.240 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:00:16.030: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 17 14:00:16.080: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Dec 17 14:00:16.567: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 17 14:00:18.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:00:20.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:00:22.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:00:24.646: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:00:26.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712188016, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:00:29.885: INFO: Waited 1.23371791s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:00:30.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9125" for this suite.
Dec 17 14:00:36.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:00:36.519: INFO: namespace aggregator-9125 deletion completed in 6.199391092s

• [SLOW TEST:20.489 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:00:36.519: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-c90504e8-2dd1-46f1-9572-f4e46343f9e8
STEP: Creating secret with name s-test-opt-upd-873d5ae6-5985-4c6d-883a-874ce10827ac
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c90504e8-2dd1-46f1-9572-f4e46343f9e8
STEP: Updating secret s-test-opt-upd-873d5ae6-5985-4c6d-883a-874ce10827ac
STEP: Creating secret with name s-test-opt-create-708c3b9f-98b3-4b31-9971-dded1fa58b74
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:00:42.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-307" for this suite.
Dec 17 14:01:04.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:01:04.839: INFO: namespace projected-307 deletion completed in 22.123814523s

• [SLOW TEST:28.320 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:01:04.839: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 17 14:01:04.887: INFO: Waiting up to 5m0s for pod "downward-api-16b31330-d32e-4f69-9dc3-ce85cdaf37b9" in namespace "downward-api-5965" to be "success or failure"
Dec 17 14:01:04.890: INFO: Pod "downward-api-16b31330-d32e-4f69-9dc3-ce85cdaf37b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.373554ms
Dec 17 14:01:06.896: INFO: Pod "downward-api-16b31330-d32e-4f69-9dc3-ce85cdaf37b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008898765s
Dec 17 14:01:08.901: INFO: Pod "downward-api-16b31330-d32e-4f69-9dc3-ce85cdaf37b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0144996s
STEP: Saw pod success
Dec 17 14:01:08.902: INFO: Pod "downward-api-16b31330-d32e-4f69-9dc3-ce85cdaf37b9" satisfied condition "success or failure"
Dec 17 14:01:08.905: INFO: Trying to get logs from node conformance-k8s-node-2 pod downward-api-16b31330-d32e-4f69-9dc3-ce85cdaf37b9 container dapi-container: <nil>
STEP: delete the pod
Dec 17 14:01:08.936: INFO: Waiting for pod downward-api-16b31330-d32e-4f69-9dc3-ce85cdaf37b9 to disappear
Dec 17 14:01:08.940: INFO: Pod downward-api-16b31330-d32e-4f69-9dc3-ce85cdaf37b9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:01:08.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5965" for this suite.
Dec 17 14:01:14.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:01:15.061: INFO: namespace downward-api-5965 deletion completed in 6.117027468s

• [SLOW TEST:10.222 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:01:15.062: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-rpd8
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 14:01:15.125: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rpd8" in namespace "subpath-7974" to be "success or failure"
Dec 17 14:01:15.147: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.631874ms
Dec 17 14:01:17.153: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027917714s
Dec 17 14:01:19.156: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Running", Reason="", readiness=true. Elapsed: 4.031618485s
Dec 17 14:01:21.160: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Running", Reason="", readiness=true. Elapsed: 6.035849074s
Dec 17 14:01:23.165: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Running", Reason="", readiness=true. Elapsed: 8.040235028s
Dec 17 14:01:25.170: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Running", Reason="", readiness=true. Elapsed: 10.044878694s
Dec 17 14:01:27.174: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Running", Reason="", readiness=true. Elapsed: 12.048952285s
Dec 17 14:01:29.178: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Running", Reason="", readiness=true. Elapsed: 14.053602462s
Dec 17 14:01:31.184: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Running", Reason="", readiness=true. Elapsed: 16.059470083s
Dec 17 14:01:33.190: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Running", Reason="", readiness=true. Elapsed: 18.064983294s
Dec 17 14:01:35.196: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Running", Reason="", readiness=true. Elapsed: 20.071668518s
Dec 17 14:01:37.201: INFO: Pod "pod-subpath-test-configmap-rpd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.075959956s
STEP: Saw pod success
Dec 17 14:01:37.201: INFO: Pod "pod-subpath-test-configmap-rpd8" satisfied condition "success or failure"
Dec 17 14:01:37.204: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-subpath-test-configmap-rpd8 container test-container-subpath-configmap-rpd8: <nil>
STEP: delete the pod
Dec 17 14:01:37.231: INFO: Waiting for pod pod-subpath-test-configmap-rpd8 to disappear
Dec 17 14:01:37.246: INFO: Pod pod-subpath-test-configmap-rpd8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rpd8
Dec 17 14:01:37.246: INFO: Deleting pod "pod-subpath-test-configmap-rpd8" in namespace "subpath-7974"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:01:37.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7974" for this suite.
Dec 17 14:01:43.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:01:43.381: INFO: namespace subpath-7974 deletion completed in 6.119172634s

• [SLOW TEST:28.319 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:01:43.381: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9183
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 17 14:01:43.426: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 17 14:02:01.569: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.90:8080/dial?request=hostName&protocol=udp&host=10.233.93.48&port=8081&tries=1'] Namespace:pod-network-test-9183 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:02:01.569: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:02:01.748: INFO: Waiting for endpoints: map[]
Dec 17 14:02:01.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.90:8080/dial?request=hostName&protocol=udp&host=10.233.95.22&port=8081&tries=1'] Namespace:pod-network-test-9183 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:02:01.752: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:02:01.925: INFO: Waiting for endpoints: map[]
Dec 17 14:02:01.930: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.90:8080/dial?request=hostName&protocol=udp&host=10.233.100.89&port=8081&tries=1'] Namespace:pod-network-test-9183 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:02:01.930: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:02:02.084: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:02:02.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9183" for this suite.
Dec 17 14:02:24.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:02:24.225: INFO: namespace pod-network-test-9183 deletion completed in 22.118693655s

• [SLOW TEST:40.844 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:02:24.227: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Dec 17 14:02:24.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 cluster-info'
Dec 17 14:02:24.354: INFO: stderr: ""
Dec 17 14:02:24.354: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:02:24.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7563" for this suite.
Dec 17 14:02:30.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:02:30.463: INFO: namespace kubectl-7563 deletion completed in 6.105460919s

• [SLOW TEST:6.237 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:02:30.469: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:02:30.525: INFO: Waiting up to 5m0s for pod "downwardapi-volume-812f6182-9409-41b7-b9f3-0ccb7ec52f51" in namespace "projected-4689" to be "success or failure"
Dec 17 14:02:30.532: INFO: Pod "downwardapi-volume-812f6182-9409-41b7-b9f3-0ccb7ec52f51": Phase="Pending", Reason="", readiness=false. Elapsed: 7.349545ms
Dec 17 14:02:32.536: INFO: Pod "downwardapi-volume-812f6182-9409-41b7-b9f3-0ccb7ec52f51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011679721s
STEP: Saw pod success
Dec 17 14:02:32.536: INFO: Pod "downwardapi-volume-812f6182-9409-41b7-b9f3-0ccb7ec52f51" satisfied condition "success or failure"
Dec 17 14:02:32.540: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-812f6182-9409-41b7-b9f3-0ccb7ec52f51 container client-container: <nil>
STEP: delete the pod
Dec 17 14:02:32.564: INFO: Waiting for pod downwardapi-volume-812f6182-9409-41b7-b9f3-0ccb7ec52f51 to disappear
Dec 17 14:02:32.571: INFO: Pod downwardapi-volume-812f6182-9409-41b7-b9f3-0ccb7ec52f51 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:02:32.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4689" for this suite.
Dec 17 14:02:38.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:02:38.690: INFO: namespace projected-4689 deletion completed in 6.11371126s

• [SLOW TEST:8.222 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:02:38.692: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 17 14:02:38.781: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1072,SelfLink:/api/v1/namespaces/watch-1072/configmaps/e2e-watch-test-resource-version,UID:a6ddf90f-493e-4596-8d05-eb044919b791,ResourceVersion:11151,Generation:0,CreationTimestamp:2019-12-17 14:02:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 17 14:02:38.781: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1072,SelfLink:/api/v1/namespaces/watch-1072/configmaps/e2e-watch-test-resource-version,UID:a6ddf90f-493e-4596-8d05-eb044919b791,ResourceVersion:11152,Generation:0,CreationTimestamp:2019-12-17 14:02:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:02:38.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1072" for this suite.
Dec 17 14:02:44.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:02:44.906: INFO: namespace watch-1072 deletion completed in 6.120489118s

• [SLOW TEST:6.214 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:02:44.906: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 17 14:02:47.991: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:02:49.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-537" for this suite.
Dec 17 14:03:11.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:03:11.163: INFO: namespace replicaset-537 deletion completed in 22.145757333s

• [SLOW TEST:26.257 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:03:11.166: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 17 14:03:11.232: INFO: Waiting up to 5m0s for pod "pod-a3fe251b-4165-416b-a0a5-5e097cc68722" in namespace "emptydir-5611" to be "success or failure"
Dec 17 14:03:11.242: INFO: Pod "pod-a3fe251b-4165-416b-a0a5-5e097cc68722": Phase="Pending", Reason="", readiness=false. Elapsed: 9.466134ms
Dec 17 14:03:13.247: INFO: Pod "pod-a3fe251b-4165-416b-a0a5-5e097cc68722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014741356s
STEP: Saw pod success
Dec 17 14:03:13.247: INFO: Pod "pod-a3fe251b-4165-416b-a0a5-5e097cc68722" satisfied condition "success or failure"
Dec 17 14:03:13.251: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-a3fe251b-4165-416b-a0a5-5e097cc68722 container test-container: <nil>
STEP: delete the pod
Dec 17 14:03:13.309: INFO: Waiting for pod pod-a3fe251b-4165-416b-a0a5-5e097cc68722 to disappear
Dec 17 14:03:13.312: INFO: Pod pod-a3fe251b-4165-416b-a0a5-5e097cc68722 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:03:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5611" for this suite.
Dec 17 14:03:19.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:03:19.484: INFO: namespace emptydir-5611 deletion completed in 6.168178788s

• [SLOW TEST:8.319 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:03:19.485: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1217 14:03:50.068631      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 17 14:03:50.068: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:03:50.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4477" for this suite.
Dec 17 14:03:56.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:03:56.162: INFO: namespace gc-4477 deletion completed in 6.090809924s

• [SLOW TEST:36.677 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:03:56.162: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-6e490510-66a1-4b21-9fe5-9ee71fa78922
STEP: Creating a pod to test consume secrets
Dec 17 14:03:56.210: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a3d425c-5345-498b-b40a-8da5725e4753" in namespace "projected-8580" to be "success or failure"
Dec 17 14:03:56.223: INFO: Pod "pod-projected-secrets-0a3d425c-5345-498b-b40a-8da5725e4753": Phase="Pending", Reason="", readiness=false. Elapsed: 12.677737ms
Dec 17 14:03:58.229: INFO: Pod "pod-projected-secrets-0a3d425c-5345-498b-b40a-8da5725e4753": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018707944s
STEP: Saw pod success
Dec 17 14:03:58.229: INFO: Pod "pod-projected-secrets-0a3d425c-5345-498b-b40a-8da5725e4753" satisfied condition "success or failure"
Dec 17 14:03:58.232: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-secrets-0a3d425c-5345-498b-b40a-8da5725e4753 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 14:03:58.251: INFO: Waiting for pod pod-projected-secrets-0a3d425c-5345-498b-b40a-8da5725e4753 to disappear
Dec 17 14:03:58.253: INFO: Pod pod-projected-secrets-0a3d425c-5345-498b-b40a-8da5725e4753 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:03:58.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8580" for this suite.
Dec 17 14:04:04.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:04:04.370: INFO: namespace projected-8580 deletion completed in 6.113495504s

• [SLOW TEST:8.208 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:04:04.374: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec 17 14:04:04.418: INFO: namespace kubectl-9332
Dec 17 14:04:04.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-9332'
Dec 17 14:04:04.727: INFO: stderr: ""
Dec 17 14:04:04.727: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 17 14:04:05.737: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 14:04:05.738: INFO: Found 0 / 1
Dec 17 14:04:06.732: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 14:04:06.732: INFO: Found 1 / 1
Dec 17 14:04:06.732: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 17 14:04:06.735: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 14:04:06.735: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 17 14:04:06.735: INFO: wait on redis-master startup in kubectl-9332 
Dec 17 14:04:06.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 logs redis-master-kjxcj redis-master --namespace=kubectl-9332'
Dec 17 14:04:06.833: INFO: stderr: ""
Dec 17 14:04:06.833: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 17 Dec 14:04:05.979 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 17 Dec 14:04:05.979 # Server started, Redis version 3.2.12\n1:M 17 Dec 14:04:05.979 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 17 Dec 14:04:05.979 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec 17 14:04:06.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9332'
Dec 17 14:04:06.932: INFO: stderr: ""
Dec 17 14:04:06.932: INFO: stdout: "service/rm2 exposed\n"
Dec 17 14:04:06.939: INFO: Service rm2 in namespace kubectl-9332 found.
STEP: exposing service
Dec 17 14:04:08.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9332'
Dec 17 14:04:09.045: INFO: stderr: ""
Dec 17 14:04:09.045: INFO: stdout: "service/rm3 exposed\n"
Dec 17 14:04:09.053: INFO: Service rm3 in namespace kubectl-9332 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:04:11.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9332" for this suite.
Dec 17 14:04:33.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:04:33.189: INFO: namespace kubectl-9332 deletion completed in 22.124826299s

• [SLOW TEST:28.815 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:04:33.191: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-c9a787be-fba7-4415-ad7d-60a926f6e014
STEP: Creating a pod to test consume configMaps
Dec 17 14:04:33.263: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-84a2f977-72e1-4f40-9c7a-e781db11421d" in namespace "projected-1364" to be "success or failure"
Dec 17 14:04:33.267: INFO: Pod "pod-projected-configmaps-84a2f977-72e1-4f40-9c7a-e781db11421d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.32871ms
Dec 17 14:04:35.273: INFO: Pod "pod-projected-configmaps-84a2f977-72e1-4f40-9c7a-e781db11421d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009963123s
Dec 17 14:04:37.277: INFO: Pod "pod-projected-configmaps-84a2f977-72e1-4f40-9c7a-e781db11421d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014199829s
STEP: Saw pod success
Dec 17 14:04:37.277: INFO: Pod "pod-projected-configmaps-84a2f977-72e1-4f40-9c7a-e781db11421d" satisfied condition "success or failure"
Dec 17 14:04:37.279: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-configmaps-84a2f977-72e1-4f40-9c7a-e781db11421d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 14:04:37.307: INFO: Waiting for pod pod-projected-configmaps-84a2f977-72e1-4f40-9c7a-e781db11421d to disappear
Dec 17 14:04:37.311: INFO: Pod pod-projected-configmaps-84a2f977-72e1-4f40-9c7a-e781db11421d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:04:37.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1364" for this suite.
Dec 17 14:04:43.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:04:43.435: INFO: namespace projected-1364 deletion completed in 6.120155546s

• [SLOW TEST:10.245 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:04:43.437: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Dec 17 14:04:43.490: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 17 14:04:43.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-4360'
Dec 17 14:04:43.707: INFO: stderr: ""
Dec 17 14:04:43.707: INFO: stdout: "service/redis-slave created\n"
Dec 17 14:04:43.708: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 17 14:04:43.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-4360'
Dec 17 14:04:43.960: INFO: stderr: ""
Dec 17 14:04:43.960: INFO: stdout: "service/redis-master created\n"
Dec 17 14:04:43.960: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 17 14:04:43.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-4360'
Dec 17 14:04:44.198: INFO: stderr: ""
Dec 17 14:04:44.198: INFO: stdout: "service/frontend created\n"
Dec 17 14:04:44.198: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 17 14:04:44.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-4360'
Dec 17 14:04:44.453: INFO: stderr: ""
Dec 17 14:04:44.453: INFO: stdout: "deployment.apps/frontend created\n"
Dec 17 14:04:44.454: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 17 14:04:44.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-4360'
Dec 17 14:04:44.699: INFO: stderr: ""
Dec 17 14:04:44.699: INFO: stdout: "deployment.apps/redis-master created\n"
Dec 17 14:04:44.699: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 17 14:04:44.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-4360'
Dec 17 14:04:44.932: INFO: stderr: ""
Dec 17 14:04:44.932: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec 17 14:04:44.932: INFO: Waiting for all frontend pods to be Running.
Dec 17 14:05:04.985: INFO: Waiting for frontend to serve content.
Dec 17 14:05:05.998: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec 17 14:05:11.022: INFO: Trying to add a new entry to the guestbook.
Dec 17 14:05:11.041: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 17 14:05:11.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-4360'
Dec 17 14:05:11.176: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 14:05:11.176: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 14:05:11.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-4360'
Dec 17 14:05:11.278: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 14:05:11.278: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 14:05:11.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-4360'
Dec 17 14:05:11.408: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 14:05:11.408: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 14:05:11.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-4360'
Dec 17 14:05:11.501: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 14:05:11.501: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 14:05:11.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-4360'
Dec 17 14:05:11.595: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 14:05:11.595: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 14:05:11.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-4360'
Dec 17 14:05:11.674: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 14:05:11.674: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:05:11.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4360" for this suite.
Dec 17 14:05:49.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:05:49.785: INFO: namespace kubectl-4360 deletion completed in 38.107803686s

• [SLOW TEST:66.349 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:05:49.786: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:05:49.825: INFO: Creating ReplicaSet my-hostname-basic-debf60de-00d6-47f1-957b-bae21d3a615e
Dec 17 14:05:49.836: INFO: Pod name my-hostname-basic-debf60de-00d6-47f1-957b-bae21d3a615e: Found 0 pods out of 1
Dec 17 14:05:54.842: INFO: Pod name my-hostname-basic-debf60de-00d6-47f1-957b-bae21d3a615e: Found 1 pods out of 1
Dec 17 14:05:54.842: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-debf60de-00d6-47f1-957b-bae21d3a615e" is running
Dec 17 14:05:54.846: INFO: Pod "my-hostname-basic-debf60de-00d6-47f1-957b-bae21d3a615e-dglsv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-17 14:05:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-17 14:05:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-17 14:05:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-17 14:05:49 +0000 UTC Reason: Message:}])
Dec 17 14:05:54.847: INFO: Trying to dial the pod
Dec 17 14:05:59.868: INFO: Controller my-hostname-basic-debf60de-00d6-47f1-957b-bae21d3a615e: Got expected result from replica 1 [my-hostname-basic-debf60de-00d6-47f1-957b-bae21d3a615e-dglsv]: "my-hostname-basic-debf60de-00d6-47f1-957b-bae21d3a615e-dglsv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:05:59.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3746" for this suite.
Dec 17 14:06:05.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:06:06.010: INFO: namespace replicaset-3746 deletion completed in 6.135855193s

• [SLOW TEST:16.224 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:06:06.010: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-ebb17ba0-bd0e-4492-9cfe-a07c5d4ebc40 in namespace container-probe-1987
Dec 17 14:06:10.087: INFO: Started pod busybox-ebb17ba0-bd0e-4492-9cfe-a07c5d4ebc40 in namespace container-probe-1987
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 14:06:10.090: INFO: Initial restart count of pod busybox-ebb17ba0-bd0e-4492-9cfe-a07c5d4ebc40 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:10:10.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1987" for this suite.
Dec 17 14:10:17.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:10:17.115: INFO: namespace container-probe-1987 deletion completed in 6.129592829s

• [SLOW TEST:251.105 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:10:17.119: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Dec 17 14:10:17.165: INFO: Waiting up to 5m0s for pod "client-containers-1209f040-4453-4f8a-a112-6bce76317ff3" in namespace "containers-6544" to be "success or failure"
Dec 17 14:10:17.169: INFO: Pod "client-containers-1209f040-4453-4f8a-a112-6bce76317ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.351745ms
Dec 17 14:10:19.172: INFO: Pod "client-containers-1209f040-4453-4f8a-a112-6bce76317ff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006548751s
STEP: Saw pod success
Dec 17 14:10:19.172: INFO: Pod "client-containers-1209f040-4453-4f8a-a112-6bce76317ff3" satisfied condition "success or failure"
Dec 17 14:10:19.174: INFO: Trying to get logs from node conformance-k8s-node-2 pod client-containers-1209f040-4453-4f8a-a112-6bce76317ff3 container test-container: <nil>
STEP: delete the pod
Dec 17 14:10:19.202: INFO: Waiting for pod client-containers-1209f040-4453-4f8a-a112-6bce76317ff3 to disappear
Dec 17 14:10:19.216: INFO: Pod client-containers-1209f040-4453-4f8a-a112-6bce76317ff3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:10:19.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6544" for this suite.
Dec 17 14:10:25.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:10:25.338: INFO: namespace containers-6544 deletion completed in 6.103389992s

• [SLOW TEST:8.219 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:10:25.339: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:10:25.408: INFO: (0) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 21.252043ms)
Dec 17 14:10:25.414: INFO: (1) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.982789ms)
Dec 17 14:10:25.417: INFO: (2) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.424826ms)
Dec 17 14:10:25.421: INFO: (3) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.275472ms)
Dec 17 14:10:25.424: INFO: (4) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.451967ms)
Dec 17 14:10:25.428: INFO: (5) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.28142ms)
Dec 17 14:10:25.439: INFO: (6) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.974265ms)
Dec 17 14:10:25.443: INFO: (7) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.816703ms)
Dec 17 14:10:25.446: INFO: (8) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.029472ms)
Dec 17 14:10:25.450: INFO: (9) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.002751ms)
Dec 17 14:10:25.454: INFO: (10) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.819804ms)
Dec 17 14:10:25.457: INFO: (11) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.49423ms)
Dec 17 14:10:25.461: INFO: (12) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.831033ms)
Dec 17 14:10:25.471: INFO: (13) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.342862ms)
Dec 17 14:10:25.475: INFO: (14) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.855693ms)
Dec 17 14:10:25.479: INFO: (15) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.41136ms)
Dec 17 14:10:25.483: INFO: (16) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.204221ms)
Dec 17 14:10:25.487: INFO: (17) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.016253ms)
Dec 17 14:10:25.492: INFO: (18) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.028429ms)
Dec 17 14:10:25.496: INFO: (19) /api/v1/nodes/conformance-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.007984ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:10:25.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-307" for this suite.
Dec 17 14:10:31.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:10:31.618: INFO: namespace proxy-307 deletion completed in 6.116678834s

• [SLOW TEST:6.279 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:10:31.619: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 17 14:10:31.660: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 14:10:31.668: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 14:10:31.671: INFO: 
Logging pods the kubelet thinks is on node conformance-k8s-node-1 before test
Dec 17 14:10:31.681: INFO: nginx-proxy-conformance-k8s-node-1 from kube-system started at 2019-12-17 13:27:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.681: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 14:10:31.681: INFO: coredns-74c9d4d795-2w8nz from kube-system started at 2019-12-17 13:28:57 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.682: INFO: 	Container coredns ready: true, restart count 0
Dec 17 14:10:31.682: INFO: calico-node-fqsjm from kube-system started at 2019-12-17 13:27:45 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.682: INFO: 	Container calico-node ready: true, restart count 1
Dec 17 14:10:31.682: INFO: calico-kube-controllers-58f76c7dcf-mfnlw from kube-system started at 2019-12-17 13:28:21 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.682: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 17 14:10:31.682: INFO: nodelocaldns-llm5v from kube-system started at 2019-12-17 13:28:56 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.683: INFO: 	Container node-cache ready: true, restart count 0
Dec 17 14:10:31.683: INFO: sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-s4k7l from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:10:31.683: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:10:31.683: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 14:10:31.683: INFO: kube-proxy-6kv7k from kube-system started at 2019-12-17 13:27:22 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.683: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 14:10:31.683: INFO: 
Logging pods the kubelet thinks is on node conformance-k8s-node-2 before test
Dec 17 14:10:31.690: INFO: sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-l45d2 from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:10:31.690: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:10:31.690: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 14:10:31.690: INFO: nginx-proxy-conformance-k8s-node-2 from kube-system started at 2019-12-17 13:27:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.690: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 14:10:31.690: INFO: nodelocaldns-j44d9 from kube-system started at 2019-12-17 13:28:56 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.690: INFO: 	Container node-cache ready: true, restart count 0
Dec 17 14:10:31.690: INFO: calico-node-mjmxr from kube-system started at 2019-12-17 13:27:45 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.690: INFO: 	Container calico-node ready: true, restart count 1
Dec 17 14:10:31.690: INFO: kubernetes-dashboard-7c547b4c64-f72ch from kube-system started at 2019-12-17 13:28:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.690: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 17 14:10:31.690: INFO: kube-proxy-flwq7 from kube-system started at 2019-12-17 13:27:22 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.690: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 14:10:31.690: INFO: 
Logging pods the kubelet thinks is on node conformance-k8s-node-3 before test
Dec 17 14:10:31.711: INFO: calico-node-dfzdd from kube-system started at 2019-12-17 13:27:45 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.712: INFO: 	Container calico-node ready: true, restart count 1
Dec 17 14:10:31.712: INFO: kube-proxy-ghh8l from kube-system started at 2019-12-17 13:27:22 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.712: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 14:10:31.712: INFO: nodelocaldns-d4tqp from kube-system started at 2019-12-17 13:28:56 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.712: INFO: 	Container node-cache ready: true, restart count 0
Dec 17 14:10:31.712: INFO: sonobuoy from sonobuoy started at 2019-12-17 13:31:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.712: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 14:10:31.712: INFO: nginx-proxy-conformance-k8s-node-3 from kube-system started at 2019-12-17 13:27:19 +0000 UTC (1 container statuses recorded)
Dec 17 14:10:31.712: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 14:10:31.712: INFO: sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-zf47h from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:10:31.713: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:10:31.713: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 14:10:31.713: INFO: sonobuoy-e2e-job-897112f537434934 from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:10:31.713: INFO: 	Container e2e ready: true, restart count 0
Dec 17 14:10:31.713: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node conformance-k8s-node-1
STEP: verifying the node has the label node conformance-k8s-node-2
STEP: verifying the node has the label node conformance-k8s-node-3
Dec 17 14:10:31.781: INFO: Pod calico-kube-controllers-58f76c7dcf-mfnlw requesting resource cpu=30m on Node conformance-k8s-node-1
Dec 17 14:10:31.781: INFO: Pod calico-node-dfzdd requesting resource cpu=150m on Node conformance-k8s-node-3
Dec 17 14:10:31.781: INFO: Pod calico-node-fqsjm requesting resource cpu=150m on Node conformance-k8s-node-1
Dec 17 14:10:31.781: INFO: Pod calico-node-mjmxr requesting resource cpu=150m on Node conformance-k8s-node-2
Dec 17 14:10:31.781: INFO: Pod coredns-74c9d4d795-2w8nz requesting resource cpu=100m on Node conformance-k8s-node-1
Dec 17 14:10:31.781: INFO: Pod kube-proxy-6kv7k requesting resource cpu=0m on Node conformance-k8s-node-1
Dec 17 14:10:31.781: INFO: Pod kube-proxy-flwq7 requesting resource cpu=0m on Node conformance-k8s-node-2
Dec 17 14:10:31.781: INFO: Pod kube-proxy-ghh8l requesting resource cpu=0m on Node conformance-k8s-node-3
Dec 17 14:10:31.781: INFO: Pod kubernetes-dashboard-7c547b4c64-f72ch requesting resource cpu=50m on Node conformance-k8s-node-2
Dec 17 14:10:31.781: INFO: Pod nginx-proxy-conformance-k8s-node-1 requesting resource cpu=25m on Node conformance-k8s-node-1
Dec 17 14:10:31.781: INFO: Pod nginx-proxy-conformance-k8s-node-2 requesting resource cpu=25m on Node conformance-k8s-node-2
Dec 17 14:10:31.781: INFO: Pod nginx-proxy-conformance-k8s-node-3 requesting resource cpu=25m on Node conformance-k8s-node-3
Dec 17 14:10:31.781: INFO: Pod nodelocaldns-d4tqp requesting resource cpu=100m on Node conformance-k8s-node-3
Dec 17 14:10:31.781: INFO: Pod nodelocaldns-j44d9 requesting resource cpu=100m on Node conformance-k8s-node-2
Dec 17 14:10:31.781: INFO: Pod nodelocaldns-llm5v requesting resource cpu=100m on Node conformance-k8s-node-1
Dec 17 14:10:31.781: INFO: Pod sonobuoy requesting resource cpu=0m on Node conformance-k8s-node-3
Dec 17 14:10:31.781: INFO: Pod sonobuoy-e2e-job-897112f537434934 requesting resource cpu=0m on Node conformance-k8s-node-3
Dec 17 14:10:31.781: INFO: Pod sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-l45d2 requesting resource cpu=0m on Node conformance-k8s-node-2
Dec 17 14:10:31.781: INFO: Pod sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-s4k7l requesting resource cpu=0m on Node conformance-k8s-node-1
Dec 17 14:10:31.781: INFO: Pod sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-zf47h requesting resource cpu=0m on Node conformance-k8s-node-3
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-482a8dcb-f314-4122-bd3f-77c158604a6e.15e12deafb135ade], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7394/filler-pod-482a8dcb-f314-4122-bd3f-77c158604a6e to conformance-k8s-node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-482a8dcb-f314-4122-bd3f-77c158604a6e.15e12deb35f2573c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-482a8dcb-f314-4122-bd3f-77c158604a6e.15e12deb3afefc33], Reason = [Created], Message = [Created container filler-pod-482a8dcb-f314-4122-bd3f-77c158604a6e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-482a8dcb-f314-4122-bd3f-77c158604a6e.15e12deb473e2145], Reason = [Started], Message = [Started container filler-pod-482a8dcb-f314-4122-bd3f-77c158604a6e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ab88d0b9-4b7c-4be0-9329-e1f5226fcdb6.15e12deafc6c90f4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7394/filler-pod-ab88d0b9-4b7c-4be0-9329-e1f5226fcdb6 to conformance-k8s-node-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ab88d0b9-4b7c-4be0-9329-e1f5226fcdb6.15e12deb38f1b4c8], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ab88d0b9-4b7c-4be0-9329-e1f5226fcdb6.15e12deb6b462251], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ab88d0b9-4b7c-4be0-9329-e1f5226fcdb6.15e12deb6f6a67f7], Reason = [Created], Message = [Created container filler-pod-ab88d0b9-4b7c-4be0-9329-e1f5226fcdb6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ab88d0b9-4b7c-4be0-9329-e1f5226fcdb6.15e12deb7ee61505], Reason = [Started], Message = [Started container filler-pod-ab88d0b9-4b7c-4be0-9329-e1f5226fcdb6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-efefe915-b620-4263-ab4e-82b00ffbcdbb.15e12deafa9a57c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7394/filler-pod-efefe915-b620-4263-ab4e-82b00ffbcdbb to conformance-k8s-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-efefe915-b620-4263-ab4e-82b00ffbcdbb.15e12deb390df044], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-efefe915-b620-4263-ab4e-82b00ffbcdbb.15e12deb68447366], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-efefe915-b620-4263-ab4e-82b00ffbcdbb.15e12deb6c2b7f10], Reason = [Created], Message = [Created container filler-pod-efefe915-b620-4263-ab4e-82b00ffbcdbb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-efefe915-b620-4263-ab4e-82b00ffbcdbb.15e12deb7ad59bd4], Reason = [Started], Message = [Started container filler-pod-efefe915-b620-4263-ab4e-82b00ffbcdbb]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e12debec531653], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node conformance-k8s-node-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node conformance-k8s-node-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node conformance-k8s-node-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:10:36.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7394" for this suite.
Dec 17 14:10:42.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:10:43.030: INFO: namespace sched-pred-7394 deletion completed in 6.115004189s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.412 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:10:43.033: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 17 14:10:47.124: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 14:10:47.126: INFO: Pod pod-with-prestop-http-hook still exists
Dec 17 14:10:49.127: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 14:10:49.131: INFO: Pod pod-with-prestop-http-hook still exists
Dec 17 14:10:51.127: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 14:10:51.130: INFO: Pod pod-with-prestop-http-hook still exists
Dec 17 14:10:53.127: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 14:10:53.132: INFO: Pod pod-with-prestop-http-hook still exists
Dec 17 14:10:55.127: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 14:10:55.133: INFO: Pod pod-with-prestop-http-hook still exists
Dec 17 14:10:57.127: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 14:10:57.131: INFO: Pod pod-with-prestop-http-hook still exists
Dec 17 14:10:59.127: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 14:10:59.132: INFO: Pod pod-with-prestop-http-hook still exists
Dec 17 14:11:01.127: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 14:11:01.137: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:11:01.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5305" for this suite.
Dec 17 14:11:23.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:11:23.269: INFO: namespace container-lifecycle-hook-5305 deletion completed in 22.116204653s

• [SLOW TEST:40.237 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:11:23.274: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-07693c09-f4de-481c-8962-e65fdc13a88a
STEP: Creating a pod to test consume configMaps
Dec 17 14:11:23.351: INFO: Waiting up to 5m0s for pod "pod-configmaps-9459b434-bae5-4c5f-b7f8-2628e0af8b56" in namespace "configmap-7838" to be "success or failure"
Dec 17 14:11:23.355: INFO: Pod "pod-configmaps-9459b434-bae5-4c5f-b7f8-2628e0af8b56": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468209ms
Dec 17 14:11:25.359: INFO: Pod "pod-configmaps-9459b434-bae5-4c5f-b7f8-2628e0af8b56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007742776s
Dec 17 14:11:27.369: INFO: Pod "pod-configmaps-9459b434-bae5-4c5f-b7f8-2628e0af8b56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017378473s
STEP: Saw pod success
Dec 17 14:11:27.369: INFO: Pod "pod-configmaps-9459b434-bae5-4c5f-b7f8-2628e0af8b56" satisfied condition "success or failure"
Dec 17 14:11:27.371: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-9459b434-bae5-4c5f-b7f8-2628e0af8b56 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 14:11:27.390: INFO: Waiting for pod pod-configmaps-9459b434-bae5-4c5f-b7f8-2628e0af8b56 to disappear
Dec 17 14:11:27.395: INFO: Pod pod-configmaps-9459b434-bae5-4c5f-b7f8-2628e0af8b56 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:11:27.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7838" for this suite.
Dec 17 14:11:33.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:11:33.509: INFO: namespace configmap-7838 deletion completed in 6.109552004s

• [SLOW TEST:10.236 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:11:33.510: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Dec 17 14:11:33.566: INFO: Waiting up to 5m0s for pod "client-containers-256fcf5f-2d46-4138-9a81-8bd3e703ed23" in namespace "containers-3607" to be "success or failure"
Dec 17 14:11:33.574: INFO: Pod "client-containers-256fcf5f-2d46-4138-9a81-8bd3e703ed23": Phase="Pending", Reason="", readiness=false. Elapsed: 7.899357ms
Dec 17 14:11:35.578: INFO: Pod "client-containers-256fcf5f-2d46-4138-9a81-8bd3e703ed23": Phase="Running", Reason="", readiness=true. Elapsed: 2.012337627s
Dec 17 14:11:37.583: INFO: Pod "client-containers-256fcf5f-2d46-4138-9a81-8bd3e703ed23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017193479s
STEP: Saw pod success
Dec 17 14:11:37.583: INFO: Pod "client-containers-256fcf5f-2d46-4138-9a81-8bd3e703ed23" satisfied condition "success or failure"
Dec 17 14:11:37.587: INFO: Trying to get logs from node conformance-k8s-node-2 pod client-containers-256fcf5f-2d46-4138-9a81-8bd3e703ed23 container test-container: <nil>
STEP: delete the pod
Dec 17 14:11:37.612: INFO: Waiting for pod client-containers-256fcf5f-2d46-4138-9a81-8bd3e703ed23 to disappear
Dec 17 14:11:37.615: INFO: Pod client-containers-256fcf5f-2d46-4138-9a81-8bd3e703ed23 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:11:37.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3607" for this suite.
Dec 17 14:11:43.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:11:43.744: INFO: namespace containers-3607 deletion completed in 6.126310937s

• [SLOW TEST:10.234 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:11:43.745: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:11:43.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aef690ca-aaa0-4ab7-bce7-b5556751b881" in namespace "projected-1341" to be "success or failure"
Dec 17 14:11:43.807: INFO: Pod "downwardapi-volume-aef690ca-aaa0-4ab7-bce7-b5556751b881": Phase="Pending", Reason="", readiness=false. Elapsed: 10.291677ms
Dec 17 14:11:45.811: INFO: Pod "downwardapi-volume-aef690ca-aaa0-4ab7-bce7-b5556751b881": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014752948s
Dec 17 14:11:47.817: INFO: Pod "downwardapi-volume-aef690ca-aaa0-4ab7-bce7-b5556751b881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020166903s
STEP: Saw pod success
Dec 17 14:11:47.817: INFO: Pod "downwardapi-volume-aef690ca-aaa0-4ab7-bce7-b5556751b881" satisfied condition "success or failure"
Dec 17 14:11:47.820: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-aef690ca-aaa0-4ab7-bce7-b5556751b881 container client-container: <nil>
STEP: delete the pod
Dec 17 14:11:47.841: INFO: Waiting for pod downwardapi-volume-aef690ca-aaa0-4ab7-bce7-b5556751b881 to disappear
Dec 17 14:11:47.843: INFO: Pod downwardapi-volume-aef690ca-aaa0-4ab7-bce7-b5556751b881 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:11:47.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1341" for this suite.
Dec 17 14:11:53.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:11:53.976: INFO: namespace projected-1341 deletion completed in 6.128091777s

• [SLOW TEST:10.232 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:11:53.981: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:11:59.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4949" for this suite.
Dec 17 14:12:21.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:12:21.227: INFO: namespace replication-controller-4949 deletion completed in 22.109152989s

• [SLOW TEST:27.246 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:12:21.228: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-fa1db90a-8492-4896-91ab-570cab8d269f
STEP: Creating a pod to test consume secrets
Dec 17 14:12:21.300: INFO: Waiting up to 5m0s for pod "pod-secrets-8c011415-1e9f-4175-86fc-c9affc2de02c" in namespace "secrets-1908" to be "success or failure"
Dec 17 14:12:21.304: INFO: Pod "pod-secrets-8c011415-1e9f-4175-86fc-c9affc2de02c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181153ms
Dec 17 14:12:23.309: INFO: Pod "pod-secrets-8c011415-1e9f-4175-86fc-c9affc2de02c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008830263s
STEP: Saw pod success
Dec 17 14:12:23.309: INFO: Pod "pod-secrets-8c011415-1e9f-4175-86fc-c9affc2de02c" satisfied condition "success or failure"
Dec 17 14:12:23.312: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-secrets-8c011415-1e9f-4175-86fc-c9affc2de02c container secret-env-test: <nil>
STEP: delete the pod
Dec 17 14:12:23.337: INFO: Waiting for pod pod-secrets-8c011415-1e9f-4175-86fc-c9affc2de02c to disappear
Dec 17 14:12:23.340: INFO: Pod pod-secrets-8c011415-1e9f-4175-86fc-c9affc2de02c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:12:23.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1908" for this suite.
Dec 17 14:12:29.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:12:29.461: INFO: namespace secrets-1908 deletion completed in 6.105440855s

• [SLOW TEST:8.233 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:12:29.464: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 17 14:12:29.519: INFO: Waiting up to 5m0s for pod "pod-babf22a8-342c-4563-a3b3-fb48a9d7b289" in namespace "emptydir-1154" to be "success or failure"
Dec 17 14:12:29.523: INFO: Pod "pod-babf22a8-342c-4563-a3b3-fb48a9d7b289": Phase="Pending", Reason="", readiness=false. Elapsed: 3.410809ms
Dec 17 14:12:31.527: INFO: Pod "pod-babf22a8-342c-4563-a3b3-fb48a9d7b289": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008081488s
STEP: Saw pod success
Dec 17 14:12:31.527: INFO: Pod "pod-babf22a8-342c-4563-a3b3-fb48a9d7b289" satisfied condition "success or failure"
Dec 17 14:12:31.530: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-babf22a8-342c-4563-a3b3-fb48a9d7b289 container test-container: <nil>
STEP: delete the pod
Dec 17 14:12:31.555: INFO: Waiting for pod pod-babf22a8-342c-4563-a3b3-fb48a9d7b289 to disappear
Dec 17 14:12:31.559: INFO: Pod pod-babf22a8-342c-4563-a3b3-fb48a9d7b289 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:12:31.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1154" for this suite.
Dec 17 14:12:37.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:12:37.679: INFO: namespace emptydir-1154 deletion completed in 6.115127781s

• [SLOW TEST:8.215 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:12:37.679: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Dec 17 14:12:37.747: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-643038348 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:12:37.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-839" for this suite.
Dec 17 14:12:43.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:12:44.002: INFO: namespace kubectl-839 deletion completed in 6.171375152s

• [SLOW TEST:6.323 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:12:44.005: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8223
I1217 14:12:44.065958      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8223, replica count: 1
I1217 14:12:45.116459      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 14:12:46.117444      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 17 14:12:46.232: INFO: Created: latency-svc-drq2d
Dec 17 14:12:46.242: INFO: Got endpoints: latency-svc-drq2d [23.523433ms]
Dec 17 14:12:46.259: INFO: Created: latency-svc-7vqhc
Dec 17 14:12:46.275: INFO: Created: latency-svc-hsq6w
Dec 17 14:12:46.275: INFO: Got endpoints: latency-svc-7vqhc [32.289873ms]
Dec 17 14:12:46.283: INFO: Got endpoints: latency-svc-hsq6w [39.335393ms]
Dec 17 14:12:46.295: INFO: Created: latency-svc-j5jz6
Dec 17 14:12:46.305: INFO: Got endpoints: latency-svc-j5jz6 [61.704032ms]
Dec 17 14:12:46.308: INFO: Created: latency-svc-rz94w
Dec 17 14:12:46.318: INFO: Got endpoints: latency-svc-rz94w [74.385314ms]
Dec 17 14:12:46.324: INFO: Created: latency-svc-2745b
Dec 17 14:12:46.334: INFO: Created: latency-svc-lv64r
Dec 17 14:12:46.335: INFO: Got endpoints: latency-svc-2745b [91.212662ms]
Dec 17 14:12:46.338: INFO: Got endpoints: latency-svc-lv64r [93.81003ms]
Dec 17 14:12:46.351: INFO: Created: latency-svc-cz6wn
Dec 17 14:12:46.360: INFO: Created: latency-svc-hxrjw
Dec 17 14:12:46.362: INFO: Got endpoints: latency-svc-cz6wn [120.011497ms]
Dec 17 14:12:46.367: INFO: Created: latency-svc-xcxg9
Dec 17 14:12:46.367: INFO: Got endpoints: latency-svc-hxrjw [28.87607ms]
Dec 17 14:12:46.387: INFO: Got endpoints: latency-svc-xcxg9 [142.233186ms]
Dec 17 14:12:46.388: INFO: Created: latency-svc-6p2cv
Dec 17 14:12:46.395: INFO: Created: latency-svc-nnmr8
Dec 17 14:12:46.396: INFO: Got endpoints: latency-svc-6p2cv [150.959585ms]
Dec 17 14:12:46.405: INFO: Created: latency-svc-jj5gx
Dec 17 14:12:46.405: INFO: Got endpoints: latency-svc-nnmr8 [159.797886ms]
Dec 17 14:12:46.411: INFO: Got endpoints: latency-svc-jj5gx [165.452685ms]
Dec 17 14:12:46.413: INFO: Created: latency-svc-nfx64
Dec 17 14:12:46.416: INFO: Got endpoints: latency-svc-nfx64 [170.687974ms]
Dec 17 14:12:46.420: INFO: Created: latency-svc-79q69
Dec 17 14:12:46.430: INFO: Got endpoints: latency-svc-79q69 [185.876016ms]
Dec 17 14:12:46.435: INFO: Created: latency-svc-5h4z4
Dec 17 14:12:46.447: INFO: Got endpoints: latency-svc-5h4z4 [200.937164ms]
Dec 17 14:12:46.447: INFO: Created: latency-svc-7whpq
Dec 17 14:12:46.450: INFO: Got endpoints: latency-svc-7whpq [204.38309ms]
Dec 17 14:12:46.453: INFO: Created: latency-svc-qrt6m
Dec 17 14:12:46.456: INFO: Got endpoints: latency-svc-qrt6m [180.045778ms]
Dec 17 14:12:46.459: INFO: Created: latency-svc-gnvhb
Dec 17 14:12:46.465: INFO: Got endpoints: latency-svc-gnvhb [182.735245ms]
Dec 17 14:12:46.476: INFO: Created: latency-svc-bg94m
Dec 17 14:12:46.481: INFO: Got endpoints: latency-svc-bg94m [175.618242ms]
Dec 17 14:12:46.482: INFO: Created: latency-svc-dm49f
Dec 17 14:12:46.499: INFO: Created: latency-svc-7lp8g
Dec 17 14:12:46.500: INFO: Got endpoints: latency-svc-dm49f [181.227002ms]
Dec 17 14:12:46.506: INFO: Created: latency-svc-8tll8
Dec 17 14:12:46.506: INFO: Got endpoints: latency-svc-7lp8g [170.651552ms]
Dec 17 14:12:46.511: INFO: Got endpoints: latency-svc-8tll8 [148.59199ms]
Dec 17 14:12:46.514: INFO: Created: latency-svc-5chdh
Dec 17 14:12:46.519: INFO: Got endpoints: latency-svc-5chdh [151.990666ms]
Dec 17 14:12:46.525: INFO: Created: latency-svc-qw25n
Dec 17 14:12:46.532: INFO: Created: latency-svc-czqpq
Dec 17 14:12:46.533: INFO: Got endpoints: latency-svc-qw25n [145.596356ms]
Dec 17 14:12:46.542: INFO: Got endpoints: latency-svc-czqpq [146.336979ms]
Dec 17 14:12:46.543: INFO: Created: latency-svc-vzhjp
Dec 17 14:12:46.552: INFO: Got endpoints: latency-svc-vzhjp [146.754034ms]
Dec 17 14:12:46.557: INFO: Created: latency-svc-bhsgr
Dec 17 14:12:46.566: INFO: Got endpoints: latency-svc-bhsgr [155.376148ms]
Dec 17 14:12:46.573: INFO: Created: latency-svc-96xtk
Dec 17 14:12:46.579: INFO: Got endpoints: latency-svc-96xtk [162.795942ms]
Dec 17 14:12:46.589: INFO: Created: latency-svc-l2px8
Dec 17 14:12:46.595: INFO: Got endpoints: latency-svc-l2px8 [164.125405ms]
Dec 17 14:12:46.603: INFO: Created: latency-svc-gqpkx
Dec 17 14:12:46.612: INFO: Created: latency-svc-9s5f4
Dec 17 14:12:46.614: INFO: Got endpoints: latency-svc-gqpkx [167.222338ms]
Dec 17 14:12:46.619: INFO: Got endpoints: latency-svc-9s5f4 [168.68184ms]
Dec 17 14:12:46.620: INFO: Created: latency-svc-54z24
Dec 17 14:12:46.627: INFO: Got endpoints: latency-svc-54z24 [171.645045ms]
Dec 17 14:12:46.632: INFO: Created: latency-svc-rtvdv
Dec 17 14:12:46.641: INFO: Got endpoints: latency-svc-rtvdv [174.667566ms]
Dec 17 14:12:46.643: INFO: Created: latency-svc-l6584
Dec 17 14:12:46.650: INFO: Got endpoints: latency-svc-l6584 [169.320914ms]
Dec 17 14:12:46.651: INFO: Created: latency-svc-zpr6r
Dec 17 14:12:46.664: INFO: Got endpoints: latency-svc-zpr6r [164.643536ms]
Dec 17 14:12:46.665: INFO: Created: latency-svc-h85v8
Dec 17 14:12:46.670: INFO: Created: latency-svc-d7pgq
Dec 17 14:12:46.673: INFO: Got endpoints: latency-svc-h85v8 [166.917361ms]
Dec 17 14:12:46.674: INFO: Got endpoints: latency-svc-d7pgq [163.34838ms]
Dec 17 14:12:46.676: INFO: Created: latency-svc-6bb94
Dec 17 14:12:46.693: INFO: Created: latency-svc-nzcts
Dec 17 14:12:46.706: INFO: Created: latency-svc-dhl7s
Dec 17 14:12:46.709: INFO: Got endpoints: latency-svc-6bb94 [189.3798ms]
Dec 17 14:12:46.716: INFO: Created: latency-svc-5t9wd
Dec 17 14:12:46.726: INFO: Created: latency-svc-j2drm
Dec 17 14:12:46.733: INFO: Created: latency-svc-qpvv8
Dec 17 14:12:46.741: INFO: Created: latency-svc-lsm8w
Dec 17 14:12:46.747: INFO: Got endpoints: latency-svc-nzcts [214.380632ms]
Dec 17 14:12:46.747: INFO: Created: latency-svc-6scm7
Dec 17 14:12:46.757: INFO: Created: latency-svc-46bqw
Dec 17 14:12:46.766: INFO: Created: latency-svc-6hdc6
Dec 17 14:12:46.772: INFO: Created: latency-svc-jfzb9
Dec 17 14:12:46.781: INFO: Created: latency-svc-v4rlx
Dec 17 14:12:46.791: INFO: Created: latency-svc-gjpdk
Dec 17 14:12:46.792: INFO: Got endpoints: latency-svc-dhl7s [249.559793ms]
Dec 17 14:12:46.805: INFO: Created: latency-svc-lp8n4
Dec 17 14:12:46.808: INFO: Created: latency-svc-lchft
Dec 17 14:12:46.826: INFO: Created: latency-svc-4dt6n
Dec 17 14:12:46.830: INFO: Created: latency-svc-89nr5
Dec 17 14:12:46.839: INFO: Got endpoints: latency-svc-5t9wd [287.041987ms]
Dec 17 14:12:46.841: INFO: Created: latency-svc-2ftnd
Dec 17 14:12:46.851: INFO: Created: latency-svc-qj5xk
Dec 17 14:12:46.889: INFO: Got endpoints: latency-svc-j2drm [322.84054ms]
Dec 17 14:12:46.901: INFO: Created: latency-svc-fmm9m
Dec 17 14:12:46.940: INFO: Got endpoints: latency-svc-qpvv8 [360.213189ms]
Dec 17 14:12:46.952: INFO: Created: latency-svc-4jj2t
Dec 17 14:12:46.991: INFO: Got endpoints: latency-svc-lsm8w [396.446817ms]
Dec 17 14:12:47.000: INFO: Created: latency-svc-86qr7
Dec 17 14:12:47.046: INFO: Got endpoints: latency-svc-6scm7 [431.97841ms]
Dec 17 14:12:47.056: INFO: Created: latency-svc-z2hzc
Dec 17 14:12:47.089: INFO: Got endpoints: latency-svc-46bqw [470.03539ms]
Dec 17 14:12:47.104: INFO: Created: latency-svc-l6zbw
Dec 17 14:12:47.146: INFO: Got endpoints: latency-svc-6hdc6 [518.826911ms]
Dec 17 14:12:47.158: INFO: Created: latency-svc-ptb7j
Dec 17 14:12:47.188: INFO: Got endpoints: latency-svc-jfzb9 [546.97594ms]
Dec 17 14:12:47.200: INFO: Created: latency-svc-xxlv7
Dec 17 14:12:47.239: INFO: Got endpoints: latency-svc-v4rlx [588.651931ms]
Dec 17 14:12:47.257: INFO: Created: latency-svc-z87th
Dec 17 14:12:47.290: INFO: Got endpoints: latency-svc-gjpdk [626.087469ms]
Dec 17 14:12:47.302: INFO: Created: latency-svc-8tgwx
Dec 17 14:12:47.343: INFO: Got endpoints: latency-svc-lp8n4 [669.472291ms]
Dec 17 14:12:47.355: INFO: Created: latency-svc-cv7ld
Dec 17 14:12:47.389: INFO: Got endpoints: latency-svc-lchft [715.216289ms]
Dec 17 14:12:47.401: INFO: Created: latency-svc-6szb8
Dec 17 14:12:47.446: INFO: Got endpoints: latency-svc-4dt6n [737.213799ms]
Dec 17 14:12:47.461: INFO: Created: latency-svc-lsm7r
Dec 17 14:12:47.496: INFO: Got endpoints: latency-svc-89nr5 [748.867009ms]
Dec 17 14:12:47.513: INFO: Created: latency-svc-khkff
Dec 17 14:12:47.538: INFO: Got endpoints: latency-svc-2ftnd [746.066538ms]
Dec 17 14:12:47.545: INFO: Created: latency-svc-jctjx
Dec 17 14:12:47.593: INFO: Got endpoints: latency-svc-qj5xk [754.474006ms]
Dec 17 14:12:47.605: INFO: Created: latency-svc-h2qx9
Dec 17 14:12:47.642: INFO: Got endpoints: latency-svc-fmm9m [752.419014ms]
Dec 17 14:12:47.655: INFO: Created: latency-svc-58ss4
Dec 17 14:12:47.691: INFO: Got endpoints: latency-svc-4jj2t [751.261256ms]
Dec 17 14:12:47.702: INFO: Created: latency-svc-4927x
Dec 17 14:12:47.739: INFO: Got endpoints: latency-svc-86qr7 [747.368548ms]
Dec 17 14:12:47.752: INFO: Created: latency-svc-nfntt
Dec 17 14:12:47.791: INFO: Got endpoints: latency-svc-z2hzc [744.618652ms]
Dec 17 14:12:47.809: INFO: Created: latency-svc-cc4ff
Dec 17 14:12:47.841: INFO: Got endpoints: latency-svc-l6zbw [751.097019ms]
Dec 17 14:12:47.850: INFO: Created: latency-svc-pnllz
Dec 17 14:12:47.890: INFO: Got endpoints: latency-svc-ptb7j [744.186081ms]
Dec 17 14:12:47.910: INFO: Created: latency-svc-fkxmz
Dec 17 14:12:47.939: INFO: Got endpoints: latency-svc-xxlv7 [750.22227ms]
Dec 17 14:12:47.955: INFO: Created: latency-svc-27p9j
Dec 17 14:12:47.989: INFO: Got endpoints: latency-svc-z87th [749.657236ms]
Dec 17 14:12:48.001: INFO: Created: latency-svc-k725f
Dec 17 14:12:48.039: INFO: Got endpoints: latency-svc-8tgwx [748.118237ms]
Dec 17 14:12:48.053: INFO: Created: latency-svc-jplbx
Dec 17 14:12:48.091: INFO: Got endpoints: latency-svc-cv7ld [748.043263ms]
Dec 17 14:12:48.100: INFO: Created: latency-svc-xq9l5
Dec 17 14:12:48.140: INFO: Got endpoints: latency-svc-6szb8 [750.558081ms]
Dec 17 14:12:48.154: INFO: Created: latency-svc-mvc9d
Dec 17 14:12:48.189: INFO: Got endpoints: latency-svc-lsm7r [743.08458ms]
Dec 17 14:12:48.202: INFO: Created: latency-svc-bfwkm
Dec 17 14:12:48.239: INFO: Got endpoints: latency-svc-khkff [742.720981ms]
Dec 17 14:12:48.252: INFO: Created: latency-svc-d2xwn
Dec 17 14:12:48.292: INFO: Got endpoints: latency-svc-jctjx [753.97327ms]
Dec 17 14:12:48.307: INFO: Created: latency-svc-qfjht
Dec 17 14:12:48.343: INFO: Got endpoints: latency-svc-h2qx9 [749.049608ms]
Dec 17 14:12:48.351: INFO: Created: latency-svc-rszcz
Dec 17 14:12:48.390: INFO: Got endpoints: latency-svc-58ss4 [748.0061ms]
Dec 17 14:12:48.401: INFO: Created: latency-svc-k2vvl
Dec 17 14:12:48.440: INFO: Got endpoints: latency-svc-4927x [749.031204ms]
Dec 17 14:12:48.473: INFO: Created: latency-svc-jw76d
Dec 17 14:12:48.491: INFO: Got endpoints: latency-svc-nfntt [750.286392ms]
Dec 17 14:12:48.509: INFO: Created: latency-svc-2xgjl
Dec 17 14:12:48.539: INFO: Got endpoints: latency-svc-cc4ff [747.715677ms]
Dec 17 14:12:48.558: INFO: Created: latency-svc-476zs
Dec 17 14:12:48.595: INFO: Got endpoints: latency-svc-pnllz [752.868055ms]
Dec 17 14:12:48.605: INFO: Created: latency-svc-gzgns
Dec 17 14:12:48.642: INFO: Got endpoints: latency-svc-fkxmz [750.621423ms]
Dec 17 14:12:48.651: INFO: Created: latency-svc-wxct9
Dec 17 14:12:48.690: INFO: Got endpoints: latency-svc-27p9j [751.394244ms]
Dec 17 14:12:48.698: INFO: Created: latency-svc-c4jvt
Dec 17 14:12:48.743: INFO: Got endpoints: latency-svc-k725f [753.428205ms]
Dec 17 14:12:48.766: INFO: Created: latency-svc-qcb2n
Dec 17 14:12:48.799: INFO: Got endpoints: latency-svc-jplbx [760.097349ms]
Dec 17 14:12:48.815: INFO: Created: latency-svc-8h74z
Dec 17 14:12:48.839: INFO: Got endpoints: latency-svc-xq9l5 [747.610082ms]
Dec 17 14:12:48.856: INFO: Created: latency-svc-xnpc5
Dec 17 14:12:48.895: INFO: Got endpoints: latency-svc-mvc9d [754.302562ms]
Dec 17 14:12:48.909: INFO: Created: latency-svc-wpksd
Dec 17 14:12:48.940: INFO: Got endpoints: latency-svc-bfwkm [750.480159ms]
Dec 17 14:12:48.949: INFO: Created: latency-svc-lvtqc
Dec 17 14:12:48.990: INFO: Got endpoints: latency-svc-d2xwn [750.110223ms]
Dec 17 14:12:49.008: INFO: Created: latency-svc-5l95d
Dec 17 14:12:49.047: INFO: Got endpoints: latency-svc-qfjht [754.469369ms]
Dec 17 14:12:49.063: INFO: Created: latency-svc-8nnxg
Dec 17 14:12:49.093: INFO: Got endpoints: latency-svc-rszcz [749.977413ms]
Dec 17 14:12:49.130: INFO: Created: latency-svc-xgrkp
Dec 17 14:12:49.143: INFO: Got endpoints: latency-svc-k2vvl [752.856215ms]
Dec 17 14:12:49.158: INFO: Created: latency-svc-wh74z
Dec 17 14:12:49.193: INFO: Got endpoints: latency-svc-jw76d [753.072185ms]
Dec 17 14:12:49.204: INFO: Created: latency-svc-cklcf
Dec 17 14:12:49.245: INFO: Got endpoints: latency-svc-2xgjl [753.670573ms]
Dec 17 14:12:49.259: INFO: Created: latency-svc-zbzp8
Dec 17 14:12:49.293: INFO: Got endpoints: latency-svc-476zs [753.910665ms]
Dec 17 14:12:49.309: INFO: Created: latency-svc-tngzg
Dec 17 14:12:49.344: INFO: Got endpoints: latency-svc-gzgns [748.706844ms]
Dec 17 14:12:49.357: INFO: Created: latency-svc-jjh72
Dec 17 14:12:49.394: INFO: Got endpoints: latency-svc-wxct9 [751.814321ms]
Dec 17 14:12:49.409: INFO: Created: latency-svc-xw5tp
Dec 17 14:12:49.440: INFO: Got endpoints: latency-svc-c4jvt [749.319297ms]
Dec 17 14:12:49.455: INFO: Created: latency-svc-jst45
Dec 17 14:12:49.489: INFO: Got endpoints: latency-svc-qcb2n [746.040787ms]
Dec 17 14:12:49.498: INFO: Created: latency-svc-9cwmq
Dec 17 14:12:49.541: INFO: Got endpoints: latency-svc-8h74z [742.344171ms]
Dec 17 14:12:49.550: INFO: Created: latency-svc-kvlxt
Dec 17 14:12:49.592: INFO: Got endpoints: latency-svc-xnpc5 [752.284809ms]
Dec 17 14:12:49.621: INFO: Created: latency-svc-xwjvm
Dec 17 14:12:49.644: INFO: Got endpoints: latency-svc-wpksd [749.155277ms]
Dec 17 14:12:49.669: INFO: Created: latency-svc-rvmc6
Dec 17 14:12:49.699: INFO: Got endpoints: latency-svc-lvtqc [759.142477ms]
Dec 17 14:12:49.729: INFO: Created: latency-svc-ljlxb
Dec 17 14:12:49.743: INFO: Got endpoints: latency-svc-5l95d [752.91182ms]
Dec 17 14:12:49.761: INFO: Created: latency-svc-vts8r
Dec 17 14:12:49.789: INFO: Got endpoints: latency-svc-8nnxg [742.245585ms]
Dec 17 14:12:49.804: INFO: Created: latency-svc-6qsn6
Dec 17 14:12:49.841: INFO: Got endpoints: latency-svc-xgrkp [748.130921ms]
Dec 17 14:12:49.867: INFO: Created: latency-svc-94jrl
Dec 17 14:12:49.889: INFO: Got endpoints: latency-svc-wh74z [746.389291ms]
Dec 17 14:12:49.898: INFO: Created: latency-svc-47mp7
Dec 17 14:12:49.940: INFO: Got endpoints: latency-svc-cklcf [746.078487ms]
Dec 17 14:12:49.953: INFO: Created: latency-svc-rj8m4
Dec 17 14:12:49.993: INFO: Got endpoints: latency-svc-zbzp8 [747.889732ms]
Dec 17 14:12:50.006: INFO: Created: latency-svc-tq7rk
Dec 17 14:12:50.041: INFO: Got endpoints: latency-svc-tngzg [747.470682ms]
Dec 17 14:12:50.053: INFO: Created: latency-svc-ldjt5
Dec 17 14:12:50.090: INFO: Got endpoints: latency-svc-jjh72 [746.528897ms]
Dec 17 14:12:50.103: INFO: Created: latency-svc-znn57
Dec 17 14:12:50.142: INFO: Got endpoints: latency-svc-xw5tp [746.870783ms]
Dec 17 14:12:50.154: INFO: Created: latency-svc-rpnd7
Dec 17 14:12:50.190: INFO: Got endpoints: latency-svc-jst45 [750.283627ms]
Dec 17 14:12:50.199: INFO: Created: latency-svc-dbn22
Dec 17 14:12:50.241: INFO: Got endpoints: latency-svc-9cwmq [751.683321ms]
Dec 17 14:12:50.251: INFO: Created: latency-svc-srvff
Dec 17 14:12:50.298: INFO: Got endpoints: latency-svc-kvlxt [756.705411ms]
Dec 17 14:12:50.309: INFO: Created: latency-svc-s5v4r
Dec 17 14:12:50.342: INFO: Got endpoints: latency-svc-xwjvm [749.287908ms]
Dec 17 14:12:50.355: INFO: Created: latency-svc-hrrsj
Dec 17 14:12:50.389: INFO: Got endpoints: latency-svc-rvmc6 [743.984002ms]
Dec 17 14:12:50.408: INFO: Created: latency-svc-7kwbt
Dec 17 14:12:50.438: INFO: Got endpoints: latency-svc-ljlxb [739.172128ms]
Dec 17 14:12:50.451: INFO: Created: latency-svc-s4zzj
Dec 17 14:12:50.489: INFO: Got endpoints: latency-svc-vts8r [746.324488ms]
Dec 17 14:12:50.502: INFO: Created: latency-svc-4h2zq
Dec 17 14:12:50.542: INFO: Got endpoints: latency-svc-6qsn6 [752.290701ms]
Dec 17 14:12:50.552: INFO: Created: latency-svc-wmvd8
Dec 17 14:12:50.590: INFO: Got endpoints: latency-svc-94jrl [749.16745ms]
Dec 17 14:12:50.607: INFO: Created: latency-svc-htfbz
Dec 17 14:12:50.641: INFO: Got endpoints: latency-svc-47mp7 [751.376043ms]
Dec 17 14:12:50.652: INFO: Created: latency-svc-97bjk
Dec 17 14:12:50.689: INFO: Got endpoints: latency-svc-rj8m4 [749.701943ms]
Dec 17 14:12:50.701: INFO: Created: latency-svc-z2lf6
Dec 17 14:12:50.739: INFO: Got endpoints: latency-svc-tq7rk [746.447797ms]
Dec 17 14:12:50.751: INFO: Created: latency-svc-h79mk
Dec 17 14:12:50.793: INFO: Got endpoints: latency-svc-ldjt5 [751.938345ms]
Dec 17 14:12:50.809: INFO: Created: latency-svc-6fk4s
Dec 17 14:12:50.843: INFO: Got endpoints: latency-svc-znn57 [752.573648ms]
Dec 17 14:12:50.856: INFO: Created: latency-svc-29p7c
Dec 17 14:12:50.893: INFO: Got endpoints: latency-svc-rpnd7 [751.48372ms]
Dec 17 14:12:50.902: INFO: Created: latency-svc-t2kkj
Dec 17 14:12:50.947: INFO: Got endpoints: latency-svc-dbn22 [756.973375ms]
Dec 17 14:12:50.959: INFO: Created: latency-svc-hdwqv
Dec 17 14:12:50.989: INFO: Got endpoints: latency-svc-srvff [747.695706ms]
Dec 17 14:12:51.000: INFO: Created: latency-svc-chq4z
Dec 17 14:12:51.039: INFO: Got endpoints: latency-svc-s5v4r [740.367002ms]
Dec 17 14:12:51.055: INFO: Created: latency-svc-xn4mn
Dec 17 14:12:51.096: INFO: Got endpoints: latency-svc-hrrsj [754.164066ms]
Dec 17 14:12:51.109: INFO: Created: latency-svc-vdtdn
Dec 17 14:12:51.141: INFO: Got endpoints: latency-svc-7kwbt [751.66727ms]
Dec 17 14:12:51.153: INFO: Created: latency-svc-dd7pf
Dec 17 14:12:51.190: INFO: Got endpoints: latency-svc-s4zzj [751.575729ms]
Dec 17 14:12:51.216: INFO: Created: latency-svc-jp4hn
Dec 17 14:12:51.239: INFO: Got endpoints: latency-svc-4h2zq [749.700711ms]
Dec 17 14:12:51.247: INFO: Created: latency-svc-8ppq4
Dec 17 14:12:51.291: INFO: Got endpoints: latency-svc-wmvd8 [749.3383ms]
Dec 17 14:12:51.303: INFO: Created: latency-svc-4s87b
Dec 17 14:12:51.341: INFO: Got endpoints: latency-svc-htfbz [750.359983ms]
Dec 17 14:12:51.355: INFO: Created: latency-svc-dmf96
Dec 17 14:12:51.389: INFO: Got endpoints: latency-svc-97bjk [748.097873ms]
Dec 17 14:12:51.404: INFO: Created: latency-svc-wlcc5
Dec 17 14:12:51.443: INFO: Got endpoints: latency-svc-z2lf6 [753.455577ms]
Dec 17 14:12:51.457: INFO: Created: latency-svc-b5xn5
Dec 17 14:12:51.490: INFO: Got endpoints: latency-svc-h79mk [750.568414ms]
Dec 17 14:12:51.503: INFO: Created: latency-svc-9fqnj
Dec 17 14:12:51.540: INFO: Got endpoints: latency-svc-6fk4s [746.99349ms]
Dec 17 14:12:51.549: INFO: Created: latency-svc-7hjhz
Dec 17 14:12:51.596: INFO: Got endpoints: latency-svc-29p7c [752.431511ms]
Dec 17 14:12:51.607: INFO: Created: latency-svc-682sd
Dec 17 14:12:51.639: INFO: Got endpoints: latency-svc-t2kkj [745.129929ms]
Dec 17 14:12:51.652: INFO: Created: latency-svc-dnp7g
Dec 17 14:12:51.690: INFO: Got endpoints: latency-svc-hdwqv [742.184094ms]
Dec 17 14:12:51.710: INFO: Created: latency-svc-9wj28
Dec 17 14:12:51.741: INFO: Got endpoints: latency-svc-chq4z [752.473933ms]
Dec 17 14:12:51.755: INFO: Created: latency-svc-99sws
Dec 17 14:12:51.790: INFO: Got endpoints: latency-svc-xn4mn [751.106243ms]
Dec 17 14:12:51.803: INFO: Created: latency-svc-rxj97
Dec 17 14:12:51.865: INFO: Got endpoints: latency-svc-vdtdn [768.554506ms]
Dec 17 14:12:51.891: INFO: Created: latency-svc-92hw5
Dec 17 14:12:51.892: INFO: Got endpoints: latency-svc-dd7pf [750.340167ms]
Dec 17 14:12:51.902: INFO: Created: latency-svc-vh95l
Dec 17 14:12:51.945: INFO: Got endpoints: latency-svc-jp4hn [754.938669ms]
Dec 17 14:12:51.954: INFO: Created: latency-svc-2nmj4
Dec 17 14:12:51.990: INFO: Got endpoints: latency-svc-8ppq4 [751.161507ms]
Dec 17 14:12:52.002: INFO: Created: latency-svc-5ntr9
Dec 17 14:12:52.039: INFO: Got endpoints: latency-svc-4s87b [747.822975ms]
Dec 17 14:12:52.056: INFO: Created: latency-svc-gtkvl
Dec 17 14:12:52.091: INFO: Got endpoints: latency-svc-dmf96 [749.211772ms]
Dec 17 14:12:52.110: INFO: Created: latency-svc-scc2d
Dec 17 14:12:52.138: INFO: Got endpoints: latency-svc-wlcc5 [748.735767ms]
Dec 17 14:12:52.149: INFO: Created: latency-svc-9bgfm
Dec 17 14:12:52.189: INFO: Got endpoints: latency-svc-b5xn5 [746.069607ms]
Dec 17 14:12:52.207: INFO: Created: latency-svc-b7nds
Dec 17 14:12:52.242: INFO: Got endpoints: latency-svc-9fqnj [751.766403ms]
Dec 17 14:12:52.255: INFO: Created: latency-svc-6mzjt
Dec 17 14:12:52.293: INFO: Got endpoints: latency-svc-7hjhz [752.696406ms]
Dec 17 14:12:52.300: INFO: Created: latency-svc-7sszz
Dec 17 14:12:52.340: INFO: Got endpoints: latency-svc-682sd [744.067079ms]
Dec 17 14:12:52.355: INFO: Created: latency-svc-csvvr
Dec 17 14:12:52.390: INFO: Got endpoints: latency-svc-dnp7g [750.470556ms]
Dec 17 14:12:52.405: INFO: Created: latency-svc-wfpn6
Dec 17 14:12:52.443: INFO: Got endpoints: latency-svc-9wj28 [752.574564ms]
Dec 17 14:12:52.471: INFO: Created: latency-svc-pxmlb
Dec 17 14:12:52.490: INFO: Got endpoints: latency-svc-99sws [748.654333ms]
Dec 17 14:12:52.502: INFO: Created: latency-svc-lm84f
Dec 17 14:12:52.539: INFO: Got endpoints: latency-svc-rxj97 [749.043217ms]
Dec 17 14:12:52.551: INFO: Created: latency-svc-2sp9d
Dec 17 14:12:52.589: INFO: Got endpoints: latency-svc-92hw5 [720.341342ms]
Dec 17 14:12:52.606: INFO: Created: latency-svc-tkxc8
Dec 17 14:12:52.640: INFO: Got endpoints: latency-svc-vh95l [748.233609ms]
Dec 17 14:12:52.655: INFO: Created: latency-svc-wwl4x
Dec 17 14:12:52.691: INFO: Got endpoints: latency-svc-2nmj4 [745.901633ms]
Dec 17 14:12:52.700: INFO: Created: latency-svc-srpd2
Dec 17 14:12:52.741: INFO: Got endpoints: latency-svc-5ntr9 [749.677122ms]
Dec 17 14:12:52.749: INFO: Created: latency-svc-zp8sc
Dec 17 14:12:52.790: INFO: Got endpoints: latency-svc-gtkvl [750.766282ms]
Dec 17 14:12:52.808: INFO: Created: latency-svc-p6smn
Dec 17 14:12:52.841: INFO: Got endpoints: latency-svc-scc2d [750.198933ms]
Dec 17 14:12:52.853: INFO: Created: latency-svc-9frbl
Dec 17 14:12:52.890: INFO: Got endpoints: latency-svc-9bgfm [751.906607ms]
Dec 17 14:12:52.907: INFO: Created: latency-svc-kq9vv
Dec 17 14:12:52.940: INFO: Got endpoints: latency-svc-b7nds [751.244333ms]
Dec 17 14:12:52.955: INFO: Created: latency-svc-7j9ws
Dec 17 14:12:52.999: INFO: Got endpoints: latency-svc-6mzjt [756.600176ms]
Dec 17 14:12:53.013: INFO: Created: latency-svc-p89w6
Dec 17 14:12:53.039: INFO: Got endpoints: latency-svc-7sszz [745.199364ms]
Dec 17 14:12:53.048: INFO: Created: latency-svc-g65m6
Dec 17 14:12:53.088: INFO: Got endpoints: latency-svc-csvvr [747.130258ms]
Dec 17 14:12:53.096: INFO: Created: latency-svc-q2n4t
Dec 17 14:12:53.139: INFO: Got endpoints: latency-svc-wfpn6 [748.944696ms]
Dec 17 14:12:53.150: INFO: Created: latency-svc-45v4s
Dec 17 14:12:53.188: INFO: Got endpoints: latency-svc-pxmlb [745.194921ms]
Dec 17 14:12:53.201: INFO: Created: latency-svc-bs4rj
Dec 17 14:12:53.243: INFO: Got endpoints: latency-svc-lm84f [752.471865ms]
Dec 17 14:12:53.254: INFO: Created: latency-svc-mwnrn
Dec 17 14:12:53.289: INFO: Got endpoints: latency-svc-2sp9d [749.233807ms]
Dec 17 14:12:53.299: INFO: Created: latency-svc-fbf2q
Dec 17 14:12:53.339: INFO: Got endpoints: latency-svc-tkxc8 [748.514117ms]
Dec 17 14:12:53.351: INFO: Created: latency-svc-wn58z
Dec 17 14:12:53.391: INFO: Got endpoints: latency-svc-wwl4x [750.623854ms]
Dec 17 14:12:53.399: INFO: Created: latency-svc-l9dmd
Dec 17 14:12:53.443: INFO: Got endpoints: latency-svc-srpd2 [752.009467ms]
Dec 17 14:12:53.451: INFO: Created: latency-svc-ljsbz
Dec 17 14:12:53.489: INFO: Got endpoints: latency-svc-zp8sc [748.173853ms]
Dec 17 14:12:53.503: INFO: Created: latency-svc-zm7t7
Dec 17 14:12:53.542: INFO: Got endpoints: latency-svc-p6smn [751.928718ms]
Dec 17 14:12:53.554: INFO: Created: latency-svc-pblwn
Dec 17 14:12:53.599: INFO: Got endpoints: latency-svc-9frbl [756.752257ms]
Dec 17 14:12:53.611: INFO: Created: latency-svc-fm4b8
Dec 17 14:12:53.640: INFO: Got endpoints: latency-svc-kq9vv [750.268366ms]
Dec 17 14:12:53.651: INFO: Created: latency-svc-s948k
Dec 17 14:12:53.690: INFO: Got endpoints: latency-svc-7j9ws [749.81618ms]
Dec 17 14:12:53.703: INFO: Created: latency-svc-fl62f
Dec 17 14:12:53.742: INFO: Got endpoints: latency-svc-p89w6 [742.868343ms]
Dec 17 14:12:53.759: INFO: Created: latency-svc-gdst4
Dec 17 14:12:53.790: INFO: Got endpoints: latency-svc-g65m6 [751.525933ms]
Dec 17 14:12:53.802: INFO: Created: latency-svc-rc68w
Dec 17 14:12:53.839: INFO: Got endpoints: latency-svc-q2n4t [750.413934ms]
Dec 17 14:12:53.851: INFO: Created: latency-svc-flp24
Dec 17 14:12:53.890: INFO: Got endpoints: latency-svc-45v4s [750.532228ms]
Dec 17 14:12:53.901: INFO: Created: latency-svc-lzmmf
Dec 17 14:12:53.939: INFO: Got endpoints: latency-svc-bs4rj [750.906944ms]
Dec 17 14:12:53.952: INFO: Created: latency-svc-9l2zw
Dec 17 14:12:53.991: INFO: Got endpoints: latency-svc-mwnrn [747.846178ms]
Dec 17 14:12:54.015: INFO: Created: latency-svc-xhj7j
Dec 17 14:12:54.041: INFO: Got endpoints: latency-svc-fbf2q [751.669854ms]
Dec 17 14:12:54.048: INFO: Created: latency-svc-p7kzr
Dec 17 14:12:54.091: INFO: Got endpoints: latency-svc-wn58z [752.055377ms]
Dec 17 14:12:54.139: INFO: Got endpoints: latency-svc-l9dmd [748.202094ms]
Dec 17 14:12:54.189: INFO: Got endpoints: latency-svc-ljsbz [745.76369ms]
Dec 17 14:12:54.240: INFO: Got endpoints: latency-svc-zm7t7 [750.365811ms]
Dec 17 14:12:54.294: INFO: Got endpoints: latency-svc-pblwn [751.717757ms]
Dec 17 14:12:54.342: INFO: Got endpoints: latency-svc-fm4b8 [742.474993ms]
Dec 17 14:12:54.391: INFO: Got endpoints: latency-svc-s948k [750.380334ms]
Dec 17 14:12:54.447: INFO: Got endpoints: latency-svc-fl62f [756.666663ms]
Dec 17 14:12:54.489: INFO: Got endpoints: latency-svc-gdst4 [747.584712ms]
Dec 17 14:12:54.539: INFO: Got endpoints: latency-svc-rc68w [747.8288ms]
Dec 17 14:12:54.589: INFO: Got endpoints: latency-svc-flp24 [749.547228ms]
Dec 17 14:12:54.639: INFO: Got endpoints: latency-svc-lzmmf [749.343024ms]
Dec 17 14:12:54.691: INFO: Got endpoints: latency-svc-9l2zw [751.573059ms]
Dec 17 14:12:54.740: INFO: Got endpoints: latency-svc-xhj7j [749.263118ms]
Dec 17 14:12:54.791: INFO: Got endpoints: latency-svc-p7kzr [750.004936ms]
Dec 17 14:12:54.791: INFO: Latencies: [28.87607ms 32.289873ms 39.335393ms 61.704032ms 74.385314ms 91.212662ms 93.81003ms 120.011497ms 142.233186ms 145.596356ms 146.336979ms 146.754034ms 148.59199ms 150.959585ms 151.990666ms 155.376148ms 159.797886ms 162.795942ms 163.34838ms 164.125405ms 164.643536ms 165.452685ms 166.917361ms 167.222338ms 168.68184ms 169.320914ms 170.651552ms 170.687974ms 171.645045ms 174.667566ms 175.618242ms 180.045778ms 181.227002ms 182.735245ms 185.876016ms 189.3798ms 200.937164ms 204.38309ms 214.380632ms 249.559793ms 287.041987ms 322.84054ms 360.213189ms 396.446817ms 431.97841ms 470.03539ms 518.826911ms 546.97594ms 588.651931ms 626.087469ms 669.472291ms 715.216289ms 720.341342ms 737.213799ms 739.172128ms 740.367002ms 742.184094ms 742.245585ms 742.344171ms 742.474993ms 742.720981ms 742.868343ms 743.08458ms 743.984002ms 744.067079ms 744.186081ms 744.618652ms 745.129929ms 745.194921ms 745.199364ms 745.76369ms 745.901633ms 746.040787ms 746.066538ms 746.069607ms 746.078487ms 746.324488ms 746.389291ms 746.447797ms 746.528897ms 746.870783ms 746.99349ms 747.130258ms 747.368548ms 747.470682ms 747.584712ms 747.610082ms 747.695706ms 747.715677ms 747.822975ms 747.8288ms 747.846178ms 747.889732ms 748.0061ms 748.043263ms 748.097873ms 748.118237ms 748.130921ms 748.173853ms 748.202094ms 748.233609ms 748.514117ms 748.654333ms 748.706844ms 748.735767ms 748.867009ms 748.944696ms 749.031204ms 749.043217ms 749.049608ms 749.155277ms 749.16745ms 749.211772ms 749.233807ms 749.263118ms 749.287908ms 749.319297ms 749.3383ms 749.343024ms 749.547228ms 749.657236ms 749.677122ms 749.700711ms 749.701943ms 749.81618ms 749.977413ms 750.004936ms 750.110223ms 750.198933ms 750.22227ms 750.268366ms 750.283627ms 750.286392ms 750.340167ms 750.359983ms 750.365811ms 750.380334ms 750.413934ms 750.470556ms 750.480159ms 750.532228ms 750.558081ms 750.568414ms 750.621423ms 750.623854ms 750.766282ms 750.906944ms 751.097019ms 751.106243ms 751.161507ms 751.244333ms 751.261256ms 751.376043ms 751.394244ms 751.48372ms 751.525933ms 751.573059ms 751.575729ms 751.66727ms 751.669854ms 751.683321ms 751.717757ms 751.766403ms 751.814321ms 751.906607ms 751.928718ms 751.938345ms 752.009467ms 752.055377ms 752.284809ms 752.290701ms 752.419014ms 752.431511ms 752.471865ms 752.473933ms 752.573648ms 752.574564ms 752.696406ms 752.856215ms 752.868055ms 752.91182ms 753.072185ms 753.428205ms 753.455577ms 753.670573ms 753.910665ms 753.97327ms 754.164066ms 754.302562ms 754.469369ms 754.474006ms 754.938669ms 756.600176ms 756.666663ms 756.705411ms 756.752257ms 756.973375ms 759.142477ms 760.097349ms 768.554506ms]
Dec 17 14:12:54.791: INFO: 50 %ile: 748.233609ms
Dec 17 14:12:54.791: INFO: 90 %ile: 752.91182ms
Dec 17 14:12:54.791: INFO: 99 %ile: 760.097349ms
Dec 17 14:12:54.791: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:12:54.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8223" for this suite.
Dec 17 14:13:12.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:13:12.923: INFO: namespace svc-latency-8223 deletion completed in 18.12550906s

• [SLOW TEST:28.918 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:13:12.923: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:13:12.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-031bc277-6491-4476-968d-ae78aea80437" in namespace "downward-api-9120" to be "success or failure"
Dec 17 14:13:12.977: INFO: Pod "downwardapi-volume-031bc277-6491-4476-968d-ae78aea80437": Phase="Pending", Reason="", readiness=false. Elapsed: 7.915415ms
Dec 17 14:13:14.982: INFO: Pod "downwardapi-volume-031bc277-6491-4476-968d-ae78aea80437": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012916252s
Dec 17 14:13:16.986: INFO: Pod "downwardapi-volume-031bc277-6491-4476-968d-ae78aea80437": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017165219s
STEP: Saw pod success
Dec 17 14:13:16.986: INFO: Pod "downwardapi-volume-031bc277-6491-4476-968d-ae78aea80437" satisfied condition "success or failure"
Dec 17 14:13:16.989: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-031bc277-6491-4476-968d-ae78aea80437 container client-container: <nil>
STEP: delete the pod
Dec 17 14:13:17.006: INFO: Waiting for pod downwardapi-volume-031bc277-6491-4476-968d-ae78aea80437 to disappear
Dec 17 14:13:17.019: INFO: Pod downwardapi-volume-031bc277-6491-4476-968d-ae78aea80437 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:13:17.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9120" for this suite.
Dec 17 14:13:23.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:13:23.123: INFO: namespace downward-api-9120 deletion completed in 6.097286055s

• [SLOW TEST:10.200 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:13:23.124: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 17 14:13:23.192: INFO: Waiting up to 5m0s for pod "downward-api-3c87a294-23f4-4d61-899f-92a54e5dc79d" in namespace "downward-api-7308" to be "success or failure"
Dec 17 14:13:23.202: INFO: Pod "downward-api-3c87a294-23f4-4d61-899f-92a54e5dc79d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.759764ms
Dec 17 14:13:25.208: INFO: Pod "downward-api-3c87a294-23f4-4d61-899f-92a54e5dc79d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015689561s
Dec 17 14:13:27.212: INFO: Pod "downward-api-3c87a294-23f4-4d61-899f-92a54e5dc79d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019441876s
STEP: Saw pod success
Dec 17 14:13:27.212: INFO: Pod "downward-api-3c87a294-23f4-4d61-899f-92a54e5dc79d" satisfied condition "success or failure"
Dec 17 14:13:27.214: INFO: Trying to get logs from node conformance-k8s-node-2 pod downward-api-3c87a294-23f4-4d61-899f-92a54e5dc79d container dapi-container: <nil>
STEP: delete the pod
Dec 17 14:13:27.246: INFO: Waiting for pod downward-api-3c87a294-23f4-4d61-899f-92a54e5dc79d to disappear
Dec 17 14:13:27.254: INFO: Pod downward-api-3c87a294-23f4-4d61-899f-92a54e5dc79d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:13:27.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7308" for this suite.
Dec 17 14:13:33.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:13:33.388: INFO: namespace downward-api-7308 deletion completed in 6.12874125s

• [SLOW TEST:10.264 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:13:33.388: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-5aa46b99-461a-40e6-9504-10b0dfa728b3
STEP: Creating a pod to test consume configMaps
Dec 17 14:13:33.454: INFO: Waiting up to 5m0s for pod "pod-configmaps-20e072bf-fe56-4e66-8222-a59e0ad85ecc" in namespace "configmap-1984" to be "success or failure"
Dec 17 14:13:33.460: INFO: Pod "pod-configmaps-20e072bf-fe56-4e66-8222-a59e0ad85ecc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.32303ms
Dec 17 14:13:35.467: INFO: Pod "pod-configmaps-20e072bf-fe56-4e66-8222-a59e0ad85ecc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01285832s
Dec 17 14:13:37.474: INFO: Pod "pod-configmaps-20e072bf-fe56-4e66-8222-a59e0ad85ecc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019771611s
STEP: Saw pod success
Dec 17 14:13:37.474: INFO: Pod "pod-configmaps-20e072bf-fe56-4e66-8222-a59e0ad85ecc" satisfied condition "success or failure"
Dec 17 14:13:37.478: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-20e072bf-fe56-4e66-8222-a59e0ad85ecc container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 14:13:37.512: INFO: Waiting for pod pod-configmaps-20e072bf-fe56-4e66-8222-a59e0ad85ecc to disappear
Dec 17 14:13:37.517: INFO: Pod pod-configmaps-20e072bf-fe56-4e66-8222-a59e0ad85ecc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:13:37.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1984" for this suite.
Dec 17 14:13:43.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:13:43.642: INFO: namespace configmap-1984 deletion completed in 6.119531721s

• [SLOW TEST:10.255 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:13:43.647: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-29ff8c8e-defd-4788-bf95-5e582760d233
STEP: Creating configMap with name cm-test-opt-upd-687f44ff-a776-444a-968c-369fb6a0aed2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-29ff8c8e-defd-4788-bf95-5e582760d233
STEP: Updating configmap cm-test-opt-upd-687f44ff-a776-444a-968c-369fb6a0aed2
STEP: Creating configMap with name cm-test-opt-create-43d59b4d-99a9-4a4c-aa88-801139726d17
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:14:54.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5792" for this suite.
Dec 17 14:15:16.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:15:16.445: INFO: namespace configmap-5792 deletion completed in 22.114240817s

• [SLOW TEST:92.798 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:15:16.445: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 17 14:15:16.486: INFO: Waiting up to 5m0s for pod "pod-71d04c89-3d30-4430-8abb-723d1b83e1ac" in namespace "emptydir-8890" to be "success or failure"
Dec 17 14:15:16.494: INFO: Pod "pod-71d04c89-3d30-4430-8abb-723d1b83e1ac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.750358ms
Dec 17 14:15:18.498: INFO: Pod "pod-71d04c89-3d30-4430-8abb-723d1b83e1ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012027176s
STEP: Saw pod success
Dec 17 14:15:18.498: INFO: Pod "pod-71d04c89-3d30-4430-8abb-723d1b83e1ac" satisfied condition "success or failure"
Dec 17 14:15:18.502: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-71d04c89-3d30-4430-8abb-723d1b83e1ac container test-container: <nil>
STEP: delete the pod
Dec 17 14:15:18.520: INFO: Waiting for pod pod-71d04c89-3d30-4430-8abb-723d1b83e1ac to disappear
Dec 17 14:15:18.526: INFO: Pod pod-71d04c89-3d30-4430-8abb-723d1b83e1ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:15:18.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8890" for this suite.
Dec 17 14:15:24.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:15:24.657: INFO: namespace emptydir-8890 deletion completed in 6.12610666s

• [SLOW TEST:8.212 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:15:24.658: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3366
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3366
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3366
Dec 17 14:15:24.711: INFO: Found 0 stateful pods, waiting for 1
Dec 17 14:15:34.716: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 17 14:15:34.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-3366 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 14:15:35.391: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 14:15:35.391: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 14:15:35.391: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 17 14:15:35.396: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 17 14:15:45.400: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 14:15:45.400: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 14:15:45.419: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999903s
Dec 17 14:15:46.425: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991634006s
Dec 17 14:15:47.431: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.986121851s
Dec 17 14:15:48.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980146161s
Dec 17 14:15:49.443: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.97426892s
Dec 17 14:15:50.448: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.967738963s
Dec 17 14:15:51.459: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.962890989s
Dec 17 14:15:52.464: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.951780867s
Dec 17 14:15:53.471: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.94674143s
Dec 17 14:15:54.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 940.012079ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3366
Dec 17 14:15:55.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-3366 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 17 14:15:55.731: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 17 14:15:55.731: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 17 14:15:55.731: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 17 14:15:55.737: INFO: Found 1 stateful pods, waiting for 3
Dec 17 14:16:05.744: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 14:16:05.744: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 14:16:05.744: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 17 14:16:05.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-3366 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 14:16:06.022: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 14:16:06.022: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 14:16:06.022: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 17 14:16:06.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-3366 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 14:16:06.255: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 14:16:06.255: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 14:16:06.255: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 17 14:16:06.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-3366 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 14:16:06.550: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 14:16:06.550: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 14:16:06.550: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 17 14:16:06.550: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 14:16:06.555: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 17 14:16:16.565: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 14:16:16.565: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 14:16:16.565: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 14:16:16.577: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999s
Dec 17 14:16:17.583: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995098906s
Dec 17 14:16:18.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988916991s
Dec 17 14:16:19.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983534356s
Dec 17 14:16:20.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976131376s
Dec 17 14:16:21.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968446614s
Dec 17 14:16:22.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960751205s
Dec 17 14:16:23.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.953944675s
Dec 17 14:16:24.631: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.947465008s
Dec 17 14:16:25.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 941.418715ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3366
Dec 17 14:16:26.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-3366 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 17 14:16:26.911: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 17 14:16:26.911: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 17 14:16:26.911: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 17 14:16:26.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-3366 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 17 14:16:27.154: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 17 14:16:27.154: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 17 14:16:27.154: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 17 14:16:27.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-3366 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 17 14:16:27.409: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 17 14:16:27.409: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 17 14:16:27.409: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 17 14:16:27.409: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 17 14:16:47.429: INFO: Deleting all statefulset in ns statefulset-3366
Dec 17 14:16:47.432: INFO: Scaling statefulset ss to 0
Dec 17 14:16:47.442: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 14:16:47.445: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:16:47.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3366" for this suite.
Dec 17 14:16:53.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:16:53.573: INFO: namespace statefulset-3366 deletion completed in 6.109667355s

• [SLOW TEST:88.916 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:16:53.579: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 17 14:16:53.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4349'
Dec 17 14:16:53.733: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 17 14:16:53.734: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Dec 17 14:16:53.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete jobs e2e-test-nginx-job --namespace=kubectl-4349'
Dec 17 14:16:53.812: INFO: stderr: ""
Dec 17 14:16:53.812: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:16:53.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4349" for this suite.
Dec 17 14:17:15.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:17:15.933: INFO: namespace kubectl-4349 deletion completed in 22.117037049s

• [SLOW TEST:22.355 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:17:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4856
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec 17 14:17:16.008: INFO: Found 0 stateful pods, waiting for 3
Dec 17 14:17:26.021: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 14:17:26.021: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 14:17:26.021: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 14:17:26.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4856 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 14:17:26.295: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 14:17:26.295: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 14:17:26.295: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 17 14:17:36.335: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 17 14:17:46.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4856 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 17 14:17:46.600: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 17 14:17:46.600: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 17 14:17:46.600: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 17 14:18:06.628: INFO: Waiting for StatefulSet statefulset-4856/ss2 to complete update
Dec 17 14:18:06.629: INFO: Waiting for Pod statefulset-4856/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Dec 17 14:18:16.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4856 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 17 14:18:16.887: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 17 14:18:16.887: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 17 14:18:16.887: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 17 14:18:26.921: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 17 14:18:36.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 exec --namespace=statefulset-4856 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 17 14:18:37.264: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 17 14:18:37.264: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 17 14:18:37.264: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 17 14:18:57.289: INFO: Waiting for StatefulSet statefulset-4856/ss2 to complete update
Dec 17 14:18:57.289: INFO: Waiting for Pod statefulset-4856/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 17 14:19:07.297: INFO: Deleting all statefulset in ns statefulset-4856
Dec 17 14:19:07.301: INFO: Scaling statefulset ss2 to 0
Dec 17 14:19:37.317: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 14:19:37.321: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:19:37.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4856" for this suite.
Dec 17 14:19:43.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:19:43.449: INFO: namespace statefulset-4856 deletion completed in 6.110987279s

• [SLOW TEST:147.514 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:19:43.452: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec 17 14:19:43.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-3552'
Dec 17 14:19:43.983: INFO: stderr: ""
Dec 17 14:19:43.983: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 14:19:43.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3552'
Dec 17 14:19:44.054: INFO: stderr: ""
Dec 17 14:19:44.054: INFO: stdout: "update-demo-nautilus-22hf7 update-demo-nautilus-7xjmf "
Dec 17 14:19:44.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-22hf7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3552'
Dec 17 14:19:44.132: INFO: stderr: ""
Dec 17 14:19:44.132: INFO: stdout: ""
Dec 17 14:19:44.132: INFO: update-demo-nautilus-22hf7 is created but not running
Dec 17 14:19:49.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3552'
Dec 17 14:19:49.249: INFO: stderr: ""
Dec 17 14:19:49.250: INFO: stdout: "update-demo-nautilus-22hf7 update-demo-nautilus-7xjmf "
Dec 17 14:19:49.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-22hf7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3552'
Dec 17 14:19:49.326: INFO: stderr: ""
Dec 17 14:19:49.326: INFO: stdout: "true"
Dec 17 14:19:49.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-22hf7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3552'
Dec 17 14:19:49.416: INFO: stderr: ""
Dec 17 14:19:49.416: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:19:49.416: INFO: validating pod update-demo-nautilus-22hf7
Dec 17 14:19:49.421: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:19:49.421: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:19:49.421: INFO: update-demo-nautilus-22hf7 is verified up and running
Dec 17 14:19:49.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-7xjmf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3552'
Dec 17 14:19:49.500: INFO: stderr: ""
Dec 17 14:19:49.500: INFO: stdout: "true"
Dec 17 14:19:49.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-7xjmf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3552'
Dec 17 14:19:49.580: INFO: stderr: ""
Dec 17 14:19:49.580: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:19:49.580: INFO: validating pod update-demo-nautilus-7xjmf
Dec 17 14:19:49.586: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:19:49.587: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:19:49.587: INFO: update-demo-nautilus-7xjmf is verified up and running
STEP: using delete to clean up resources
Dec 17 14:19:49.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-3552'
Dec 17 14:19:49.673: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 14:19:49.673: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 17 14:19:49.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3552'
Dec 17 14:19:49.755: INFO: stderr: "No resources found.\n"
Dec 17 14:19:49.755: INFO: stdout: ""
Dec 17 14:19:49.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -l name=update-demo --namespace=kubectl-3552 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 14:19:49.838: INFO: stderr: ""
Dec 17 14:19:49.838: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:19:49.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3552" for this suite.
Dec 17 14:20:11.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:20:11.984: INFO: namespace kubectl-3552 deletion completed in 22.140441413s

• [SLOW TEST:28.533 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:20:11.985: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1217 14:20:52.085454      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 17 14:20:52.085: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:20:52.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7992" for this suite.
Dec 17 14:20:58.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:20:58.277: INFO: namespace gc-7992 deletion completed in 6.187078376s

• [SLOW TEST:46.292 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:20:58.277: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5399
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 17 14:20:58.319: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 17 14:21:20.443: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.130:8080/dial?request=hostName&protocol=http&host=10.233.95.33&port=8080&tries=1'] Namespace:pod-network-test-5399 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:21:20.443: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:21:20.614: INFO: Waiting for endpoints: map[]
Dec 17 14:21:20.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.130:8080/dial?request=hostName&protocol=http&host=10.233.100.129&port=8080&tries=1'] Namespace:pod-network-test-5399 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:21:20.620: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:21:20.794: INFO: Waiting for endpoints: map[]
Dec 17 14:21:20.800: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.100.130:8080/dial?request=hostName&protocol=http&host=10.233.93.60&port=8080&tries=1'] Namespace:pod-network-test-5399 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:21:20.800: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:21:20.956: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:21:20.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5399" for this suite.
Dec 17 14:21:42.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:21:43.083: INFO: namespace pod-network-test-5399 deletion completed in 22.121403096s

• [SLOW TEST:44.806 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:21:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-43f2fa73-7a7c-46bc-96e3-e12a615fbeb9 in namespace container-probe-853
Dec 17 14:21:45.199: INFO: Started pod busybox-43f2fa73-7a7c-46bc-96e3-e12a615fbeb9 in namespace container-probe-853
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 14:21:45.202: INFO: Initial restart count of pod busybox-43f2fa73-7a7c-46bc-96e3-e12a615fbeb9 is 0
Dec 17 14:22:39.351: INFO: Restart count of pod container-probe-853/busybox-43f2fa73-7a7c-46bc-96e3-e12a615fbeb9 is now 1 (54.149147303s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:22:39.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-853" for this suite.
Dec 17 14:22:45.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:22:45.504: INFO: namespace container-probe-853 deletion completed in 6.127839733s

• [SLOW TEST:62.421 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:22:45.507: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:22:45.630: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 17 14:22:45.647: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:45.650: INFO: Number of nodes with available pods: 0
Dec 17 14:22:45.650: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 14:22:46.656: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:46.661: INFO: Number of nodes with available pods: 0
Dec 17 14:22:46.661: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 14:22:47.657: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:47.661: INFO: Number of nodes with available pods: 1
Dec 17 14:22:47.661: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 14:22:48.658: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:48.661: INFO: Number of nodes with available pods: 3
Dec 17 14:22:48.661: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 17 14:22:48.705: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:48.705: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:48.705: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:48.719: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:49.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:49.724: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:49.724: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:49.729: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:50.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:50.724: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:50.724: INFO: Pod daemon-set-b8czs is not available
Dec 17 14:22:50.724: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:50.728: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:51.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:51.726: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:51.726: INFO: Pod daemon-set-b8czs is not available
Dec 17 14:22:51.726: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:51.731: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:52.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:52.725: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:52.725: INFO: Pod daemon-set-b8czs is not available
Dec 17 14:22:52.725: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:52.756: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:53.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:53.724: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:53.724: INFO: Pod daemon-set-b8czs is not available
Dec 17 14:22:53.724: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:53.729: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:54.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:54.724: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:54.724: INFO: Pod daemon-set-b8czs is not available
Dec 17 14:22:54.724: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:54.729: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:55.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:55.724: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:55.725: INFO: Pod daemon-set-b8czs is not available
Dec 17 14:22:55.725: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:55.730: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:56.727: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:56.727: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:56.727: INFO: Pod daemon-set-b8czs is not available
Dec 17 14:22:56.727: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:56.733: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:57.726: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:57.726: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:57.726: INFO: Pod daemon-set-b8czs is not available
Dec 17 14:22:57.726: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:57.731: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:58.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:58.725: INFO: Wrong image for pod: daemon-set-b8czs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:58.725: INFO: Pod daemon-set-b8czs is not available
Dec 17 14:22:58.725: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:58.730: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:22:59.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:59.725: INFO: Pod daemon-set-jlv6b is not available
Dec 17 14:22:59.725: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:22:59.730: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:00.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:00.724: INFO: Pod daemon-set-jlv6b is not available
Dec 17 14:23:00.724: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:00.730: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:01.722: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:01.722: INFO: Pod daemon-set-jlv6b is not available
Dec 17 14:23:01.722: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:01.725: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:02.726: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:02.726: INFO: Pod daemon-set-jlv6b is not available
Dec 17 14:23:02.726: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:02.732: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:03.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:03.724: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:03.731: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:04.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:04.726: INFO: Wrong image for pod: daemon-set-mfmr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:04.726: INFO: Pod daemon-set-mfmr9 is not available
Dec 17 14:23:04.730: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:05.729: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:05.729: INFO: Pod daemon-set-d8llt is not available
Dec 17 14:23:05.746: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:06.727: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:06.727: INFO: Pod daemon-set-d8llt is not available
Dec 17 14:23:06.735: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:07.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:07.725: INFO: Pod daemon-set-d8llt is not available
Dec 17 14:23:07.729: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:08.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:08.725: INFO: Pod daemon-set-d8llt is not available
Dec 17 14:23:08.731: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:09.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:09.730: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:10.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:10.725: INFO: Pod daemon-set-5fvbt is not available
Dec 17 14:23:10.731: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:11.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:11.725: INFO: Pod daemon-set-5fvbt is not available
Dec 17 14:23:11.733: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:12.728: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:12.728: INFO: Pod daemon-set-5fvbt is not available
Dec 17 14:23:12.733: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:13.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:13.725: INFO: Pod daemon-set-5fvbt is not available
Dec 17 14:23:13.730: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:14.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:14.725: INFO: Pod daemon-set-5fvbt is not available
Dec 17 14:23:14.732: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:15.723: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:15.723: INFO: Pod daemon-set-5fvbt is not available
Dec 17 14:23:15.729: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:16.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:16.724: INFO: Pod daemon-set-5fvbt is not available
Dec 17 14:23:16.729: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:17.724: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:17.724: INFO: Pod daemon-set-5fvbt is not available
Dec 17 14:23:17.729: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:18.725: INFO: Wrong image for pod: daemon-set-5fvbt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 17 14:23:18.725: INFO: Pod daemon-set-5fvbt is not available
Dec 17 14:23:18.730: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:19.726: INFO: Pod daemon-set-g4gfm is not available
Dec 17 14:23:19.730: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 17 14:23:19.734: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:19.737: INFO: Number of nodes with available pods: 2
Dec 17 14:23:19.737: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 14:23:20.743: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:20.752: INFO: Number of nodes with available pods: 2
Dec 17 14:23:20.752: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 14:23:21.743: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:21.747: INFO: Number of nodes with available pods: 2
Dec 17 14:23:21.747: INFO: Node conformance-k8s-node-2 is running more than one daemon pod
Dec 17 14:23:22.743: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:23:22.749: INFO: Number of nodes with available pods: 3
Dec 17 14:23:22.749: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7283, will wait for the garbage collector to delete the pods
Dec 17 14:23:22.829: INFO: Deleting DaemonSet.extensions daemon-set took: 7.537445ms
Dec 17 14:23:23.129: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.384239ms
Dec 17 14:23:29.534: INFO: Number of nodes with available pods: 0
Dec 17 14:23:29.534: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 14:23:29.537: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7283/daemonsets","resourceVersion":"17354"},"items":null}

Dec 17 14:23:29.539: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7283/pods","resourceVersion":"17354"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:23:29.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7283" for this suite.
Dec 17 14:23:35.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:23:35.676: INFO: namespace daemonsets-7283 deletion completed in 6.120455094s

• [SLOW TEST:50.170 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:23:35.679: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:23:35.720: INFO: Creating deployment "test-recreate-deployment"
Dec 17 14:23:35.723: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 17 14:23:35.742: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 17 14:23:37.756: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 17 14:23:37.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712189415, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712189415, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712189415, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712189415, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:23:39.763: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 17 14:23:39.771: INFO: Updating deployment test-recreate-deployment
Dec 17 14:23:39.771: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 17 14:23:39.893: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-8583,SelfLink:/apis/apps/v1/namespaces/deployment-8583/deployments/test-recreate-deployment,UID:e2d2f867-663d-4c01-a376-04d324441d95,ResourceVersion:17469,Generation:2,CreationTimestamp:2019-12-17 14:23:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-17 14:23:39 +0000 UTC 2019-12-17 14:23:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-17 14:23:39 +0000 UTC 2019-12-17 14:23:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec 17 14:23:39.896: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-8583,SelfLink:/apis/apps/v1/namespaces/deployment-8583/replicasets/test-recreate-deployment-5c8c9cc69d,UID:8cc6f6c0-9c8d-433a-8fc9-c4d110062a34,ResourceVersion:17466,Generation:1,CreationTimestamp:2019-12-17 14:23:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e2d2f867-663d-4c01-a376-04d324441d95 0xc003539377 0xc003539378}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 17 14:23:39.896: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 17 14:23:39.896: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-8583,SelfLink:/apis/apps/v1/namespaces/deployment-8583/replicasets/test-recreate-deployment-6df85df6b9,UID:3bcddb05-5f7f-4a61-b213-82736ec4e0ac,ResourceVersion:17457,Generation:2,CreationTimestamp:2019-12-17 14:23:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e2d2f867-663d-4c01-a376-04d324441d95 0xc003539577 0xc003539578}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 17 14:23:39.900: INFO: Pod "test-recreate-deployment-5c8c9cc69d-l6rml" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-l6rml,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-8583,SelfLink:/api/v1/namespaces/deployment-8583/pods/test-recreate-deployment-5c8c9cc69d-l6rml,UID:58a59a8e-e553-4ff6-bcab-81eb6d4fe8c2,ResourceVersion:17471,Generation:0,CreationTimestamp:2019-12-17 14:23:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 8cc6f6c0-9c8d-433a-8fc9-c4d110062a34 0xc00367c417 0xc00367c418}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m8mjq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m8mjq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m8mjq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00367c480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00367c4a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:23:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:23:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:23:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:23:39 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2019-12-17 14:23:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:23:39.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8583" for this suite.
Dec 17 14:23:45.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:23:46.011: INFO: namespace deployment-8583 deletion completed in 6.106835041s

• [SLOW TEST:10.332 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:23:46.011: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 14:23:48.088: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:23:48.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5225" for this suite.
Dec 17 14:23:54.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:23:54.224: INFO: namespace container-runtime-5225 deletion completed in 6.109665143s

• [SLOW TEST:8.213 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:23:54.224: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-5x64
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 14:23:54.276: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5x64" in namespace "subpath-6687" to be "success or failure"
Dec 17 14:23:54.286: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Pending", Reason="", readiness=false. Elapsed: 9.896299ms
Dec 17 14:23:56.292: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015840829s
Dec 17 14:23:58.297: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Running", Reason="", readiness=true. Elapsed: 4.020646182s
Dec 17 14:24:00.302: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Running", Reason="", readiness=true. Elapsed: 6.025910953s
Dec 17 14:24:02.307: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Running", Reason="", readiness=true. Elapsed: 8.031236961s
Dec 17 14:24:04.313: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Running", Reason="", readiness=true. Elapsed: 10.036387384s
Dec 17 14:24:06.317: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Running", Reason="", readiness=true. Elapsed: 12.04098392s
Dec 17 14:24:08.321: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Running", Reason="", readiness=true. Elapsed: 14.04513092s
Dec 17 14:24:10.325: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Running", Reason="", readiness=true. Elapsed: 16.048837064s
Dec 17 14:24:12.331: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Running", Reason="", readiness=true. Elapsed: 18.054894576s
Dec 17 14:24:14.335: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Running", Reason="", readiness=true. Elapsed: 20.059013437s
Dec 17 14:24:16.340: INFO: Pod "pod-subpath-test-configmap-5x64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.063725325s
STEP: Saw pod success
Dec 17 14:24:16.340: INFO: Pod "pod-subpath-test-configmap-5x64" satisfied condition "success or failure"
Dec 17 14:24:16.343: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-subpath-test-configmap-5x64 container test-container-subpath-configmap-5x64: <nil>
STEP: delete the pod
Dec 17 14:24:16.381: INFO: Waiting for pod pod-subpath-test-configmap-5x64 to disappear
Dec 17 14:24:16.384: INFO: Pod pod-subpath-test-configmap-5x64 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5x64
Dec 17 14:24:16.384: INFO: Deleting pod "pod-subpath-test-configmap-5x64" in namespace "subpath-6687"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:24:16.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6687" for this suite.
Dec 17 14:24:22.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:24:22.491: INFO: namespace subpath-6687 deletion completed in 6.098332727s

• [SLOW TEST:28.267 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:24:22.495: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-e3a443cb-305b-4a77-b60f-0071a7acd105
STEP: Creating secret with name secret-projected-all-test-volume-d2bb20a8-b239-4623-996d-16d6a94e9194
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 17 14:24:22.541: INFO: Waiting up to 5m0s for pod "projected-volume-78342226-a413-4996-9025-3924589e8acd" in namespace "projected-3953" to be "success or failure"
Dec 17 14:24:22.549: INFO: Pod "projected-volume-78342226-a413-4996-9025-3924589e8acd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.228495ms
Dec 17 14:24:24.553: INFO: Pod "projected-volume-78342226-a413-4996-9025-3924589e8acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011962828s
STEP: Saw pod success
Dec 17 14:24:24.553: INFO: Pod "projected-volume-78342226-a413-4996-9025-3924589e8acd" satisfied condition "success or failure"
Dec 17 14:24:24.557: INFO: Trying to get logs from node conformance-k8s-node-2 pod projected-volume-78342226-a413-4996-9025-3924589e8acd container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 17 14:24:24.585: INFO: Waiting for pod projected-volume-78342226-a413-4996-9025-3924589e8acd to disappear
Dec 17 14:24:24.589: INFO: Pod projected-volume-78342226-a413-4996-9025-3924589e8acd no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:24:24.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3953" for this suite.
Dec 17 14:24:30.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:24:30.702: INFO: namespace projected-3953 deletion completed in 6.109873142s

• [SLOW TEST:8.208 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:24:30.706: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:24:30.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 version'
Dec 17 14:24:30.832: INFO: stderr: ""
Dec 17 14:24:30.832: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:24:30.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8631" for this suite.
Dec 17 14:24:36.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:24:36.961: INFO: namespace kubectl-8631 deletion completed in 6.123921799s

• [SLOW TEST:6.256 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:24:36.964: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 17 14:24:37.001: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 14:24:37.019: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 14:24:37.023: INFO: 
Logging pods the kubelet thinks is on node conformance-k8s-node-1 before test
Dec 17 14:24:37.038: INFO: nginx-proxy-conformance-k8s-node-1 from kube-system started at 2019-12-17 13:27:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.038: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 14:24:37.038: INFO: coredns-74c9d4d795-2w8nz from kube-system started at 2019-12-17 13:28:57 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.038: INFO: 	Container coredns ready: true, restart count 0
Dec 17 14:24:37.038: INFO: calico-node-fqsjm from kube-system started at 2019-12-17 13:27:45 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.038: INFO: 	Container calico-node ready: true, restart count 1
Dec 17 14:24:37.038: INFO: calico-kube-controllers-58f76c7dcf-mfnlw from kube-system started at 2019-12-17 13:28:21 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.038: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 17 14:24:37.038: INFO: nodelocaldns-llm5v from kube-system started at 2019-12-17 13:28:56 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.038: INFO: 	Container node-cache ready: true, restart count 0
Dec 17 14:24:37.038: INFO: sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-s4k7l from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:24:37.038: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:24:37.038: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 14:24:37.038: INFO: kube-proxy-6kv7k from kube-system started at 2019-12-17 13:27:22 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.038: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 14:24:37.038: INFO: 
Logging pods the kubelet thinks is on node conformance-k8s-node-2 before test
Dec 17 14:24:37.048: INFO: calico-node-mjmxr from kube-system started at 2019-12-17 13:27:45 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.048: INFO: 	Container calico-node ready: true, restart count 1
Dec 17 14:24:37.048: INFO: nodelocaldns-j44d9 from kube-system started at 2019-12-17 13:28:56 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.048: INFO: 	Container node-cache ready: true, restart count 0
Dec 17 14:24:37.048: INFO: kube-proxy-flwq7 from kube-system started at 2019-12-17 13:27:22 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.048: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 14:24:37.048: INFO: kubernetes-dashboard-7c547b4c64-f72ch from kube-system started at 2019-12-17 13:28:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.048: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 17 14:24:37.048: INFO: nginx-proxy-conformance-k8s-node-2 from kube-system started at 2019-12-17 13:27:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.048: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 14:24:37.048: INFO: sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-l45d2 from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:24:37.048: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:24:37.048: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 14:24:37.048: INFO: 
Logging pods the kubelet thinks is on node conformance-k8s-node-3 before test
Dec 17 14:24:37.059: INFO: calico-node-dfzdd from kube-system started at 2019-12-17 13:27:45 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.059: INFO: 	Container calico-node ready: true, restart count 1
Dec 17 14:24:37.059: INFO: nginx-proxy-conformance-k8s-node-3 from kube-system started at 2019-12-17 13:27:19 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.059: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 14:24:37.059: INFO: kube-proxy-ghh8l from kube-system started at 2019-12-17 13:27:22 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.059: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 14:24:37.059: INFO: nodelocaldns-d4tqp from kube-system started at 2019-12-17 13:28:56 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.059: INFO: 	Container node-cache ready: true, restart count 0
Dec 17 14:24:37.059: INFO: sonobuoy from sonobuoy started at 2019-12-17 13:31:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:24:37.059: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 14:24:37.059: INFO: sonobuoy-e2e-job-897112f537434934 from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:24:37.059: INFO: 	Container e2e ready: true, restart count 0
Dec 17 14:24:37.059: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:24:37.059: INFO: sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-zf47h from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:24:37.059: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:24:37.059: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9395d364-01bd-4946-9fb4-95333fadeb3f 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9395d364-01bd-4946-9fb4-95333fadeb3f off the node conformance-k8s-node-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9395d364-01bd-4946-9fb4-95333fadeb3f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:24:41.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3886" for this suite.
Dec 17 14:24:49.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:24:49.287: INFO: namespace sched-pred-3886 deletion completed in 8.133743022s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:12.323 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:24:49.292: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 17 14:24:49.357: INFO: Waiting up to 5m0s for pod "pod-8569f886-36df-4271-ad5c-728968d3852b" in namespace "emptydir-2787" to be "success or failure"
Dec 17 14:24:49.361: INFO: Pod "pod-8569f886-36df-4271-ad5c-728968d3852b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.909636ms
Dec 17 14:24:51.365: INFO: Pod "pod-8569f886-36df-4271-ad5c-728968d3852b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008045873s
STEP: Saw pod success
Dec 17 14:24:51.365: INFO: Pod "pod-8569f886-36df-4271-ad5c-728968d3852b" satisfied condition "success or failure"
Dec 17 14:24:51.368: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-8569f886-36df-4271-ad5c-728968d3852b container test-container: <nil>
STEP: delete the pod
Dec 17 14:24:51.396: INFO: Waiting for pod pod-8569f886-36df-4271-ad5c-728968d3852b to disappear
Dec 17 14:24:51.398: INFO: Pod pod-8569f886-36df-4271-ad5c-728968d3852b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:24:51.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2787" for this suite.
Dec 17 14:24:57.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:24:57.561: INFO: namespace emptydir-2787 deletion completed in 6.155061761s

• [SLOW TEST:8.269 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:24:57.562: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:24:59.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2077" for this suite.
Dec 17 14:25:41.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:25:41.777: INFO: namespace kubelet-test-2077 deletion completed in 42.128084319s

• [SLOW TEST:44.215 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:25:41.777: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8798
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 17 14:25:41.816: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 17 14:26:07.966: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.93.63 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8798 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:26:07.966: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:26:09.111: INFO: Found all expected endpoints: [netserver-0]
Dec 17 14:26:09.116: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.95.36 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8798 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:26:09.116: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:26:10.275: INFO: Found all expected endpoints: [netserver-1]
Dec 17 14:26:10.279: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.100.143 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8798 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:26:10.279: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:26:11.455: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:26:11.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8798" for this suite.
Dec 17 14:26:33.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:26:33.585: INFO: namespace pod-network-test-8798 deletion completed in 22.123223008s

• [SLOW TEST:51.808 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:26:33.585: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 17 14:26:33.661: INFO: Waiting up to 5m0s for pod "downward-api-7e8fcfff-be5c-4f63-971a-c390046929aa" in namespace "downward-api-6646" to be "success or failure"
Dec 17 14:26:33.677: INFO: Pod "downward-api-7e8fcfff-be5c-4f63-971a-c390046929aa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.573048ms
Dec 17 14:26:35.682: INFO: Pod "downward-api-7e8fcfff-be5c-4f63-971a-c390046929aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021073075s
STEP: Saw pod success
Dec 17 14:26:35.682: INFO: Pod "downward-api-7e8fcfff-be5c-4f63-971a-c390046929aa" satisfied condition "success or failure"
Dec 17 14:26:35.697: INFO: Trying to get logs from node conformance-k8s-node-2 pod downward-api-7e8fcfff-be5c-4f63-971a-c390046929aa container dapi-container: <nil>
STEP: delete the pod
Dec 17 14:26:35.729: INFO: Waiting for pod downward-api-7e8fcfff-be5c-4f63-971a-c390046929aa to disappear
Dec 17 14:26:35.735: INFO: Pod downward-api-7e8fcfff-be5c-4f63-971a-c390046929aa no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:26:35.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6646" for this suite.
Dec 17 14:26:41.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:26:41.888: INFO: namespace downward-api-6646 deletion completed in 6.148607139s

• [SLOW TEST:8.302 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:26:41.893: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:26:41.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3951" for this suite.
Dec 17 14:27:13.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:27:14.095: INFO: namespace pods-3951 deletion completed in 32.123282619s

• [SLOW TEST:32.202 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:27:14.096: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 17 14:27:14.184: INFO: Waiting up to 5m0s for pod "downward-api-a45ffac7-86f3-41b9-89b4-abca0b095b5a" in namespace "downward-api-9884" to be "success or failure"
Dec 17 14:27:14.193: INFO: Pod "downward-api-a45ffac7-86f3-41b9-89b4-abca0b095b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087113ms
Dec 17 14:27:16.197: INFO: Pod "downward-api-a45ffac7-86f3-41b9-89b4-abca0b095b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012597284s
Dec 17 14:27:18.203: INFO: Pod "downward-api-a45ffac7-86f3-41b9-89b4-abca0b095b5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01849431s
STEP: Saw pod success
Dec 17 14:27:18.203: INFO: Pod "downward-api-a45ffac7-86f3-41b9-89b4-abca0b095b5a" satisfied condition "success or failure"
Dec 17 14:27:18.206: INFO: Trying to get logs from node conformance-k8s-node-2 pod downward-api-a45ffac7-86f3-41b9-89b4-abca0b095b5a container dapi-container: <nil>
STEP: delete the pod
Dec 17 14:27:18.237: INFO: Waiting for pod downward-api-a45ffac7-86f3-41b9-89b4-abca0b095b5a to disappear
Dec 17 14:27:18.240: INFO: Pod downward-api-a45ffac7-86f3-41b9-89b4-abca0b095b5a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:27:18.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9884" for this suite.
Dec 17 14:27:24.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:27:24.352: INFO: namespace downward-api-9884 deletion completed in 6.108965081s

• [SLOW TEST:10.257 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:27:24.353: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-e3c7d8a3-5514-48a1-bf05-4bcbf6796a8d
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:27:24.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3294" for this suite.
Dec 17 14:27:30.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:27:30.531: INFO: namespace configmap-3294 deletion completed in 6.125713254s

• [SLOW TEST:6.179 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:27:30.532: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-44b569c2-7fba-46cf-b96c-46388882d487
STEP: Creating a pod to test consume configMaps
Dec 17 14:27:30.600: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-34d419e2-14e1-4817-9e8c-c3cb619cbb14" in namespace "projected-1797" to be "success or failure"
Dec 17 14:27:30.606: INFO: Pod "pod-projected-configmaps-34d419e2-14e1-4817-9e8c-c3cb619cbb14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055372ms
Dec 17 14:27:32.612: INFO: Pod "pod-projected-configmaps-34d419e2-14e1-4817-9e8c-c3cb619cbb14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011845283s
Dec 17 14:27:34.616: INFO: Pod "pod-projected-configmaps-34d419e2-14e1-4817-9e8c-c3cb619cbb14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015917577s
STEP: Saw pod success
Dec 17 14:27:34.616: INFO: Pod "pod-projected-configmaps-34d419e2-14e1-4817-9e8c-c3cb619cbb14" satisfied condition "success or failure"
Dec 17 14:27:34.619: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-configmaps-34d419e2-14e1-4817-9e8c-c3cb619cbb14 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 14:27:34.640: INFO: Waiting for pod pod-projected-configmaps-34d419e2-14e1-4817-9e8c-c3cb619cbb14 to disappear
Dec 17 14:27:34.643: INFO: Pod pod-projected-configmaps-34d419e2-14e1-4817-9e8c-c3cb619cbb14 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:27:34.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1797" for this suite.
Dec 17 14:27:40.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:27:40.774: INFO: namespace projected-1797 deletion completed in 6.127943006s

• [SLOW TEST:10.242 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:27:40.775: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-16accfaa-ccf7-4cc6-95c4-a70003b68c1d
STEP: Creating configMap with name cm-test-opt-upd-3fa4d0d6-d7ae-4ae8-823a-ca120e4efc67
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-16accfaa-ccf7-4cc6-95c4-a70003b68c1d
STEP: Updating configmap cm-test-opt-upd-3fa4d0d6-d7ae-4ae8-823a-ca120e4efc67
STEP: Creating configMap with name cm-test-opt-create-206b8a13-ac5e-4ef3-abcb-6aba4578cc01
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:28:55.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-202" for this suite.
Dec 17 14:29:17.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:29:17.601: INFO: namespace projected-202 deletion completed in 22.113778538s

• [SLOW TEST:96.826 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:29:17.602: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 17 14:29:22.193: INFO: Successfully updated pod "pod-update-activedeadlineseconds-91897de5-f085-40e9-ba59-dd0bba12cc07"
Dec 17 14:29:22.193: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-91897de5-f085-40e9-ba59-dd0bba12cc07" in namespace "pods-7256" to be "terminated due to deadline exceeded"
Dec 17 14:29:22.199: INFO: Pod "pod-update-activedeadlineseconds-91897de5-f085-40e9-ba59-dd0bba12cc07": Phase="Running", Reason="", readiness=true. Elapsed: 5.801777ms
Dec 17 14:29:24.204: INFO: Pod "pod-update-activedeadlineseconds-91897de5-f085-40e9-ba59-dd0bba12cc07": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.010444382s
Dec 17 14:29:24.204: INFO: Pod "pod-update-activedeadlineseconds-91897de5-f085-40e9-ba59-dd0bba12cc07" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:29:24.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7256" for this suite.
Dec 17 14:29:30.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:29:30.329: INFO: namespace pods-7256 deletion completed in 6.120092888s

• [SLOW TEST:12.728 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:29:30.330: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 17 14:29:30.479: INFO: Waiting up to 5m0s for pod "pod-84146180-5b5e-4e3c-9d7a-6c3e2f1d96dc" in namespace "emptydir-3528" to be "success or failure"
Dec 17 14:29:30.486: INFO: Pod "pod-84146180-5b5e-4e3c-9d7a-6c3e2f1d96dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.967516ms
Dec 17 14:29:32.492: INFO: Pod "pod-84146180-5b5e-4e3c-9d7a-6c3e2f1d96dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012339875s
STEP: Saw pod success
Dec 17 14:29:32.492: INFO: Pod "pod-84146180-5b5e-4e3c-9d7a-6c3e2f1d96dc" satisfied condition "success or failure"
Dec 17 14:29:32.495: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-84146180-5b5e-4e3c-9d7a-6c3e2f1d96dc container test-container: <nil>
STEP: delete the pod
Dec 17 14:29:32.520: INFO: Waiting for pod pod-84146180-5b5e-4e3c-9d7a-6c3e2f1d96dc to disappear
Dec 17 14:29:32.523: INFO: Pod pod-84146180-5b5e-4e3c-9d7a-6c3e2f1d96dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:29:32.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3528" for this suite.
Dec 17 14:29:38.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:29:38.654: INFO: namespace emptydir-3528 deletion completed in 6.126313703s

• [SLOW TEST:8.324 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:29:38.663: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 17 14:29:38.715: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 14:29:38.725: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 14:29:38.728: INFO: 
Logging pods the kubelet thinks is on node conformance-k8s-node-1 before test
Dec 17 14:29:38.745: INFO: nginx-proxy-conformance-k8s-node-1 from kube-system started at 2019-12-17 13:27:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.745: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 14:29:38.745: INFO: coredns-74c9d4d795-2w8nz from kube-system started at 2019-12-17 13:28:57 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.745: INFO: 	Container coredns ready: true, restart count 0
Dec 17 14:29:38.746: INFO: calico-node-fqsjm from kube-system started at 2019-12-17 13:27:45 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.746: INFO: 	Container calico-node ready: true, restart count 1
Dec 17 14:29:38.746: INFO: calico-kube-controllers-58f76c7dcf-mfnlw from kube-system started at 2019-12-17 13:28:21 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.746: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 17 14:29:38.746: INFO: nodelocaldns-llm5v from kube-system started at 2019-12-17 13:28:56 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.746: INFO: 	Container node-cache ready: true, restart count 0
Dec 17 14:29:38.746: INFO: sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-s4k7l from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:29:38.746: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:29:38.746: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 14:29:38.746: INFO: kube-proxy-6kv7k from kube-system started at 2019-12-17 13:27:22 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.746: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 14:29:38.747: INFO: 
Logging pods the kubelet thinks is on node conformance-k8s-node-2 before test
Dec 17 14:29:38.756: INFO: nginx-proxy-conformance-k8s-node-2 from kube-system started at 2019-12-17 13:27:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.756: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 14:29:38.756: INFO: sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-l45d2 from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:29:38.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:29:38.756: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 14:29:38.756: INFO: calico-node-mjmxr from kube-system started at 2019-12-17 13:27:45 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.756: INFO: 	Container calico-node ready: true, restart count 1
Dec 17 14:29:38.756: INFO: nodelocaldns-j44d9 from kube-system started at 2019-12-17 13:28:56 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.756: INFO: 	Container node-cache ready: true, restart count 0
Dec 17 14:29:38.756: INFO: kube-proxy-flwq7 from kube-system started at 2019-12-17 13:27:22 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.757: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 14:29:38.757: INFO: kubernetes-dashboard-7c547b4c64-f72ch from kube-system started at 2019-12-17 13:28:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.757: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec 17 14:29:38.757: INFO: 
Logging pods the kubelet thinks is on node conformance-k8s-node-3 before test
Dec 17 14:29:38.767: INFO: calico-node-dfzdd from kube-system started at 2019-12-17 13:27:45 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.767: INFO: 	Container calico-node ready: true, restart count 1
Dec 17 14:29:38.767: INFO: nginx-proxy-conformance-k8s-node-3 from kube-system started at 2019-12-17 13:27:19 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.767: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 17 14:29:38.767: INFO: kube-proxy-ghh8l from kube-system started at 2019-12-17 13:27:22 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.767: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 14:29:38.767: INFO: nodelocaldns-d4tqp from kube-system started at 2019-12-17 13:28:56 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.767: INFO: 	Container node-cache ready: true, restart count 0
Dec 17 14:29:38.767: INFO: sonobuoy from sonobuoy started at 2019-12-17 13:31:59 +0000 UTC (1 container statuses recorded)
Dec 17 14:29:38.767: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 14:29:38.767: INFO: sonobuoy-e2e-job-897112f537434934 from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:29:38.767: INFO: 	Container e2e ready: true, restart count 0
Dec 17 14:29:38.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:29:38.767: INFO: sonobuoy-systemd-logs-daemon-set-893ee45c0df14a60-zf47h from sonobuoy started at 2019-12-17 13:32:05 +0000 UTC (2 container statuses recorded)
Dec 17 14:29:38.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 14:29:38.767: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e12ef6086856c2], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:29:39.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7738" for this suite.
Dec 17 14:29:45.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:29:45.905: INFO: namespace sched-pred-7738 deletion completed in 6.105038805s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.243 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:29:45.908: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 17 14:29:56.025: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:29:56.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1217 14:29:56.025099      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1900" for this suite.
Dec 17 14:30:02.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:30:02.194: INFO: namespace gc-1900 deletion completed in 6.166197866s

• [SLOW TEST:16.287 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:30:02.195: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-373.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-373.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-373.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-373.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-373.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-373.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 14:30:14.354: INFO: DNS probes using dns-373/dns-test-aab2450b-f8df-46ef-bdc5-ea72986a005f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:30:14.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-373" for this suite.
Dec 17 14:30:20.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:30:20.500: INFO: namespace dns-373 deletion completed in 6.12787588s

• [SLOW TEST:18.305 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:30:20.504: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 17 14:30:22.629: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-643038348 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 17 14:30:32.706: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:30:32.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6769" for this suite.
Dec 17 14:30:38.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:30:38.836: INFO: namespace pods-6769 deletion completed in 6.120143711s

• [SLOW TEST:18.333 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:30:38.838: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Dec 17 14:30:38.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 api-versions'
Dec 17 14:30:38.970: INFO: stderr: ""
Dec 17 14:30:38.970: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:30:38.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3012" for this suite.
Dec 17 14:30:44.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:30:45.079: INFO: namespace kubectl-3012 deletion completed in 6.101860815s

• [SLOW TEST:6.241 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:30:45.084: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 17 14:30:45.130: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:30:48.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5647" for this suite.
Dec 17 14:30:54.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:30:54.288: INFO: namespace init-container-5647 deletion completed in 6.1154817s

• [SLOW TEST:9.204 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:30:54.297: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 17 14:30:54.357: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-a,UID:02a4804b-253b-4b92-98ea-7955187690f8,ResourceVersion:19412,Generation:0,CreationTimestamp:2019-12-17 14:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 17 14:30:54.357: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-a,UID:02a4804b-253b-4b92-98ea-7955187690f8,ResourceVersion:19412,Generation:0,CreationTimestamp:2019-12-17 14:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 17 14:31:04.369: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-a,UID:02a4804b-253b-4b92-98ea-7955187690f8,ResourceVersion:19432,Generation:0,CreationTimestamp:2019-12-17 14:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 17 14:31:04.369: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-a,UID:02a4804b-253b-4b92-98ea-7955187690f8,ResourceVersion:19432,Generation:0,CreationTimestamp:2019-12-17 14:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 17 14:31:14.378: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-a,UID:02a4804b-253b-4b92-98ea-7955187690f8,ResourceVersion:19452,Generation:0,CreationTimestamp:2019-12-17 14:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 17 14:31:14.378: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-a,UID:02a4804b-253b-4b92-98ea-7955187690f8,ResourceVersion:19452,Generation:0,CreationTimestamp:2019-12-17 14:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 17 14:31:24.386: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-a,UID:02a4804b-253b-4b92-98ea-7955187690f8,ResourceVersion:19472,Generation:0,CreationTimestamp:2019-12-17 14:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 17 14:31:24.386: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-a,UID:02a4804b-253b-4b92-98ea-7955187690f8,ResourceVersion:19472,Generation:0,CreationTimestamp:2019-12-17 14:30:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 17 14:31:34.394: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-b,UID:6c6d89d6-907e-4108-af3d-ffc63fbf729e,ResourceVersion:19493,Generation:0,CreationTimestamp:2019-12-17 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 17 14:31:34.394: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-b,UID:6c6d89d6-907e-4108-af3d-ffc63fbf729e,ResourceVersion:19493,Generation:0,CreationTimestamp:2019-12-17 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 17 14:31:44.405: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-b,UID:6c6d89d6-907e-4108-af3d-ffc63fbf729e,ResourceVersion:19513,Generation:0,CreationTimestamp:2019-12-17 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 17 14:31:44.405: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1473,SelfLink:/api/v1/namespaces/watch-1473/configmaps/e2e-watch-test-configmap-b,UID:6c6d89d6-907e-4108-af3d-ffc63fbf729e,ResourceVersion:19513,Generation:0,CreationTimestamp:2019-12-17 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:31:54.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1473" for this suite.
Dec 17 14:32:00.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:32:00.518: INFO: namespace watch-1473 deletion completed in 6.107765052s

• [SLOW TEST:66.221 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:32:00.518: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-d6f2d5ce-ce72-4dff-b32e-02c56eaa79db
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d6f2d5ce-ce72-4dff-b32e-02c56eaa79db
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:32:04.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9642" for this suite.
Dec 17 14:32:26.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:32:26.779: INFO: namespace configmap-9642 deletion completed in 22.147138304s

• [SLOW TEST:26.261 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:32:26.780: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:32:26.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c792f14a-fb02-4f64-a2ec-437953deb812" in namespace "projected-5479" to be "success or failure"
Dec 17 14:32:26.835: INFO: Pod "downwardapi-volume-c792f14a-fb02-4f64-a2ec-437953deb812": Phase="Pending", Reason="", readiness=false. Elapsed: 9.274424ms
Dec 17 14:32:28.839: INFO: Pod "downwardapi-volume-c792f14a-fb02-4f64-a2ec-437953deb812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013920064s
STEP: Saw pod success
Dec 17 14:32:28.840: INFO: Pod "downwardapi-volume-c792f14a-fb02-4f64-a2ec-437953deb812" satisfied condition "success or failure"
Dec 17 14:32:28.843: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-c792f14a-fb02-4f64-a2ec-437953deb812 container client-container: <nil>
STEP: delete the pod
Dec 17 14:32:28.887: INFO: Waiting for pod downwardapi-volume-c792f14a-fb02-4f64-a2ec-437953deb812 to disappear
Dec 17 14:32:28.890: INFO: Pod downwardapi-volume-c792f14a-fb02-4f64-a2ec-437953deb812 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:32:28.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5479" for this suite.
Dec 17 14:32:34.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:32:35.011: INFO: namespace projected-5479 deletion completed in 6.115528144s

• [SLOW TEST:8.231 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:32:35.012: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 17 14:32:35.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2529'
Dec 17 14:32:35.603: INFO: stderr: ""
Dec 17 14:32:35.603: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Dec 17 14:32:35.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete pods e2e-test-nginx-pod --namespace=kubectl-2529'
Dec 17 14:32:49.444: INFO: stderr: ""
Dec 17 14:32:49.444: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:32:49.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2529" for this suite.
Dec 17 14:32:55.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:32:55.557: INFO: namespace kubectl-2529 deletion completed in 6.105944564s

• [SLOW TEST:20.546 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:32:55.560: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:33:19.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1250" for this suite.
Dec 17 14:33:25.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:33:25.836: INFO: namespace namespaces-1250 deletion completed in 6.112603291s
STEP: Destroying namespace "nsdeletetest-547" for this suite.
Dec 17 14:33:25.839: INFO: Namespace nsdeletetest-547 was already deleted
STEP: Destroying namespace "nsdeletetest-7293" for this suite.
Dec 17 14:33:31.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:33:31.966: INFO: namespace nsdeletetest-7293 deletion completed in 6.126367709s

• [SLOW TEST:36.405 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:33:31.968: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:33:32.024: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 17 14:33:37.030: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 17 14:33:37.030: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 17 14:33:39.036: INFO: Creating deployment "test-rollover-deployment"
Dec 17 14:33:39.046: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 17 14:33:41.054: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 17 14:33:41.063: INFO: Ensure that both replica sets have 1 created replica
Dec 17 14:33:41.070: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 17 14:33:41.080: INFO: Updating deployment test-rollover-deployment
Dec 17 14:33:41.080: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 17 14:33:43.092: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 17 14:33:43.117: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 17 14:33:43.124: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 14:33:43.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190023, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:33:45.143: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 14:33:45.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190023, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:33:47.132: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 14:33:47.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190023, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:33:49.131: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 14:33:49.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190023, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:33:51.133: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 14:33:51.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190023, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:33:53.133: INFO: 
Dec 17 14:33:53.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190023, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712190019, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 14:33:55.134: INFO: 
Dec 17 14:33:55.134: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 17 14:33:55.145: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-8940,SelfLink:/apis/apps/v1/namespaces/deployment-8940/deployments/test-rollover-deployment,UID:449c734c-62c9-44d5-90f9-426886c9272b,ResourceVersion:20045,Generation:2,CreationTimestamp:2019-12-17 14:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-17 14:33:39 +0000 UTC 2019-12-17 14:33:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-17 14:33:53 +0000 UTC 2019-12-17 14:33:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 17 14:33:55.150: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-8940,SelfLink:/apis/apps/v1/namespaces/deployment-8940/replicasets/test-rollover-deployment-854595fc44,UID:6e691e93-c398-4f35-9393-135bde7a1e13,ResourceVersion:20034,Generation:2,CreationTimestamp:2019-12-17 14:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 449c734c-62c9-44d5-90f9-426886c9272b 0xc002989197 0xc002989198}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 17 14:33:55.150: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 17 14:33:55.151: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-8940,SelfLink:/apis/apps/v1/namespaces/deployment-8940/replicasets/test-rollover-controller,UID:963f2507-9f2b-488a-a343-75922c817b80,ResourceVersion:20044,Generation:2,CreationTimestamp:2019-12-17 14:33:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 449c734c-62c9-44d5-90f9-426886c9272b 0xc0029890c7 0xc0029890c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 17 14:33:55.151: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-8940,SelfLink:/apis/apps/v1/namespaces/deployment-8940/replicasets/test-rollover-deployment-9b8b997cf,UID:3805d4af-6860-442c-87b0-8651d7f41598,ResourceVersion:19997,Generation:2,CreationTimestamp:2019-12-17 14:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 449c734c-62c9-44d5-90f9-426886c9272b 0xc002989260 0xc002989261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 17 14:33:55.156: INFO: Pod "test-rollover-deployment-854595fc44-wbmkm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-wbmkm,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-8940,SelfLink:/api/v1/namespaces/deployment-8940/pods/test-rollover-deployment-854595fc44-wbmkm,UID:a8881187-1bf2-4b09-a520-d9ade1721681,ResourceVersion:20012,Generation:0,CreationTimestamp:2019-12-17 14:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 6e691e93-c398-4f35-9393-135bde7a1e13 0xc003e54257 0xc003e54258}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjg2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjg2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-cjg2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003e542c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003e542e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:33:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:33:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:33:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:33:41 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:10.233.100.165,StartTime:2019-12-17 14:33:41 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-17 14:33:42 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://e9acd843e988441987ad40e88652f7223d83f38033219cbe614ce711f289bff7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:33:55.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8940" for this suite.
Dec 17 14:34:01.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:34:01.302: INFO: namespace deployment-8940 deletion completed in 6.139313661s

• [SLOW TEST:29.335 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:34:01.305: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-27bs
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 14:34:01.383: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-27bs" in namespace "subpath-8302" to be "success or failure"
Dec 17 14:34:01.386: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Pending", Reason="", readiness=false. Elapsed: 3.577926ms
Dec 17 14:34:03.391: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 2.008642623s
Dec 17 14:34:05.397: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 4.013848558s
Dec 17 14:34:07.402: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 6.019688029s
Dec 17 14:34:09.407: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 8.024174235s
Dec 17 14:34:11.420: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 10.037051805s
Dec 17 14:34:13.424: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 12.040885253s
Dec 17 14:34:15.429: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 14.045770666s
Dec 17 14:34:17.433: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 16.050204216s
Dec 17 14:34:19.442: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 18.058912205s
Dec 17 14:34:21.448: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Running", Reason="", readiness=true. Elapsed: 20.065473335s
Dec 17 14:34:23.453: INFO: Pod "pod-subpath-test-secret-27bs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.069802935s
STEP: Saw pod success
Dec 17 14:34:23.453: INFO: Pod "pod-subpath-test-secret-27bs" satisfied condition "success or failure"
Dec 17 14:34:23.461: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-subpath-test-secret-27bs container test-container-subpath-secret-27bs: <nil>
STEP: delete the pod
Dec 17 14:34:23.488: INFO: Waiting for pod pod-subpath-test-secret-27bs to disappear
Dec 17 14:34:23.492: INFO: Pod pod-subpath-test-secret-27bs no longer exists
STEP: Deleting pod pod-subpath-test-secret-27bs
Dec 17 14:34:23.493: INFO: Deleting pod "pod-subpath-test-secret-27bs" in namespace "subpath-8302"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:34:23.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8302" for this suite.
Dec 17 14:34:29.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:34:29.610: INFO: namespace subpath-8302 deletion completed in 6.111124962s

• [SLOW TEST:28.305 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:34:29.612: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 17 14:34:29.661: INFO: Waiting up to 5m0s for pod "pod-60102973-dd82-436f-9d02-da27c7d85ef0" in namespace "emptydir-5025" to be "success or failure"
Dec 17 14:34:29.667: INFO: Pod "pod-60102973-dd82-436f-9d02-da27c7d85ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.697797ms
Dec 17 14:34:31.672: INFO: Pod "pod-60102973-dd82-436f-9d02-da27c7d85ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010460715s
STEP: Saw pod success
Dec 17 14:34:31.673: INFO: Pod "pod-60102973-dd82-436f-9d02-da27c7d85ef0" satisfied condition "success or failure"
Dec 17 14:34:31.677: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-60102973-dd82-436f-9d02-da27c7d85ef0 container test-container: <nil>
STEP: delete the pod
Dec 17 14:34:31.696: INFO: Waiting for pod pod-60102973-dd82-436f-9d02-da27c7d85ef0 to disappear
Dec 17 14:34:31.704: INFO: Pod pod-60102973-dd82-436f-9d02-da27c7d85ef0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:34:31.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5025" for this suite.
Dec 17 14:34:37.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:34:37.814: INFO: namespace emptydir-5025 deletion completed in 6.105471629s

• [SLOW TEST:8.203 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:34:37.823: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 17 14:34:37.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-587'
Dec 17 14:34:37.953: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 17 14:34:37.953: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Dec 17 14:34:37.961: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec 17 14:34:37.966: INFO: scanned /root for discovery docs: <nil>
Dec 17 14:34:37.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-587'
Dec 17 14:34:53.811: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 17 14:34:53.811: INFO: stdout: "Created e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2\nScaling up e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec 17 14:34:53.811: INFO: stdout: "Created e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2\nScaling up e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec 17 14:34:53.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-587'
Dec 17 14:34:53.898: INFO: stderr: ""
Dec 17 14:34:53.898: INFO: stdout: "e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2-lwg54 "
Dec 17 14:34:53.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2-lwg54 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-587'
Dec 17 14:34:53.984: INFO: stderr: ""
Dec 17 14:34:53.984: INFO: stdout: "true"
Dec 17 14:34:53.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2-lwg54 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-587'
Dec 17 14:34:54.061: INFO: stderr: ""
Dec 17 14:34:54.061: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec 17 14:34:54.061: INFO: e2e-test-nginx-rc-205d427a8f3b5f99e4cfe87473fb68e2-lwg54 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Dec 17 14:34:54.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete rc e2e-test-nginx-rc --namespace=kubectl-587'
Dec 17 14:34:54.139: INFO: stderr: ""
Dec 17 14:34:54.139: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:34:54.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-587" for this suite.
Dec 17 14:35:00.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:35:00.274: INFO: namespace kubectl-587 deletion completed in 6.129959866s

• [SLOW TEST:22.452 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:35:00.276: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 17 14:35:08.384: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:08.391: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:10.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:10.397: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:12.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:12.396: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:14.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:14.397: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:16.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:16.396: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:18.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:18.396: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:20.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:20.397: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:22.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:22.401: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:24.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:24.397: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:26.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:26.397: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:28.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:28.396: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 14:35:30.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 14:35:30.395: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:35:30.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-300" for this suite.
Dec 17 14:35:52.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:35:52.538: INFO: namespace container-lifecycle-hook-300 deletion completed in 22.127846917s

• [SLOW TEST:52.262 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:35:52.538: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:35:52.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7987e998-1642-4c99-91fd-a7f926ee2982" in namespace "downward-api-7216" to be "success or failure"
Dec 17 14:35:52.593: INFO: Pod "downwardapi-volume-7987e998-1642-4c99-91fd-a7f926ee2982": Phase="Pending", Reason="", readiness=false. Elapsed: 6.792298ms
Dec 17 14:35:54.598: INFO: Pod "downwardapi-volume-7987e998-1642-4c99-91fd-a7f926ee2982": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011775384s
STEP: Saw pod success
Dec 17 14:35:54.598: INFO: Pod "downwardapi-volume-7987e998-1642-4c99-91fd-a7f926ee2982" satisfied condition "success or failure"
Dec 17 14:35:54.602: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-7987e998-1642-4c99-91fd-a7f926ee2982 container client-container: <nil>
STEP: delete the pod
Dec 17 14:35:54.639: INFO: Waiting for pod downwardapi-volume-7987e998-1642-4c99-91fd-a7f926ee2982 to disappear
Dec 17 14:35:54.644: INFO: Pod downwardapi-volume-7987e998-1642-4c99-91fd-a7f926ee2982 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:35:54.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7216" for this suite.
Dec 17 14:36:00.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:36:00.782: INFO: namespace downward-api-7216 deletion completed in 6.13263024s

• [SLOW TEST:8.244 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:36:00.785: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:36:06.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5690" for this suite.
Dec 17 14:36:13.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:36:13.138: INFO: namespace namespaces-5690 deletion completed in 6.166947233s
STEP: Destroying namespace "nsdeletetest-9378" for this suite.
Dec 17 14:36:13.141: INFO: Namespace nsdeletetest-9378 was already deleted
STEP: Destroying namespace "nsdeletetest-6386" for this suite.
Dec 17 14:36:19.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:36:19.247: INFO: namespace nsdeletetest-6386 deletion completed in 6.105996308s

• [SLOW TEST:18.464 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:36:19.249: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:36:19.299: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 17 14:36:21.340: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:36:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4195" for this suite.
Dec 17 14:36:28.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:36:28.467: INFO: namespace replication-controller-4195 deletion completed in 6.112589399s

• [SLOW TEST:9.218 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:36:28.468: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:36:28.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5b3539f-13be-41af-a39d-33aa4a8780dc" in namespace "downward-api-5951" to be "success or failure"
Dec 17 14:36:28.525: INFO: Pod "downwardapi-volume-f5b3539f-13be-41af-a39d-33aa4a8780dc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.439895ms
Dec 17 14:36:30.529: INFO: Pod "downwardapi-volume-f5b3539f-13be-41af-a39d-33aa4a8780dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021624836s
STEP: Saw pod success
Dec 17 14:36:30.529: INFO: Pod "downwardapi-volume-f5b3539f-13be-41af-a39d-33aa4a8780dc" satisfied condition "success or failure"
Dec 17 14:36:30.532: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-f5b3539f-13be-41af-a39d-33aa4a8780dc container client-container: <nil>
STEP: delete the pod
Dec 17 14:36:30.554: INFO: Waiting for pod downwardapi-volume-f5b3539f-13be-41af-a39d-33aa4a8780dc to disappear
Dec 17 14:36:30.559: INFO: Pod downwardapi-volume-f5b3539f-13be-41af-a39d-33aa4a8780dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:36:30.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5951" for this suite.
Dec 17 14:36:36.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:36:36.662: INFO: namespace downward-api-5951 deletion completed in 6.097509612s

• [SLOW TEST:8.194 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:36:36.669: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 17 14:36:36.721: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:36:40.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7674" for this suite.
Dec 17 14:36:58.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:36:58.772: INFO: namespace init-container-7674 deletion completed in 18.129900067s

• [SLOW TEST:22.104 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:36:58.775: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 17 14:36:58.827: INFO: Waiting up to 5m0s for pod "pod-728b500b-94fe-4ddd-9052-187c6a3891cc" in namespace "emptydir-9904" to be "success or failure"
Dec 17 14:36:58.837: INFO: Pod "pod-728b500b-94fe-4ddd-9052-187c6a3891cc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.875923ms
Dec 17 14:37:00.844: INFO: Pod "pod-728b500b-94fe-4ddd-9052-187c6a3891cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016609022s
Dec 17 14:37:02.850: INFO: Pod "pod-728b500b-94fe-4ddd-9052-187c6a3891cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022555781s
STEP: Saw pod success
Dec 17 14:37:02.850: INFO: Pod "pod-728b500b-94fe-4ddd-9052-187c6a3891cc" satisfied condition "success or failure"
Dec 17 14:37:02.853: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-728b500b-94fe-4ddd-9052-187c6a3891cc container test-container: <nil>
STEP: delete the pod
Dec 17 14:37:02.874: INFO: Waiting for pod pod-728b500b-94fe-4ddd-9052-187c6a3891cc to disappear
Dec 17 14:37:02.877: INFO: Pod pod-728b500b-94fe-4ddd-9052-187c6a3891cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:37:02.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9904" for this suite.
Dec 17 14:37:08.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:37:09.025: INFO: namespace emptydir-9904 deletion completed in 6.143249496s

• [SLOW TEST:10.251 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:37:09.029: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-ec8fe2f8-476c-4347-88b3-cd6ff8318858 in namespace container-probe-4072
Dec 17 14:37:13.092: INFO: Started pod test-webserver-ec8fe2f8-476c-4347-88b3-cd6ff8318858 in namespace container-probe-4072
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 14:37:13.096: INFO: Initial restart count of pod test-webserver-ec8fe2f8-476c-4347-88b3-cd6ff8318858 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:41:13.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4072" for this suite.
Dec 17 14:41:19.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:41:19.823: INFO: namespace container-probe-4072 deletion completed in 6.118339986s

• [SLOW TEST:250.794 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:41:19.825: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 17 14:41:23.946: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 17 14:41:23.949: INFO: Pod pod-with-poststart-http-hook still exists
Dec 17 14:41:25.949: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 17 14:41:25.953: INFO: Pod pod-with-poststart-http-hook still exists
Dec 17 14:41:27.949: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 17 14:41:27.953: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:41:27.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8090" for this suite.
Dec 17 14:41:49.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:41:50.078: INFO: namespace container-lifecycle-hook-8090 deletion completed in 22.11950041s

• [SLOW TEST:30.253 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:41:50.085: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-5nw7
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 14:41:50.155: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5nw7" in namespace "subpath-9600" to be "success or failure"
Dec 17 14:41:50.165: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016814ms
Dec 17 14:41:52.170: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014849946s
Dec 17 14:41:54.175: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 4.019898134s
Dec 17 14:41:56.182: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 6.026147765s
Dec 17 14:41:58.187: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 8.031471706s
Dec 17 14:42:00.190: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 10.035046827s
Dec 17 14:42:02.196: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 12.04056762s
Dec 17 14:42:04.211: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 14.055824853s
Dec 17 14:42:06.218: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 16.062253152s
Dec 17 14:42:08.227: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 18.071912004s
Dec 17 14:42:10.232: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 20.076837237s
Dec 17 14:42:12.235: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Running", Reason="", readiness=true. Elapsed: 22.079976338s
Dec 17 14:42:14.240: INFO: Pod "pod-subpath-test-projected-5nw7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.084732518s
STEP: Saw pod success
Dec 17 14:42:14.240: INFO: Pod "pod-subpath-test-projected-5nw7" satisfied condition "success or failure"
Dec 17 14:42:14.243: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-subpath-test-projected-5nw7 container test-container-subpath-projected-5nw7: <nil>
STEP: delete the pod
Dec 17 14:42:14.274: INFO: Waiting for pod pod-subpath-test-projected-5nw7 to disappear
Dec 17 14:42:14.278: INFO: Pod pod-subpath-test-projected-5nw7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-5nw7
Dec 17 14:42:14.278: INFO: Deleting pod "pod-subpath-test-projected-5nw7" in namespace "subpath-9600"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:42:14.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9600" for this suite.
Dec 17 14:42:20.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:42:20.399: INFO: namespace subpath-9600 deletion completed in 6.111702286s

• [SLOW TEST:30.314 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:42:20.400: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec 17 14:42:20.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-5603'
Dec 17 14:42:20.690: INFO: stderr: ""
Dec 17 14:42:20.690: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 14:42:20.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5603'
Dec 17 14:42:20.770: INFO: stderr: ""
Dec 17 14:42:20.770: INFO: stdout: "update-demo-nautilus-2pblb update-demo-nautilus-p7tsn "
Dec 17 14:42:20.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-2pblb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:20.840: INFO: stderr: ""
Dec 17 14:42:20.840: INFO: stdout: ""
Dec 17 14:42:20.840: INFO: update-demo-nautilus-2pblb is created but not running
Dec 17 14:42:25.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5603'
Dec 17 14:42:25.917: INFO: stderr: ""
Dec 17 14:42:25.917: INFO: stdout: "update-demo-nautilus-2pblb update-demo-nautilus-p7tsn "
Dec 17 14:42:25.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-2pblb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:25.998: INFO: stderr: ""
Dec 17 14:42:25.998: INFO: stdout: "true"
Dec 17 14:42:25.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-2pblb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:26.077: INFO: stderr: ""
Dec 17 14:42:26.077: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:42:26.077: INFO: validating pod update-demo-nautilus-2pblb
Dec 17 14:42:26.082: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:42:26.082: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:42:26.082: INFO: update-demo-nautilus-2pblb is verified up and running
Dec 17 14:42:26.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-p7tsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:26.155: INFO: stderr: ""
Dec 17 14:42:26.155: INFO: stdout: "true"
Dec 17 14:42:26.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-p7tsn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:26.214: INFO: stderr: ""
Dec 17 14:42:26.214: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:42:26.214: INFO: validating pod update-demo-nautilus-p7tsn
Dec 17 14:42:26.220: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:42:26.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:42:26.220: INFO: update-demo-nautilus-p7tsn is verified up and running
STEP: scaling down the replication controller
Dec 17 14:42:26.222: INFO: scanned /root for discovery docs: <nil>
Dec 17 14:42:26.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5603'
Dec 17 14:42:27.327: INFO: stderr: ""
Dec 17 14:42:27.327: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 14:42:27.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5603'
Dec 17 14:42:27.402: INFO: stderr: ""
Dec 17 14:42:27.402: INFO: stdout: "update-demo-nautilus-2pblb update-demo-nautilus-p7tsn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 17 14:42:32.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5603'
Dec 17 14:42:32.493: INFO: stderr: ""
Dec 17 14:42:32.493: INFO: stdout: "update-demo-nautilus-2pblb update-demo-nautilus-p7tsn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 17 14:42:37.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5603'
Dec 17 14:42:38.055: INFO: stderr: ""
Dec 17 14:42:38.055: INFO: stdout: "update-demo-nautilus-2pblb update-demo-nautilus-p7tsn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 17 14:42:43.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5603'
Dec 17 14:42:43.141: INFO: stderr: ""
Dec 17 14:42:43.141: INFO: stdout: "update-demo-nautilus-p7tsn "
Dec 17 14:42:43.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-p7tsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:43.228: INFO: stderr: ""
Dec 17 14:42:43.228: INFO: stdout: "true"
Dec 17 14:42:43.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-p7tsn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:43.317: INFO: stderr: ""
Dec 17 14:42:43.317: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:42:43.317: INFO: validating pod update-demo-nautilus-p7tsn
Dec 17 14:42:43.322: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:42:43.323: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:42:43.323: INFO: update-demo-nautilus-p7tsn is verified up and running
STEP: scaling up the replication controller
Dec 17 14:42:43.325: INFO: scanned /root for discovery docs: <nil>
Dec 17 14:42:43.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5603'
Dec 17 14:42:44.446: INFO: stderr: ""
Dec 17 14:42:44.446: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 14:42:44.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5603'
Dec 17 14:42:44.533: INFO: stderr: ""
Dec 17 14:42:44.533: INFO: stdout: "update-demo-nautilus-p7tsn update-demo-nautilus-tzl6b "
Dec 17 14:42:44.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-p7tsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:44.606: INFO: stderr: ""
Dec 17 14:42:44.606: INFO: stdout: "true"
Dec 17 14:42:44.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-p7tsn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:44.689: INFO: stderr: ""
Dec 17 14:42:44.689: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:42:44.689: INFO: validating pod update-demo-nautilus-p7tsn
Dec 17 14:42:44.693: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:42:44.693: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:42:44.693: INFO: update-demo-nautilus-p7tsn is verified up and running
Dec 17 14:42:44.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-tzl6b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:44.785: INFO: stderr: ""
Dec 17 14:42:44.785: INFO: stdout: ""
Dec 17 14:42:44.785: INFO: update-demo-nautilus-tzl6b is created but not running
Dec 17 14:42:49.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5603'
Dec 17 14:42:49.884: INFO: stderr: ""
Dec 17 14:42:49.884: INFO: stdout: "update-demo-nautilus-p7tsn update-demo-nautilus-tzl6b "
Dec 17 14:42:49.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-p7tsn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:49.955: INFO: stderr: ""
Dec 17 14:42:49.955: INFO: stdout: "true"
Dec 17 14:42:49.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-p7tsn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:50.043: INFO: stderr: ""
Dec 17 14:42:50.043: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:42:50.043: INFO: validating pod update-demo-nautilus-p7tsn
Dec 17 14:42:50.051: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:42:50.051: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:42:50.051: INFO: update-demo-nautilus-p7tsn is verified up and running
Dec 17 14:42:50.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-tzl6b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:50.130: INFO: stderr: ""
Dec 17 14:42:50.130: INFO: stdout: "true"
Dec 17 14:42:50.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-tzl6b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5603'
Dec 17 14:42:50.213: INFO: stderr: ""
Dec 17 14:42:50.213: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:42:50.213: INFO: validating pod update-demo-nautilus-tzl6b
Dec 17 14:42:50.219: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:42:50.219: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:42:50.219: INFO: update-demo-nautilus-tzl6b is verified up and running
STEP: using delete to clean up resources
Dec 17 14:42:50.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-5603'
Dec 17 14:42:50.292: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 14:42:50.292: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 17 14:42:50.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5603'
Dec 17 14:42:50.385: INFO: stderr: "No resources found.\n"
Dec 17 14:42:50.385: INFO: stdout: ""
Dec 17 14:42:50.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -l name=update-demo --namespace=kubectl-5603 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 14:42:50.461: INFO: stderr: ""
Dec 17 14:42:50.461: INFO: stdout: "update-demo-nautilus-p7tsn\nupdate-demo-nautilus-tzl6b\n"
Dec 17 14:42:50.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5603'
Dec 17 14:42:51.041: INFO: stderr: "No resources found.\n"
Dec 17 14:42:51.041: INFO: stdout: ""
Dec 17 14:42:51.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -l name=update-demo --namespace=kubectl-5603 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 14:42:51.138: INFO: stderr: ""
Dec 17 14:42:51.138: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:42:51.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5603" for this suite.
Dec 17 14:43:13.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:43:13.287: INFO: namespace kubectl-5603 deletion completed in 22.137963482s

• [SLOW TEST:52.887 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:43:13.289: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:43:13.338: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:43:17.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7136" for this suite.
Dec 17 14:44:01.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:44:01.665: INFO: namespace pods-7136 deletion completed in 44.144336375s

• [SLOW TEST:48.376 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:44:01.668: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-3ffe5258-dc13-4bd6-95fe-f5a57b9d6d9f
STEP: Creating a pod to test consume configMaps
Dec 17 14:44:01.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-9635a49b-1f58-46d7-a39f-ba6ac62d1062" in namespace "configmap-7042" to be "success or failure"
Dec 17 14:44:01.758: INFO: Pod "pod-configmaps-9635a49b-1f58-46d7-a39f-ba6ac62d1062": Phase="Pending", Reason="", readiness=false. Elapsed: 6.840152ms
Dec 17 14:44:03.763: INFO: Pod "pod-configmaps-9635a49b-1f58-46d7-a39f-ba6ac62d1062": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011833948s
Dec 17 14:44:05.766: INFO: Pod "pod-configmaps-9635a49b-1f58-46d7-a39f-ba6ac62d1062": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01524894s
STEP: Saw pod success
Dec 17 14:44:05.766: INFO: Pod "pod-configmaps-9635a49b-1f58-46d7-a39f-ba6ac62d1062" satisfied condition "success or failure"
Dec 17 14:44:05.768: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-9635a49b-1f58-46d7-a39f-ba6ac62d1062 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 14:44:05.793: INFO: Waiting for pod pod-configmaps-9635a49b-1f58-46d7-a39f-ba6ac62d1062 to disappear
Dec 17 14:44:05.796: INFO: Pod pod-configmaps-9635a49b-1f58-46d7-a39f-ba6ac62d1062 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:44:05.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7042" for this suite.
Dec 17 14:44:11.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:44:11.936: INFO: namespace configmap-7042 deletion completed in 6.135105724s

• [SLOW TEST:10.277 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:44:11.948: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Dec 17 14:44:16.017: INFO: Pod pod-hostip-20f98cc4-40e2-4bca-b8da-4e16bdd6dd3a has hostIP: 192.168.1.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:44:16.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5066" for this suite.
Dec 17 14:44:38.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:44:38.138: INFO: namespace pods-5066 deletion completed in 22.117014907s

• [SLOW TEST:26.190 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:44:38.140: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-9422
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9422 to expose endpoints map[]
Dec 17 14:44:38.210: INFO: Get endpoints failed (4.926423ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 17 14:44:39.216: INFO: successfully validated that service multi-endpoint-test in namespace services-9422 exposes endpoints map[] (1.0107083s elapsed)
STEP: Creating pod pod1 in namespace services-9422
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9422 to expose endpoints map[pod1:[100]]
Dec 17 14:44:41.256: INFO: successfully validated that service multi-endpoint-test in namespace services-9422 exposes endpoints map[pod1:[100]] (2.029982974s elapsed)
STEP: Creating pod pod2 in namespace services-9422
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9422 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 17 14:44:43.350: INFO: successfully validated that service multi-endpoint-test in namespace services-9422 exposes endpoints map[pod1:[100] pod2:[101]] (2.087393109s elapsed)
STEP: Deleting pod pod1 in namespace services-9422
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9422 to expose endpoints map[pod2:[101]]
Dec 17 14:44:44.374: INFO: successfully validated that service multi-endpoint-test in namespace services-9422 exposes endpoints map[pod2:[101]] (1.017356038s elapsed)
STEP: Deleting pod pod2 in namespace services-9422
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9422 to expose endpoints map[]
Dec 17 14:44:45.393: INFO: successfully validated that service multi-endpoint-test in namespace services-9422 exposes endpoints map[] (1.00803062s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:44:45.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9422" for this suite.
Dec 17 14:45:07.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:45:07.560: INFO: namespace services-9422 deletion completed in 22.13603517s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:29.420 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:45:07.562: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:45:07.638: INFO: (0) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.64207ms)
Dec 17 14:45:07.644: INFO: (1) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.121791ms)
Dec 17 14:45:07.648: INFO: (2) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.223981ms)
Dec 17 14:45:07.652: INFO: (3) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.307137ms)
Dec 17 14:45:07.656: INFO: (4) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.158682ms)
Dec 17 14:45:07.661: INFO: (5) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.457557ms)
Dec 17 14:45:07.665: INFO: (6) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.224353ms)
Dec 17 14:45:07.669: INFO: (7) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.905036ms)
Dec 17 14:45:07.673: INFO: (8) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.489032ms)
Dec 17 14:45:07.678: INFO: (9) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.7642ms)
Dec 17 14:45:07.684: INFO: (10) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.651262ms)
Dec 17 14:45:07.688: INFO: (11) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.375211ms)
Dec 17 14:45:07.693: INFO: (12) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.243517ms)
Dec 17 14:45:07.697: INFO: (13) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.821427ms)
Dec 17 14:45:07.701: INFO: (14) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.722067ms)
Dec 17 14:45:07.706: INFO: (15) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.644767ms)
Dec 17 14:45:07.710: INFO: (16) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.689317ms)
Dec 17 14:45:07.724: INFO: (17) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.974107ms)
Dec 17 14:45:07.729: INFO: (18) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.111208ms)
Dec 17 14:45:07.733: INFO: (19) /api/v1/nodes/conformance-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.383823ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:45:07.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5093" for this suite.
Dec 17 14:45:13.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:45:13.844: INFO: namespace proxy-5093 deletion completed in 6.105446044s

• [SLOW TEST:6.282 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:45:13.845: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 17 14:45:13.897: INFO: Waiting up to 5m0s for pod "pod-2de0cc1c-1a16-4f86-b153-7c8d06cd3b8d" in namespace "emptydir-4737" to be "success or failure"
Dec 17 14:45:13.903: INFO: Pod "pod-2de0cc1c-1a16-4f86-b153-7c8d06cd3b8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346149ms
Dec 17 14:45:15.915: INFO: Pod "pod-2de0cc1c-1a16-4f86-b153-7c8d06cd3b8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017860923s
Dec 17 14:45:17.927: INFO: Pod "pod-2de0cc1c-1a16-4f86-b153-7c8d06cd3b8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029881281s
STEP: Saw pod success
Dec 17 14:45:17.927: INFO: Pod "pod-2de0cc1c-1a16-4f86-b153-7c8d06cd3b8d" satisfied condition "success or failure"
Dec 17 14:45:17.931: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-2de0cc1c-1a16-4f86-b153-7c8d06cd3b8d container test-container: <nil>
STEP: delete the pod
Dec 17 14:45:17.954: INFO: Waiting for pod pod-2de0cc1c-1a16-4f86-b153-7c8d06cd3b8d to disappear
Dec 17 14:45:17.958: INFO: Pod pod-2de0cc1c-1a16-4f86-b153-7c8d06cd3b8d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:45:17.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4737" for this suite.
Dec 17 14:45:23.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:45:24.084: INFO: namespace emptydir-4737 deletion completed in 6.119878857s

• [SLOW TEST:10.239 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:45:24.086: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 17 14:45:28.142: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-96019ac1-20fd-4da7-ba9f-8bb31e90c777,GenerateName:,Namespace:events-8214,SelfLink:/api/v1/namespaces/events-8214/pods/send-events-96019ac1-20fd-4da7-ba9f-8bb31e90c777,UID:966d2e0e-f5c8-40e6-9867-d7018e47bf63,ResourceVersion:22397,Generation:0,CreationTimestamp:2019-12-17 14:45:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 120505967,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-57sxg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-57sxg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-57sxg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023386d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023386f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:45:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:45:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:45:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-17 14:45:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:10.233.100.188,StartTime:2019-12-17 14:45:24 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-17 14:45:25 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://aca38239272716d78ddf8e96dd16642eb12fe5dabaf1e015ca29103d8418c487}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec 17 14:45:30.153: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 17 14:45:32.159: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:45:32.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8214" for this suite.
Dec 17 14:46:10.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:46:10.278: INFO: namespace events-8214 deletion completed in 38.108768492s

• [SLOW TEST:46.192 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:46:10.278: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:46:10.366: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9270992e-3266-43d8-9b99-64aef4cf9891", Controller:(*bool)(0xc003f9c606), BlockOwnerDeletion:(*bool)(0xc003f9c607)}}
Dec 17 14:46:10.378: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b5963f17-05d3-457a-9868-ce3ea8fadf93", Controller:(*bool)(0xc002912956), BlockOwnerDeletion:(*bool)(0xc002912957)}}
Dec 17 14:46:10.383: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f1698dd9-0535-4320-8a51-bad2242c14f0", Controller:(*bool)(0xc003f9c7d6), BlockOwnerDeletion:(*bool)(0xc003f9c7d7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:46:15.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6456" for this suite.
Dec 17 14:46:21.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:46:21.512: INFO: namespace gc-6456 deletion completed in 6.107956746s

• [SLOW TEST:11.233 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:46:21.513: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Dec 17 14:46:21.563: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-643038348 proxy --unix-socket=/tmp/kubectl-proxy-unix336307679/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:46:21.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8572" for this suite.
Dec 17 14:46:27.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:46:27.752: INFO: namespace kubectl-8572 deletion completed in 6.127260776s

• [SLOW TEST:6.239 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:46:27.754: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 17 14:46:27.808: INFO: Waiting up to 5m0s for pod "pod-fa560d00-e0c6-4b43-a7d1-847f125ba4dd" in namespace "emptydir-3447" to be "success or failure"
Dec 17 14:46:27.813: INFO: Pod "pod-fa560d00-e0c6-4b43-a7d1-847f125ba4dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525028ms
Dec 17 14:46:29.823: INFO: Pod "pod-fa560d00-e0c6-4b43-a7d1-847f125ba4dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014896297s
STEP: Saw pod success
Dec 17 14:46:29.823: INFO: Pod "pod-fa560d00-e0c6-4b43-a7d1-847f125ba4dd" satisfied condition "success or failure"
Dec 17 14:46:29.828: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-fa560d00-e0c6-4b43-a7d1-847f125ba4dd container test-container: <nil>
STEP: delete the pod
Dec 17 14:46:29.863: INFO: Waiting for pod pod-fa560d00-e0c6-4b43-a7d1-847f125ba4dd to disappear
Dec 17 14:46:29.866: INFO: Pod pod-fa560d00-e0c6-4b43-a7d1-847f125ba4dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:46:29.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3447" for this suite.
Dec 17 14:46:35.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:46:35.997: INFO: namespace emptydir-3447 deletion completed in 6.126067987s

• [SLOW TEST:8.243 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:46:35.997: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 17 14:46:36.057: INFO: Waiting up to 5m0s for pod "pod-9939a68b-de9a-4b00-8f7e-2c139a539b61" in namespace "emptydir-2723" to be "success or failure"
Dec 17 14:46:36.072: INFO: Pod "pod-9939a68b-de9a-4b00-8f7e-2c139a539b61": Phase="Pending", Reason="", readiness=false. Elapsed: 15.438893ms
Dec 17 14:46:38.079: INFO: Pod "pod-9939a68b-de9a-4b00-8f7e-2c139a539b61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02218834s
STEP: Saw pod success
Dec 17 14:46:38.079: INFO: Pod "pod-9939a68b-de9a-4b00-8f7e-2c139a539b61" satisfied condition "success or failure"
Dec 17 14:46:38.084: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-9939a68b-de9a-4b00-8f7e-2c139a539b61 container test-container: <nil>
STEP: delete the pod
Dec 17 14:46:38.118: INFO: Waiting for pod pod-9939a68b-de9a-4b00-8f7e-2c139a539b61 to disappear
Dec 17 14:46:38.122: INFO: Pod pod-9939a68b-de9a-4b00-8f7e-2c139a539b61 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:46:38.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2723" for this suite.
Dec 17 14:46:44.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:46:44.258: INFO: namespace emptydir-2723 deletion completed in 6.131029047s

• [SLOW TEST:8.261 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:46:44.258: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:46:48.399: INFO: Waiting up to 5m0s for pod "client-envvars-42b44e14-66af-4ce2-b561-5a7eca837af8" in namespace "pods-334" to be "success or failure"
Dec 17 14:46:48.405: INFO: Pod "client-envvars-42b44e14-66af-4ce2-b561-5a7eca837af8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452698ms
Dec 17 14:46:50.410: INFO: Pod "client-envvars-42b44e14-66af-4ce2-b561-5a7eca837af8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010160014s
Dec 17 14:46:52.414: INFO: Pod "client-envvars-42b44e14-66af-4ce2-b561-5a7eca837af8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015045392s
STEP: Saw pod success
Dec 17 14:46:52.415: INFO: Pod "client-envvars-42b44e14-66af-4ce2-b561-5a7eca837af8" satisfied condition "success or failure"
Dec 17 14:46:52.417: INFO: Trying to get logs from node conformance-k8s-node-2 pod client-envvars-42b44e14-66af-4ce2-b561-5a7eca837af8 container env3cont: <nil>
STEP: delete the pod
Dec 17 14:46:52.461: INFO: Waiting for pod client-envvars-42b44e14-66af-4ce2-b561-5a7eca837af8 to disappear
Dec 17 14:46:52.464: INFO: Pod client-envvars-42b44e14-66af-4ce2-b561-5a7eca837af8 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:46:52.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-334" for this suite.
Dec 17 14:47:42.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:47:42.618: INFO: namespace pods-334 deletion completed in 50.150269868s

• [SLOW TEST:58.359 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:47:42.621: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 17 14:47:45.239: INFO: Successfully updated pod "annotationupdate583bbd4c-5593-4e15-bdf3-b4a022a13fc1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:47:47.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3303" for this suite.
Dec 17 14:48:09.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:48:09.383: INFO: namespace downward-api-3303 deletion completed in 22.113341162s

• [SLOW TEST:26.762 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:48:09.384: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-65fbab2d-03a6-4046-ac97-d7ccc82e2b3b
STEP: Creating a pod to test consume configMaps
Dec 17 14:48:09.445: INFO: Waiting up to 5m0s for pod "pod-configmaps-83ff1c01-7923-4ca7-8217-5bfe5c065e34" in namespace "configmap-9301" to be "success or failure"
Dec 17 14:48:09.449: INFO: Pod "pod-configmaps-83ff1c01-7923-4ca7-8217-5bfe5c065e34": Phase="Pending", Reason="", readiness=false. Elapsed: 3.819793ms
Dec 17 14:48:11.454: INFO: Pod "pod-configmaps-83ff1c01-7923-4ca7-8217-5bfe5c065e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00913736s
STEP: Saw pod success
Dec 17 14:48:11.454: INFO: Pod "pod-configmaps-83ff1c01-7923-4ca7-8217-5bfe5c065e34" satisfied condition "success or failure"
Dec 17 14:48:11.457: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-83ff1c01-7923-4ca7-8217-5bfe5c065e34 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 14:48:11.492: INFO: Waiting for pod pod-configmaps-83ff1c01-7923-4ca7-8217-5bfe5c065e34 to disappear
Dec 17 14:48:11.495: INFO: Pod pod-configmaps-83ff1c01-7923-4ca7-8217-5bfe5c065e34 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:48:11.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9301" for this suite.
Dec 17 14:48:17.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:48:17.643: INFO: namespace configmap-9301 deletion completed in 6.142840154s

• [SLOW TEST:8.259 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:48:17.644: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 14:48:20.719: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:48:20.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1765" for this suite.
Dec 17 14:48:26.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:48:26.843: INFO: namespace container-runtime-1765 deletion completed in 6.101825174s

• [SLOW TEST:9.200 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:48:26.843: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 17 14:48:26.905: INFO: Waiting up to 5m0s for pod "pod-bdeb2c19-f9a0-4f08-b23b-2d16398a31cc" in namespace "emptydir-8207" to be "success or failure"
Dec 17 14:48:26.908: INFO: Pod "pod-bdeb2c19-f9a0-4f08-b23b-2d16398a31cc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.787037ms
Dec 17 14:48:28.913: INFO: Pod "pod-bdeb2c19-f9a0-4f08-b23b-2d16398a31cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008497597s
Dec 17 14:48:30.919: INFO: Pod "pod-bdeb2c19-f9a0-4f08-b23b-2d16398a31cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013848963s
STEP: Saw pod success
Dec 17 14:48:30.919: INFO: Pod "pod-bdeb2c19-f9a0-4f08-b23b-2d16398a31cc" satisfied condition "success or failure"
Dec 17 14:48:30.922: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-bdeb2c19-f9a0-4f08-b23b-2d16398a31cc container test-container: <nil>
STEP: delete the pod
Dec 17 14:48:30.948: INFO: Waiting for pod pod-bdeb2c19-f9a0-4f08-b23b-2d16398a31cc to disappear
Dec 17 14:48:30.951: INFO: Pod pod-bdeb2c19-f9a0-4f08-b23b-2d16398a31cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:48:30.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8207" for this suite.
Dec 17 14:48:36.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:48:37.093: INFO: namespace emptydir-8207 deletion completed in 6.136394401s

• [SLOW TEST:10.249 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:48:37.095: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 17 14:48:37.180: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:48:37.185: INFO: Number of nodes with available pods: 0
Dec 17 14:48:37.185: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 14:48:38.192: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:48:38.204: INFO: Number of nodes with available pods: 0
Dec 17 14:48:38.204: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 14:48:39.190: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:48:39.195: INFO: Number of nodes with available pods: 0
Dec 17 14:48:39.195: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 14:48:40.191: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:48:40.195: INFO: Number of nodes with available pods: 3
Dec 17 14:48:40.195: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 17 14:48:40.215: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:48:40.226: INFO: Number of nodes with available pods: 2
Dec 17 14:48:40.226: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 14:48:41.234: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:48:41.238: INFO: Number of nodes with available pods: 2
Dec 17 14:48:41.239: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 14:48:42.232: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:48:42.236: INFO: Number of nodes with available pods: 2
Dec 17 14:48:42.236: INFO: Node conformance-k8s-node-1 is running more than one daemon pod
Dec 17 14:48:43.233: INFO: DaemonSet pods can't tolerate node conformance-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 14:48:43.237: INFO: Number of nodes with available pods: 3
Dec 17 14:48:43.237: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9850, will wait for the garbage collector to delete the pods
Dec 17 14:48:43.306: INFO: Deleting DaemonSet.extensions daemon-set took: 10.227593ms
Dec 17 14:48:43.606: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.304809ms
Dec 17 14:48:49.510: INFO: Number of nodes with available pods: 0
Dec 17 14:48:49.510: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 14:48:49.514: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9850/daemonsets","resourceVersion":"23223"},"items":null}

Dec 17 14:48:49.518: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9850/pods","resourceVersion":"23223"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:48:49.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9850" for this suite.
Dec 17 14:48:55.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:48:55.668: INFO: namespace daemonsets-9850 deletion completed in 6.126269331s

• [SLOW TEST:18.573 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:48:55.668: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 17 14:48:58.255: INFO: Successfully updated pod "pod-update-09d64735-f268-46eb-b973-153ceec29a85"
STEP: verifying the updated pod is in kubernetes
Dec 17 14:48:58.267: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:48:58.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4500" for this suite.
Dec 17 14:49:20.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:49:20.392: INFO: namespace pods-4500 deletion completed in 22.118376348s

• [SLOW TEST:24.724 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:49:20.396: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8989, will wait for the garbage collector to delete the pods
Dec 17 14:49:22.506: INFO: Deleting Job.batch foo took: 7.964384ms
Dec 17 14:49:22.807: INFO: Terminating Job.batch foo pods took: 300.26711ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:50:09.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8989" for this suite.
Dec 17 14:50:15.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:50:15.658: INFO: namespace job-8989 deletion completed in 6.141832349s

• [SLOW TEST:55.262 seconds]
[sig-apps] Job
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:50:15.660: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:50:15.707: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:50:17.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2679" for this suite.
Dec 17 14:50:55.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:50:55.892: INFO: namespace pods-2679 deletion completed in 38.135143293s

• [SLOW TEST:40.232 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:50:55.893: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Dec 17 14:50:56.457: INFO: created pod pod-service-account-defaultsa
Dec 17 14:50:56.457: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 17 14:50:56.464: INFO: created pod pod-service-account-mountsa
Dec 17 14:50:56.464: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 17 14:50:56.480: INFO: created pod pod-service-account-nomountsa
Dec 17 14:50:56.480: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 17 14:50:56.485: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 17 14:50:56.485: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 17 14:50:56.498: INFO: created pod pod-service-account-mountsa-mountspec
Dec 17 14:50:56.498: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 17 14:50:56.506: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 17 14:50:56.506: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 17 14:50:56.524: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 17 14:50:56.524: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 17 14:50:56.532: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 17 14:50:56.532: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 17 14:50:56.542: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 17 14:50:56.542: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:50:56.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8637" for this suite.
Dec 17 14:51:02.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:51:02.698: INFO: namespace svcaccounts-8637 deletion completed in 6.143067054s

• [SLOW TEST:6.805 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:51:02.698: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 14:51:04.775: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:51:04.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9069" for this suite.
Dec 17 14:51:10.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:51:10.925: INFO: namespace container-runtime-9069 deletion completed in 6.124278368s

• [SLOW TEST:8.227 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:51:10.928: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:51:13.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5760" for this suite.
Dec 17 14:52:03.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:52:03.159: INFO: namespace kubelet-test-5760 deletion completed in 50.144292509s

• [SLOW TEST:52.231 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:52:03.159: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 17 14:52:09.281: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:09.286: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:11.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:11.293: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:13.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:13.294: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:15.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:15.293: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:17.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:17.292: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:19.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:19.293: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:21.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:21.293: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:23.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:23.291: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:25.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:25.292: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:27.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:27.292: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:29.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:29.297: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 14:52:31.287: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 14:52:31.292: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:52:31.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9714" for this suite.
Dec 17 14:52:53.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:52:53.423: INFO: namespace container-lifecycle-hook-9714 deletion completed in 22.125997743s

• [SLOW TEST:50.265 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:52:53.424: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Dec 17 14:52:53.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-2544'
Dec 17 14:52:54.154: INFO: stderr: ""
Dec 17 14:52:54.154: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 14:52:54.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2544'
Dec 17 14:52:54.244: INFO: stderr: ""
Dec 17 14:52:54.244: INFO: stdout: "update-demo-nautilus-26d4l update-demo-nautilus-mggxk "
Dec 17 14:52:54.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-26d4l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2544'
Dec 17 14:52:54.330: INFO: stderr: ""
Dec 17 14:52:54.330: INFO: stdout: ""
Dec 17 14:52:54.330: INFO: update-demo-nautilus-26d4l is created but not running
Dec 17 14:52:59.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2544'
Dec 17 14:52:59.434: INFO: stderr: ""
Dec 17 14:52:59.434: INFO: stdout: "update-demo-nautilus-26d4l update-demo-nautilus-mggxk "
Dec 17 14:52:59.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-26d4l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2544'
Dec 17 14:52:59.511: INFO: stderr: ""
Dec 17 14:52:59.511: INFO: stdout: "true"
Dec 17 14:52:59.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-26d4l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2544'
Dec 17 14:52:59.579: INFO: stderr: ""
Dec 17 14:52:59.579: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:52:59.579: INFO: validating pod update-demo-nautilus-26d4l
Dec 17 14:52:59.586: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:52:59.586: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:52:59.586: INFO: update-demo-nautilus-26d4l is verified up and running
Dec 17 14:52:59.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-mggxk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2544'
Dec 17 14:52:59.655: INFO: stderr: ""
Dec 17 14:52:59.655: INFO: stdout: "true"
Dec 17 14:52:59.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-nautilus-mggxk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2544'
Dec 17 14:52:59.737: INFO: stderr: ""
Dec 17 14:52:59.737: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 14:52:59.737: INFO: validating pod update-demo-nautilus-mggxk
Dec 17 14:52:59.743: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 14:52:59.743: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 14:52:59.743: INFO: update-demo-nautilus-mggxk is verified up and running
STEP: rolling-update to new replication controller
Dec 17 14:52:59.745: INFO: scanned /root for discovery docs: <nil>
Dec 17 14:52:59.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2544'
Dec 17 14:53:22.255: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 17 14:53:22.255: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 14:53:22.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2544'
Dec 17 14:53:22.346: INFO: stderr: ""
Dec 17 14:53:22.346: INFO: stdout: "update-demo-kitten-9d4bp update-demo-kitten-mh4pb "
Dec 17 14:53:22.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-kitten-9d4bp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2544'
Dec 17 14:53:22.416: INFO: stderr: ""
Dec 17 14:53:22.416: INFO: stdout: "true"
Dec 17 14:53:22.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-kitten-9d4bp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2544'
Dec 17 14:53:22.497: INFO: stderr: ""
Dec 17 14:53:22.497: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 17 14:53:22.497: INFO: validating pod update-demo-kitten-9d4bp
Dec 17 14:53:22.511: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 17 14:53:22.511: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 17 14:53:22.511: INFO: update-demo-kitten-9d4bp is verified up and running
Dec 17 14:53:22.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-kitten-mh4pb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2544'
Dec 17 14:53:22.591: INFO: stderr: ""
Dec 17 14:53:22.592: INFO: stdout: "true"
Dec 17 14:53:22.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods update-demo-kitten-mh4pb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2544'
Dec 17 14:53:22.679: INFO: stderr: ""
Dec 17 14:53:22.679: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 17 14:53:22.679: INFO: validating pod update-demo-kitten-mh4pb
Dec 17 14:53:22.685: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 17 14:53:22.685: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 17 14:53:22.685: INFO: update-demo-kitten-mh4pb is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:53:22.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2544" for this suite.
Dec 17 14:53:44.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:53:44.799: INFO: namespace kubectl-2544 deletion completed in 22.108694246s

• [SLOW TEST:51.375 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:53:44.799: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-8dbe168d-cbf9-4bc2-b6d8-ec49d0ef62f7
STEP: Creating a pod to test consume secrets
Dec 17 14:53:44.860: INFO: Waiting up to 5m0s for pod "pod-secrets-7a29aa2e-fbfb-4893-bec8-31ef4e93edcd" in namespace "secrets-1011" to be "success or failure"
Dec 17 14:53:44.868: INFO: Pod "pod-secrets-7a29aa2e-fbfb-4893-bec8-31ef4e93edcd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.596572ms
Dec 17 14:53:46.873: INFO: Pod "pod-secrets-7a29aa2e-fbfb-4893-bec8-31ef4e93edcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012682354s
STEP: Saw pod success
Dec 17 14:53:46.873: INFO: Pod "pod-secrets-7a29aa2e-fbfb-4893-bec8-31ef4e93edcd" satisfied condition "success or failure"
Dec 17 14:53:46.877: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-secrets-7a29aa2e-fbfb-4893-bec8-31ef4e93edcd container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 14:53:46.902: INFO: Waiting for pod pod-secrets-7a29aa2e-fbfb-4893-bec8-31ef4e93edcd to disappear
Dec 17 14:53:46.905: INFO: Pod pod-secrets-7a29aa2e-fbfb-4893-bec8-31ef4e93edcd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:53:46.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1011" for this suite.
Dec 17 14:53:52.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:53:53.023: INFO: namespace secrets-1011 deletion completed in 6.113906821s

• [SLOW TEST:8.225 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:53:53.024: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 17 14:53:53.084: INFO: Waiting up to 5m0s for pod "pod-7db61a2f-8adc-4c39-a3ff-ea6280826452" in namespace "emptydir-57" to be "success or failure"
Dec 17 14:53:53.089: INFO: Pod "pod-7db61a2f-8adc-4c39-a3ff-ea6280826452": Phase="Pending", Reason="", readiness=false. Elapsed: 4.558312ms
Dec 17 14:53:55.093: INFO: Pod "pod-7db61a2f-8adc-4c39-a3ff-ea6280826452": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008415672s
STEP: Saw pod success
Dec 17 14:53:55.093: INFO: Pod "pod-7db61a2f-8adc-4c39-a3ff-ea6280826452" satisfied condition "success or failure"
Dec 17 14:53:55.095: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-7db61a2f-8adc-4c39-a3ff-ea6280826452 container test-container: <nil>
STEP: delete the pod
Dec 17 14:53:55.123: INFO: Waiting for pod pod-7db61a2f-8adc-4c39-a3ff-ea6280826452 to disappear
Dec 17 14:53:55.127: INFO: Pod pod-7db61a2f-8adc-4c39-a3ff-ea6280826452 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:53:55.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-57" for this suite.
Dec 17 14:54:01.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:54:01.248: INFO: namespace emptydir-57 deletion completed in 6.116119998s

• [SLOW TEST:8.224 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:54:01.249: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Dec 17 14:54:01.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 --namespace=kubectl-6281 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 17 14:54:03.433: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 17 14:54:03.433: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:54:05.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6281" for this suite.
Dec 17 14:54:11.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:54:11.559: INFO: namespace kubectl-6281 deletion completed in 6.111044465s

• [SLOW TEST:10.310 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:54:11.560: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-5a85ebf4-f51a-4765-86a0-fca64ad78e7f
STEP: Creating a pod to test consume secrets
Dec 17 14:54:11.622: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-73028a0f-7038-436b-96f0-182b0b6d088b" in namespace "projected-2058" to be "success or failure"
Dec 17 14:54:11.629: INFO: Pod "pod-projected-secrets-73028a0f-7038-436b-96f0-182b0b6d088b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.176566ms
Dec 17 14:54:13.633: INFO: Pod "pod-projected-secrets-73028a0f-7038-436b-96f0-182b0b6d088b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011745805s
STEP: Saw pod success
Dec 17 14:54:13.634: INFO: Pod "pod-projected-secrets-73028a0f-7038-436b-96f0-182b0b6d088b" satisfied condition "success or failure"
Dec 17 14:54:13.637: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-secrets-73028a0f-7038-436b-96f0-182b0b6d088b container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 14:54:13.662: INFO: Waiting for pod pod-projected-secrets-73028a0f-7038-436b-96f0-182b0b6d088b to disappear
Dec 17 14:54:13.665: INFO: Pod pod-projected-secrets-73028a0f-7038-436b-96f0-182b0b6d088b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:54:13.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2058" for this suite.
Dec 17 14:54:19.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:54:19.773: INFO: namespace projected-2058 deletion completed in 6.103581242s

• [SLOW TEST:8.213 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:54:19.774: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 17 14:54:27.854: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:27.855: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:28.013: INFO: Exec stderr: ""
Dec 17 14:54:28.013: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:28.013: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:28.119: INFO: Exec stderr: ""
Dec 17 14:54:28.119: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:28.119: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:28.254: INFO: Exec stderr: ""
Dec 17 14:54:28.254: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:28.254: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:28.398: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 17 14:54:28.398: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:28.398: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:28.556: INFO: Exec stderr: ""
Dec 17 14:54:28.556: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:28.556: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:28.717: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 17 14:54:28.717: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:28.717: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:28.839: INFO: Exec stderr: ""
Dec 17 14:54:28.839: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:28.839: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:29.012: INFO: Exec stderr: ""
Dec 17 14:54:29.012: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:29.012: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:29.169: INFO: Exec stderr: ""
Dec 17 14:54:29.169: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9393 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 14:54:29.169: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
Dec 17 14:54:29.332: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:54:29.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9393" for this suite.
Dec 17 14:55:11.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:55:11.451: INFO: namespace e2e-kubelet-etc-hosts-9393 deletion completed in 42.114744717s

• [SLOW TEST:51.677 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:55:11.451: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-fa124296-9bbf-4b70-8f01-38b1184ee086
STEP: Creating a pod to test consume secrets
Dec 17 14:55:11.514: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6a9fe116-91c6-47b6-986a-63f3505fd96e" in namespace "projected-81" to be "success or failure"
Dec 17 14:55:11.518: INFO: Pod "pod-projected-secrets-6a9fe116-91c6-47b6-986a-63f3505fd96e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.720801ms
Dec 17 14:55:13.523: INFO: Pod "pod-projected-secrets-6a9fe116-91c6-47b6-986a-63f3505fd96e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00926873s
STEP: Saw pod success
Dec 17 14:55:13.524: INFO: Pod "pod-projected-secrets-6a9fe116-91c6-47b6-986a-63f3505fd96e" satisfied condition "success or failure"
Dec 17 14:55:13.527: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-secrets-6a9fe116-91c6-47b6-986a-63f3505fd96e container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 14:55:13.555: INFO: Waiting for pod pod-projected-secrets-6a9fe116-91c6-47b6-986a-63f3505fd96e to disappear
Dec 17 14:55:13.558: INFO: Pod pod-projected-secrets-6a9fe116-91c6-47b6-986a-63f3505fd96e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:55:13.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-81" for this suite.
Dec 17 14:55:19.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:55:19.706: INFO: namespace projected-81 deletion completed in 6.143453843s

• [SLOW TEST:8.255 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:55:19.707: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 17 14:55:19.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4691'
Dec 17 14:55:19.847: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 17 14:55:19.847: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec 17 14:55:19.859: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-lb9k4]
Dec 17 14:55:19.859: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-lb9k4" in namespace "kubectl-4691" to be "running and ready"
Dec 17 14:55:19.863: INFO: Pod "e2e-test-nginx-rc-lb9k4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060682ms
Dec 17 14:55:21.871: INFO: Pod "e2e-test-nginx-rc-lb9k4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01178923s
Dec 17 14:55:21.871: INFO: Pod "e2e-test-nginx-rc-lb9k4" satisfied condition "running and ready"
Dec 17 14:55:21.871: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-lb9k4]
Dec 17 14:55:21.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 logs rc/e2e-test-nginx-rc --namespace=kubectl-4691'
Dec 17 14:55:22.000: INFO: stderr: ""
Dec 17 14:55:22.000: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Dec 17 14:55:22.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete rc e2e-test-nginx-rc --namespace=kubectl-4691'
Dec 17 14:55:22.078: INFO: stderr: ""
Dec 17 14:55:22.078: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:55:22.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4691" for this suite.
Dec 17 14:55:44.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:55:44.208: INFO: namespace kubectl-4691 deletion completed in 22.124071275s

• [SLOW TEST:24.501 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:55:44.209: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:55:44.261: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:55:45.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3588" for this suite.
Dec 17 14:55:51.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:55:51.468: INFO: namespace custom-resource-definition-3588 deletion completed in 6.128856561s

• [SLOW TEST:7.259 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:55:51.471: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 17 14:56:09.546: INFO: Container started at 2019-12-17 14:55:52 +0000 UTC, pod became ready at 2019-12-17 14:56:09 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:56:09.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8012" for this suite.
Dec 17 14:56:31.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:56:31.680: INFO: namespace container-probe-8012 deletion completed in 22.128376636s

• [SLOW TEST:40.209 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:56:31.689: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:56:35.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1024" for this suite.
Dec 17 14:57:21.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:57:21.910: INFO: namespace kubelet-test-1024 deletion completed in 46.12326466s

• [SLOW TEST:50.222 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:57:21.912: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:57:21.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6da13f46-b6dd-4278-b675-d8dab477ab70" in namespace "downward-api-1059" to be "success or failure"
Dec 17 14:57:21.966: INFO: Pod "downwardapi-volume-6da13f46-b6dd-4278-b675-d8dab477ab70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538841ms
Dec 17 14:57:23.970: INFO: Pod "downwardapi-volume-6da13f46-b6dd-4278-b675-d8dab477ab70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009249209s
STEP: Saw pod success
Dec 17 14:57:23.970: INFO: Pod "downwardapi-volume-6da13f46-b6dd-4278-b675-d8dab477ab70" satisfied condition "success or failure"
Dec 17 14:57:23.973: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-6da13f46-b6dd-4278-b675-d8dab477ab70 container client-container: <nil>
STEP: delete the pod
Dec 17 14:57:23.999: INFO: Waiting for pod downwardapi-volume-6da13f46-b6dd-4278-b675-d8dab477ab70 to disappear
Dec 17 14:57:24.002: INFO: Pod downwardapi-volume-6da13f46-b6dd-4278-b675-d8dab477ab70 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:57:24.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1059" for this suite.
Dec 17 14:57:30.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:57:30.119: INFO: namespace downward-api-1059 deletion completed in 6.113069412s

• [SLOW TEST:8.207 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:57:30.119: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-250bddaa-29f8-49f5-bf51-b047eeb78ca7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-250bddaa-29f8-49f5-bf51-b047eeb78ca7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:57:34.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5106" for this suite.
Dec 17 14:57:56.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:57:56.330: INFO: namespace projected-5106 deletion completed in 22.116328032s

• [SLOW TEST:26.211 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:57:56.333: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-9070a31f-c135-4ada-be96-cef2dd379209
STEP: Creating a pod to test consume configMaps
Dec 17 14:57:56.403: INFO: Waiting up to 5m0s for pod "pod-configmaps-0514f808-0ad3-4bcc-a911-faee8504baf0" in namespace "configmap-3356" to be "success or failure"
Dec 17 14:57:56.408: INFO: Pod "pod-configmaps-0514f808-0ad3-4bcc-a911-faee8504baf0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.085936ms
Dec 17 14:57:58.412: INFO: Pod "pod-configmaps-0514f808-0ad3-4bcc-a911-faee8504baf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009619876s
STEP: Saw pod success
Dec 17 14:57:58.412: INFO: Pod "pod-configmaps-0514f808-0ad3-4bcc-a911-faee8504baf0" satisfied condition "success or failure"
Dec 17 14:57:58.415: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-0514f808-0ad3-4bcc-a911-faee8504baf0 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 14:57:58.447: INFO: Waiting for pod pod-configmaps-0514f808-0ad3-4bcc-a911-faee8504baf0 to disappear
Dec 17 14:57:58.453: INFO: Pod pod-configmaps-0514f808-0ad3-4bcc-a911-faee8504baf0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:57:58.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3356" for this suite.
Dec 17 14:58:04.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:58:04.593: INFO: namespace configmap-3356 deletion completed in 6.128638377s

• [SLOW TEST:8.260 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:58:04.597: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:58:04.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d09f4573-31c5-43d9-8ea8-570312e5d877" in namespace "projected-7975" to be "success or failure"
Dec 17 14:58:04.667: INFO: Pod "downwardapi-volume-d09f4573-31c5-43d9-8ea8-570312e5d877": Phase="Pending", Reason="", readiness=false. Elapsed: 6.767225ms
Dec 17 14:58:06.672: INFO: Pod "downwardapi-volume-d09f4573-31c5-43d9-8ea8-570312e5d877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011818781s
STEP: Saw pod success
Dec 17 14:58:06.672: INFO: Pod "downwardapi-volume-d09f4573-31c5-43d9-8ea8-570312e5d877" satisfied condition "success or failure"
Dec 17 14:58:06.677: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-d09f4573-31c5-43d9-8ea8-570312e5d877 container client-container: <nil>
STEP: delete the pod
Dec 17 14:58:06.701: INFO: Waiting for pod downwardapi-volume-d09f4573-31c5-43d9-8ea8-570312e5d877 to disappear
Dec 17 14:58:06.705: INFO: Pod downwardapi-volume-d09f4573-31c5-43d9-8ea8-570312e5d877 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:58:06.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7975" for this suite.
Dec 17 14:58:12.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:58:12.825: INFO: namespace projected-7975 deletion completed in 6.11544935s

• [SLOW TEST:8.229 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:58:12.827: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Dec 17 14:58:12.869: INFO: Waiting up to 5m0s for pod "var-expansion-40576b07-a4fb-4f07-84e0-89d7bc461acb" in namespace "var-expansion-9379" to be "success or failure"
Dec 17 14:58:12.876: INFO: Pod "var-expansion-40576b07-a4fb-4f07-84e0-89d7bc461acb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.836639ms
Dec 17 14:58:14.880: INFO: Pod "var-expansion-40576b07-a4fb-4f07-84e0-89d7bc461acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010929195s
STEP: Saw pod success
Dec 17 14:58:14.880: INFO: Pod "var-expansion-40576b07-a4fb-4f07-84e0-89d7bc461acb" satisfied condition "success or failure"
Dec 17 14:58:14.884: INFO: Trying to get logs from node conformance-k8s-node-2 pod var-expansion-40576b07-a4fb-4f07-84e0-89d7bc461acb container dapi-container: <nil>
STEP: delete the pod
Dec 17 14:58:14.906: INFO: Waiting for pod var-expansion-40576b07-a4fb-4f07-84e0-89d7bc461acb to disappear
Dec 17 14:58:14.915: INFO: Pod var-expansion-40576b07-a4fb-4f07-84e0-89d7bc461acb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:58:14.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9379" for this suite.
Dec 17 14:58:20.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:58:21.027: INFO: namespace var-expansion-9379 deletion completed in 6.107726726s

• [SLOW TEST:8.201 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:58:21.028: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-4d02b097-028b-4505-bbe0-4ee019494aaa
STEP: Creating a pod to test consume configMaps
Dec 17 14:58:21.087: INFO: Waiting up to 5m0s for pod "pod-configmaps-b051e3ae-da9f-47d8-a4bb-cd47459f2620" in namespace "configmap-7463" to be "success or failure"
Dec 17 14:58:21.105: INFO: Pod "pod-configmaps-b051e3ae-da9f-47d8-a4bb-cd47459f2620": Phase="Pending", Reason="", readiness=false. Elapsed: 17.274193ms
Dec 17 14:58:23.110: INFO: Pod "pod-configmaps-b051e3ae-da9f-47d8-a4bb-cd47459f2620": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022209632s
STEP: Saw pod success
Dec 17 14:58:23.110: INFO: Pod "pod-configmaps-b051e3ae-da9f-47d8-a4bb-cd47459f2620" satisfied condition "success or failure"
Dec 17 14:58:23.113: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-b051e3ae-da9f-47d8-a4bb-cd47459f2620 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 14:58:23.141: INFO: Waiting for pod pod-configmaps-b051e3ae-da9f-47d8-a4bb-cd47459f2620 to disappear
Dec 17 14:58:23.144: INFO: Pod pod-configmaps-b051e3ae-da9f-47d8-a4bb-cd47459f2620 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:58:23.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7463" for this suite.
Dec 17 14:58:29.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:58:29.255: INFO: namespace configmap-7463 deletion completed in 6.106250966s

• [SLOW TEST:8.227 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:58:29.257: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Dec 17 14:58:29.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 create -f - --namespace=kubectl-3275'
Dec 17 14:58:29.598: INFO: stderr: ""
Dec 17 14:58:29.598: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Dec 17 14:58:30.603: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 14:58:30.603: INFO: Found 0 / 1
Dec 17 14:58:31.605: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 14:58:31.605: INFO: Found 1 / 1
Dec 17 14:58:31.605: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 17 14:58:31.607: INFO: Selector matched 1 pods for map[app:redis]
Dec 17 14:58:31.607: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec 17 14:58:31.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 logs redis-master-sjcms redis-master --namespace=kubectl-3275'
Dec 17 14:58:31.704: INFO: stderr: ""
Dec 17 14:58:31.704: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 17 Dec 14:58:30.796 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 17 Dec 14:58:30.796 # Server started, Redis version 3.2.12\n1:M 17 Dec 14:58:30.797 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 17 Dec 14:58:30.797 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec 17 14:58:31.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 log redis-master-sjcms redis-master --namespace=kubectl-3275 --tail=1'
Dec 17 14:58:31.831: INFO: stderr: ""
Dec 17 14:58:31.831: INFO: stdout: "1:M 17 Dec 14:58:30.797 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec 17 14:58:31.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 log redis-master-sjcms redis-master --namespace=kubectl-3275 --limit-bytes=1'
Dec 17 14:58:31.995: INFO: stderr: ""
Dec 17 14:58:31.995: INFO: stdout: " "
STEP: exposing timestamps
Dec 17 14:58:31.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 log redis-master-sjcms redis-master --namespace=kubectl-3275 --tail=1 --timestamps'
Dec 17 14:58:32.082: INFO: stderr: ""
Dec 17 14:58:32.082: INFO: stdout: "2019-12-17T14:58:30.798286967Z 1:M 17 Dec 14:58:30.797 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec 17 14:58:34.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 log redis-master-sjcms redis-master --namespace=kubectl-3275 --since=1s'
Dec 17 14:58:34.688: INFO: stderr: ""
Dec 17 14:58:34.688: INFO: stdout: ""
Dec 17 14:58:34.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 log redis-master-sjcms redis-master --namespace=kubectl-3275 --since=24h'
Dec 17 14:58:34.784: INFO: stderr: ""
Dec 17 14:58:34.784: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 17 Dec 14:58:30.796 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 17 Dec 14:58:30.796 # Server started, Redis version 3.2.12\n1:M 17 Dec 14:58:30.797 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 17 Dec 14:58:30.797 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Dec 17 14:58:34.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 delete --grace-period=0 --force -f - --namespace=kubectl-3275'
Dec 17 14:58:34.872: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 14:58:34.872: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec 17 14:58:34.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get rc,svc -l name=nginx --no-headers --namespace=kubectl-3275'
Dec 17 14:58:34.958: INFO: stderr: "No resources found.\n"
Dec 17 14:58:34.958: INFO: stdout: ""
Dec 17 14:58:34.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-643038348 get pods -l name=nginx --namespace=kubectl-3275 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 14:58:35.033: INFO: stderr: ""
Dec 17 14:58:35.033: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:58:35.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3275" for this suite.
Dec 17 14:58:41.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:58:41.164: INFO: namespace kubectl-3275 deletion completed in 6.126932106s

• [SLOW TEST:11.908 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:58:41.165: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-be2e4fda-58e3-4fd8-9a26-41893bf9fb9a
STEP: Creating a pod to test consume secrets
Dec 17 14:58:41.214: INFO: Waiting up to 5m0s for pod "pod-secrets-170655f9-ff54-4a16-b024-3953ab369ae3" in namespace "secrets-5334" to be "success or failure"
Dec 17 14:58:41.220: INFO: Pod "pod-secrets-170655f9-ff54-4a16-b024-3953ab369ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.106174ms
Dec 17 14:58:43.224: INFO: Pod "pod-secrets-170655f9-ff54-4a16-b024-3953ab369ae3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009767835s
STEP: Saw pod success
Dec 17 14:58:43.224: INFO: Pod "pod-secrets-170655f9-ff54-4a16-b024-3953ab369ae3" satisfied condition "success or failure"
Dec 17 14:58:43.227: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-secrets-170655f9-ff54-4a16-b024-3953ab369ae3 container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 14:58:43.260: INFO: Waiting for pod pod-secrets-170655f9-ff54-4a16-b024-3953ab369ae3 to disappear
Dec 17 14:58:43.263: INFO: Pod pod-secrets-170655f9-ff54-4a16-b024-3953ab369ae3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:58:43.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5334" for this suite.
Dec 17 14:58:49.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:58:49.378: INFO: namespace secrets-5334 deletion completed in 6.106201661s

• [SLOW TEST:8.214 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:58:49.384: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec 17 14:58:50.034: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:58:50.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1217 14:58:50.034530      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4166" for this suite.
Dec 17 14:58:56.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:58:56.176: INFO: namespace gc-4166 deletion completed in 6.132636712s

• [SLOW TEST:6.792 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:58:56.178: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-03389a63-f700-45a2-8837-2ead03e2fc5b
STEP: Creating a pod to test consume secrets
Dec 17 14:58:56.247: INFO: Waiting up to 5m0s for pod "pod-secrets-de9d9248-29da-4d36-b907-98e3b2fa977b" in namespace "secrets-792" to be "success or failure"
Dec 17 14:58:56.256: INFO: Pod "pod-secrets-de9d9248-29da-4d36-b907-98e3b2fa977b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.657231ms
Dec 17 14:58:58.262: INFO: Pod "pod-secrets-de9d9248-29da-4d36-b907-98e3b2fa977b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014521721s
Dec 17 14:59:00.267: INFO: Pod "pod-secrets-de9d9248-29da-4d36-b907-98e3b2fa977b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019149696s
STEP: Saw pod success
Dec 17 14:59:00.267: INFO: Pod "pod-secrets-de9d9248-29da-4d36-b907-98e3b2fa977b" satisfied condition "success or failure"
Dec 17 14:59:00.270: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-secrets-de9d9248-29da-4d36-b907-98e3b2fa977b container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 14:59:00.291: INFO: Waiting for pod pod-secrets-de9d9248-29da-4d36-b907-98e3b2fa977b to disappear
Dec 17 14:59:00.294: INFO: Pod pod-secrets-de9d9248-29da-4d36-b907-98e3b2fa977b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:59:00.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-792" for this suite.
Dec 17 14:59:06.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:59:06.420: INFO: namespace secrets-792 deletion completed in 6.120574204s

• [SLOW TEST:10.243 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:59:06.423: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 17 14:59:11.033: INFO: Successfully updated pod "annotationupdate12ca4653-4da7-45d1-a507-ab979fe9a7ee"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:59:13.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-823" for this suite.
Dec 17 14:59:35.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:59:35.193: INFO: namespace projected-823 deletion completed in 22.12057843s

• [SLOW TEST:28.770 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:59:35.196: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-ec9d176c-2f51-4857-b5bc-3e5fe38225d7
STEP: Creating a pod to test consume secrets
Dec 17 14:59:35.262: INFO: Waiting up to 5m0s for pod "pod-secrets-2db8be8f-7f18-454d-a5ea-f3cdfe4593ce" in namespace "secrets-2056" to be "success or failure"
Dec 17 14:59:35.269: INFO: Pod "pod-secrets-2db8be8f-7f18-454d-a5ea-f3cdfe4593ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.587777ms
Dec 17 14:59:37.275: INFO: Pod "pod-secrets-2db8be8f-7f18-454d-a5ea-f3cdfe4593ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012200434s
STEP: Saw pod success
Dec 17 14:59:37.276: INFO: Pod "pod-secrets-2db8be8f-7f18-454d-a5ea-f3cdfe4593ce" satisfied condition "success or failure"
Dec 17 14:59:37.279: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-secrets-2db8be8f-7f18-454d-a5ea-f3cdfe4593ce container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 14:59:37.314: INFO: Waiting for pod pod-secrets-2db8be8f-7f18-454d-a5ea-f3cdfe4593ce to disappear
Dec 17 14:59:37.318: INFO: Pod pod-secrets-2db8be8f-7f18-454d-a5ea-f3cdfe4593ce no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:59:37.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2056" for this suite.
Dec 17 14:59:43.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:59:43.436: INFO: namespace secrets-2056 deletion completed in 6.112605987s

• [SLOW TEST:8.240 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:59:43.437: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 17 14:59:43.487: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8dd68407-c536-43e4-86a1-0499d0614fad" in namespace "downward-api-8021" to be "success or failure"
Dec 17 14:59:43.503: INFO: Pod "downwardapi-volume-8dd68407-c536-43e4-86a1-0499d0614fad": Phase="Pending", Reason="", readiness=false. Elapsed: 15.883957ms
Dec 17 14:59:45.509: INFO: Pod "downwardapi-volume-8dd68407-c536-43e4-86a1-0499d0614fad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021746543s
Dec 17 14:59:47.515: INFO: Pod "downwardapi-volume-8dd68407-c536-43e4-86a1-0499d0614fad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026473425s
STEP: Saw pod success
Dec 17 14:59:47.515: INFO: Pod "downwardapi-volume-8dd68407-c536-43e4-86a1-0499d0614fad" satisfied condition "success or failure"
Dec 17 14:59:47.519: INFO: Trying to get logs from node conformance-k8s-node-2 pod downwardapi-volume-8dd68407-c536-43e4-86a1-0499d0614fad container client-container: <nil>
STEP: delete the pod
Dec 17 14:59:47.552: INFO: Waiting for pod downwardapi-volume-8dd68407-c536-43e4-86a1-0499d0614fad to disappear
Dec 17 14:59:47.554: INFO: Pod downwardapi-volume-8dd68407-c536-43e4-86a1-0499d0614fad no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:59:47.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8021" for this suite.
Dec 17 14:59:53.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 14:59:53.671: INFO: namespace downward-api-8021 deletion completed in 6.112832634s

• [SLOW TEST:10.234 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 14:59:53.673: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-f22e7195-770b-4cc0-8e23-d925eccc31ea
STEP: Creating a pod to test consume secrets
Dec 17 14:59:53.722: INFO: Waiting up to 5m0s for pod "pod-secrets-368a8ceb-0e0e-494f-b2d0-b9099478f383" in namespace "secrets-3082" to be "success or failure"
Dec 17 14:59:53.726: INFO: Pod "pod-secrets-368a8ceb-0e0e-494f-b2d0-b9099478f383": Phase="Pending", Reason="", readiness=false. Elapsed: 3.601752ms
Dec 17 14:59:55.729: INFO: Pod "pod-secrets-368a8ceb-0e0e-494f-b2d0-b9099478f383": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006829032s
STEP: Saw pod success
Dec 17 14:59:55.729: INFO: Pod "pod-secrets-368a8ceb-0e0e-494f-b2d0-b9099478f383" satisfied condition "success or failure"
Dec 17 14:59:55.732: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-secrets-368a8ceb-0e0e-494f-b2d0-b9099478f383 container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 14:59:55.780: INFO: Waiting for pod pod-secrets-368a8ceb-0e0e-494f-b2d0-b9099478f383 to disappear
Dec 17 14:59:55.783: INFO: Pod pod-secrets-368a8ceb-0e0e-494f-b2d0-b9099478f383 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 14:59:55.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3082" for this suite.
Dec 17 15:00:01.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:00:01.902: INFO: namespace secrets-3082 deletion completed in 6.113807543s

• [SLOW TEST:8.230 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:00:01.904: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f2aa91c0-ecc8-4706-a06b-ae15befbbeee
STEP: Creating a pod to test consume configMaps
Dec 17 15:00:01.967: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b4340261-03c9-4a37-b9ee-525ab0f7e08e" in namespace "projected-724" to be "success or failure"
Dec 17 15:00:01.974: INFO: Pod "pod-projected-configmaps-b4340261-03c9-4a37-b9ee-525ab0f7e08e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.788333ms
Dec 17 15:00:03.985: INFO: Pod "pod-projected-configmaps-b4340261-03c9-4a37-b9ee-525ab0f7e08e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017820724s
STEP: Saw pod success
Dec 17 15:00:03.985: INFO: Pod "pod-projected-configmaps-b4340261-03c9-4a37-b9ee-525ab0f7e08e" satisfied condition "success or failure"
Dec 17 15:00:03.990: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-projected-configmaps-b4340261-03c9-4a37-b9ee-525ab0f7e08e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 15:00:04.033: INFO: Waiting for pod pod-projected-configmaps-b4340261-03c9-4a37-b9ee-525ab0f7e08e to disappear
Dec 17 15:00:04.036: INFO: Pod pod-projected-configmaps-b4340261-03c9-4a37-b9ee-525ab0f7e08e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:00:04.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-724" for this suite.
Dec 17 15:00:10.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:00:10.162: INFO: namespace projected-724 deletion completed in 6.120879212s

• [SLOW TEST:8.258 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:00:10.163: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:00:14.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6352" for this suite.
Dec 17 15:00:20.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:00:20.358: INFO: namespace kubelet-test-6352 deletion completed in 6.118628505s

• [SLOW TEST:10.196 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:00:20.360: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5481/configmap-test-15117727-252b-4388-b5c4-70b9a2d9d248
STEP: Creating a pod to test consume configMaps
Dec 17 15:00:20.425: INFO: Waiting up to 5m0s for pod "pod-configmaps-af955518-357f-45c4-815c-06cc0973272a" in namespace "configmap-5481" to be "success or failure"
Dec 17 15:00:20.429: INFO: Pod "pod-configmaps-af955518-357f-45c4-815c-06cc0973272a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.678045ms
Dec 17 15:00:22.433: INFO: Pod "pod-configmaps-af955518-357f-45c4-815c-06cc0973272a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007775308s
STEP: Saw pod success
Dec 17 15:00:22.433: INFO: Pod "pod-configmaps-af955518-357f-45c4-815c-06cc0973272a" satisfied condition "success or failure"
Dec 17 15:00:22.436: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-af955518-357f-45c4-815c-06cc0973272a container env-test: <nil>
STEP: delete the pod
Dec 17 15:00:22.469: INFO: Waiting for pod pod-configmaps-af955518-357f-45c4-815c-06cc0973272a to disappear
Dec 17 15:00:22.473: INFO: Pod pod-configmaps-af955518-357f-45c4-815c-06cc0973272a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:00:22.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5481" for this suite.
Dec 17 15:00:28.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:00:28.583: INFO: namespace configmap-5481 deletion completed in 6.106486819s

• [SLOW TEST:8.224 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:00:28.584: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 17 15:00:31.158: INFO: Successfully updated pod "labelsupdate1a32a238-c162-47a0-af3f-32bd9890ecc2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:00:33.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8226" for this suite.
Dec 17 15:00:55.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:00:55.307: INFO: namespace projected-8226 deletion completed in 22.113975678s

• [SLOW TEST:26.723 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:00:55.308: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-c7ab8b56-afcd-4604-bdbc-f0b529a0f346
STEP: Creating a pod to test consume secrets
Dec 17 15:00:55.370: INFO: Waiting up to 5m0s for pod "pod-secrets-dc3b23b3-eba4-4797-a665-d87ae59f00fb" in namespace "secrets-3189" to be "success or failure"
Dec 17 15:00:55.376: INFO: Pod "pod-secrets-dc3b23b3-eba4-4797-a665-d87ae59f00fb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.183859ms
Dec 17 15:00:57.381: INFO: Pod "pod-secrets-dc3b23b3-eba4-4797-a665-d87ae59f00fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010661289s
STEP: Saw pod success
Dec 17 15:00:57.381: INFO: Pod "pod-secrets-dc3b23b3-eba4-4797-a665-d87ae59f00fb" satisfied condition "success or failure"
Dec 17 15:00:57.384: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-secrets-dc3b23b3-eba4-4797-a665-d87ae59f00fb container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 15:00:57.405: INFO: Waiting for pod pod-secrets-dc3b23b3-eba4-4797-a665-d87ae59f00fb to disappear
Dec 17 15:00:57.408: INFO: Pod pod-secrets-dc3b23b3-eba4-4797-a665-d87ae59f00fb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:00:57.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3189" for this suite.
Dec 17 15:01:03.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:01:03.544: INFO: namespace secrets-3189 deletion completed in 6.121296456s

• [SLOW TEST:8.237 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:01:03.546: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Dec 17 15:01:03.624: INFO: Waiting up to 5m0s for pod "client-containers-beda9b66-f501-4851-812a-1e1b6e4aa705" in namespace "containers-2338" to be "success or failure"
Dec 17 15:01:03.638: INFO: Pod "client-containers-beda9b66-f501-4851-812a-1e1b6e4aa705": Phase="Pending", Reason="", readiness=false. Elapsed: 13.203128ms
Dec 17 15:01:05.643: INFO: Pod "client-containers-beda9b66-f501-4851-812a-1e1b6e4aa705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018587493s
STEP: Saw pod success
Dec 17 15:01:05.643: INFO: Pod "client-containers-beda9b66-f501-4851-812a-1e1b6e4aa705" satisfied condition "success or failure"
Dec 17 15:01:05.647: INFO: Trying to get logs from node conformance-k8s-node-2 pod client-containers-beda9b66-f501-4851-812a-1e1b6e4aa705 container test-container: <nil>
STEP: delete the pod
Dec 17 15:01:05.676: INFO: Waiting for pod client-containers-beda9b66-f501-4851-812a-1e1b6e4aa705 to disappear
Dec 17 15:01:05.680: INFO: Pod client-containers-beda9b66-f501-4851-812a-1e1b6e4aa705 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:01:05.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2338" for this suite.
Dec 17 15:01:11.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:01:11.836: INFO: namespace containers-2338 deletion completed in 6.147632659s

• [SLOW TEST:8.291 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:01:11.838: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5157.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5157.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 15:01:15.936: INFO: DNS probes using dns-5157/dns-test-de946688-6bf5-4044-b7fe-a87628dbe79c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:01:15.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5157" for this suite.
Dec 17 15:01:21.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:01:22.086: INFO: namespace dns-5157 deletion completed in 6.117783143s

• [SLOW TEST:10.248 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:01:22.086: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:01:24.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7457" for this suite.
Dec 17 15:01:30.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:01:30.343: INFO: namespace emptydir-wrapper-7457 deletion completed in 6.107884132s

• [SLOW TEST:8.257 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:01:30.345: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-153c47bf-b25d-40d6-944e-295584f5c875
STEP: Creating a pod to test consume configMaps
Dec 17 15:01:30.403: INFO: Waiting up to 5m0s for pod "pod-configmaps-64984733-587b-4ea6-b7c7-2ca5034aa498" in namespace "configmap-438" to be "success or failure"
Dec 17 15:01:30.415: INFO: Pod "pod-configmaps-64984733-587b-4ea6-b7c7-2ca5034aa498": Phase="Pending", Reason="", readiness=false. Elapsed: 11.887779ms
Dec 17 15:01:32.418: INFO: Pod "pod-configmaps-64984733-587b-4ea6-b7c7-2ca5034aa498": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015218493s
STEP: Saw pod success
Dec 17 15:01:32.419: INFO: Pod "pod-configmaps-64984733-587b-4ea6-b7c7-2ca5034aa498" satisfied condition "success or failure"
Dec 17 15:01:32.421: INFO: Trying to get logs from node conformance-k8s-node-2 pod pod-configmaps-64984733-587b-4ea6-b7c7-2ca5034aa498 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 15:01:32.460: INFO: Waiting for pod pod-configmaps-64984733-587b-4ea6-b7c7-2ca5034aa498 to disappear
Dec 17 15:01:32.464: INFO: Pod pod-configmaps-64984733-587b-4ea6-b7c7-2ca5034aa498 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:01:32.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-438" for this suite.
Dec 17 15:01:38.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:01:38.577: INFO: namespace configmap-438 deletion completed in 6.108686172s

• [SLOW TEST:8.232 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:01:38.578: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7737.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7737.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7737.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7737.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7737.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7737.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7737.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7737.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7737.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7737.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7737.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.36.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.36.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.36.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.36.98_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7737.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7737.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7737.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7737.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7737.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7737.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7737.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7737.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7737.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7737.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7737.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.36.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.36.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.36.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.36.98_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 15:01:42.677: INFO: Unable to read wheezy_udp@dns-test-service.dns-7737.svc.cluster.local from pod dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035: the server could not find the requested resource (get pods dns-test-435a7c93-63c5-4d99-9f76-555d474f4035)
Dec 17 15:01:42.684: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7737.svc.cluster.local from pod dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035: the server could not find the requested resource (get pods dns-test-435a7c93-63c5-4d99-9f76-555d474f4035)
Dec 17 15:01:42.688: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local from pod dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035: the server could not find the requested resource (get pods dns-test-435a7c93-63c5-4d99-9f76-555d474f4035)
Dec 17 15:01:42.693: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local from pod dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035: the server could not find the requested resource (get pods dns-test-435a7c93-63c5-4d99-9f76-555d474f4035)
Dec 17 15:01:42.722: INFO: Unable to read jessie_udp@dns-test-service.dns-7737.svc.cluster.local from pod dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035: the server could not find the requested resource (get pods dns-test-435a7c93-63c5-4d99-9f76-555d474f4035)
Dec 17 15:01:42.726: INFO: Unable to read jessie_tcp@dns-test-service.dns-7737.svc.cluster.local from pod dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035: the server could not find the requested resource (get pods dns-test-435a7c93-63c5-4d99-9f76-555d474f4035)
Dec 17 15:01:42.729: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local from pod dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035: the server could not find the requested resource (get pods dns-test-435a7c93-63c5-4d99-9f76-555d474f4035)
Dec 17 15:01:42.733: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local from pod dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035: the server could not find the requested resource (get pods dns-test-435a7c93-63c5-4d99-9f76-555d474f4035)
Dec 17 15:01:42.753: INFO: Lookups using dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035 failed for: [wheezy_udp@dns-test-service.dns-7737.svc.cluster.local wheezy_tcp@dns-test-service.dns-7737.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local jessie_udp@dns-test-service.dns-7737.svc.cluster.local jessie_tcp@dns-test-service.dns-7737.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7737.svc.cluster.local]

Dec 17 15:01:47.846: INFO: DNS probes using dns-7737/dns-test-435a7c93-63c5-4d99-9f76-555d474f4035 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:01:47.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7737" for this suite.
Dec 17 15:01:53.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:01:54.069: INFO: namespace dns-7737 deletion completed in 6.120036317s

• [SLOW TEST:15.492 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:01:54.070: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-2842
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2842 to expose endpoints map[]
Dec 17 15:01:54.134: INFO: Get endpoints failed (13.183943ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 17 15:01:55.139: INFO: successfully validated that service endpoint-test2 in namespace services-2842 exposes endpoints map[] (1.018536684s elapsed)
STEP: Creating pod pod1 in namespace services-2842
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2842 to expose endpoints map[pod1:[80]]
Dec 17 15:01:57.169: INFO: successfully validated that service endpoint-test2 in namespace services-2842 exposes endpoints map[pod1:[80]] (2.021812963s elapsed)
STEP: Creating pod pod2 in namespace services-2842
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2842 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 17 15:02:00.239: INFO: successfully validated that service endpoint-test2 in namespace services-2842 exposes endpoints map[pod1:[80] pod2:[80]] (3.058839543s elapsed)
STEP: Deleting pod pod1 in namespace services-2842
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2842 to expose endpoints map[pod2:[80]]
Dec 17 15:02:01.272: INFO: successfully validated that service endpoint-test2 in namespace services-2842 exposes endpoints map[pod2:[80]] (1.019566198s elapsed)
STEP: Deleting pod pod2 in namespace services-2842
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2842 to expose endpoints map[]
Dec 17 15:02:01.288: INFO: successfully validated that service endpoint-test2 in namespace services-2842 exposes endpoints map[] (10.144258ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:02:01.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2842" for this suite.
Dec 17 15:02:07.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:02:07.453: INFO: namespace services-2842 deletion completed in 6.132395197s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:13.383 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 17 15:02:07.454: INFO: >>> kubeConfig: /tmp/kubeconfig-643038348
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6933.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6933.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6933.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6933.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 15:02:11.537: INFO: DNS probes using dns-test-0d9d9391-f2fe-498d-8dcd-4867e58268e4 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6933.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6933.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6933.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6933.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 15:02:15.597: INFO: DNS probes using dns-test-2937daaa-ec11-4226-a540-d0d53eca169d succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6933.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6933.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6933.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6933.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 15:02:19.675: INFO: DNS probes using dns-test-1ff7e405-8216-4934-bede-abcba5194c73 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 17 15:02:19.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6933" for this suite.
Dec 17 15:02:25.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 17 15:02:25.870: INFO: namespace dns-6933 deletion completed in 6.138954369s

• [SLOW TEST:18.416 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSDec 17 15:02:25.871: INFO: Running AfterSuite actions on all nodes
Dec 17 15:02:25.871: INFO: Running AfterSuite actions on node 1
Dec 17 15:02:25.871: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5392.806 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h29m54.143563838s
Test Suite Passed
