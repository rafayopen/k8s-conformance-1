I0915 15:46:33.028345      19 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-180164986
I0915 15:46:33.028779      19 e2e.go:241] Starting e2e run "a966ae7f-ebdd-4818-b578-c7e5db9a360b" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1568562391 - Will randomize all specs
Will run 215 of 4413 specs

Sep 15 15:46:33.234: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 15:46:33.236: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 15 15:46:33.252: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 15 15:46:33.292: INFO: 49 / 49 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 15 15:46:33.292: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Sep 15 15:46:33.292: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 15 15:46:33.300: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Sep 15 15:46:33.300: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Sep 15 15:46:33.300: INFO: e2e test version: v1.15.3
Sep 15 15:46:33.301: INFO: kube-apiserver version: v1.15.3
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:46:33.301: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename services
Sep 15 15:46:33.364: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep 15 15:46:33.384: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-1204
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1204 to expose endpoints map[]
Sep 15 15:46:33.543: INFO: successfully validated that service multi-endpoint-test in namespace services-1204 exposes endpoints map[] (5.13974ms elapsed)
STEP: Creating pod pod1 in namespace services-1204
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1204 to expose endpoints map[pod1:[100]]
Sep 15 15:46:35.589: INFO: successfully validated that service multi-endpoint-test in namespace services-1204 exposes endpoints map[pod1:[100]] (2.030205692s elapsed)
STEP: Creating pod pod2 in namespace services-1204
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1204 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 15 15:46:37.636: INFO: successfully validated that service multi-endpoint-test in namespace services-1204 exposes endpoints map[pod1:[100] pod2:[101]] (2.038642501s elapsed)
STEP: Deleting pod pod1 in namespace services-1204
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1204 to expose endpoints map[pod2:[101]]
Sep 15 15:46:38.668: INFO: successfully validated that service multi-endpoint-test in namespace services-1204 exposes endpoints map[pod2:[101]] (1.021517702s elapsed)
STEP: Deleting pod pod2 in namespace services-1204
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1204 to expose endpoints map[]
Sep 15 15:46:39.685: INFO: successfully validated that service multi-endpoint-test in namespace services-1204 exposes endpoints map[] (1.00831279s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:46:39.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1204" for this suite.
Sep 15 15:46:45.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:46:45.880: INFO: namespace services-1204 deletion completed in 6.153154803s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:12.578 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:46:45.880: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 15:47:10.093: INFO: Container started at 2019-09-15 15:46:48 +0000 UTC, pod became ready at 2019-09-15 15:47:09 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:47:10.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2001" for this suite.
Sep 15 15:47:32.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:47:32.260: INFO: namespace container-probe-2001 deletion completed in 22.160996152s

• [SLOW TEST:46.380 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:47:32.260: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 15 15:47:32.452: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:32.452: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:32.452: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:32.457: INFO: Number of nodes with available pods: 0
Sep 15 15:47:32.457: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 15:47:33.463: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:33.463: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:33.463: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:33.468: INFO: Number of nodes with available pods: 0
Sep 15 15:47:33.468: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 15:47:34.463: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:34.463: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:34.463: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:34.467: INFO: Number of nodes with available pods: 0
Sep 15 15:47:34.467: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 15:47:35.463: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:35.463: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:35.463: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:35.468: INFO: Number of nodes with available pods: 2
Sep 15 15:47:35.468: INFO: Node ip-172-16-74-13.ec2.internal is running more than one daemon pod
Sep 15 15:47:36.464: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:36.464: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:36.464: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:36.469: INFO: Number of nodes with available pods: 3
Sep 15 15:47:36.469: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 15 15:47:36.496: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:36.496: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:36.496: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:36.502: INFO: Number of nodes with available pods: 2
Sep 15 15:47:36.502: INFO: Node ip-172-16-74-13.ec2.internal is running more than one daemon pod
Sep 15 15:47:37.508: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:37.508: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:37.508: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:37.513: INFO: Number of nodes with available pods: 2
Sep 15 15:47:37.513: INFO: Node ip-172-16-74-13.ec2.internal is running more than one daemon pod
Sep 15 15:47:38.508: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:38.509: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:38.509: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:47:38.513: INFO: Number of nodes with available pods: 3
Sep 15 15:47:38.513: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6917, will wait for the garbage collector to delete the pods
Sep 15 15:47:38.586: INFO: Deleting DaemonSet.extensions daemon-set took: 11.703542ms
Sep 15 15:47:38.987: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.256447ms
Sep 15 15:47:41.691: INFO: Number of nodes with available pods: 0
Sep 15 15:47:41.691: INFO: Number of running nodes: 0, number of available pods: 0
Sep 15 15:47:41.697: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6917/daemonsets","resourceVersion":"4648"},"items":null}

Sep 15 15:47:41.701: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6917/pods","resourceVersion":"4648"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:47:41.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6917" for this suite.
Sep 15 15:47:47.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:47:47.895: INFO: namespace daemonsets-6917 deletion completed in 6.172243071s

• [SLOW TEST:15.636 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:47:47.896: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Sep 15 15:47:50.073: INFO: Pod pod-hostip-3a48149b-6a7c-4845-b663-746061a0ff65 has hostIP: 172.16.34.210
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:47:50.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4921" for this suite.
Sep 15 15:48:12.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:48:12.274: INFO: namespace pods-4921 deletion completed in 22.195804201s

• [SLOW TEST:24.379 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:48:12.275: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 15:48:12.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c569650a-c0e5-4780-ad4d-f773928b0662" in namespace "projected-2398" to be "success or failure"
Sep 15 15:48:12.505: INFO: Pod "downwardapi-volume-c569650a-c0e5-4780-ad4d-f773928b0662": Phase="Pending", Reason="", readiness=false. Elapsed: 6.111014ms
Sep 15 15:48:14.510: INFO: Pod "downwardapi-volume-c569650a-c0e5-4780-ad4d-f773928b0662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011019313s
Sep 15 15:48:16.515: INFO: Pod "downwardapi-volume-c569650a-c0e5-4780-ad4d-f773928b0662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015736409s
STEP: Saw pod success
Sep 15 15:48:16.515: INFO: Pod "downwardapi-volume-c569650a-c0e5-4780-ad4d-f773928b0662" satisfied condition "success or failure"
Sep 15 15:48:16.518: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-c569650a-c0e5-4780-ad4d-f773928b0662 container client-container: <nil>
STEP: delete the pod
Sep 15 15:48:16.560: INFO: Waiting for pod downwardapi-volume-c569650a-c0e5-4780-ad4d-f773928b0662 to disappear
Sep 15 15:48:16.564: INFO: Pod downwardapi-volume-c569650a-c0e5-4780-ad4d-f773928b0662 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:48:16.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2398" for this suite.
Sep 15 15:48:22.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:48:22.728: INFO: namespace projected-2398 deletion completed in 6.157848396s

• [SLOW TEST:10.453 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:48:22.728: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-4de3b0e4-3c71-45db-8a8e-0655e4463df4
STEP: Creating a pod to test consume configMaps
Sep 15 15:48:22.888: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98132c9d-8411-471b-856e-7a3ec7019eab" in namespace "projected-9083" to be "success or failure"
Sep 15 15:48:22.894: INFO: Pod "pod-projected-configmaps-98132c9d-8411-471b-856e-7a3ec7019eab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.316162ms
Sep 15 15:48:24.898: INFO: Pod "pod-projected-configmaps-98132c9d-8411-471b-856e-7a3ec7019eab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009839419s
STEP: Saw pod success
Sep 15 15:48:24.898: INFO: Pod "pod-projected-configmaps-98132c9d-8411-471b-856e-7a3ec7019eab" satisfied condition "success or failure"
Sep 15 15:48:24.902: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-configmaps-98132c9d-8411-471b-856e-7a3ec7019eab container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 15:48:24.933: INFO: Waiting for pod pod-projected-configmaps-98132c9d-8411-471b-856e-7a3ec7019eab to disappear
Sep 15 15:48:24.946: INFO: Pod pod-projected-configmaps-98132c9d-8411-471b-856e-7a3ec7019eab no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:48:24.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9083" for this suite.
Sep 15 15:48:30.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:48:31.104: INFO: namespace projected-9083 deletion completed in 6.153586181s

• [SLOW TEST:8.376 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:48:31.104: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1455
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-e0314312-498a-45f7-8111-fd10c293dfdb
STEP: Creating a pod to test consume configMaps
Sep 15 15:48:31.267: INFO: Waiting up to 5m0s for pod "pod-configmaps-fdbeb5b7-2405-4c08-a6df-fc5b81da97ba" in namespace "configmap-1455" to be "success or failure"
Sep 15 15:48:31.271: INFO: Pod "pod-configmaps-fdbeb5b7-2405-4c08-a6df-fc5b81da97ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.158738ms
Sep 15 15:48:33.276: INFO: Pod "pod-configmaps-fdbeb5b7-2405-4c08-a6df-fc5b81da97ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009079661s
STEP: Saw pod success
Sep 15 15:48:33.276: INFO: Pod "pod-configmaps-fdbeb5b7-2405-4c08-a6df-fc5b81da97ba" satisfied condition "success or failure"
Sep 15 15:48:33.280: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-fdbeb5b7-2405-4c08-a6df-fc5b81da97ba container configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 15:48:33.311: INFO: Waiting for pod pod-configmaps-fdbeb5b7-2405-4c08-a6df-fc5b81da97ba to disappear
Sep 15 15:48:33.315: INFO: Pod pod-configmaps-fdbeb5b7-2405-4c08-a6df-fc5b81da97ba no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:48:33.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1455" for this suite.
Sep 15 15:48:39.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:48:39.479: INFO: namespace configmap-1455 deletion completed in 6.158141184s

• [SLOW TEST:8.375 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:48:39.479: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-90
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 15 15:48:41.658: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:48:41.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-90" for this suite.
Sep 15 15:48:47.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:48:47.840: INFO: namespace container-runtime-90 deletion completed in 6.157042987s

• [SLOW TEST:8.361 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:48:47.840: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-9607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9607 to expose endpoints map[]
Sep 15 15:48:48.012: INFO: Get endpoints failed (6.434944ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep 15 15:48:49.016: INFO: successfully validated that service endpoint-test2 in namespace services-9607 exposes endpoints map[] (1.010810329s elapsed)
STEP: Creating pod pod1 in namespace services-9607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9607 to expose endpoints map[pod1:[80]]
Sep 15 15:48:51.054: INFO: successfully validated that service endpoint-test2 in namespace services-9607 exposes endpoints map[pod1:[80]] (2.027621552s elapsed)
STEP: Creating pod pod2 in namespace services-9607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9607 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 15 15:48:52.102: INFO: successfully validated that service endpoint-test2 in namespace services-9607 exposes endpoints map[pod1:[80] pod2:[80]] (1.039876497s elapsed)
STEP: Deleting pod pod1 in namespace services-9607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9607 to expose endpoints map[pod2:[80]]
Sep 15 15:48:53.126: INFO: successfully validated that service endpoint-test2 in namespace services-9607 exposes endpoints map[pod2:[80]] (1.015901238s elapsed)
STEP: Deleting pod pod2 in namespace services-9607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9607 to expose endpoints map[]
Sep 15 15:48:54.144: INFO: successfully validated that service endpoint-test2 in namespace services-9607 exposes endpoints map[] (1.008908379s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:48:54.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9607" for this suite.
Sep 15 15:49:16.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:49:16.351: INFO: namespace services-9607 deletion completed in 22.168214351s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:28.511 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:49:16.352: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Sep 15 15:49:16.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-2647'
Sep 15 15:49:16.917: INFO: stderr: ""
Sep 15 15:49:16.917: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 15 15:49:16.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2647'
Sep 15 15:49:17.001: INFO: stderr: ""
Sep 15 15:49:17.001: INFO: stdout: "update-demo-nautilus-bgl7d update-demo-nautilus-fgdnt "
Sep 15 15:49:17.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-bgl7d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2647'
Sep 15 15:49:17.074: INFO: stderr: ""
Sep 15 15:49:17.074: INFO: stdout: ""
Sep 15 15:49:17.074: INFO: update-demo-nautilus-bgl7d is created but not running
Sep 15 15:49:22.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2647'
Sep 15 15:49:22.145: INFO: stderr: ""
Sep 15 15:49:22.145: INFO: stdout: "update-demo-nautilus-bgl7d update-demo-nautilus-fgdnt "
Sep 15 15:49:22.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-bgl7d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2647'
Sep 15 15:49:22.212: INFO: stderr: ""
Sep 15 15:49:22.212: INFO: stdout: "true"
Sep 15 15:49:22.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-bgl7d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2647'
Sep 15 15:49:22.280: INFO: stderr: ""
Sep 15 15:49:22.280: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 15 15:49:22.280: INFO: validating pod update-demo-nautilus-bgl7d
Sep 15 15:49:22.289: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 15 15:49:22.289: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 15 15:49:22.289: INFO: update-demo-nautilus-bgl7d is verified up and running
Sep 15 15:49:22.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-fgdnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2647'
Sep 15 15:49:22.362: INFO: stderr: ""
Sep 15 15:49:22.362: INFO: stdout: "true"
Sep 15 15:49:22.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-fgdnt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2647'
Sep 15 15:49:22.430: INFO: stderr: ""
Sep 15 15:49:22.430: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 15 15:49:22.430: INFO: validating pod update-demo-nautilus-fgdnt
Sep 15 15:49:22.437: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 15 15:49:22.437: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 15 15:49:22.437: INFO: update-demo-nautilus-fgdnt is verified up and running
STEP: rolling-update to new replication controller
Sep 15 15:49:22.439: INFO: scanned /root for discovery docs: <nil>
Sep 15 15:49:22.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2647'
Sep 15 15:49:44.865: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 15 15:49:44.865: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 15 15:49:44.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2647'
Sep 15 15:49:44.941: INFO: stderr: ""
Sep 15 15:49:44.941: INFO: stdout: "update-demo-kitten-nsprl update-demo-kitten-xntn4 "
Sep 15 15:49:44.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-kitten-nsprl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2647'
Sep 15 15:49:45.017: INFO: stderr: ""
Sep 15 15:49:45.017: INFO: stdout: "true"
Sep 15 15:49:45.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-kitten-nsprl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2647'
Sep 15 15:49:45.086: INFO: stderr: ""
Sep 15 15:49:45.086: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 15 15:49:45.086: INFO: validating pod update-demo-kitten-nsprl
Sep 15 15:49:45.094: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 15 15:49:45.094: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 15 15:49:45.094: INFO: update-demo-kitten-nsprl is verified up and running
Sep 15 15:49:45.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-kitten-xntn4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2647'
Sep 15 15:49:45.166: INFO: stderr: ""
Sep 15 15:49:45.166: INFO: stdout: "true"
Sep 15 15:49:45.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-kitten-xntn4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2647'
Sep 15 15:49:45.235: INFO: stderr: ""
Sep 15 15:49:45.235: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 15 15:49:45.235: INFO: validating pod update-demo-kitten-xntn4
Sep 15 15:49:45.243: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 15 15:49:45.243: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 15 15:49:45.243: INFO: update-demo-kitten-xntn4 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:49:45.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2647" for this suite.
Sep 15 15:50:07.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:50:07.409: INFO: namespace kubectl-2647 deletion completed in 22.161529345s

• [SLOW TEST:51.058 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:50:07.409: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8229
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Sep 15 15:50:07.565: INFO: Waiting up to 5m0s for pod "var-expansion-5e1ba814-74fa-4324-8ce4-372d050c16fd" in namespace "var-expansion-8229" to be "success or failure"
Sep 15 15:50:07.571: INFO: Pod "var-expansion-5e1ba814-74fa-4324-8ce4-372d050c16fd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.695106ms
Sep 15 15:50:09.576: INFO: Pod "var-expansion-5e1ba814-74fa-4324-8ce4-372d050c16fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010525212s
STEP: Saw pod success
Sep 15 15:50:09.576: INFO: Pod "var-expansion-5e1ba814-74fa-4324-8ce4-372d050c16fd" satisfied condition "success or failure"
Sep 15 15:50:09.580: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod var-expansion-5e1ba814-74fa-4324-8ce4-372d050c16fd container dapi-container: <nil>
STEP: delete the pod
Sep 15 15:50:09.611: INFO: Waiting for pod var-expansion-5e1ba814-74fa-4324-8ce4-372d050c16fd to disappear
Sep 15 15:50:09.617: INFO: Pod var-expansion-5e1ba814-74fa-4324-8ce4-372d050c16fd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:50:09.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8229" for this suite.
Sep 15 15:50:15.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:50:15.801: INFO: namespace var-expansion-8229 deletion completed in 6.177984528s

• [SLOW TEST:8.392 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:50:15.801: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-k5w4
STEP: Creating a pod to test atomic-volume-subpath
Sep 15 15:50:15.968: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-k5w4" in namespace "subpath-2860" to be "success or failure"
Sep 15 15:50:15.974: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.594368ms
Sep 15 15:50:17.979: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011210027s
Sep 15 15:50:19.984: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 4.016185642s
Sep 15 15:50:21.989: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 6.021048393s
Sep 15 15:50:23.994: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 8.025918534s
Sep 15 15:50:25.999: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 10.031001077s
Sep 15 15:50:28.005: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 12.036819628s
Sep 15 15:50:30.011: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 14.042557733s
Sep 15 15:50:32.015: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 16.047245205s
Sep 15 15:50:34.020: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 18.051871359s
Sep 15 15:50:36.025: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Running", Reason="", readiness=true. Elapsed: 20.056575481s
Sep 15 15:50:38.030: INFO: Pod "pod-subpath-test-projected-k5w4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.061557948s
STEP: Saw pod success
Sep 15 15:50:38.030: INFO: Pod "pod-subpath-test-projected-k5w4" satisfied condition "success or failure"
Sep 15 15:50:38.035: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-subpath-test-projected-k5w4 container test-container-subpath-projected-k5w4: <nil>
STEP: delete the pod
Sep 15 15:50:38.069: INFO: Waiting for pod pod-subpath-test-projected-k5w4 to disappear
Sep 15 15:50:38.073: INFO: Pod pod-subpath-test-projected-k5w4 no longer exists
STEP: Deleting pod pod-subpath-test-projected-k5w4
Sep 15 15:50:38.073: INFO: Deleting pod "pod-subpath-test-projected-k5w4" in namespace "subpath-2860"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:50:38.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2860" for this suite.
Sep 15 15:50:44.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:50:44.239: INFO: namespace subpath-2860 deletion completed in 6.154318647s

• [SLOW TEST:28.438 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:50:44.240: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-95f76c81-c116-45ce-90df-ed9048066558
STEP: Creating a pod to test consume configMaps
Sep 15 15:50:44.400: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f59ffc7-f3ce-4881-a20d-d0a82b3067eb" in namespace "projected-3694" to be "success or failure"
Sep 15 15:50:44.407: INFO: Pod "pod-projected-configmaps-0f59ffc7-f3ce-4881-a20d-d0a82b3067eb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.699686ms
Sep 15 15:50:46.412: INFO: Pod "pod-projected-configmaps-0f59ffc7-f3ce-4881-a20d-d0a82b3067eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011360413s
STEP: Saw pod success
Sep 15 15:50:46.412: INFO: Pod "pod-projected-configmaps-0f59ffc7-f3ce-4881-a20d-d0a82b3067eb" satisfied condition "success or failure"
Sep 15 15:50:46.416: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-configmaps-0f59ffc7-f3ce-4881-a20d-d0a82b3067eb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 15:50:46.445: INFO: Waiting for pod pod-projected-configmaps-0f59ffc7-f3ce-4881-a20d-d0a82b3067eb to disappear
Sep 15 15:50:46.451: INFO: Pod pod-projected-configmaps-0f59ffc7-f3ce-4881-a20d-d0a82b3067eb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:50:46.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3694" for this suite.
Sep 15 15:50:52.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:50:52.639: INFO: namespace projected-3694 deletion completed in 6.18331922s

• [SLOW TEST:8.399 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:50:52.639: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4928
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 15 15:50:52.796: INFO: Waiting up to 5m0s for pod "pod-3c6d6abe-2c1c-4b0d-b1d7-fc46d0c9bc32" in namespace "emptydir-4928" to be "success or failure"
Sep 15 15:50:52.803: INFO: Pod "pod-3c6d6abe-2c1c-4b0d-b1d7-fc46d0c9bc32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013725ms
Sep 15 15:50:54.807: INFO: Pod "pod-3c6d6abe-2c1c-4b0d-b1d7-fc46d0c9bc32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010496179s
STEP: Saw pod success
Sep 15 15:50:54.807: INFO: Pod "pod-3c6d6abe-2c1c-4b0d-b1d7-fc46d0c9bc32" satisfied condition "success or failure"
Sep 15 15:50:54.811: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-3c6d6abe-2c1c-4b0d-b1d7-fc46d0c9bc32 container test-container: <nil>
STEP: delete the pod
Sep 15 15:50:54.840: INFO: Waiting for pod pod-3c6d6abe-2c1c-4b0d-b1d7-fc46d0c9bc32 to disappear
Sep 15 15:50:54.846: INFO: Pod pod-3c6d6abe-2c1c-4b0d-b1d7-fc46d0c9bc32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:50:54.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4928" for this suite.
Sep 15 15:51:00.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:51:01.027: INFO: namespace emptydir-4928 deletion completed in 6.176047282s

• [SLOW TEST:8.388 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:51:01.027: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-883
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 15 15:51:01.172: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 15 15:51:27.292: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.25:8080/dial?request=hostName&protocol=udp&host=100.96.1.21&port=8081&tries=1'] Namespace:pod-network-test-883 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 15:51:27.292: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 15:51:27.407: INFO: Waiting for endpoints: map[]
Sep 15 15:51:27.412: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.25:8080/dial?request=hostName&protocol=udp&host=100.96.3.23&port=8081&tries=1'] Namespace:pod-network-test-883 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 15:51:27.412: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 15:51:27.514: INFO: Waiting for endpoints: map[]
Sep 15 15:51:27.519: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.25:8080/dial?request=hostName&protocol=udp&host=100.96.2.24&port=8081&tries=1'] Namespace:pod-network-test-883 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 15:51:27.519: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 15:51:27.612: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:51:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-883" for this suite.
Sep 15 15:51:49.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:51:49.769: INFO: namespace pod-network-test-883 deletion completed in 22.151517703s

• [SLOW TEST:48.742 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:51:49.769: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5094
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep 15 15:51:55.955: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0915 15:51:55.955499      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:51:55.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5094" for this suite.
Sep 15 15:52:01.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:52:02.119: INFO: namespace gc-5094 deletion completed in 6.159251201s

• [SLOW TEST:12.350 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:52:02.120: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2120
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2120.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2120.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 15 15:52:14.334: INFO: DNS probes using dns-2120/dns-test-3c002399-9def-43ca-9349-e46e5a4aa320 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:52:14.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2120" for this suite.
Sep 15 15:52:20.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:52:20.517: INFO: namespace dns-2120 deletion completed in 6.158804478s

• [SLOW TEST:18.398 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:52:20.518: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 15:52:20.689: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 15 15:52:20.700: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:20.700: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:20.700: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:20.704: INFO: Number of nodes with available pods: 0
Sep 15 15:52:20.704: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 15:52:21.710: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:21.710: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:21.710: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:21.714: INFO: Number of nodes with available pods: 1
Sep 15 15:52:21.714: INFO: Node ip-172-16-49-162.ec2.internal is running more than one daemon pod
Sep 15 15:52:22.710: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:22.710: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:22.710: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:22.714: INFO: Number of nodes with available pods: 3
Sep 15 15:52:22.714: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 15 15:52:22.746: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:22.746: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:22.746: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:22.751: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:22.751: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:22.751: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:23.756: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:23.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:23.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:23.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:23.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:23.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:24.756: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:24.756: INFO: Pod daemon-set-ddd4p is not available
Sep 15 15:52:24.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:24.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:24.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:24.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:24.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:25.756: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:25.756: INFO: Pod daemon-set-ddd4p is not available
Sep 15 15:52:25.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:25.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:25.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:25.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:25.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:26.757: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:26.757: INFO: Pod daemon-set-ddd4p is not available
Sep 15 15:52:26.757: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:26.757: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:26.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:26.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:26.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:27.756: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:27.756: INFO: Pod daemon-set-ddd4p is not available
Sep 15 15:52:27.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:27.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:27.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:27.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:27.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:28.756: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:28.756: INFO: Pod daemon-set-ddd4p is not available
Sep 15 15:52:28.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:28.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:28.763: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:28.763: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:28.763: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:29.760: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:29.760: INFO: Pod daemon-set-ddd4p is not available
Sep 15 15:52:29.760: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:29.760: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:29.765: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:29.765: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:29.765: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:30.756: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:30.756: INFO: Pod daemon-set-ddd4p is not available
Sep 15 15:52:30.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:30.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:30.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:30.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:30.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:31.756: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:31.756: INFO: Pod daemon-set-ddd4p is not available
Sep 15 15:52:31.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:31.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:31.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:31.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:31.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:32.756: INFO: Wrong image for pod: daemon-set-ddd4p. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:32.756: INFO: Pod daemon-set-ddd4p is not available
Sep 15 15:52:32.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:32.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:32.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:32.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:32.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:33.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:33.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:33.756: INFO: Pod daemon-set-sqfzb is not available
Sep 15 15:52:33.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:33.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:33.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:34.757: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:34.757: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:34.757: INFO: Pod daemon-set-sqfzb is not available
Sep 15 15:52:34.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:34.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:34.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:35.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:35.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:35.756: INFO: Pod daemon-set-sqfzb is not available
Sep 15 15:52:35.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:35.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:35.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:36.756: INFO: Wrong image for pod: daemon-set-pvb7c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:36.756: INFO: Pod daemon-set-pvb7c is not available
Sep 15 15:52:36.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:36.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:36.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:36.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:37.756: INFO: Pod daemon-set-4jnbv is not available
Sep 15 15:52:37.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:37.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:37.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:37.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:38.757: INFO: Pod daemon-set-4jnbv is not available
Sep 15 15:52:38.757: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:38.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:38.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:38.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:39.756: INFO: Pod daemon-set-4jnbv is not available
Sep 15 15:52:39.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:39.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:39.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:39.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:40.757: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:40.763: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:40.763: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:40.763: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:41.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:41.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:41.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:41.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:41.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:42.757: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:42.757: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:42.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:42.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:42.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:43.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:43.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:43.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:43.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:43.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:44.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:44.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:44.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:44.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:44.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:45.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:45.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:45.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:45.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:45.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:46.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:46.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:46.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:46.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:46.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:47.757: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:47.757: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:47.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:47.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:47.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:48.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:48.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:48.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:48.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:48.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:49.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:49.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:49.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:49.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:49.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:50.757: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:50.757: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:50.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:50.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:50.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:51.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:51.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:51.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:51.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:51.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:52.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:52.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:52.761: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:52.761: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:52.761: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:53.756: INFO: Wrong image for pod: daemon-set-qj9rg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 15 15:52:53.756: INFO: Pod daemon-set-qj9rg is not available
Sep 15 15:52:53.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:53.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:53.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:54.756: INFO: Pod daemon-set-rdhmf is not available
Sep 15 15:52:54.762: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:54.762: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:54.762: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 15 15:52:54.767: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:54.767: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:54.767: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:54.771: INFO: Number of nodes with available pods: 2
Sep 15 15:52:54.771: INFO: Node ip-172-16-49-162.ec2.internal is running more than one daemon pod
Sep 15 15:52:55.777: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:55.777: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:55.777: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:55.781: INFO: Number of nodes with available pods: 2
Sep 15 15:52:55.781: INFO: Node ip-172-16-49-162.ec2.internal is running more than one daemon pod
Sep 15 15:52:56.777: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:56.777: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:56.777: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:56.781: INFO: Number of nodes with available pods: 2
Sep 15 15:52:56.781: INFO: Node ip-172-16-49-162.ec2.internal is running more than one daemon pod
Sep 15 15:52:57.777: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:57.777: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:57.777: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 15:52:57.782: INFO: Number of nodes with available pods: 3
Sep 15 15:52:57.782: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5752, will wait for the garbage collector to delete the pods
Sep 15 15:52:57.868: INFO: Deleting DaemonSet.extensions daemon-set took: 12.022209ms
Sep 15 15:52:58.268: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.253249ms
Sep 15 15:53:09.173: INFO: Number of nodes with available pods: 0
Sep 15 15:53:09.173: INFO: Number of running nodes: 0, number of available pods: 0
Sep 15 15:53:09.177: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5752/daemonsets","resourceVersion":"6343"},"items":null}

Sep 15 15:53:09.181: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5752/pods","resourceVersion":"6343"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:53:09.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5752" for this suite.
Sep 15 15:53:15.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:53:15.359: INFO: namespace daemonsets-5752 deletion completed in 6.155103573s

• [SLOW TEST:54.841 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:53:15.359: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-448
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6664
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:53:39.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4655" for this suite.
Sep 15 15:53:45.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:53:45.997: INFO: namespace namespaces-4655 deletion completed in 6.159910608s
STEP: Destroying namespace "nsdeletetest-448" for this suite.
Sep 15 15:53:46.000: INFO: Namespace nsdeletetest-448 was already deleted
STEP: Destroying namespace "nsdeletetest-6664" for this suite.
Sep 15 15:53:52.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:53:52.183: INFO: namespace nsdeletetest-6664 deletion completed in 6.182315252s

• [SLOW TEST:36.824 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:53:52.183: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4799
I0915 15:53:52.340022      19 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4799, replica count: 1
I0915 15:53:53.390496      19 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 15 15:53:53.509: INFO: Created: latency-svc-ch8bf
Sep 15 15:53:53.514: INFO: Got endpoints: latency-svc-ch8bf [23.341783ms]
Sep 15 15:53:53.532: INFO: Created: latency-svc-rd7sl
Sep 15 15:53:53.541: INFO: Got endpoints: latency-svc-rd7sl [27.411739ms]
Sep 15 15:53:53.546: INFO: Created: latency-svc-r94bn
Sep 15 15:53:53.551: INFO: Got endpoints: latency-svc-r94bn [36.867276ms]
Sep 15 15:53:53.559: INFO: Created: latency-svc-pwvtk
Sep 15 15:53:53.568: INFO: Got endpoints: latency-svc-pwvtk [54.191561ms]
Sep 15 15:53:53.583: INFO: Created: latency-svc-6cbsv
Sep 15 15:53:53.587: INFO: Got endpoints: latency-svc-6cbsv [73.385947ms]
Sep 15 15:53:53.628: INFO: Created: latency-svc-fbmbf
Sep 15 15:53:53.653: INFO: Got endpoints: latency-svc-fbmbf [139.362694ms]
Sep 15 15:53:53.657: INFO: Created: latency-svc-hzbfq
Sep 15 15:53:53.738: INFO: Got endpoints: latency-svc-hzbfq [223.849879ms]
Sep 15 15:53:53.743: INFO: Created: latency-svc-9dtpp
Sep 15 15:53:53.752: INFO: Got endpoints: latency-svc-9dtpp [238.285761ms]
Sep 15 15:53:53.757: INFO: Created: latency-svc-mvhpb
Sep 15 15:53:53.763: INFO: Got endpoints: latency-svc-mvhpb [249.003632ms]
Sep 15 15:53:53.772: INFO: Created: latency-svc-nbfxx
Sep 15 15:53:53.779: INFO: Got endpoints: latency-svc-nbfxx [264.77513ms]
Sep 15 15:53:53.784: INFO: Created: latency-svc-rhb6x
Sep 15 15:53:53.793: INFO: Got endpoints: latency-svc-rhb6x [278.689771ms]
Sep 15 15:53:53.798: INFO: Created: latency-svc-fb4qf
Sep 15 15:53:53.809: INFO: Got endpoints: latency-svc-fb4qf [294.687669ms]
Sep 15 15:53:53.815: INFO: Created: latency-svc-v75cz
Sep 15 15:53:53.825: INFO: Got endpoints: latency-svc-v75cz [310.861415ms]
Sep 15 15:53:53.832: INFO: Created: latency-svc-cvjcv
Sep 15 15:53:53.836: INFO: Got endpoints: latency-svc-cvjcv [321.529699ms]
Sep 15 15:53:53.850: INFO: Created: latency-svc-xm2l2
Sep 15 15:53:53.858: INFO: Got endpoints: latency-svc-xm2l2 [343.963386ms]
Sep 15 15:53:53.861: INFO: Created: latency-svc-tmmxj
Sep 15 15:53:53.867: INFO: Got endpoints: latency-svc-tmmxj [352.830077ms]
Sep 15 15:53:53.877: INFO: Created: latency-svc-mnxxk
Sep 15 15:53:53.882: INFO: Got endpoints: latency-svc-mnxxk [340.941828ms]
Sep 15 15:53:53.897: INFO: Created: latency-svc-8db82
Sep 15 15:53:53.902: INFO: Got endpoints: latency-svc-8db82 [350.903955ms]
Sep 15 15:53:53.911: INFO: Created: latency-svc-845nn
Sep 15 15:53:53.922: INFO: Got endpoints: latency-svc-845nn [353.710273ms]
Sep 15 15:53:53.926: INFO: Created: latency-svc-g2dpt
Sep 15 15:53:53.937: INFO: Got endpoints: latency-svc-g2dpt [349.557213ms]
Sep 15 15:53:53.942: INFO: Created: latency-svc-s6lxj
Sep 15 15:53:53.947: INFO: Got endpoints: latency-svc-s6lxj [293.415549ms]
Sep 15 15:53:53.958: INFO: Created: latency-svc-5bc7t
Sep 15 15:53:53.966: INFO: Got endpoints: latency-svc-5bc7t [228.044142ms]
Sep 15 15:53:53.971: INFO: Created: latency-svc-psqgz
Sep 15 15:53:53.980: INFO: Got endpoints: latency-svc-psqgz [227.207409ms]
Sep 15 15:53:53.986: INFO: Created: latency-svc-xldc8
Sep 15 15:53:53.989: INFO: Got endpoints: latency-svc-xldc8 [225.870361ms]
Sep 15 15:53:54.001: INFO: Created: latency-svc-jdch9
Sep 15 15:53:54.016: INFO: Got endpoints: latency-svc-jdch9 [236.534339ms]
Sep 15 15:53:54.022: INFO: Created: latency-svc-kzmvm
Sep 15 15:53:54.027: INFO: Got endpoints: latency-svc-kzmvm [233.583945ms]
Sep 15 15:53:54.036: INFO: Created: latency-svc-qxpk5
Sep 15 15:53:54.048: INFO: Got endpoints: latency-svc-qxpk5 [238.60408ms]
Sep 15 15:53:54.052: INFO: Created: latency-svc-spbbr
Sep 15 15:53:54.062: INFO: Got endpoints: latency-svc-spbbr [236.491641ms]
Sep 15 15:53:54.067: INFO: Created: latency-svc-jlnj5
Sep 15 15:53:54.081: INFO: Got endpoints: latency-svc-jlnj5 [244.979576ms]
Sep 15 15:53:54.089: INFO: Created: latency-svc-8wzk9
Sep 15 15:53:54.099: INFO: Got endpoints: latency-svc-8wzk9 [240.899923ms]
Sep 15 15:53:54.103: INFO: Created: latency-svc-j2sqm
Sep 15 15:53:54.109: INFO: Got endpoints: latency-svc-j2sqm [242.16256ms]
Sep 15 15:53:54.118: INFO: Created: latency-svc-r84x5
Sep 15 15:53:54.128: INFO: Got endpoints: latency-svc-r84x5 [245.916878ms]
Sep 15 15:53:54.134: INFO: Created: latency-svc-9jmc7
Sep 15 15:53:54.144: INFO: Got endpoints: latency-svc-9jmc7 [242.024082ms]
Sep 15 15:53:54.147: INFO: Created: latency-svc-9skn7
Sep 15 15:53:54.156: INFO: Got endpoints: latency-svc-9skn7 [233.696974ms]
Sep 15 15:53:54.161: INFO: Created: latency-svc-9vzqq
Sep 15 15:53:54.170: INFO: Got endpoints: latency-svc-9vzqq [233.404988ms]
Sep 15 15:53:54.175: INFO: Created: latency-svc-sfv5l
Sep 15 15:53:54.183: INFO: Got endpoints: latency-svc-sfv5l [236.326529ms]
Sep 15 15:53:54.187: INFO: Created: latency-svc-kkv48
Sep 15 15:53:54.194: INFO: Got endpoints: latency-svc-kkv48 [228.391247ms]
Sep 15 15:53:54.200: INFO: Created: latency-svc-m6nsv
Sep 15 15:53:54.209: INFO: Got endpoints: latency-svc-m6nsv [229.502824ms]
Sep 15 15:53:54.215: INFO: Created: latency-svc-8srz4
Sep 15 15:53:54.224: INFO: Got endpoints: latency-svc-8srz4 [234.805468ms]
Sep 15 15:53:54.229: INFO: Created: latency-svc-5q9zv
Sep 15 15:53:54.237: INFO: Got endpoints: latency-svc-5q9zv [27.398629ms]
Sep 15 15:53:54.241: INFO: Created: latency-svc-ghk98
Sep 15 15:53:54.250: INFO: Got endpoints: latency-svc-ghk98 [234.464431ms]
Sep 15 15:53:54.256: INFO: Created: latency-svc-kpkgv
Sep 15 15:53:54.261: INFO: Got endpoints: latency-svc-kpkgv [234.040996ms]
Sep 15 15:53:54.271: INFO: Created: latency-svc-5rt2p
Sep 15 15:53:54.281: INFO: Got endpoints: latency-svc-5rt2p [233.005973ms]
Sep 15 15:53:54.284: INFO: Created: latency-svc-zjd88
Sep 15 15:53:54.294: INFO: Got endpoints: latency-svc-zjd88 [232.163315ms]
Sep 15 15:53:54.298: INFO: Created: latency-svc-d87td
Sep 15 15:53:54.305: INFO: Got endpoints: latency-svc-d87td [224.435784ms]
Sep 15 15:53:54.312: INFO: Created: latency-svc-5qqrs
Sep 15 15:53:54.320: INFO: Got endpoints: latency-svc-5qqrs [221.147343ms]
Sep 15 15:53:54.325: INFO: Created: latency-svc-gvttt
Sep 15 15:53:54.339: INFO: Created: latency-svc-fs7v6
Sep 15 15:53:54.349: INFO: Created: latency-svc-nsfc8
Sep 15 15:53:54.367: INFO: Got endpoints: latency-svc-gvttt [257.088013ms]
Sep 15 15:53:54.367: INFO: Created: latency-svc-44fs6
Sep 15 15:53:54.379: INFO: Created: latency-svc-cfwc8
Sep 15 15:53:54.392: INFO: Created: latency-svc-xs7vw
Sep 15 15:53:54.404: INFO: Created: latency-svc-j9wvh
Sep 15 15:53:54.419: INFO: Got endpoints: latency-svc-fs7v6 [290.585347ms]
Sep 15 15:53:54.424: INFO: Created: latency-svc-4gwzz
Sep 15 15:53:54.435: INFO: Created: latency-svc-xhj7f
Sep 15 15:53:54.449: INFO: Created: latency-svc-qqqjk
Sep 15 15:53:54.463: INFO: Created: latency-svc-gm6r7
Sep 15 15:53:54.464: INFO: Got endpoints: latency-svc-nsfc8 [320.250107ms]
Sep 15 15:53:54.474: INFO: Created: latency-svc-pcsjn
Sep 15 15:53:54.488: INFO: Created: latency-svc-ntxln
Sep 15 15:53:54.500: INFO: Created: latency-svc-fwrsb
Sep 15 15:53:54.511: INFO: Created: latency-svc-gj98j
Sep 15 15:53:54.514: INFO: Got endpoints: latency-svc-44fs6 [358.03725ms]
Sep 15 15:53:54.524: INFO: Created: latency-svc-qsb8t
Sep 15 15:53:54.536: INFO: Created: latency-svc-njsbt
Sep 15 15:53:54.548: INFO: Created: latency-svc-tf96f
Sep 15 15:53:54.560: INFO: Created: latency-svc-qhw9w
Sep 15 15:53:54.564: INFO: Got endpoints: latency-svc-cfwc8 [393.574046ms]
Sep 15 15:53:54.595: INFO: Created: latency-svc-4vnn6
Sep 15 15:53:54.615: INFO: Got endpoints: latency-svc-xs7vw [432.092079ms]
Sep 15 15:53:54.632: INFO: Created: latency-svc-hmz4m
Sep 15 15:53:54.664: INFO: Got endpoints: latency-svc-j9wvh [469.618034ms]
Sep 15 15:53:54.682: INFO: Created: latency-svc-6zgk4
Sep 15 15:53:54.714: INFO: Got endpoints: latency-svc-4gwzz [490.041886ms]
Sep 15 15:53:54.732: INFO: Created: latency-svc-dnxbp
Sep 15 15:53:54.764: INFO: Got endpoints: latency-svc-xhj7f [527.239871ms]
Sep 15 15:53:54.782: INFO: Created: latency-svc-9m4kg
Sep 15 15:53:54.814: INFO: Got endpoints: latency-svc-qqqjk [564.353098ms]
Sep 15 15:53:54.834: INFO: Created: latency-svc-mg6zn
Sep 15 15:53:54.864: INFO: Got endpoints: latency-svc-gm6r7 [603.010291ms]
Sep 15 15:53:54.884: INFO: Created: latency-svc-jrnvw
Sep 15 15:53:54.914: INFO: Got endpoints: latency-svc-pcsjn [633.420406ms]
Sep 15 15:53:54.951: INFO: Created: latency-svc-fc292
Sep 15 15:53:54.964: INFO: Got endpoints: latency-svc-ntxln [669.891354ms]
Sep 15 15:53:54.983: INFO: Created: latency-svc-psl9x
Sep 15 15:53:55.015: INFO: Got endpoints: latency-svc-fwrsb [709.430044ms]
Sep 15 15:53:55.033: INFO: Created: latency-svc-b8hjp
Sep 15 15:53:55.064: INFO: Got endpoints: latency-svc-gj98j [743.886462ms]
Sep 15 15:53:55.082: INFO: Created: latency-svc-qf5dh
Sep 15 15:53:55.115: INFO: Got endpoints: latency-svc-qsb8t [748.007556ms]
Sep 15 15:53:55.133: INFO: Created: latency-svc-l522g
Sep 15 15:53:55.164: INFO: Got endpoints: latency-svc-njsbt [745.525778ms]
Sep 15 15:53:55.182: INFO: Created: latency-svc-ccl54
Sep 15 15:53:55.215: INFO: Got endpoints: latency-svc-tf96f [751.179289ms]
Sep 15 15:53:55.234: INFO: Created: latency-svc-vw5xk
Sep 15 15:53:55.264: INFO: Got endpoints: latency-svc-qhw9w [750.102384ms]
Sep 15 15:53:55.283: INFO: Created: latency-svc-wvz4n
Sep 15 15:53:55.314: INFO: Got endpoints: latency-svc-4vnn6 [749.85956ms]
Sep 15 15:53:55.332: INFO: Created: latency-svc-ktsqf
Sep 15 15:53:55.365: INFO: Got endpoints: latency-svc-hmz4m [749.255854ms]
Sep 15 15:53:55.382: INFO: Created: latency-svc-xz8hk
Sep 15 15:53:55.415: INFO: Got endpoints: latency-svc-6zgk4 [750.972665ms]
Sep 15 15:53:55.433: INFO: Created: latency-svc-vgqpm
Sep 15 15:53:55.465: INFO: Got endpoints: latency-svc-dnxbp [750.68131ms]
Sep 15 15:53:55.493: INFO: Created: latency-svc-kb4jw
Sep 15 15:53:55.514: INFO: Got endpoints: latency-svc-9m4kg [750.265447ms]
Sep 15 15:53:55.532: INFO: Created: latency-svc-mt589
Sep 15 15:53:55.564: INFO: Got endpoints: latency-svc-mg6zn [749.644008ms]
Sep 15 15:53:55.582: INFO: Created: latency-svc-x8lkz
Sep 15 15:53:55.615: INFO: Got endpoints: latency-svc-jrnvw [750.79945ms]
Sep 15 15:53:55.635: INFO: Created: latency-svc-nj7r9
Sep 15 15:53:55.664: INFO: Got endpoints: latency-svc-fc292 [749.785341ms]
Sep 15 15:53:55.682: INFO: Created: latency-svc-ktpl8
Sep 15 15:53:55.716: INFO: Got endpoints: latency-svc-psl9x [752.114352ms]
Sep 15 15:53:55.733: INFO: Created: latency-svc-gd6f8
Sep 15 15:53:55.764: INFO: Got endpoints: latency-svc-b8hjp [749.569604ms]
Sep 15 15:53:55.783: INFO: Created: latency-svc-tkf5p
Sep 15 15:53:55.815: INFO: Got endpoints: latency-svc-qf5dh [750.301585ms]
Sep 15 15:53:55.833: INFO: Created: latency-svc-t5cx6
Sep 15 15:53:55.864: INFO: Got endpoints: latency-svc-l522g [749.56072ms]
Sep 15 15:53:55.880: INFO: Created: latency-svc-xrmk2
Sep 15 15:53:55.913: INFO: Got endpoints: latency-svc-ccl54 [749.145882ms]
Sep 15 15:53:55.932: INFO: Created: latency-svc-rmbz9
Sep 15 15:53:55.964: INFO: Got endpoints: latency-svc-vw5xk [748.674502ms]
Sep 15 15:53:55.981: INFO: Created: latency-svc-nbj2j
Sep 15 15:53:56.015: INFO: Got endpoints: latency-svc-wvz4n [751.525665ms]
Sep 15 15:53:56.034: INFO: Created: latency-svc-sgmnk
Sep 15 15:53:56.064: INFO: Got endpoints: latency-svc-ktsqf [750.503195ms]
Sep 15 15:53:56.082: INFO: Created: latency-svc-ln67b
Sep 15 15:53:56.114: INFO: Got endpoints: latency-svc-xz8hk [749.697291ms]
Sep 15 15:53:56.134: INFO: Created: latency-svc-zphx9
Sep 15 15:53:56.164: INFO: Got endpoints: latency-svc-vgqpm [749.182013ms]
Sep 15 15:53:56.183: INFO: Created: latency-svc-nwwqj
Sep 15 15:53:56.214: INFO: Got endpoints: latency-svc-kb4jw [749.371751ms]
Sep 15 15:53:56.229: INFO: Created: latency-svc-pvnjk
Sep 15 15:53:56.264: INFO: Got endpoints: latency-svc-mt589 [749.914576ms]
Sep 15 15:53:56.283: INFO: Created: latency-svc-c2mzr
Sep 15 15:53:56.314: INFO: Got endpoints: latency-svc-x8lkz [750.057862ms]
Sep 15 15:53:56.332: INFO: Created: latency-svc-b5nnf
Sep 15 15:53:56.364: INFO: Got endpoints: latency-svc-nj7r9 [749.56936ms]
Sep 15 15:53:56.381: INFO: Created: latency-svc-r8tqp
Sep 15 15:53:56.414: INFO: Got endpoints: latency-svc-ktpl8 [750.016962ms]
Sep 15 15:53:56.429: INFO: Created: latency-svc-lqzpt
Sep 15 15:53:56.464: INFO: Got endpoints: latency-svc-gd6f8 [747.847135ms]
Sep 15 15:53:56.480: INFO: Created: latency-svc-wk2gp
Sep 15 15:53:56.514: INFO: Got endpoints: latency-svc-tkf5p [749.462369ms]
Sep 15 15:53:56.531: INFO: Created: latency-svc-ms2nr
Sep 15 15:53:56.564: INFO: Got endpoints: latency-svc-t5cx6 [749.173088ms]
Sep 15 15:53:56.581: INFO: Created: latency-svc-5pz2p
Sep 15 15:53:56.614: INFO: Got endpoints: latency-svc-xrmk2 [749.459349ms]
Sep 15 15:53:56.631: INFO: Created: latency-svc-xsqk2
Sep 15 15:53:56.665: INFO: Got endpoints: latency-svc-rmbz9 [751.033171ms]
Sep 15 15:53:56.681: INFO: Created: latency-svc-tk6fk
Sep 15 15:53:56.714: INFO: Got endpoints: latency-svc-nbj2j [750.077418ms]
Sep 15 15:53:56.736: INFO: Created: latency-svc-c2lwp
Sep 15 15:53:56.767: INFO: Got endpoints: latency-svc-sgmnk [751.233895ms]
Sep 15 15:53:56.798: INFO: Created: latency-svc-ffxqf
Sep 15 15:53:56.814: INFO: Got endpoints: latency-svc-ln67b [749.367968ms]
Sep 15 15:53:56.831: INFO: Created: latency-svc-sbhhh
Sep 15 15:53:56.864: INFO: Got endpoints: latency-svc-zphx9 [750.152831ms]
Sep 15 15:53:56.883: INFO: Created: latency-svc-pm55x
Sep 15 15:53:56.915: INFO: Got endpoints: latency-svc-nwwqj [750.49482ms]
Sep 15 15:53:56.945: INFO: Created: latency-svc-hvltn
Sep 15 15:53:56.966: INFO: Got endpoints: latency-svc-pvnjk [751.438333ms]
Sep 15 15:53:56.988: INFO: Created: latency-svc-sffqj
Sep 15 15:53:57.014: INFO: Got endpoints: latency-svc-c2mzr [750.045082ms]
Sep 15 15:53:57.030: INFO: Created: latency-svc-rntg5
Sep 15 15:53:57.064: INFO: Got endpoints: latency-svc-b5nnf [749.679607ms]
Sep 15 15:53:57.083: INFO: Created: latency-svc-cj284
Sep 15 15:53:57.114: INFO: Got endpoints: latency-svc-r8tqp [749.980517ms]
Sep 15 15:53:57.130: INFO: Created: latency-svc-ztz6h
Sep 15 15:53:57.165: INFO: Got endpoints: latency-svc-lqzpt [751.024603ms]
Sep 15 15:53:57.186: INFO: Created: latency-svc-rh8zq
Sep 15 15:53:57.215: INFO: Got endpoints: latency-svc-wk2gp [750.624667ms]
Sep 15 15:53:57.233: INFO: Created: latency-svc-k957r
Sep 15 15:53:57.263: INFO: Got endpoints: latency-svc-ms2nr [749.203054ms]
Sep 15 15:53:57.282: INFO: Created: latency-svc-gmw49
Sep 15 15:53:57.314: INFO: Got endpoints: latency-svc-5pz2p [749.788396ms]
Sep 15 15:53:57.332: INFO: Created: latency-svc-c9w2q
Sep 15 15:53:57.366: INFO: Got endpoints: latency-svc-xsqk2 [752.42679ms]
Sep 15 15:53:57.386: INFO: Created: latency-svc-cb2ft
Sep 15 15:53:57.414: INFO: Got endpoints: latency-svc-tk6fk [749.782815ms]
Sep 15 15:53:57.432: INFO: Created: latency-svc-pcsl8
Sep 15 15:53:57.463: INFO: Got endpoints: latency-svc-c2lwp [749.476496ms]
Sep 15 15:53:57.480: INFO: Created: latency-svc-rlmbw
Sep 15 15:53:57.513: INFO: Got endpoints: latency-svc-ffxqf [746.632902ms]
Sep 15 15:53:57.531: INFO: Created: latency-svc-5b9vj
Sep 15 15:53:57.565: INFO: Got endpoints: latency-svc-sbhhh [750.875553ms]
Sep 15 15:53:57.590: INFO: Created: latency-svc-cmclr
Sep 15 15:53:57.614: INFO: Got endpoints: latency-svc-pm55x [749.581012ms]
Sep 15 15:53:57.630: INFO: Created: latency-svc-phwzv
Sep 15 15:53:57.664: INFO: Got endpoints: latency-svc-hvltn [749.157387ms]
Sep 15 15:53:57.681: INFO: Created: latency-svc-67xcx
Sep 15 15:53:57.714: INFO: Got endpoints: latency-svc-sffqj [748.088046ms]
Sep 15 15:53:57.731: INFO: Created: latency-svc-cvln2
Sep 15 15:53:57.764: INFO: Got endpoints: latency-svc-rntg5 [749.816135ms]
Sep 15 15:53:57.781: INFO: Created: latency-svc-5cwlm
Sep 15 15:53:57.814: INFO: Got endpoints: latency-svc-cj284 [749.95264ms]
Sep 15 15:53:57.830: INFO: Created: latency-svc-68xv4
Sep 15 15:53:57.865: INFO: Got endpoints: latency-svc-ztz6h [750.350668ms]
Sep 15 15:53:57.883: INFO: Created: latency-svc-5wvqx
Sep 15 15:53:57.914: INFO: Got endpoints: latency-svc-rh8zq [748.9784ms]
Sep 15 15:53:57.932: INFO: Created: latency-svc-bc64n
Sep 15 15:53:57.964: INFO: Got endpoints: latency-svc-k957r [749.916149ms]
Sep 15 15:53:57.983: INFO: Created: latency-svc-q2xsf
Sep 15 15:53:58.014: INFO: Got endpoints: latency-svc-gmw49 [750.704882ms]
Sep 15 15:53:58.034: INFO: Created: latency-svc-zl4g5
Sep 15 15:53:58.064: INFO: Got endpoints: latency-svc-c9w2q [750.413317ms]
Sep 15 15:53:58.087: INFO: Created: latency-svc-sl4xw
Sep 15 15:53:58.121: INFO: Got endpoints: latency-svc-cb2ft [754.315065ms]
Sep 15 15:53:58.138: INFO: Created: latency-svc-mnc7j
Sep 15 15:53:58.164: INFO: Got endpoints: latency-svc-pcsl8 [749.415556ms]
Sep 15 15:53:58.182: INFO: Created: latency-svc-kdb2n
Sep 15 15:53:58.213: INFO: Got endpoints: latency-svc-rlmbw [749.799658ms]
Sep 15 15:53:58.230: INFO: Created: latency-svc-6lbhx
Sep 15 15:53:58.264: INFO: Got endpoints: latency-svc-5b9vj [750.027788ms]
Sep 15 15:53:58.280: INFO: Created: latency-svc-fsw58
Sep 15 15:53:58.314: INFO: Got endpoints: latency-svc-cmclr [749.339081ms]
Sep 15 15:53:58.332: INFO: Created: latency-svc-fj27v
Sep 15 15:53:58.364: INFO: Got endpoints: latency-svc-phwzv [750.127995ms]
Sep 15 15:53:58.382: INFO: Created: latency-svc-dk4ww
Sep 15 15:53:58.414: INFO: Got endpoints: latency-svc-67xcx [749.722066ms]
Sep 15 15:53:58.435: INFO: Created: latency-svc-wtvf2
Sep 15 15:53:58.464: INFO: Got endpoints: latency-svc-cvln2 [750.248311ms]
Sep 15 15:53:58.484: INFO: Created: latency-svc-xt5l8
Sep 15 15:53:58.515: INFO: Got endpoints: latency-svc-5cwlm [750.821665ms]
Sep 15 15:53:58.559: INFO: Created: latency-svc-mjz5n
Sep 15 15:53:58.640: INFO: Got endpoints: latency-svc-68xv4 [826.008874ms]
Sep 15 15:53:58.643: INFO: Got endpoints: latency-svc-5wvqx [778.209151ms]
Sep 15 15:53:58.672: INFO: Created: latency-svc-792g8
Sep 15 15:53:58.674: INFO: Got endpoints: latency-svc-bc64n [759.84049ms]
Sep 15 15:53:58.691: INFO: Created: latency-svc-ql8xq
Sep 15 15:53:58.705: INFO: Created: latency-svc-tbtfl
Sep 15 15:53:58.714: INFO: Got endpoints: latency-svc-q2xsf [749.515875ms]
Sep 15 15:53:58.731: INFO: Created: latency-svc-l5cs6
Sep 15 15:53:58.764: INFO: Got endpoints: latency-svc-zl4g5 [749.888811ms]
Sep 15 15:53:58.781: INFO: Created: latency-svc-mfmth
Sep 15 15:53:58.816: INFO: Got endpoints: latency-svc-sl4xw [752.162939ms]
Sep 15 15:53:58.834: INFO: Created: latency-svc-vgr4q
Sep 15 15:53:58.864: INFO: Got endpoints: latency-svc-mnc7j [743.166179ms]
Sep 15 15:53:58.881: INFO: Created: latency-svc-lndhz
Sep 15 15:53:58.914: INFO: Got endpoints: latency-svc-kdb2n [749.507035ms]
Sep 15 15:53:58.930: INFO: Created: latency-svc-frfb9
Sep 15 15:53:58.965: INFO: Got endpoints: latency-svc-6lbhx [751.453414ms]
Sep 15 15:53:58.981: INFO: Created: latency-svc-g5cnj
Sep 15 15:53:59.013: INFO: Got endpoints: latency-svc-fsw58 [749.912217ms]
Sep 15 15:53:59.034: INFO: Created: latency-svc-p9rk2
Sep 15 15:53:59.067: INFO: Got endpoints: latency-svc-fj27v [752.984692ms]
Sep 15 15:53:59.083: INFO: Created: latency-svc-hj6sx
Sep 15 15:53:59.114: INFO: Got endpoints: latency-svc-dk4ww [749.65243ms]
Sep 15 15:53:59.132: INFO: Created: latency-svc-xsc8r
Sep 15 15:53:59.164: INFO: Got endpoints: latency-svc-wtvf2 [750.038647ms]
Sep 15 15:53:59.185: INFO: Created: latency-svc-glxml
Sep 15 15:53:59.214: INFO: Got endpoints: latency-svc-xt5l8 [750.376086ms]
Sep 15 15:53:59.232: INFO: Created: latency-svc-rhfbw
Sep 15 15:53:59.264: INFO: Got endpoints: latency-svc-mjz5n [749.398223ms]
Sep 15 15:53:59.283: INFO: Created: latency-svc-8bk8x
Sep 15 15:53:59.314: INFO: Got endpoints: latency-svc-792g8 [673.849904ms]
Sep 15 15:53:59.331: INFO: Created: latency-svc-57q6r
Sep 15 15:53:59.365: INFO: Got endpoints: latency-svc-ql8xq [721.700337ms]
Sep 15 15:53:59.384: INFO: Created: latency-svc-wz4gd
Sep 15 15:53:59.415: INFO: Got endpoints: latency-svc-tbtfl [740.506675ms]
Sep 15 15:53:59.432: INFO: Created: latency-svc-mk84p
Sep 15 15:53:59.465: INFO: Got endpoints: latency-svc-l5cs6 [750.474166ms]
Sep 15 15:53:59.485: INFO: Created: latency-svc-82hzs
Sep 15 15:53:59.514: INFO: Got endpoints: latency-svc-mfmth [750.243227ms]
Sep 15 15:53:59.532: INFO: Created: latency-svc-9mn72
Sep 15 15:53:59.564: INFO: Got endpoints: latency-svc-vgr4q [747.56751ms]
Sep 15 15:53:59.580: INFO: Created: latency-svc-2djzf
Sep 15 15:53:59.614: INFO: Got endpoints: latency-svc-lndhz [750.467582ms]
Sep 15 15:53:59.635: INFO: Created: latency-svc-pr7pd
Sep 15 15:53:59.665: INFO: Got endpoints: latency-svc-frfb9 [750.995723ms]
Sep 15 15:53:59.684: INFO: Created: latency-svc-ml5zw
Sep 15 15:53:59.714: INFO: Got endpoints: latency-svc-g5cnj [749.082501ms]
Sep 15 15:53:59.730: INFO: Created: latency-svc-s5plt
Sep 15 15:53:59.764: INFO: Got endpoints: latency-svc-p9rk2 [750.633218ms]
Sep 15 15:53:59.780: INFO: Created: latency-svc-fbxqx
Sep 15 15:53:59.815: INFO: Got endpoints: latency-svc-hj6sx [747.715613ms]
Sep 15 15:53:59.836: INFO: Created: latency-svc-r9c49
Sep 15 15:53:59.864: INFO: Got endpoints: latency-svc-xsc8r [750.262795ms]
Sep 15 15:53:59.884: INFO: Created: latency-svc-vfq6z
Sep 15 15:53:59.914: INFO: Got endpoints: latency-svc-glxml [750.315043ms]
Sep 15 15:53:59.930: INFO: Created: latency-svc-7h6js
Sep 15 15:53:59.964: INFO: Got endpoints: latency-svc-rhfbw [749.096912ms]
Sep 15 15:53:59.986: INFO: Created: latency-svc-5mb4k
Sep 15 15:54:00.015: INFO: Got endpoints: latency-svc-8bk8x [750.909776ms]
Sep 15 15:54:00.034: INFO: Created: latency-svc-nwzkl
Sep 15 15:54:00.064: INFO: Got endpoints: latency-svc-57q6r [750.381929ms]
Sep 15 15:54:00.083: INFO: Created: latency-svc-fv5p2
Sep 15 15:54:00.114: INFO: Got endpoints: latency-svc-wz4gd [749.276193ms]
Sep 15 15:54:00.131: INFO: Created: latency-svc-62fnc
Sep 15 15:54:00.165: INFO: Got endpoints: latency-svc-mk84p [750.204814ms]
Sep 15 15:54:00.183: INFO: Created: latency-svc-hrhtr
Sep 15 15:54:00.215: INFO: Got endpoints: latency-svc-82hzs [750.786447ms]
Sep 15 15:54:00.232: INFO: Created: latency-svc-xdvvl
Sep 15 15:54:00.264: INFO: Got endpoints: latency-svc-9mn72 [749.419906ms]
Sep 15 15:54:00.281: INFO: Created: latency-svc-5k6hb
Sep 15 15:54:00.318: INFO: Got endpoints: latency-svc-2djzf [754.107462ms]
Sep 15 15:54:00.337: INFO: Created: latency-svc-jxzkz
Sep 15 15:54:00.364: INFO: Got endpoints: latency-svc-pr7pd [749.827132ms]
Sep 15 15:54:00.383: INFO: Created: latency-svc-z5dww
Sep 15 15:54:00.414: INFO: Got endpoints: latency-svc-ml5zw [748.834805ms]
Sep 15 15:54:00.435: INFO: Created: latency-svc-4vplt
Sep 15 15:54:00.464: INFO: Got endpoints: latency-svc-s5plt [750.247318ms]
Sep 15 15:54:00.482: INFO: Created: latency-svc-qnbvh
Sep 15 15:54:00.515: INFO: Got endpoints: latency-svc-fbxqx [751.108733ms]
Sep 15 15:54:00.543: INFO: Created: latency-svc-2q445
Sep 15 15:54:00.564: INFO: Got endpoints: latency-svc-r9c49 [749.406229ms]
Sep 15 15:54:00.581: INFO: Created: latency-svc-twkgl
Sep 15 15:54:00.626: INFO: Got endpoints: latency-svc-vfq6z [761.508397ms]
Sep 15 15:54:00.646: INFO: Created: latency-svc-8425l
Sep 15 15:54:00.665: INFO: Got endpoints: latency-svc-7h6js [750.104662ms]
Sep 15 15:54:00.683: INFO: Created: latency-svc-gfpq6
Sep 15 15:54:00.714: INFO: Got endpoints: latency-svc-5mb4k [750.502205ms]
Sep 15 15:54:00.735: INFO: Created: latency-svc-lqjwq
Sep 15 15:54:00.765: INFO: Got endpoints: latency-svc-nwzkl [749.445007ms]
Sep 15 15:54:00.786: INFO: Created: latency-svc-br22q
Sep 15 15:54:00.815: INFO: Got endpoints: latency-svc-fv5p2 [750.57752ms]
Sep 15 15:54:00.832: INFO: Created: latency-svc-dr45c
Sep 15 15:54:00.864: INFO: Got endpoints: latency-svc-62fnc [749.647729ms]
Sep 15 15:54:00.882: INFO: Created: latency-svc-42qr9
Sep 15 15:54:00.914: INFO: Got endpoints: latency-svc-hrhtr [748.719263ms]
Sep 15 15:54:00.931: INFO: Created: latency-svc-mpx96
Sep 15 15:54:00.964: INFO: Got endpoints: latency-svc-xdvvl [748.235929ms]
Sep 15 15:54:00.982: INFO: Created: latency-svc-zpgpp
Sep 15 15:54:01.014: INFO: Got endpoints: latency-svc-5k6hb [750.194265ms]
Sep 15 15:54:01.030: INFO: Created: latency-svc-lqr2w
Sep 15 15:54:01.064: INFO: Got endpoints: latency-svc-jxzkz [746.244115ms]
Sep 15 15:54:01.082: INFO: Created: latency-svc-h6bgg
Sep 15 15:54:01.114: INFO: Got endpoints: latency-svc-z5dww [750.026538ms]
Sep 15 15:54:01.131: INFO: Created: latency-svc-fw9td
Sep 15 15:54:01.165: INFO: Got endpoints: latency-svc-4vplt [751.242215ms]
Sep 15 15:54:01.182: INFO: Created: latency-svc-hlpq5
Sep 15 15:54:01.215: INFO: Got endpoints: latency-svc-qnbvh [751.117362ms]
Sep 15 15:54:01.235: INFO: Created: latency-svc-89dzz
Sep 15 15:54:01.266: INFO: Got endpoints: latency-svc-2q445 [750.580608ms]
Sep 15 15:54:01.284: INFO: Created: latency-svc-59pzm
Sep 15 15:54:01.314: INFO: Got endpoints: latency-svc-twkgl [749.864986ms]
Sep 15 15:54:01.332: INFO: Created: latency-svc-9vfx7
Sep 15 15:54:01.364: INFO: Got endpoints: latency-svc-8425l [737.994585ms]
Sep 15 15:54:01.414: INFO: Got endpoints: latency-svc-gfpq6 [749.832795ms]
Sep 15 15:54:01.466: INFO: Got endpoints: latency-svc-lqjwq [751.340636ms]
Sep 15 15:54:01.515: INFO: Got endpoints: latency-svc-br22q [749.89532ms]
Sep 15 15:54:01.564: INFO: Got endpoints: latency-svc-dr45c [749.248434ms]
Sep 15 15:54:01.615: INFO: Got endpoints: latency-svc-42qr9 [751.183221ms]
Sep 15 15:54:01.664: INFO: Got endpoints: latency-svc-mpx96 [750.424913ms]
Sep 15 15:54:01.714: INFO: Got endpoints: latency-svc-zpgpp [750.216422ms]
Sep 15 15:54:01.766: INFO: Got endpoints: latency-svc-lqr2w [752.109326ms]
Sep 15 15:54:01.814: INFO: Got endpoints: latency-svc-h6bgg [749.803382ms]
Sep 15 15:54:01.864: INFO: Got endpoints: latency-svc-fw9td [750.130066ms]
Sep 15 15:54:01.914: INFO: Got endpoints: latency-svc-hlpq5 [749.117317ms]
Sep 15 15:54:01.964: INFO: Got endpoints: latency-svc-89dzz [749.111271ms]
Sep 15 15:54:02.015: INFO: Got endpoints: latency-svc-59pzm [749.057265ms]
Sep 15 15:54:02.067: INFO: Got endpoints: latency-svc-9vfx7 [753.054541ms]
Sep 15 15:54:02.067: INFO: Latencies: [27.398629ms 27.411739ms 36.867276ms 54.191561ms 73.385947ms 139.362694ms 221.147343ms 223.849879ms 224.435784ms 225.870361ms 227.207409ms 228.044142ms 228.391247ms 229.502824ms 232.163315ms 233.005973ms 233.404988ms 233.583945ms 233.696974ms 234.040996ms 234.464431ms 234.805468ms 236.326529ms 236.491641ms 236.534339ms 238.285761ms 238.60408ms 240.899923ms 242.024082ms 242.16256ms 244.979576ms 245.916878ms 249.003632ms 257.088013ms 264.77513ms 278.689771ms 290.585347ms 293.415549ms 294.687669ms 310.861415ms 320.250107ms 321.529699ms 340.941828ms 343.963386ms 349.557213ms 350.903955ms 352.830077ms 353.710273ms 358.03725ms 393.574046ms 432.092079ms 469.618034ms 490.041886ms 527.239871ms 564.353098ms 603.010291ms 633.420406ms 669.891354ms 673.849904ms 709.430044ms 721.700337ms 737.994585ms 740.506675ms 743.166179ms 743.886462ms 745.525778ms 746.244115ms 746.632902ms 747.56751ms 747.715613ms 747.847135ms 748.007556ms 748.088046ms 748.235929ms 748.674502ms 748.719263ms 748.834805ms 748.9784ms 749.057265ms 749.082501ms 749.096912ms 749.111271ms 749.117317ms 749.145882ms 749.157387ms 749.173088ms 749.182013ms 749.203054ms 749.248434ms 749.255854ms 749.276193ms 749.339081ms 749.367968ms 749.371751ms 749.398223ms 749.406229ms 749.415556ms 749.419906ms 749.445007ms 749.459349ms 749.462369ms 749.476496ms 749.507035ms 749.515875ms 749.56072ms 749.56936ms 749.569604ms 749.581012ms 749.644008ms 749.647729ms 749.65243ms 749.679607ms 749.697291ms 749.722066ms 749.782815ms 749.785341ms 749.788396ms 749.799658ms 749.803382ms 749.816135ms 749.827132ms 749.832795ms 749.85956ms 749.864986ms 749.888811ms 749.89532ms 749.912217ms 749.914576ms 749.916149ms 749.95264ms 749.980517ms 750.016962ms 750.026538ms 750.027788ms 750.038647ms 750.045082ms 750.057862ms 750.077418ms 750.102384ms 750.104662ms 750.127995ms 750.130066ms 750.152831ms 750.194265ms 750.204814ms 750.216422ms 750.243227ms 750.247318ms 750.248311ms 750.262795ms 750.265447ms 750.301585ms 750.315043ms 750.350668ms 750.376086ms 750.381929ms 750.413317ms 750.424913ms 750.467582ms 750.474166ms 750.49482ms 750.502205ms 750.503195ms 750.57752ms 750.580608ms 750.624667ms 750.633218ms 750.68131ms 750.704882ms 750.786447ms 750.79945ms 750.821665ms 750.875553ms 750.909776ms 750.972665ms 750.995723ms 751.024603ms 751.033171ms 751.108733ms 751.117362ms 751.179289ms 751.183221ms 751.233895ms 751.242215ms 751.340636ms 751.438333ms 751.453414ms 751.525665ms 752.109326ms 752.114352ms 752.162939ms 752.42679ms 752.984692ms 753.054541ms 754.107462ms 754.315065ms 759.84049ms 761.508397ms 778.209151ms 826.008874ms]
Sep 15 15:54:02.068: INFO: 50 %ile: 749.462369ms
Sep 15 15:54:02.068: INFO: 90 %ile: 751.179289ms
Sep 15 15:54:02.068: INFO: 99 %ile: 778.209151ms
Sep 15 15:54:02.068: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:54:02.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4799" for this suite.
Sep 15 15:54:16.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:54:16.256: INFO: namespace svc-latency-4799 deletion completed in 14.182410436s

• [SLOW TEST:24.074 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:54:16.257: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-788
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 15:54:16.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-788'
Sep 15 15:54:16.601: INFO: stderr: ""
Sep 15 15:54:16.601: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep 15 15:54:16.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-788'
Sep 15 15:54:16.857: INFO: stderr: ""
Sep 15 15:54:16.857: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 15 15:54:17.862: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 15:54:17.862: INFO: Found 1 / 1
Sep 15 15:54:17.862: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 15 15:54:17.866: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 15:54:17.866: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 15 15:54:17.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 describe pod redis-master-fjm6z --namespace=kubectl-788'
Sep 15 15:54:17.996: INFO: stderr: ""
Sep 15 15:54:17.996: INFO: stdout: "Name:           redis-master-fjm6z\nNamespace:      kubectl-788\nPriority:       0\nNode:           ip-172-16-34-210.ec2.internal/172.16.34.210\nStart Time:     Sun, 15 Sep 2019 15:54:16 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 100.96.2.35/32\n                kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             100.96.2.35\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://9e113ae389f5451e20b49e6d9dc94530dc91f7ceb578d46eea66366e40f22840\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 15 Sep 2019 15:54:17 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-t8ddt (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-t8ddt:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-t8ddt\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                    Message\n  ----    ------     ----  ----                                    -------\n  Normal  Scheduled  1s    default-scheduler                       Successfully assigned kubectl-788/redis-master-fjm6z to ip-172-16-34-210.ec2.internal\n  Normal  Pulled     0s    kubelet, ip-172-16-34-210.ec2.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    0s    kubelet, ip-172-16-34-210.ec2.internal  Created container redis-master\n  Normal  Started    0s    kubelet, ip-172-16-34-210.ec2.internal  Started container redis-master\n"
Sep 15 15:54:17.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 describe rc redis-master --namespace=kubectl-788'
Sep 15 15:54:18.099: INFO: stderr: ""
Sep 15 15:54:18.099: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-788\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-fjm6z\n"
Sep 15 15:54:18.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 describe service redis-master --namespace=kubectl-788'
Sep 15 15:54:18.214: INFO: stderr: ""
Sep 15 15:54:18.214: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-788\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.67.139.43\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         <none>\nSession Affinity:  None\nEvents:            <none>\n"
Sep 15 15:54:18.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 describe node ip-172-16-34-210.ec2.internal'
Sep 15 15:54:18.367: INFO: stderr: ""
Sep 15 15:54:18.367: INFO: stdout: "Name:               ip-172-16-34-210.ec2.internal\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t2.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-16-34-210.ec2.internal\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=node\n                    kublr.io/node-group=group1\n                    node-role.kubernetes.io/node=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"72:48:3f:ba:c4:ec\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.16.34.210\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.96.2.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 15 Sep 2019 15:35:58 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sun, 15 Sep 2019 15:53:49 +0000   Sun, 15 Sep 2019 15:35:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sun, 15 Sep 2019 15:53:49 +0000   Sun, 15 Sep 2019 15:35:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sun, 15 Sep 2019 15:53:49 +0000   Sun, 15 Sep 2019 15:35:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sun, 15 Sep 2019 15:53:49 +0000   Sun, 15 Sep 2019 15:36:18 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   172.16.34.210\n  ExternalIP:   34.229.173.210\n  InternalDNS:  ip-172-16-34-210.ec2.internal\n  ExternalDNS:  ec2-34-229-173-210.compute-1.amazonaws.com\n  Hostname:     ip-172-16-34-210.ec2.internal\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         4\n ephemeral-storage:           41930732Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      16265040Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         4\n ephemeral-storage:           38643362548\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      14851920Ki\n pods:                        110\nSystem Info:\n Machine ID:                 1b80af216c984cd2a362c07e65d755d1\n System UUID:                EC2086F7-8716-4B64-73A1-0ECB175D0490\n Boot ID:                    973bedd5-3922-4250-a541-f6841ce801c7\n Kernel Version:             3.10.0-1062.el7.x86_64\n OS Image:                   Red Hat Enterprise Linux Server 7.7 (Maipo)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nPodCIDR:                     100.96.2.0/24\nProviderID:                  aws:///us-east-1b/i-0eefcb61283293a98\nNon-terminated Pods:         (12 in total)\n  Namespace                  Name                                                                                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                                                                              ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-df7b335343554494-7psrx                                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m1s\n  kube-system                canal-8zmc6                                                                                                       50m (1%)      0 (0%)      114Mi (0%)       128Mi (0%)     18m\n  kube-system                heapster-v1.6.0-beta.1-59cc44564c-zhc8x                                                                           138m (3%)     138m (3%)   324Mi (2%)       324Mi (2%)     17m\n  kube-system                k8s-api-haproxy-30491611cba52aec18f9e69ca06c97ca1f7bdbbd34703396d270209b01ad231c-ip-172-16-34-210.ec2.internal    1m (0%)       0 (0%)      20Mi (0%)        20Mi (0%)      18m\n  kube-system                kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-34-210.ec2.internal         5m (0%)       250m (6%)   48Mi (0%)        48Mi (0%)      18m\n  kube-system                kublr-system-shell-bd4545595-l6rcb                                                                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                node-local-dns-gz49t                                                                                              25m (0%)      0 (0%)      5Mi (0%)         30Mi (0%)      18m\n  kubectl-788                redis-master-fjm6z                                                                                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  kublr                      kublr-logging-elasticsearch-data-0                                                                                25m (0%)      0 (0%)      4Gi (28%)        4Gi (28%)      16m\n  kublr                      kublr-logging-fluentd-es-bh9xf                                                                                    150m (3%)     0 (0%)      512Mi (3%)       512Mi (3%)     16m\n  kublr                      kublr-logging-kibana-6c5f8fd758-nmztf                                                                             220m (5%)     0 (0%)      2240Mi (15%)     2240Mi (15%)   16m\n  kublr                      kublr-logging-port-fwd-app-7b6c88769c-ffdkg                                                                       100m (2%)     0 (0%)      42Mi (0%)        228Mi (1%)     16m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         714m (17%)    388m (9%)\n  memory                      7401Mi (51%)  7626Mi (52%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:                       <none>\n"
Sep 15 15:54:18.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 describe namespace kubectl-788'
Sep 15 15:54:18.458: INFO: stderr: ""
Sep 15 15:54:18.458: INFO: stdout: "Name:         kubectl-788\nLabels:       e2e-framework=kubectl\n              e2e-run=a966ae7f-ebdd-4818-b578-c7e5db9a360b\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:54:18.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-788" for this suite.
Sep 15 15:54:40.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:54:40.627: INFO: namespace kubectl-788 deletion completed in 22.161237143s

• [SLOW TEST:24.370 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:54:40.627: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1100
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-4f714f39-0c10-4252-87d3-7bf8bf423924
STEP: Creating a pod to test consume secrets
Sep 15 15:54:40.791: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4711f168-2cf7-4b05-bd83-a29adc7ad23f" in namespace "projected-1100" to be "success or failure"
Sep 15 15:54:40.797: INFO: Pod "pod-projected-secrets-4711f168-2cf7-4b05-bd83-a29adc7ad23f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.964507ms
Sep 15 15:54:42.802: INFO: Pod "pod-projected-secrets-4711f168-2cf7-4b05-bd83-a29adc7ad23f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010908541s
STEP: Saw pod success
Sep 15 15:54:42.802: INFO: Pod "pod-projected-secrets-4711f168-2cf7-4b05-bd83-a29adc7ad23f" satisfied condition "success or failure"
Sep 15 15:54:42.806: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-secrets-4711f168-2cf7-4b05-bd83-a29adc7ad23f container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 15 15:54:42.838: INFO: Waiting for pod pod-projected-secrets-4711f168-2cf7-4b05-bd83-a29adc7ad23f to disappear
Sep 15 15:54:42.844: INFO: Pod pod-projected-secrets-4711f168-2cf7-4b05-bd83-a29adc7ad23f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:54:42.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1100" for this suite.
Sep 15 15:54:48.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:54:49.003: INFO: namespace projected-1100 deletion completed in 6.152450363s

• [SLOW TEST:8.375 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:54:49.003: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 15:54:49.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af36b5bd-e8a8-4267-bedc-c779c6100167" in namespace "downward-api-9199" to be "success or failure"
Sep 15 15:54:49.165: INFO: Pod "downwardapi-volume-af36b5bd-e8a8-4267-bedc-c779c6100167": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905602ms
Sep 15 15:54:51.170: INFO: Pod "downwardapi-volume-af36b5bd-e8a8-4267-bedc-c779c6100167": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009939028s
STEP: Saw pod success
Sep 15 15:54:51.170: INFO: Pod "downwardapi-volume-af36b5bd-e8a8-4267-bedc-c779c6100167" satisfied condition "success or failure"
Sep 15 15:54:51.174: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-af36b5bd-e8a8-4267-bedc-c779c6100167 container client-container: <nil>
STEP: delete the pod
Sep 15 15:54:51.202: INFO: Waiting for pod downwardapi-volume-af36b5bd-e8a8-4267-bedc-c779c6100167 to disappear
Sep 15 15:54:51.206: INFO: Pod downwardapi-volume-af36b5bd-e8a8-4267-bedc-c779c6100167 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:54:51.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9199" for this suite.
Sep 15 15:54:57.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:54:57.366: INFO: namespace downward-api-9199 deletion completed in 6.154978611s

• [SLOW TEST:8.363 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:54:57.366: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 15:54:57.523: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1609b0e-7fea-4642-b2ca-9c3570646792" in namespace "projected-9997" to be "success or failure"
Sep 15 15:54:57.531: INFO: Pod "downwardapi-volume-e1609b0e-7fea-4642-b2ca-9c3570646792": Phase="Pending", Reason="", readiness=false. Elapsed: 7.300985ms
Sep 15 15:54:59.536: INFO: Pod "downwardapi-volume-e1609b0e-7fea-4642-b2ca-9c3570646792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012131464s
STEP: Saw pod success
Sep 15 15:54:59.536: INFO: Pod "downwardapi-volume-e1609b0e-7fea-4642-b2ca-9c3570646792" satisfied condition "success or failure"
Sep 15 15:54:59.539: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-e1609b0e-7fea-4642-b2ca-9c3570646792 container client-container: <nil>
STEP: delete the pod
Sep 15 15:54:59.569: INFO: Waiting for pod downwardapi-volume-e1609b0e-7fea-4642-b2ca-9c3570646792 to disappear
Sep 15 15:54:59.577: INFO: Pod downwardapi-volume-e1609b0e-7fea-4642-b2ca-9c3570646792 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:54:59.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9997" for this suite.
Sep 15 15:55:05.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:55:05.735: INFO: namespace projected-9997 deletion completed in 6.153845633s

• [SLOW TEST:8.369 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:55:05.736: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep 15 15:55:08.441: INFO: Successfully updated pod "labelsupdatec2d78493-0852-468b-9af8-4a27288dae5e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:55:12.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3077" for this suite.
Sep 15 15:55:34.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:55:34.642: INFO: namespace projected-3077 deletion completed in 22.157569427s

• [SLOW TEST:28.906 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:55:34.642: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Sep 15 15:55:34.796: INFO: Waiting up to 5m0s for pod "client-containers-eededc49-19fa-4329-8143-ce5a572a16fc" in namespace "containers-1905" to be "success or failure"
Sep 15 15:55:34.800: INFO: Pod "client-containers-eededc49-19fa-4329-8143-ce5a572a16fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.262963ms
Sep 15 15:55:36.805: INFO: Pod "client-containers-eededc49-19fa-4329-8143-ce5a572a16fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008669003s
Sep 15 15:55:38.810: INFO: Pod "client-containers-eededc49-19fa-4329-8143-ce5a572a16fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013752171s
STEP: Saw pod success
Sep 15 15:55:38.810: INFO: Pod "client-containers-eededc49-19fa-4329-8143-ce5a572a16fc" satisfied condition "success or failure"
Sep 15 15:55:38.814: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod client-containers-eededc49-19fa-4329-8143-ce5a572a16fc container test-container: <nil>
STEP: delete the pod
Sep 15 15:55:38.844: INFO: Waiting for pod client-containers-eededc49-19fa-4329-8143-ce5a572a16fc to disappear
Sep 15 15:55:38.849: INFO: Pod client-containers-eededc49-19fa-4329-8143-ce5a572a16fc no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:55:38.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1905" for this suite.
Sep 15 15:55:44.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:55:45.033: INFO: namespace containers-1905 deletion completed in 6.176839485s

• [SLOW TEST:10.391 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:55:45.033: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 15 15:55:45.194: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5103,SelfLink:/api/v1/namespaces/watch-5103/configmaps/e2e-watch-test-watch-closed,UID:2269f2f6-a76e-4006-8d4f-6191d0800ba0,ResourceVersion:8312,Generation:0,CreationTimestamp:2019-09-15 15:55:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 15 15:55:45.194: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5103,SelfLink:/api/v1/namespaces/watch-5103/configmaps/e2e-watch-test-watch-closed,UID:2269f2f6-a76e-4006-8d4f-6191d0800ba0,ResourceVersion:8313,Generation:0,CreationTimestamp:2019-09-15 15:55:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 15 15:55:45.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5103,SelfLink:/api/v1/namespaces/watch-5103/configmaps/e2e-watch-test-watch-closed,UID:2269f2f6-a76e-4006-8d4f-6191d0800ba0,ResourceVersion:8314,Generation:0,CreationTimestamp:2019-09-15 15:55:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 15 15:55:45.216: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5103,SelfLink:/api/v1/namespaces/watch-5103/configmaps/e2e-watch-test-watch-closed,UID:2269f2f6-a76e-4006-8d4f-6191d0800ba0,ResourceVersion:8315,Generation:0,CreationTimestamp:2019-09-15 15:55:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:55:45.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5103" for this suite.
Sep 15 15:55:51.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:55:51.374: INFO: namespace watch-5103 deletion completed in 6.152058997s

• [SLOW TEST:6.340 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:55:51.374: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Sep 15 15:55:51.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 api-versions'
Sep 15 15:55:51.597: INFO: stderr: ""
Sep 15 15:55:51.597: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:55:51.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8658" for this suite.
Sep 15 15:55:57.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:55:57.767: INFO: namespace kubectl-8658 deletion completed in 6.163955974s

• [SLOW TEST:6.393 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:55:57.767: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-46ef76c8-40d1-486f-a3db-1a9e911d8a80
STEP: Creating secret with name secret-projected-all-test-volume-c8effeae-e5cc-4fa5-bde3-92e1e9a8d982
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 15 15:55:57.937: INFO: Waiting up to 5m0s for pod "projected-volume-59c05b8e-61db-424c-8bc7-107111e657cf" in namespace "projected-6600" to be "success or failure"
Sep 15 15:55:57.944: INFO: Pod "projected-volume-59c05b8e-61db-424c-8bc7-107111e657cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41599ms
Sep 15 15:55:59.949: INFO: Pod "projected-volume-59c05b8e-61db-424c-8bc7-107111e657cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011649939s
STEP: Saw pod success
Sep 15 15:55:59.949: INFO: Pod "projected-volume-59c05b8e-61db-424c-8bc7-107111e657cf" satisfied condition "success or failure"
Sep 15 15:55:59.953: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod projected-volume-59c05b8e-61db-424c-8bc7-107111e657cf container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 15 15:55:59.987: INFO: Waiting for pod projected-volume-59c05b8e-61db-424c-8bc7-107111e657cf to disappear
Sep 15 15:55:59.998: INFO: Pod projected-volume-59c05b8e-61db-424c-8bc7-107111e657cf no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:55:59.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6600" for this suite.
Sep 15 15:56:06.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:56:06.163: INFO: namespace projected-6600 deletion completed in 6.158873181s

• [SLOW TEST:8.395 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:56:06.163: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1911
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Sep 15 15:56:06.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 --namespace=kubectl-1911 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep 15 15:56:08.535: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep 15 15:56:08.535: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:56:10.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1911" for this suite.
Sep 15 15:56:16.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:56:16.710: INFO: namespace kubectl-1911 deletion completed in 6.16111785s

• [SLOW TEST:10.548 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:56:16.710: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 15 15:56:20.892: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-0294cdab-1361-4d01-b509-7e61b585ee71,GenerateName:,Namespace:events-6650,SelfLink:/api/v1/namespaces/events-6650/pods/send-events-0294cdab-1361-4d01-b509-7e61b585ee71,UID:7433dd61-9e48-4384-abfd-07a93bd27ae0,ResourceVersion:8481,Generation:0,CreationTimestamp:2019-09-15 15:56:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 863746983,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.43/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-2d5qf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-2d5qf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-2d5qf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bc9ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bc9f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 15:56:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 15:56:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 15:56:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 15:56:16 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:100.96.2.43,StartTime:2019-09-15 15:56:16 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-15 15:56:19 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://4048be6b394e081cdc222722ba2169703b68569100102c1fcfed2f1981015c73}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep 15 15:56:22.897: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 15 15:56:24.902: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:56:24.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6650" for this suite.
Sep 15 15:57:04.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:57:05.078: INFO: namespace events-6650 deletion completed in 40.161270688s

• [SLOW TEST:48.368 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:57:05.078: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 15:57:05.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f0bf05e-983a-490a-b265-5c32e57c4091" in namespace "downward-api-5366" to be "success or failure"
Sep 15 15:57:05.241: INFO: Pod "downwardapi-volume-8f0bf05e-983a-490a-b265-5c32e57c4091": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913133ms
Sep 15 15:57:07.246: INFO: Pod "downwardapi-volume-8f0bf05e-983a-490a-b265-5c32e57c4091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011894768s
STEP: Saw pod success
Sep 15 15:57:07.246: INFO: Pod "downwardapi-volume-8f0bf05e-983a-490a-b265-5c32e57c4091" satisfied condition "success or failure"
Sep 15 15:57:07.252: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-8f0bf05e-983a-490a-b265-5c32e57c4091 container client-container: <nil>
STEP: delete the pod
Sep 15 15:57:07.283: INFO: Waiting for pod downwardapi-volume-8f0bf05e-983a-490a-b265-5c32e57c4091 to disappear
Sep 15 15:57:07.287: INFO: Pod downwardapi-volume-8f0bf05e-983a-490a-b265-5c32e57c4091 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:57:07.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5366" for this suite.
Sep 15 15:57:13.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:57:13.455: INFO: namespace downward-api-5366 deletion completed in 6.161536158s

• [SLOW TEST:8.377 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:57:13.455: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 15 15:57:15.634: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:57:15.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5883" for this suite.
Sep 15 15:57:21.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:57:21.982: INFO: namespace container-runtime-5883 deletion completed in 6.250516623s

• [SLOW TEST:8.527 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:57:21.982: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 15:57:22.130: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 15 15:57:22.142: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 15 15:57:27.147: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 15 15:57:27.147: INFO: Creating deployment "test-rolling-update-deployment"
Sep 15 15:57:27.153: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 15 15:57:27.163: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 15 15:57:29.173: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 15 15:57:29.177: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 15 15:57:29.190: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4776,SelfLink:/apis/apps/v1/namespaces/deployment-4776/deployments/test-rolling-update-deployment,UID:dd5a3eb5-43f7-4f96-9dd2-b4a71c73d5f7,ResourceVersion:8750,Generation:1,CreationTimestamp:2019-09-15 15:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-15 15:57:27 +0000 UTC 2019-09-15 15:57:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-15 15:57:28 +0000 UTC 2019-09-15 15:57:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 15 15:57:29.195: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-4776,SelfLink:/apis/apps/v1/namespaces/deployment-4776/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:93edaf46-2dd9-4cc2-bc14-a186f5b283e6,ResourceVersion:8739,Generation:1,CreationTimestamp:2019-09-15 15:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment dd5a3eb5-43f7-4f96-9dd2-b4a71c73d5f7 0xc00083d867 0xc00083d868}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 15 15:57:29.195: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 15 15:57:29.195: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4776,SelfLink:/apis/apps/v1/namespaces/deployment-4776/replicasets/test-rolling-update-controller,UID:ce95d429-070c-4acc-b178-0391980be4a0,ResourceVersion:8748,Generation:2,CreationTimestamp:2019-09-15 15:57:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment dd5a3eb5-43f7-4f96-9dd2-b4a71c73d5f7 0xc00083d77f 0xc00083d790}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 15 15:57:29.199: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-vzhts" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-vzhts,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-4776,SelfLink:/api/v1/namespaces/deployment-4776/pods/test-rolling-update-deployment-79f6b9d75c-vzhts,UID:f0a657e2-2ea3-4482-96c4-0123740f8c59,ResourceVersion:8738,Generation:0,CreationTimestamp:2019-09-15 15:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.47/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 93edaf46-2dd9-4cc2-bc14-a186f5b283e6 0xc00289f237 0xc00289f238}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xpg8c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xpg8c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-xpg8c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00289f2a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00289f2c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 15:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 15:57:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 15:57:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 15:57:27 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:100.96.2.47,StartTime:2019-09-15 15:57:27 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-15 15:57:27 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2160c4caf3903540ee3c19a73b8b2424c59f2000cb45e1165b1323be66596510}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:57:29.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4776" for this suite.
Sep 15 15:57:35.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:57:35.364: INFO: namespace deployment-4776 deletion completed in 6.159177286s

• [SLOW TEST:13.382 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:57:35.364: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-ccc76274-3912-462f-94ef-9b4953356a9b
STEP: Creating a pod to test consume configMaps
Sep 15 15:57:35.525: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-27089d65-454c-4fdf-827b-1ee8ef9ce15f" in namespace "projected-3269" to be "success or failure"
Sep 15 15:57:35.530: INFO: Pod "pod-projected-configmaps-27089d65-454c-4fdf-827b-1ee8ef9ce15f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.600022ms
Sep 15 15:57:37.536: INFO: Pod "pod-projected-configmaps-27089d65-454c-4fdf-827b-1ee8ef9ce15f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010857094s
STEP: Saw pod success
Sep 15 15:57:37.536: INFO: Pod "pod-projected-configmaps-27089d65-454c-4fdf-827b-1ee8ef9ce15f" satisfied condition "success or failure"
Sep 15 15:57:37.540: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-configmaps-27089d65-454c-4fdf-827b-1ee8ef9ce15f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 15:57:37.571: INFO: Waiting for pod pod-projected-configmaps-27089d65-454c-4fdf-827b-1ee8ef9ce15f to disappear
Sep 15 15:57:37.575: INFO: Pod pod-projected-configmaps-27089d65-454c-4fdf-827b-1ee8ef9ce15f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:57:37.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3269" for this suite.
Sep 15 15:57:43.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:57:43.746: INFO: namespace projected-3269 deletion completed in 6.164731554s

• [SLOW TEST:8.382 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:57:43.746: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-735
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-dbac7b21-d540-43b9-807b-b09c799edc53
STEP: Creating secret with name s-test-opt-upd-13813ff5-f8d6-442d-bf4d-5f3e66206d9b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-dbac7b21-d540-43b9-807b-b09c799edc53
STEP: Updating secret s-test-opt-upd-13813ff5-f8d6-442d-bf4d-5f3e66206d9b
STEP: Creating secret with name s-test-opt-create-590a2271-18b4-4b41-a190-1da00c526db9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:57:52.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-735" for this suite.
Sep 15 15:58:14.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:58:14.253: INFO: namespace projected-735 deletion completed in 22.154491778s

• [SLOW TEST:30.507 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:58:14.253: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 15:58:14.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5f5162c-6607-465d-bdbc-d8d320c48823" in namespace "projected-3909" to be "success or failure"
Sep 15 15:58:14.413: INFO: Pod "downwardapi-volume-a5f5162c-6607-465d-bdbc-d8d320c48823": Phase="Pending", Reason="", readiness=false. Elapsed: 5.087067ms
Sep 15 15:58:16.418: INFO: Pod "downwardapi-volume-a5f5162c-6607-465d-bdbc-d8d320c48823": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009736924s
STEP: Saw pod success
Sep 15 15:58:16.418: INFO: Pod "downwardapi-volume-a5f5162c-6607-465d-bdbc-d8d320c48823" satisfied condition "success or failure"
Sep 15 15:58:16.422: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-a5f5162c-6607-465d-bdbc-d8d320c48823 container client-container: <nil>
STEP: delete the pod
Sep 15 15:58:16.454: INFO: Waiting for pod downwardapi-volume-a5f5162c-6607-465d-bdbc-d8d320c48823 to disappear
Sep 15 15:58:16.459: INFO: Pod downwardapi-volume-a5f5162c-6607-465d-bdbc-d8d320c48823 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:58:16.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3909" for this suite.
Sep 15 15:58:22.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:58:22.623: INFO: namespace projected-3909 deletion completed in 6.158035027s

• [SLOW TEST:8.371 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:58:22.624: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-52cacc1d-3c30-4f8c-a4b9-be4f7b917a97
STEP: Creating a pod to test consume secrets
Sep 15 15:58:22.786: INFO: Waiting up to 5m0s for pod "pod-secrets-4dbefead-83d1-4782-a4b0-82c157de4b5f" in namespace "secrets-8658" to be "success or failure"
Sep 15 15:58:22.793: INFO: Pod "pod-secrets-4dbefead-83d1-4782-a4b0-82c157de4b5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.829666ms
Sep 15 15:58:24.798: INFO: Pod "pod-secrets-4dbefead-83d1-4782-a4b0-82c157de4b5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012237311s
STEP: Saw pod success
Sep 15 15:58:24.798: INFO: Pod "pod-secrets-4dbefead-83d1-4782-a4b0-82c157de4b5f" satisfied condition "success or failure"
Sep 15 15:58:24.802: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-secrets-4dbefead-83d1-4782-a4b0-82c157de4b5f container secret-volume-test: <nil>
STEP: delete the pod
Sep 15 15:58:24.838: INFO: Waiting for pod pod-secrets-4dbefead-83d1-4782-a4b0-82c157de4b5f to disappear
Sep 15 15:58:24.842: INFO: Pod pod-secrets-4dbefead-83d1-4782-a4b0-82c157de4b5f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:58:24.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8658" for this suite.
Sep 15 15:58:30.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:58:31.018: INFO: namespace secrets-8658 deletion completed in 6.169176027s

• [SLOW TEST:8.395 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:58:31.018: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 15 15:58:41.203: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0915 15:58:41.203625      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 15:58:41.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5425" for this suite.
Sep 15 15:58:47.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 15:58:47.372: INFO: namespace gc-5425 deletion completed in 6.163503777s

• [SLOW TEST:16.353 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 15:58:47.372: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 15 15:58:47.799: INFO: Pod name wrapped-volume-race-9f2a0dec-4f17-4f0d-bf16-5199e73cba81: Found 0 pods out of 5
Sep 15 15:58:52.806: INFO: Pod name wrapped-volume-race-9f2a0dec-4f17-4f0d-bf16-5199e73cba81: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9f2a0dec-4f17-4f0d-bf16-5199e73cba81 in namespace emptydir-wrapper-5139, will wait for the garbage collector to delete the pods
Sep 15 15:59:03.029: INFO: Deleting ReplicationController wrapped-volume-race-9f2a0dec-4f17-4f0d-bf16-5199e73cba81 took: 13.455803ms
Sep 15 15:59:03.429: INFO: Terminating ReplicationController wrapped-volume-race-9f2a0dec-4f17-4f0d-bf16-5199e73cba81 pods took: 400.253367ms
STEP: Creating RC which spawns configmap-volume pods
Sep 15 15:59:37.558: INFO: Pod name wrapped-volume-race-425ea621-5b93-45dc-b0af-63fcbc064560: Found 0 pods out of 5
Sep 15 15:59:42.564: INFO: Pod name wrapped-volume-race-425ea621-5b93-45dc-b0af-63fcbc064560: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-425ea621-5b93-45dc-b0af-63fcbc064560 in namespace emptydir-wrapper-5139, will wait for the garbage collector to delete the pods
Sep 15 15:59:54.666: INFO: Deleting ReplicationController wrapped-volume-race-425ea621-5b93-45dc-b0af-63fcbc064560 took: 12.494523ms
Sep 15 15:59:55.066: INFO: Terminating ReplicationController wrapped-volume-race-425ea621-5b93-45dc-b0af-63fcbc064560 pods took: 400.257635ms
STEP: Creating RC which spawns configmap-volume pods
Sep 15 16:00:33.888: INFO: Pod name wrapped-volume-race-1887402e-18ba-4ac6-945e-4e5ce5698693: Found 0 pods out of 5
Sep 15 16:00:38.896: INFO: Pod name wrapped-volume-race-1887402e-18ba-4ac6-945e-4e5ce5698693: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1887402e-18ba-4ac6-945e-4e5ce5698693 in namespace emptydir-wrapper-5139, will wait for the garbage collector to delete the pods
Sep 15 16:00:50.988: INFO: Deleting ReplicationController wrapped-volume-race-1887402e-18ba-4ac6-945e-4e5ce5698693 took: 11.971526ms
Sep 15 16:00:51.388: INFO: Terminating ReplicationController wrapped-volume-race-1887402e-18ba-4ac6-945e-4e5ce5698693 pods took: 400.247661ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:01:33.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5139" for this suite.
Sep 15 16:01:41.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:01:41.939: INFO: namespace emptydir-wrapper-5139 deletion completed in 8.200965667s

• [SLOW TEST:174.568 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:01:41.940: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-bfa58eea-956a-49d5-9aa4-df71bb8945a4
STEP: Creating a pod to test consume secrets
Sep 15 16:01:42.118: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6247b0db-dd1e-4cdf-939b-164064c10458" in namespace "projected-9554" to be "success or failure"
Sep 15 16:01:42.123: INFO: Pod "pod-projected-secrets-6247b0db-dd1e-4cdf-939b-164064c10458": Phase="Pending", Reason="", readiness=false. Elapsed: 5.196094ms
Sep 15 16:01:44.128: INFO: Pod "pod-projected-secrets-6247b0db-dd1e-4cdf-939b-164064c10458": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010205424s
STEP: Saw pod success
Sep 15 16:01:44.128: INFO: Pod "pod-projected-secrets-6247b0db-dd1e-4cdf-939b-164064c10458" satisfied condition "success or failure"
Sep 15 16:01:44.132: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-secrets-6247b0db-dd1e-4cdf-939b-164064c10458 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:01:44.165: INFO: Waiting for pod pod-projected-secrets-6247b0db-dd1e-4cdf-939b-164064c10458 to disappear
Sep 15 16:01:44.169: INFO: Pod pod-projected-secrets-6247b0db-dd1e-4cdf-939b-164064c10458 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:01:44.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9554" for this suite.
Sep 15 16:01:50.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:01:50.909: INFO: namespace projected-9554 deletion completed in 6.734091852s

• [SLOW TEST:8.969 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:01:50.909: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:01:51.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-940" for this suite.
Sep 15 16:02:13.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:02:13.238: INFO: namespace kubelet-test-940 deletion completed in 22.15614667s

• [SLOW TEST:22.329 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:02:13.238: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7910
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:02:13.395: INFO: Creating deployment "nginx-deployment"
Sep 15 16:02:13.402: INFO: Waiting for observed generation 1
Sep 15 16:02:15.416: INFO: Waiting for all required pods to come up
Sep 15 16:02:15.421: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 15 16:02:17.432: INFO: Waiting for deployment "nginx-deployment" to complete
Sep 15 16:02:17.440: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep 15 16:02:17.450: INFO: Updating deployment nginx-deployment
Sep 15 16:02:17.450: INFO: Waiting for observed generation 2
Sep 15 16:02:19.462: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 15 16:02:19.466: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 15 16:02:19.470: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep 15 16:02:19.482: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 15 16:02:19.482: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 15 16:02:19.486: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep 15 16:02:19.493: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep 15 16:02:19.493: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep 15 16:02:19.503: INFO: Updating deployment nginx-deployment
Sep 15 16:02:19.503: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep 15 16:02:19.511: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 15 16:02:19.521: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 15 16:02:19.569: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7910,SelfLink:/apis/apps/v1/namespaces/deployment-7910/deployments/nginx-deployment,UID:8cf74d53-712d-4c92-9ba5-2f1f2422092b,ResourceVersion:10698,Generation:3,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-09-15 16:02:17 +0000 UTC 2019-09-15 16:02:13 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-09-15 16:02:19 +0000 UTC 2019-09-15 16:02:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep 15 16:02:19.736: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-7910,SelfLink:/apis/apps/v1/namespaces/deployment-7910/replicasets/nginx-deployment-55fb7cb77f,UID:95e717ea-7663-4725-9917-7ed3c18394ca,ResourceVersion:10693,Generation:3,CreationTimestamp:2019-09-15 16:02:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 8cf74d53-712d-4c92-9ba5-2f1f2422092b 0xc001357037 0xc001357038}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 15 16:02:19.736: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep 15 16:02:19.736: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-7910,SelfLink:/apis/apps/v1/namespaces/deployment-7910/replicasets/nginx-deployment-7b8c6f4498,UID:141ca562-2aa2-423c-a5bd-aef4e5f1087b,ResourceVersion:10690,Generation:3,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 8cf74d53-712d-4c92-9ba5-2f1f2422092b 0xc001357117 0xc001357118}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep 15 16:02:19.837: INFO: Pod "nginx-deployment-55fb7cb77f-2qkxx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2qkxx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-2qkxx,UID:94481689-44af-4965-976e-b48531525537,ResourceVersion:10679,Generation:0,CreationTimestamp:2019-09-15 16:02:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.3.34/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc001357ee7 0xc001357ee8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-49-162.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001357f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001357f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  }],Message:,Reason:,HostIP:172.16.49.162,PodIP:,StartTime:2019-09-15 16:02:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.837: INFO: Pod "nginx-deployment-55fb7cb77f-2zt8f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2zt8f,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-2zt8f,UID:2aa003f0-7ebe-470e-aa8f-f57c646f6c57,ResourceVersion:10722,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc0033780a0 0xc0033780a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.842: INFO: Pod "nginx-deployment-55fb7cb77f-7mp42" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7mp42,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-7mp42,UID:f54b1066-aeb7-4507-9be7-7acdecdc4613,ResourceVersion:10733,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc003378197 0xc003378198}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378210} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.842: INFO: Pod "nginx-deployment-55fb7cb77f-7pzcz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7pzcz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-7pzcz,UID:6bc2cc59-6c6a-4692-bbfd-3c1f41de1f69,ResourceVersion:10678,Generation:0,CreationTimestamp:2019-09-15 16:02:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.73/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc0033782a7 0xc0033782a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:,StartTime:2019-09-15 16:02:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.842: INFO: Pod "nginx-deployment-55fb7cb77f-8qgrv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8qgrv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-8qgrv,UID:7bc44080-97d5-4124-a1d6-0fb309e13b95,ResourceVersion:10712,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc003378400 0xc003378401}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.843: INFO: Pod "nginx-deployment-55fb7cb77f-8wjbp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8wjbp,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-8wjbp,UID:ab72971d-7068-4cc5-8a9c-53c2bc65151a,ResourceVersion:10721,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc003378510 0xc003378511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033785a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.843: INFO: Pod "nginx-deployment-55fb7cb77f-c79rf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-c79rf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-c79rf,UID:f2a1a508-33dc-4212-90dc-037b5a36b57f,ResourceVersion:10683,Generation:0,CreationTimestamp:2019-09-15 16:02:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.3.35/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc003378617 0xc003378618}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-49-162.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033786a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  }],Message:,Reason:,HostIP:172.16.49.162,PodIP:,StartTime:2019-09-15 16:02:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.843: INFO: Pod "nginx-deployment-55fb7cb77f-dkgs6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dkgs6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-dkgs6,UID:0ac8c5fa-182f-4c1f-9f11-374cc1bb1e52,ResourceVersion:10677,Generation:0,CreationTimestamp:2019-09-15 16:02:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.30/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc003378780 0xc003378781}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-74-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0033787f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  }],Message:,Reason:,HostIP:172.16.74.13,PodIP:,StartTime:2019-09-15 16:02:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.843: INFO: Pod "nginx-deployment-55fb7cb77f-lvnbq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lvnbq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-lvnbq,UID:82510e15-1570-4d92-8477-cde6e157489d,ResourceVersion:10707,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc0033788e0 0xc0033788e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-74-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.843: INFO: Pod "nginx-deployment-55fb7cb77f-pf5n9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pf5n9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-pf5n9,UID:faede76b-488b-4ee9-9771-b13f91033cbf,ResourceVersion:10675,Generation:0,CreationTimestamp:2019-09-15 16:02:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.72/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc003378a00 0xc003378a01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:17 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:,StartTime:2019-09-15 16:02:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.843: INFO: Pod "nginx-deployment-55fb7cb77f-pm67t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pm67t,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-pm67t,UID:7d07d2b7-850a-4d97-9fac-daf91ac1e1fc,ResourceVersion:10724,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc003378b60 0xc003378b61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.843: INFO: Pod "nginx-deployment-55fb7cb77f-tns4c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tns4c,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-tns4c,UID:20fa3deb-1cca-4447-b03f-3abdc641a5a1,ResourceVersion:10725,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc003378c57 0xc003378c58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-74-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.844: INFO: Pod "nginx-deployment-55fb7cb77f-xxkht" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xxkht,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-55fb7cb77f-xxkht,UID:0dd10e85-8e5a-4b13-8873-a1da546b3317,ResourceVersion:10726,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 95e717ea-7663-4725-9917-7ed3c18394ca 0xc003378d60 0xc003378d61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378df0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.844: INFO: Pod "nginx-deployment-7b8c6f4498-2bdtw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2bdtw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-2bdtw,UID:72bb4ea2-b032-4aa3-a40e-9c05e352d471,ResourceVersion:10708,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003378e57 0xc003378e58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-74-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.844: INFO: Pod "nginx-deployment-7b8c6f4498-2jrdv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2jrdv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-2jrdv,UID:26ec3866-a877-484a-bde7-a72817a4eec8,ResourceVersion:10587,Generation:0,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.70/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003378f70 0xc003378f71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003378fd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003378ff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:100.96.2.70,StartTime:2019-09-15 16:02:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-15 16:02:14 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://91513f5663716f510de05defeb2ca1372927d2362bcb497768b34a92f171c545}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.844: INFO: Pod "nginx-deployment-7b8c6f4498-4c56f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4c56f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-4c56f,UID:63c79906-b4b3-4a23-b8d9-0caa65e0081c,ResourceVersion:10600,Generation:0,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.3.33/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc0033790d0 0xc0033790d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-49-162.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  }],Message:,Reason:,HostIP:172.16.49.162,PodIP:100.96.3.33,StartTime:2019-09-15 16:02:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-15 16:02:14 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a0386ef6d679952c0e7b24b587a16b3e9fbf28d6fe5f8e0a7cee4c1412febf20}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.844: INFO: Pod "nginx-deployment-7b8c6f4498-7dr85" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7dr85,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-7dr85,UID:a5afef60-70aa-4bc8-944a-4f9ef261147e,ResourceVersion:10609,Generation:0,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.27/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003379230 0xc003379231}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-74-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033792b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  }],Message:,Reason:,HostIP:172.16.74.13,PodIP:100.96.1.27,StartTime:2019-09-15 16:02:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-15 16:02:14 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e8ecab1f983a024b41623f75201e770c762a23b0619a4d72383906b8d44b5647}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.844: INFO: Pod "nginx-deployment-7b8c6f4498-8wqsp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8wqsp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-8wqsp,UID:f227cdf0-9987-476f-a4b7-3b0250d8861c,ResourceVersion:10590,Generation:0,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.69/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003379390 0xc003379391}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0033793f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:100.96.2.69,StartTime:2019-09-15 16:02:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-15 16:02:14 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d233c6a421902e188637d039402dc10275b4d8e9aed4847a7a0a8ef10db4ca03}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.844: INFO: Pod "nginx-deployment-7b8c6f4498-bkc9s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bkc9s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-bkc9s,UID:b01aa664-6633-45e7-bd0f-93389ec68e00,ResourceVersion:10716,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc0033794e0 0xc0033794e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-49-162.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.845: INFO: Pod "nginx-deployment-7b8c6f4498-czrzg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-czrzg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-czrzg,UID:b54529e8-765f-4d79-9af9-40cec71eedae,ResourceVersion:10735,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc0033795e0 0xc0033795e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.845: INFO: Pod "nginx-deployment-7b8c6f4498-gvdsw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gvdsw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-gvdsw,UID:67102107-1493-430d-af2f-ab71a2ee187f,ResourceVersion:10729,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc0033796e0 0xc0033796e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.846: INFO: Pod "nginx-deployment-7b8c6f4498-gznsp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gznsp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-gznsp,UID:1f78e30b-a7da-4ba5-975d-9b509bce2450,ResourceVersion:10730,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc0033797c7 0xc0033797c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.846: INFO: Pod "nginx-deployment-7b8c6f4498-hltvd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hltvd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-hltvd,UID:eb7b087c-43c1-43a3-86c5-7aa4a0b0fb00,ResourceVersion:10719,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc0033798b7 0xc0033798b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-49-162.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:172.16.49.162,PodIP:,StartTime:2019-09-15 16:02:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.846: INFO: Pod "nginx-deployment-7b8c6f4498-hrqpj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hrqpj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-hrqpj,UID:a8458114-2394-45aa-9548-cac0dc6f4b3e,ResourceVersion:10596,Generation:0,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.68/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003379a17 0xc003379a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:100.96.2.68,StartTime:2019-09-15 16:02:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-15 16:02:14 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://40ff6603c475d15137c8712b8b84de1a66e32ea99916805db34eacd4d1edc104}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.846: INFO: Pod "nginx-deployment-7b8c6f4498-kgfns" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kgfns,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-kgfns,UID:e56c4646-3f05-4220-87a5-6669b8e1ad97,ResourceVersion:10603,Generation:0,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.3.32/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003379b80 0xc003379b81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-49-162.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  }],Message:,Reason:,HostIP:172.16.49.162,PodIP:100.96.3.32,StartTime:2019-09-15 16:02:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-15 16:02:14 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://808da8baf0ed3d23ea76bce3ce3f4c0898daf2ec1245fd4550da30db0415020a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.846: INFO: Pod "nginx-deployment-7b8c6f4498-l4brl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-l4brl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-l4brl,UID:65c50b31-de93-405e-9336-93c50222d915,ResourceVersion:10612,Generation:0,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.29/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003379ce0 0xc003379ce1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-74-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379d40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  }],Message:,Reason:,HostIP:172.16.74.13,PodIP:100.96.1.29,StartTime:2019-09-15 16:02:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-15 16:02:14 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d036953d83190354f3e109ead63619733311a2a8ee80a746d0206d9594fdec26}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.846: INFO: Pod "nginx-deployment-7b8c6f4498-nh7sz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nh7sz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-nh7sz,UID:907f1c07-6288-4061-8e99-43a56e3903b8,ResourceVersion:10593,Generation:0,CreationTimestamp:2019-09-15 16:02:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.71/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003379e40 0xc003379e41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003379ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:13 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:100.96.2.71,StartTime:2019-09-15 16:02:13 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-15 16:02:14 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f469873851f6d07df597a87fe14db33466f2118c7cd3cf57101aee9c37472748}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.847: INFO: Pod "nginx-deployment-7b8c6f4498-p5qc6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-p5qc6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-p5qc6,UID:c10dbe14-2d69-454b-8e2b-97122688cd42,ResourceVersion:10713,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003379f90 0xc003379f91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003379ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003094010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.847: INFO: Pod "nginx-deployment-7b8c6f4498-pxmpx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pxmpx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-pxmpx,UID:59f1b79d-8f46-4731-aa7f-d259ad28a64d,ResourceVersion:10731,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003094090 0xc003094091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030940f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003094110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.847: INFO: Pod "nginx-deployment-7b8c6f4498-s6dcc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-s6dcc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-s6dcc,UID:76dd926b-6368-48dc-8720-a0a52ca64810,ResourceVersion:10732,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003094177 0xc003094178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-74-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030941e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003094200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.847: INFO: Pod "nginx-deployment-7b8c6f4498-vf69m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vf69m,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-vf69m,UID:31a5a3d7-e6c4-429d-a36c-ec33bea509c3,ResourceVersion:10711,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003094280 0xc003094281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-49-162.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030942e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003094300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.847: INFO: Pod "nginx-deployment-7b8c6f4498-wbm5q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wbm5q,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-wbm5q,UID:4b58812d-acda-466d-981d-da1986103515,ResourceVersion:10723,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003094380 0xc003094381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030943e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003094400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:02:19.847: INFO: Pod "nginx-deployment-7b8c6f4498-wkkpz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wkkpz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7910,SelfLink:/api/v1/namespaces/deployment-7910/pods/nginx-deployment-7b8c6f4498-wkkpz,UID:26f77d20-3847-440d-b3a0-1b25341539c9,ResourceVersion:10717,Generation:0,CreationTimestamp:2019-09-15 16:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 141ca562-2aa2-423c-a5bd-aef4e5f1087b 0xc003094480 0xc003094481}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8bzb9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8bzb9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8bzb9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-74-13.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030944e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003094500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:02:19 +0000 UTC  }],Message:,Reason:,HostIP:172.16.74.13,PodIP:,StartTime:2019-09-15 16:02:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:02:19.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7910" for this suite.
Sep 15 16:02:27.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:02:28.101: INFO: namespace deployment-7910 deletion completed in 8.244447383s

• [SLOW TEST:14.863 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:02:28.101: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-9bd60648-3ba6-4923-a819-90b737ab8f29
STEP: Creating a pod to test consume configMaps
Sep 15 16:02:28.298: INFO: Waiting up to 5m0s for pod "pod-configmaps-30e51af8-580c-4085-b954-9b8bb470f429" in namespace "configmap-9267" to be "success or failure"
Sep 15 16:02:28.305: INFO: Pod "pod-configmaps-30e51af8-580c-4085-b954-9b8bb470f429": Phase="Pending", Reason="", readiness=false. Elapsed: 7.162636ms
Sep 15 16:02:30.309: INFO: Pod "pod-configmaps-30e51af8-580c-4085-b954-9b8bb470f429": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011820601s
STEP: Saw pod success
Sep 15 16:02:30.309: INFO: Pod "pod-configmaps-30e51af8-580c-4085-b954-9b8bb470f429" satisfied condition "success or failure"
Sep 15 16:02:30.313: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-30e51af8-580c-4085-b954-9b8bb470f429 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 16:02:30.347: INFO: Waiting for pod pod-configmaps-30e51af8-580c-4085-b954-9b8bb470f429 to disappear
Sep 15 16:02:30.351: INFO: Pod pod-configmaps-30e51af8-580c-4085-b954-9b8bb470f429 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:02:30.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9267" for this suite.
Sep 15 16:02:36.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:02:36.515: INFO: namespace configmap-9267 deletion completed in 6.158442158s

• [SLOW TEST:8.414 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:02:36.515: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9838
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Sep 15 16:02:36.671: INFO: Waiting up to 5m0s for pod "client-containers-7141f113-3e2b-475f-9cba-78848705efd1" in namespace "containers-9838" to be "success or failure"
Sep 15 16:02:36.678: INFO: Pod "client-containers-7141f113-3e2b-475f-9cba-78848705efd1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.091172ms
Sep 15 16:02:38.684: INFO: Pod "client-containers-7141f113-3e2b-475f-9cba-78848705efd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012514862s
STEP: Saw pod success
Sep 15 16:02:38.684: INFO: Pod "client-containers-7141f113-3e2b-475f-9cba-78848705efd1" satisfied condition "success or failure"
Sep 15 16:02:38.688: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod client-containers-7141f113-3e2b-475f-9cba-78848705efd1 container test-container: <nil>
STEP: delete the pod
Sep 15 16:02:38.722: INFO: Waiting for pod client-containers-7141f113-3e2b-475f-9cba-78848705efd1 to disappear
Sep 15 16:02:38.729: INFO: Pod client-containers-7141f113-3e2b-475f-9cba-78848705efd1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:02:38.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9838" for this suite.
Sep 15 16:02:44.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:02:44.902: INFO: namespace containers-9838 deletion completed in 6.167739191s

• [SLOW TEST:8.387 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:02:44.902: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:02:45.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec05daf6-5cd6-4e0f-98b1-837881dec6ab" in namespace "downward-api-4283" to be "success or failure"
Sep 15 16:02:45.068: INFO: Pod "downwardapi-volume-ec05daf6-5cd6-4e0f-98b1-837881dec6ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.450448ms
Sep 15 16:02:47.073: INFO: Pod "downwardapi-volume-ec05daf6-5cd6-4e0f-98b1-837881dec6ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.01261884s
Sep 15 16:02:49.078: INFO: Pod "downwardapi-volume-ec05daf6-5cd6-4e0f-98b1-837881dec6ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017673954s
STEP: Saw pod success
Sep 15 16:02:49.078: INFO: Pod "downwardapi-volume-ec05daf6-5cd6-4e0f-98b1-837881dec6ab" satisfied condition "success or failure"
Sep 15 16:02:49.082: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-ec05daf6-5cd6-4e0f-98b1-837881dec6ab container client-container: <nil>
STEP: delete the pod
Sep 15 16:02:49.112: INFO: Waiting for pod downwardapi-volume-ec05daf6-5cd6-4e0f-98b1-837881dec6ab to disappear
Sep 15 16:02:49.116: INFO: Pod downwardapi-volume-ec05daf6-5cd6-4e0f-98b1-837881dec6ab no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:02:49.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4283" for this suite.
Sep 15 16:02:55.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:02:55.286: INFO: namespace downward-api-4283 deletion completed in 6.164590955s

• [SLOW TEST:10.384 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:02:55.287: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4993
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4993
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep 15 16:02:55.450: INFO: Found 0 stateful pods, waiting for 3
Sep 15 16:03:05.455: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:03:05.455: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:03:05.455: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:03:05.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-4993 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:03:05.837: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:03:05.837: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:03:05.837: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep 15 16:03:15.876: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 15 16:03:25.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-4993 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 15 16:03:26.071: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 15 16:03:26.071: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 15 16:03:26.071: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 15 16:03:36.099: INFO: Waiting for StatefulSet statefulset-4993/ss2 to complete update
Sep 15 16:03:36.099: INFO: Waiting for Pod statefulset-4993/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep 15 16:03:36.099: INFO: Waiting for Pod statefulset-4993/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep 15 16:03:36.099: INFO: Waiting for Pod statefulset-4993/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep 15 16:03:46.109: INFO: Waiting for StatefulSet statefulset-4993/ss2 to complete update
Sep 15 16:03:46.109: INFO: Waiting for Pod statefulset-4993/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Sep 15 16:03:56.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-4993 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:03:56.279: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:03:56.279: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:03:56.279: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 15 16:04:06.319: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 15 16:04:16.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-4993 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 15 16:04:16.523: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 15 16:04:16.524: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 15 16:04:16.524: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 15 16:04:26.551: INFO: Waiting for StatefulSet statefulset-4993/ss2 to complete update
Sep 15 16:04:26.551: INFO: Waiting for Pod statefulset-4993/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep 15 16:04:26.551: INFO: Waiting for Pod statefulset-4993/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep 15 16:04:26.551: INFO: Waiting for Pod statefulset-4993/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep 15 16:04:36.561: INFO: Waiting for StatefulSet statefulset-4993/ss2 to complete update
Sep 15 16:04:36.561: INFO: Waiting for Pod statefulset-4993/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 15 16:04:46.561: INFO: Deleting all statefulset in ns statefulset-4993
Sep 15 16:04:46.565: INFO: Scaling statefulset ss2 to 0
Sep 15 16:05:06.584: INFO: Waiting for statefulset status.replicas updated to 0
Sep 15 16:05:06.591: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:05:06.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4993" for this suite.
Sep 15 16:05:12.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:05:12.778: INFO: namespace statefulset-4993 deletion completed in 6.162292201s

• [SLOW TEST:137.492 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:05:12.779: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-kngh
STEP: Creating a pod to test atomic-volume-subpath
Sep 15 16:05:12.948: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kngh" in namespace "subpath-8290" to be "success or failure"
Sep 15 16:05:12.955: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.806003ms
Sep 15 16:05:14.959: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 2.011522809s
Sep 15 16:05:16.965: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 4.016769532s
Sep 15 16:05:18.970: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 6.021709141s
Sep 15 16:05:20.974: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 8.026237s
Sep 15 16:05:22.979: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 10.031235854s
Sep 15 16:05:24.985: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 12.037348129s
Sep 15 16:05:26.990: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 14.042594907s
Sep 15 16:05:28.996: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 16.047624759s
Sep 15 16:05:31.000: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 18.052540847s
Sep 15 16:05:33.005: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Running", Reason="", readiness=true. Elapsed: 20.057549748s
Sep 15 16:05:35.010: INFO: Pod "pod-subpath-test-configmap-kngh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.061941364s
STEP: Saw pod success
Sep 15 16:05:35.010: INFO: Pod "pod-subpath-test-configmap-kngh" satisfied condition "success or failure"
Sep 15 16:05:35.014: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-subpath-test-configmap-kngh container test-container-subpath-configmap-kngh: <nil>
STEP: delete the pod
Sep 15 16:05:35.046: INFO: Waiting for pod pod-subpath-test-configmap-kngh to disappear
Sep 15 16:05:35.050: INFO: Pod pod-subpath-test-configmap-kngh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kngh
Sep 15 16:05:35.050: INFO: Deleting pod "pod-subpath-test-configmap-kngh" in namespace "subpath-8290"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:05:35.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8290" for this suite.
Sep 15 16:05:41.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:05:41.302: INFO: namespace subpath-8290 deletion completed in 6.241585653s

• [SLOW TEST:28.523 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:05:41.302: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:05:41.462: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 15 16:05:46.467: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 15 16:05:46.467: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 15 16:05:48.472: INFO: Creating deployment "test-rollover-deployment"
Sep 15 16:05:48.484: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 15 16:05:50.493: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 15 16:05:50.502: INFO: Ensure that both replica sets have 1 created replica
Sep 15 16:05:50.510: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 15 16:05:50.520: INFO: Updating deployment test-rollover-deployment
Sep 15 16:05:50.520: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 15 16:05:52.532: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 15 16:05:52.541: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 15 16:05:52.550: INFO: all replica sets need to contain the pod-template-hash label
Sep 15 16:05:52.550: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160351, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 15 16:05:54.560: INFO: all replica sets need to contain the pod-template-hash label
Sep 15 16:05:54.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160351, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 15 16:05:56.559: INFO: all replica sets need to contain the pod-template-hash label
Sep 15 16:05:56.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160351, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 15 16:05:58.560: INFO: all replica sets need to contain the pod-template-hash label
Sep 15 16:05:58.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160351, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 15 16:06:00.560: INFO: all replica sets need to contain the pod-template-hash label
Sep 15 16:06:00.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160351, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704160348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 15 16:06:02.560: INFO: 
Sep 15 16:06:02.560: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 15 16:06:02.573: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-6818,SelfLink:/apis/apps/v1/namespaces/deployment-6818/deployments/test-rollover-deployment,UID:10538dc7-7d62-4dbc-912c-7d674c28b758,ResourceVersion:12041,Generation:2,CreationTimestamp:2019-09-15 16:05:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-15 16:05:48 +0000 UTC 2019-09-15 16:05:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-15 16:06:01 +0000 UTC 2019-09-15 16:05:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 15 16:06:02.578: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-6818,SelfLink:/apis/apps/v1/namespaces/deployment-6818/replicasets/test-rollover-deployment-854595fc44,UID:0d0e3e66-f477-4642-8d93-c3e31386114d,ResourceVersion:12031,Generation:2,CreationTimestamp:2019-09-15 16:05:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 10538dc7-7d62-4dbc-912c-7d674c28b758 0xc00289fea7 0xc00289fea8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 15 16:06:02.578: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 15 16:06:02.578: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-6818,SelfLink:/apis/apps/v1/namespaces/deployment-6818/replicasets/test-rollover-controller,UID:90b8f39b-e2c0-40ac-b730-f5b15e27bb0a,ResourceVersion:12040,Generation:2,CreationTimestamp:2019-09-15 16:05:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 10538dc7-7d62-4dbc-912c-7d674c28b758 0xc00289fdd7 0xc00289fdd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 15 16:06:02.578: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-6818,SelfLink:/apis/apps/v1/namespaces/deployment-6818/replicasets/test-rollover-deployment-9b8b997cf,UID:69fd4861-faf8-4086-8822-a3d3d4474358,ResourceVersion:11991,Generation:2,CreationTimestamp:2019-09-15 16:05:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 10538dc7-7d62-4dbc-912c-7d674c28b758 0xc00289ff70 0xc00289ff71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 15 16:06:02.583: INFO: Pod "test-rollover-deployment-854595fc44-gbxmm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-gbxmm,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-6818,SelfLink:/api/v1/namespaces/deployment-6818/pods/test-rollover-deployment-854595fc44-gbxmm,UID:0446eee9-df05-4cb9-88e3-f4956681f3a1,ResourceVersion:12005,Generation:0,CreationTimestamp:2019-09-15 16:05:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.92/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 0d0e3e66-f477-4642-8d93-c3e31386114d 0xc0024b4b87 0xc0024b4b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vb2cp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vb2cp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-vb2cp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024b4bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024b4c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:05:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:05:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:05:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:05:50 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:100.96.2.92,StartTime:2019-09-15 16:05:50 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-15 16:05:51 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9c0ec3b089b002585be9bc268551a66651dfbd90a76f5e76f4a9243f3dc0af0d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:06:02.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6818" for this suite.
Sep 15 16:06:08.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:06:08.754: INFO: namespace deployment-6818 deletion completed in 6.165502429s

• [SLOW TEST:27.452 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:06:08.754: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8337
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8337/configmap-test-6e9623f7-ff00-4826-bad1-6fe2a9ea39a4
STEP: Creating a pod to test consume configMaps
Sep 15 16:06:08.915: INFO: Waiting up to 5m0s for pod "pod-configmaps-3bc1e5f6-3443-40ad-aeba-5617e1da5933" in namespace "configmap-8337" to be "success or failure"
Sep 15 16:06:08.922: INFO: Pod "pod-configmaps-3bc1e5f6-3443-40ad-aeba-5617e1da5933": Phase="Pending", Reason="", readiness=false. Elapsed: 6.836445ms
Sep 15 16:06:10.927: INFO: Pod "pod-configmaps-3bc1e5f6-3443-40ad-aeba-5617e1da5933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011901597s
STEP: Saw pod success
Sep 15 16:06:10.927: INFO: Pod "pod-configmaps-3bc1e5f6-3443-40ad-aeba-5617e1da5933" satisfied condition "success or failure"
Sep 15 16:06:10.932: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-3bc1e5f6-3443-40ad-aeba-5617e1da5933 container env-test: <nil>
STEP: delete the pod
Sep 15 16:06:10.968: INFO: Waiting for pod pod-configmaps-3bc1e5f6-3443-40ad-aeba-5617e1da5933 to disappear
Sep 15 16:06:10.972: INFO: Pod pod-configmaps-3bc1e5f6-3443-40ad-aeba-5617e1da5933 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:06:10.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8337" for this suite.
Sep 15 16:06:16.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:06:17.143: INFO: namespace configmap-8337 deletion completed in 6.164309849s

• [SLOW TEST:8.389 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:06:17.143: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:06:17.299: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2c5f6a3-f4f9-4ca5-91e6-7b5325a77d21" in namespace "projected-7165" to be "success or failure"
Sep 15 16:06:17.306: INFO: Pod "downwardapi-volume-c2c5f6a3-f4f9-4ca5-91e6-7b5325a77d21": Phase="Pending", Reason="", readiness=false. Elapsed: 6.778224ms
Sep 15 16:06:19.311: INFO: Pod "downwardapi-volume-c2c5f6a3-f4f9-4ca5-91e6-7b5325a77d21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011636435s
Sep 15 16:06:21.316: INFO: Pod "downwardapi-volume-c2c5f6a3-f4f9-4ca5-91e6-7b5325a77d21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016913524s
STEP: Saw pod success
Sep 15 16:06:21.316: INFO: Pod "downwardapi-volume-c2c5f6a3-f4f9-4ca5-91e6-7b5325a77d21" satisfied condition "success or failure"
Sep 15 16:06:21.320: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-c2c5f6a3-f4f9-4ca5-91e6-7b5325a77d21 container client-container: <nil>
STEP: delete the pod
Sep 15 16:06:21.355: INFO: Waiting for pod downwardapi-volume-c2c5f6a3-f4f9-4ca5-91e6-7b5325a77d21 to disappear
Sep 15 16:06:21.362: INFO: Pod downwardapi-volume-c2c5f6a3-f4f9-4ca5-91e6-7b5325a77d21 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:06:21.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7165" for this suite.
Sep 15 16:06:27.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:06:27.533: INFO: namespace projected-7165 deletion completed in 6.165831263s

• [SLOW TEST:10.390 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:06:27.533: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6647
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 15 16:06:27.692: INFO: Waiting up to 5m0s for pod "downward-api-7160379f-7c88-4555-aa63-de32b21b50e1" in namespace "downward-api-6647" to be "success or failure"
Sep 15 16:06:27.698: INFO: Pod "downward-api-7160379f-7c88-4555-aa63-de32b21b50e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.703122ms
Sep 15 16:06:29.704: INFO: Pod "downward-api-7160379f-7c88-4555-aa63-de32b21b50e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011441351s
STEP: Saw pod success
Sep 15 16:06:29.704: INFO: Pod "downward-api-7160379f-7c88-4555-aa63-de32b21b50e1" satisfied condition "success or failure"
Sep 15 16:06:29.708: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downward-api-7160379f-7c88-4555-aa63-de32b21b50e1 container dapi-container: <nil>
STEP: delete the pod
Sep 15 16:06:29.739: INFO: Waiting for pod downward-api-7160379f-7c88-4555-aa63-de32b21b50e1 to disappear
Sep 15 16:06:29.743: INFO: Pod downward-api-7160379f-7c88-4555-aa63-de32b21b50e1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:06:29.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6647" for this suite.
Sep 15 16:06:35.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:06:35.919: INFO: namespace downward-api-6647 deletion completed in 6.171344073s

• [SLOW TEST:8.386 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:06:35.919: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8509.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8509.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8509.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8509.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 15 16:06:38.116: INFO: DNS probes using dns-test-5f3dd1e8-11c7-4eb9-8c86-1268b8170016 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8509.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8509.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8509.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8509.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 15 16:06:50.184: INFO: DNS probes using dns-test-8bb7e54f-3b99-4c1d-b8dc-def2357acbce succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8509.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8509.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8509.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8509.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 15 16:06:54.270: INFO: DNS probes using dns-test-833923ae-021c-4226-87ac-c916005814a8 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:06:54.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8509" for this suite.
Sep 15 16:07:00.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:07:00.507: INFO: namespace dns-8509 deletion completed in 6.182626223s

• [SLOW TEST:24.588 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:07:00.507: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 15 16:07:00.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-2357'
Sep 15 16:07:00.784: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 15 16:07:00.784: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Sep 15 16:07:02.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete deployment e2e-test-nginx-deployment --namespace=kubectl-2357'
Sep 15 16:07:02.885: INFO: stderr: ""
Sep 15 16:07:02.885: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:07:02.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2357" for this suite.
Sep 15 16:07:24.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:07:25.056: INFO: namespace kubectl-2357 deletion completed in 22.165310361s

• [SLOW TEST:24.549 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:07:25.056: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1519
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-35b023ae-319d-4448-a6ef-f7ceb609d998
STEP: Creating configMap with name cm-test-opt-upd-2a16be09-60cd-432e-a757-9901bfff74f5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-35b023ae-319d-4448-a6ef-f7ceb609d998
STEP: Updating configmap cm-test-opt-upd-2a16be09-60cd-432e-a757-9901bfff74f5
STEP: Creating configMap with name cm-test-opt-create-f25cd467-e5db-4334-9236-097c5f4231f3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:07:29.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1519" for this suite.
Sep 15 16:07:51.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:07:51.549: INFO: namespace projected-1519 deletion completed in 22.17748306s

• [SLOW TEST:26.493 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:07:51.549: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-cbf6f664-e05f-4bdc-a185-de7b784c984e
STEP: Creating a pod to test consume secrets
Sep 15 16:07:51.714: INFO: Waiting up to 5m0s for pod "pod-secrets-8b37c9c9-4295-4e27-bfac-ccecdd978b0f" in namespace "secrets-8178" to be "success or failure"
Sep 15 16:07:51.721: INFO: Pod "pod-secrets-8b37c9c9-4295-4e27-bfac-ccecdd978b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.511187ms
Sep 15 16:07:53.726: INFO: Pod "pod-secrets-8b37c9c9-4295-4e27-bfac-ccecdd978b0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011563782s
STEP: Saw pod success
Sep 15 16:07:53.726: INFO: Pod "pod-secrets-8b37c9c9-4295-4e27-bfac-ccecdd978b0f" satisfied condition "success or failure"
Sep 15 16:07:53.730: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-secrets-8b37c9c9-4295-4e27-bfac-ccecdd978b0f container secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:07:53.761: INFO: Waiting for pod pod-secrets-8b37c9c9-4295-4e27-bfac-ccecdd978b0f to disappear
Sep 15 16:07:53.767: INFO: Pod pod-secrets-8b37c9c9-4295-4e27-bfac-ccecdd978b0f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:07:53.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8178" for this suite.
Sep 15 16:07:59.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:07:59.959: INFO: namespace secrets-8178 deletion completed in 6.186180163s

• [SLOW TEST:8.410 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:07:59.959: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep 15 16:08:02.674: INFO: Successfully updated pod "annotationupdate9a5355ab-b567-4446-886d-853f9e3a3577"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:08:06.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4775" for this suite.
Sep 15 16:08:28.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:08:28.885: INFO: namespace downward-api-4775 deletion completed in 22.166065551s

• [SLOW TEST:28.926 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:08:28.885: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7986
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 15 16:08:31.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec pod-sharedvolume-5335df8d-468b-40f2-bb78-ee57fa7ccabb -c busybox-main-container --namespace=emptydir-7986 -- cat /usr/share/volumeshare/shareddata.txt'
Sep 15 16:08:31.236: INFO: stderr: ""
Sep 15 16:08:31.236: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:08:31.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7986" for this suite.
Sep 15 16:08:37.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:08:37.412: INFO: namespace emptydir-7986 deletion completed in 6.169869327s

• [SLOW TEST:8.526 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:08:37.412: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 15 16:08:37.570: INFO: Waiting up to 5m0s for pod "downward-api-395e7b25-b9b5-41bb-b400-ed2458aec6d1" in namespace "downward-api-5944" to be "success or failure"
Sep 15 16:08:37.577: INFO: Pod "downward-api-395e7b25-b9b5-41bb-b400-ed2458aec6d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62282ms
Sep 15 16:08:39.583: INFO: Pod "downward-api-395e7b25-b9b5-41bb-b400-ed2458aec6d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012248977s
STEP: Saw pod success
Sep 15 16:08:39.583: INFO: Pod "downward-api-395e7b25-b9b5-41bb-b400-ed2458aec6d1" satisfied condition "success or failure"
Sep 15 16:08:39.587: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downward-api-395e7b25-b9b5-41bb-b400-ed2458aec6d1 container dapi-container: <nil>
STEP: delete the pod
Sep 15 16:08:39.618: INFO: Waiting for pod downward-api-395e7b25-b9b5-41bb-b400-ed2458aec6d1 to disappear
Sep 15 16:08:39.625: INFO: Pod downward-api-395e7b25-b9b5-41bb-b400-ed2458aec6d1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:08:39.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5944" for this suite.
Sep 15 16:08:45.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:08:45.977: INFO: namespace downward-api-5944 deletion completed in 6.347183958s

• [SLOW TEST:8.565 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:08:45.978: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:09:46.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8096" for this suite.
Sep 15 16:10:08.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:10:08.436: INFO: namespace container-probe-8096 deletion completed in 22.288045682s

• [SLOW TEST:82.459 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:10:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-9rzg
STEP: Creating a pod to test atomic-volume-subpath
Sep 15 16:10:08.604: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9rzg" in namespace "subpath-8840" to be "success or failure"
Sep 15 16:10:08.608: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.173891ms
Sep 15 16:10:10.613: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009013572s
Sep 15 16:10:12.618: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 4.013628932s
Sep 15 16:10:14.623: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 6.018881141s
Sep 15 16:10:16.628: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 8.023871899s
Sep 15 16:10:18.633: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 10.028738835s
Sep 15 16:10:20.638: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 12.033868891s
Sep 15 16:10:22.643: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 14.038852283s
Sep 15 16:10:24.648: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 16.04410583s
Sep 15 16:10:26.653: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 18.049290746s
Sep 15 16:10:28.658: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Running", Reason="", readiness=true. Elapsed: 20.054258034s
Sep 15 16:10:30.663: INFO: Pod "pod-subpath-test-downwardapi-9rzg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.059104229s
STEP: Saw pod success
Sep 15 16:10:30.663: INFO: Pod "pod-subpath-test-downwardapi-9rzg" satisfied condition "success or failure"
Sep 15 16:10:30.668: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-subpath-test-downwardapi-9rzg container test-container-subpath-downwardapi-9rzg: <nil>
STEP: delete the pod
Sep 15 16:10:30.703: INFO: Waiting for pod pod-subpath-test-downwardapi-9rzg to disappear
Sep 15 16:10:30.709: INFO: Pod pod-subpath-test-downwardapi-9rzg no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-9rzg
Sep 15 16:10:30.709: INFO: Deleting pod "pod-subpath-test-downwardapi-9rzg" in namespace "subpath-8840"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:10:30.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8840" for this suite.
Sep 15 16:10:36.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:10:36.881: INFO: namespace subpath-8840 deletion completed in 6.161961865s

• [SLOW TEST:28.445 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:10:36.882: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-cd2ee80e-52e6-49d3-ad2e-38cf993578f9
STEP: Creating a pod to test consume secrets
Sep 15 16:10:37.043: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b8e12d12-c66e-4024-a32f-6fe8c64f637d" in namespace "projected-5944" to be "success or failure"
Sep 15 16:10:37.049: INFO: Pod "pod-projected-secrets-b8e12d12-c66e-4024-a32f-6fe8c64f637d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.231902ms
Sep 15 16:10:39.054: INFO: Pod "pod-projected-secrets-b8e12d12-c66e-4024-a32f-6fe8c64f637d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010245839s
STEP: Saw pod success
Sep 15 16:10:39.054: INFO: Pod "pod-projected-secrets-b8e12d12-c66e-4024-a32f-6fe8c64f637d" satisfied condition "success or failure"
Sep 15 16:10:39.058: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-secrets-b8e12d12-c66e-4024-a32f-6fe8c64f637d container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:10:39.088: INFO: Waiting for pod pod-projected-secrets-b8e12d12-c66e-4024-a32f-6fe8c64f637d to disappear
Sep 15 16:10:39.094: INFO: Pod pod-projected-secrets-b8e12d12-c66e-4024-a32f-6fe8c64f637d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:10:39.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5944" for this suite.
Sep 15 16:10:45.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:10:45.266: INFO: namespace projected-5944 deletion completed in 6.167020248s

• [SLOW TEST:8.385 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:10:45.266: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:10:45.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-807c9e7f-df0f-4400-96b3-29baa0b543fc" in namespace "projected-7973" to be "success or failure"
Sep 15 16:10:45.428: INFO: Pod "downwardapi-volume-807c9e7f-df0f-4400-96b3-29baa0b543fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666644ms
Sep 15 16:10:47.432: INFO: Pod "downwardapi-volume-807c9e7f-df0f-4400-96b3-29baa0b543fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009353181s
STEP: Saw pod success
Sep 15 16:10:47.432: INFO: Pod "downwardapi-volume-807c9e7f-df0f-4400-96b3-29baa0b543fc" satisfied condition "success or failure"
Sep 15 16:10:47.436: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-807c9e7f-df0f-4400-96b3-29baa0b543fc container client-container: <nil>
STEP: delete the pod
Sep 15 16:10:47.467: INFO: Waiting for pod downwardapi-volume-807c9e7f-df0f-4400-96b3-29baa0b543fc to disappear
Sep 15 16:10:47.471: INFO: Pod downwardapi-volume-807c9e7f-df0f-4400-96b3-29baa0b543fc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:10:47.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7973" for this suite.
Sep 15 16:10:53.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:10:53.642: INFO: namespace projected-7973 deletion completed in 6.164659076s

• [SLOW TEST:8.376 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:10:53.642: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8177
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-6e49627a-f51b-433b-8db9-6957ad70e9f7
STEP: Creating a pod to test consume secrets
Sep 15 16:10:53.854: INFO: Waiting up to 5m0s for pod "pod-secrets-7d81e6aa-a024-4ad3-acf1-b31e3bbe26f6" in namespace "secrets-8177" to be "success or failure"
Sep 15 16:10:53.860: INFO: Pod "pod-secrets-7d81e6aa-a024-4ad3-acf1-b31e3bbe26f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.6012ms
Sep 15 16:10:55.865: INFO: Pod "pod-secrets-7d81e6aa-a024-4ad3-acf1-b31e3bbe26f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010558809s
STEP: Saw pod success
Sep 15 16:10:55.865: INFO: Pod "pod-secrets-7d81e6aa-a024-4ad3-acf1-b31e3bbe26f6" satisfied condition "success or failure"
Sep 15 16:10:55.869: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-secrets-7d81e6aa-a024-4ad3-acf1-b31e3bbe26f6 container secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:10:55.897: INFO: Waiting for pod pod-secrets-7d81e6aa-a024-4ad3-acf1-b31e3bbe26f6 to disappear
Sep 15 16:10:55.901: INFO: Pod pod-secrets-7d81e6aa-a024-4ad3-acf1-b31e3bbe26f6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:10:55.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8177" for this suite.
Sep 15 16:11:01.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:11:02.077: INFO: namespace secrets-8177 deletion completed in 6.171241107s

• [SLOW TEST:8.435 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:11:02.077: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Sep 15 16:11:02.239: INFO: Waiting up to 5m0s for pod "var-expansion-6aab791f-f5e7-4145-874d-fb81d5a6d616" in namespace "var-expansion-2082" to be "success or failure"
Sep 15 16:11:02.245: INFO: Pod "var-expansion-6aab791f-f5e7-4145-874d-fb81d5a6d616": Phase="Pending", Reason="", readiness=false. Elapsed: 6.47803ms
Sep 15 16:11:04.250: INFO: Pod "var-expansion-6aab791f-f5e7-4145-874d-fb81d5a6d616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011483019s
STEP: Saw pod success
Sep 15 16:11:04.250: INFO: Pod "var-expansion-6aab791f-f5e7-4145-874d-fb81d5a6d616" satisfied condition "success or failure"
Sep 15 16:11:04.254: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod var-expansion-6aab791f-f5e7-4145-874d-fb81d5a6d616 container dapi-container: <nil>
STEP: delete the pod
Sep 15 16:11:04.283: INFO: Waiting for pod var-expansion-6aab791f-f5e7-4145-874d-fb81d5a6d616 to disappear
Sep 15 16:11:04.288: INFO: Pod var-expansion-6aab791f-f5e7-4145-874d-fb81d5a6d616 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:11:04.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2082" for this suite.
Sep 15 16:11:10.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:11:10.579: INFO: namespace var-expansion-2082 deletion completed in 6.284326066s

• [SLOW TEST:8.501 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:11:10.579: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8418
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:11:10.736: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dfd563ab-1a4a-4876-a110-3c9909202abb" in namespace "downward-api-8418" to be "success or failure"
Sep 15 16:11:10.743: INFO: Pod "downwardapi-volume-dfd563ab-1a4a-4876-a110-3c9909202abb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084469ms
Sep 15 16:11:12.748: INFO: Pod "downwardapi-volume-dfd563ab-1a4a-4876-a110-3c9909202abb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011144088s
STEP: Saw pod success
Sep 15 16:11:12.748: INFO: Pod "downwardapi-volume-dfd563ab-1a4a-4876-a110-3c9909202abb" satisfied condition "success or failure"
Sep 15 16:11:12.752: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-dfd563ab-1a4a-4876-a110-3c9909202abb container client-container: <nil>
STEP: delete the pod
Sep 15 16:11:12.782: INFO: Waiting for pod downwardapi-volume-dfd563ab-1a4a-4876-a110-3c9909202abb to disappear
Sep 15 16:11:12.788: INFO: Pod downwardapi-volume-dfd563ab-1a4a-4876-a110-3c9909202abb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:11:12.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8418" for this suite.
Sep 15 16:11:18.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:11:19.004: INFO: namespace downward-api-8418 deletion completed in 6.210559359s

• [SLOW TEST:8.425 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:11:19.004: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 15 16:11:19.158: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:11:32.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8501" for this suite.
Sep 15 16:11:38.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:11:39.036: INFO: namespace pods-8501 deletion completed in 6.159052592s

• [SLOW TEST:20.032 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:11:39.036: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Sep 15 16:11:39.180: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-180164986 proxy --unix-socket=/tmp/kubectl-proxy-unix175250577/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:11:39.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7829" for this suite.
Sep 15 16:11:45.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:11:45.410: INFO: namespace kubectl-7829 deletion completed in 6.172044882s

• [SLOW TEST:6.374 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:11:45.410: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 15 16:11:47.585: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:11:47.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2526" for this suite.
Sep 15 16:11:53.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:11:53.901: INFO: namespace container-runtime-2526 deletion completed in 6.286810219s

• [SLOW TEST:8.491 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:11:53.902: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:11:54.057: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17fdd56c-e724-4e2d-bc66-bc0f34b83d4a" in namespace "downward-api-8826" to be "success or failure"
Sep 15 16:11:54.064: INFO: Pod "downwardapi-volume-17fdd56c-e724-4e2d-bc66-bc0f34b83d4a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.510529ms
Sep 15 16:11:56.069: INFO: Pod "downwardapi-volume-17fdd56c-e724-4e2d-bc66-bc0f34b83d4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011545551s
STEP: Saw pod success
Sep 15 16:11:56.069: INFO: Pod "downwardapi-volume-17fdd56c-e724-4e2d-bc66-bc0f34b83d4a" satisfied condition "success or failure"
Sep 15 16:11:56.073: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-17fdd56c-e724-4e2d-bc66-bc0f34b83d4a container client-container: <nil>
STEP: delete the pod
Sep 15 16:11:56.105: INFO: Waiting for pod downwardapi-volume-17fdd56c-e724-4e2d-bc66-bc0f34b83d4a to disappear
Sep 15 16:11:56.109: INFO: Pod downwardapi-volume-17fdd56c-e724-4e2d-bc66-bc0f34b83d4a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:11:56.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8826" for this suite.
Sep 15 16:12:02.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:12:02.289: INFO: namespace downward-api-8826 deletion completed in 6.173820074s

• [SLOW TEST:8.387 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:12:02.289: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:12:05.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3534" for this suite.
Sep 15 16:12:27.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:12:27.661: INFO: namespace replication-controller-3534 deletion completed in 22.177223585s

• [SLOW TEST:25.372 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:12:27.661: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Sep 15 16:12:27.806: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-180164986 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:12:27.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5118" for this suite.
Sep 15 16:12:33.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:12:34.079: INFO: namespace kubectl-5118 deletion completed in 6.173544739s

• [SLOW TEST:6.418 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:12:34.079: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3648
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 15 16:12:34.253: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3648,SelfLink:/api/v1/namespaces/watch-3648/configmaps/e2e-watch-test-label-changed,UID:ac55a269-f7d9-476e-b17c-472752f71797,ResourceVersion:13661,Generation:0,CreationTimestamp:2019-09-15 16:12:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 15 16:12:34.253: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3648,SelfLink:/api/v1/namespaces/watch-3648/configmaps/e2e-watch-test-label-changed,UID:ac55a269-f7d9-476e-b17c-472752f71797,ResourceVersion:13662,Generation:0,CreationTimestamp:2019-09-15 16:12:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 15 16:12:34.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3648,SelfLink:/api/v1/namespaces/watch-3648/configmaps/e2e-watch-test-label-changed,UID:ac55a269-f7d9-476e-b17c-472752f71797,ResourceVersion:13663,Generation:0,CreationTimestamp:2019-09-15 16:12:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 15 16:12:44.293: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3648,SelfLink:/api/v1/namespaces/watch-3648/configmaps/e2e-watch-test-label-changed,UID:ac55a269-f7d9-476e-b17c-472752f71797,ResourceVersion:13685,Generation:0,CreationTimestamp:2019-09-15 16:12:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 15 16:12:44.293: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3648,SelfLink:/api/v1/namespaces/watch-3648/configmaps/e2e-watch-test-label-changed,UID:ac55a269-f7d9-476e-b17c-472752f71797,ResourceVersion:13686,Generation:0,CreationTimestamp:2019-09-15 16:12:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep 15 16:12:44.293: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3648,SelfLink:/api/v1/namespaces/watch-3648/configmaps/e2e-watch-test-label-changed,UID:ac55a269-f7d9-476e-b17c-472752f71797,ResourceVersion:13687,Generation:0,CreationTimestamp:2019-09-15 16:12:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:12:44.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3648" for this suite.
Sep 15 16:12:50.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:12:50.503: INFO: namespace watch-3648 deletion completed in 6.204886461s

• [SLOW TEST:16.424 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:12:50.503: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Sep 15 16:12:50.660: INFO: Waiting up to 5m0s for pod "client-containers-feee1dc1-e3db-459b-a2ba-c743611cd5c9" in namespace "containers-5678" to be "success or failure"
Sep 15 16:12:50.667: INFO: Pod "client-containers-feee1dc1-e3db-459b-a2ba-c743611cd5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.556722ms
Sep 15 16:12:52.673: INFO: Pod "client-containers-feee1dc1-e3db-459b-a2ba-c743611cd5c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013691007s
STEP: Saw pod success
Sep 15 16:12:52.673: INFO: Pod "client-containers-feee1dc1-e3db-459b-a2ba-c743611cd5c9" satisfied condition "success or failure"
Sep 15 16:12:52.678: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod client-containers-feee1dc1-e3db-459b-a2ba-c743611cd5c9 container test-container: <nil>
STEP: delete the pod
Sep 15 16:12:52.711: INFO: Waiting for pod client-containers-feee1dc1-e3db-459b-a2ba-c743611cd5c9 to disappear
Sep 15 16:12:52.717: INFO: Pod client-containers-feee1dc1-e3db-459b-a2ba-c743611cd5c9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:12:52.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5678" for this suite.
Sep 15 16:12:58.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:12:58.901: INFO: namespace containers-5678 deletion completed in 6.179931777s

• [SLOW TEST:8.399 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:12:58.902: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-20c9e69d-9bc1-446e-8416-50b185289047
STEP: Creating a pod to test consume configMaps
Sep 15 16:12:59.107: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4068caa-0d6c-4fb8-94cc-dc4f8c74441c" in namespace "configmap-1122" to be "success or failure"
Sep 15 16:12:59.114: INFO: Pod "pod-configmaps-d4068caa-0d6c-4fb8-94cc-dc4f8c74441c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.463362ms
Sep 15 16:13:01.119: INFO: Pod "pod-configmaps-d4068caa-0d6c-4fb8-94cc-dc4f8c74441c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012426136s
STEP: Saw pod success
Sep 15 16:13:01.119: INFO: Pod "pod-configmaps-d4068caa-0d6c-4fb8-94cc-dc4f8c74441c" satisfied condition "success or failure"
Sep 15 16:13:01.124: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-d4068caa-0d6c-4fb8-94cc-dc4f8c74441c container configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 16:13:01.155: INFO: Waiting for pod pod-configmaps-d4068caa-0d6c-4fb8-94cc-dc4f8c74441c to disappear
Sep 15 16:13:01.159: INFO: Pod pod-configmaps-d4068caa-0d6c-4fb8-94cc-dc4f8c74441c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:13:01.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1122" for this suite.
Sep 15 16:13:07.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:13:07.333: INFO: namespace configmap-1122 deletion completed in 6.167821901s

• [SLOW TEST:8.431 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:13:07.333: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5363
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 15 16:13:07.494: INFO: Waiting up to 5m0s for pod "pod-ee880657-aa17-44b1-bba3-8bc1c61f8cd6" in namespace "emptydir-5363" to be "success or failure"
Sep 15 16:13:07.501: INFO: Pod "pod-ee880657-aa17-44b1-bba3-8bc1c61f8cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.983802ms
Sep 15 16:13:09.506: INFO: Pod "pod-ee880657-aa17-44b1-bba3-8bc1c61f8cd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01198111s
STEP: Saw pod success
Sep 15 16:13:09.506: INFO: Pod "pod-ee880657-aa17-44b1-bba3-8bc1c61f8cd6" satisfied condition "success or failure"
Sep 15 16:13:09.511: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-ee880657-aa17-44b1-bba3-8bc1c61f8cd6 container test-container: <nil>
STEP: delete the pod
Sep 15 16:13:09.542: INFO: Waiting for pod pod-ee880657-aa17-44b1-bba3-8bc1c61f8cd6 to disappear
Sep 15 16:13:09.546: INFO: Pod pod-ee880657-aa17-44b1-bba3-8bc1c61f8cd6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:13:09.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5363" for this suite.
Sep 15 16:13:15.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:13:15.770: INFO: namespace emptydir-5363 deletion completed in 6.21699547s

• [SLOW TEST:8.437 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:13:15.770: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:13:15.915: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:13:17.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4586" for this suite.
Sep 15 16:13:55.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:13:56.130: INFO: namespace pods-4586 deletion completed in 38.166179828s

• [SLOW TEST:40.360 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:13:56.130: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9230
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9230
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 15 16:13:56.285: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 15 16:14:18.394: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.3.47 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9230 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:14:18.394: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:14:19.490: INFO: Found all expected endpoints: [netserver-0]
Sep 15 16:14:19.495: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.2.119 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9230 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:14:19.495: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:14:20.580: INFO: Found all expected endpoints: [netserver-1]
Sep 15 16:14:20.585: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.42 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9230 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:14:20.585: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:14:21.685: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:14:21.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9230" for this suite.
Sep 15 16:14:43.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:14:43.859: INFO: namespace pod-network-test-9230 deletion completed in 22.167909711s

• [SLOW TEST:47.729 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:14:43.859: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep 15 16:14:46.574: INFO: Successfully updated pod "annotationupdatec7fd1579-a0e1-4bc8-962a-a2e995d368c2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:14:50.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1701" for this suite.
Sep 15 16:15:12.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:15:12.789: INFO: namespace projected-1701 deletion completed in 22.170434845s

• [SLOW TEST:28.930 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:15:12.789: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2615
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep 15 16:15:23.032: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0915 16:15:23.032788      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:15:23.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2615" for this suite.
Sep 15 16:15:31.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:15:31.202: INFO: namespace gc-2615 deletion completed in 8.16468335s

• [SLOW TEST:18.413 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:15:31.203: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3492
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Sep 15 16:15:31.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 cluster-info'
Sep 15 16:15:31.616: INFO: stderr: ""
Sep 15 16:15:31.616: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\x1b[0;32mHeapster\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/heapster/proxy\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mtiller-deploy\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/tiller-deploy:tiller/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:15:31.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3492" for this suite.
Sep 15 16:15:37.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:15:37.794: INFO: namespace kubectl-3492 deletion completed in 6.172319684s

• [SLOW TEST:6.591 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:15:37.794: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4274
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-ls8w5 in namespace proxy-4274
I0915 16:15:37.966377      19 runners.go:180] Created replication controller with name: proxy-service-ls8w5, namespace: proxy-4274, replica count: 1
I0915 16:15:39.016837      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0915 16:15:40.017646      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0915 16:15:41.017891      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0915 16:15:42.018117      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0915 16:15:43.018348      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0915 16:15:44.018593      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0915 16:15:45.018822      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0915 16:15:46.019069      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0915 16:15:47.019289      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0915 16:15:48.019520      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0915 16:15:49.019742      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0915 16:15:50.019982      19 runners.go:180] proxy-service-ls8w5 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 15 16:15:50.025: INFO: setup took 12.08414563s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 15 16:15:50.033: INFO: (0) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 7.931639ms)
Sep 15 16:15:50.035: INFO: (0) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 9.879918ms)
Sep 15 16:15:50.036: INFO: (0) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.741349ms)
Sep 15 16:15:50.036: INFO: (0) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 10.864707ms)
Sep 15 16:15:50.036: INFO: (0) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 10.759324ms)
Sep 15 16:15:50.036: INFO: (0) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 11.190625ms)
Sep 15 16:15:50.036: INFO: (0) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 11.257073ms)
Sep 15 16:15:50.038: INFO: (0) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 13.450033ms)
Sep 15 16:15:50.041: INFO: (0) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 15.967908ms)
Sep 15 16:15:50.042: INFO: (0) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 16.759455ms)
Sep 15 16:15:50.042: INFO: (0) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 16.823005ms)
Sep 15 16:15:50.042: INFO: (0) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 16.932172ms)
Sep 15 16:15:50.042: INFO: (0) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 17.058523ms)
Sep 15 16:15:50.042: INFO: (0) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 16.939493ms)
Sep 15 16:15:50.042: INFO: (0) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 17.068353ms)
Sep 15 16:15:50.042: INFO: (0) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 16.950023ms)
Sep 15 16:15:50.046: INFO: (1) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 4.722537ms)
Sep 15 16:15:50.050: INFO: (1) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 7.281277ms)
Sep 15 16:15:50.050: INFO: (1) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 7.436428ms)
Sep 15 16:15:50.050: INFO: (1) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 8.080249ms)
Sep 15 16:15:50.050: INFO: (1) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 8.256337ms)
Sep 15 16:15:50.050: INFO: (1) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 7.711826ms)
Sep 15 16:15:50.051: INFO: (1) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.029584ms)
Sep 15 16:15:50.051: INFO: (1) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 8.731986ms)
Sep 15 16:15:50.051: INFO: (1) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 8.745945ms)
Sep 15 16:15:50.051: INFO: (1) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 8.389037ms)
Sep 15 16:15:50.054: INFO: (1) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 11.474159ms)
Sep 15 16:15:50.057: INFO: (1) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 13.035795ms)
Sep 15 16:15:50.057: INFO: (1) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 14.410332ms)
Sep 15 16:15:50.057: INFO: (1) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 14.203213ms)
Sep 15 16:15:50.057: INFO: (1) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 14.366495ms)
Sep 15 16:15:50.057: INFO: (1) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 14.499442ms)
Sep 15 16:15:50.067: INFO: (2) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 10.437219ms)
Sep 15 16:15:50.067: INFO: (2) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 10.106492ms)
Sep 15 16:15:50.067: INFO: (2) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 10.007583ms)
Sep 15 16:15:50.067: INFO: (2) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.09076ms)
Sep 15 16:15:50.067: INFO: (2) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 10.47429ms)
Sep 15 16:15:50.068: INFO: (2) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 10.750102ms)
Sep 15 16:15:50.068: INFO: (2) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.444681ms)
Sep 15 16:15:50.068: INFO: (2) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 10.27375ms)
Sep 15 16:15:50.068: INFO: (2) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 10.416803ms)
Sep 15 16:15:50.068: INFO: (2) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 9.825412ms)
Sep 15 16:15:50.069: INFO: (2) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 11.788473ms)
Sep 15 16:15:50.071: INFO: (2) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 13.797359ms)
Sep 15 16:15:50.072: INFO: (2) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 14.319256ms)
Sep 15 16:15:50.072: INFO: (2) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 13.9884ms)
Sep 15 16:15:50.072: INFO: (2) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 14.88177ms)
Sep 15 16:15:50.072: INFO: (2) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 14.783712ms)
Sep 15 16:15:50.083: INFO: (3) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 10.698765ms)
Sep 15 16:15:50.083: INFO: (3) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.935676ms)
Sep 15 16:15:50.083: INFO: (3) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 10.778443ms)
Sep 15 16:15:50.083: INFO: (3) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 10.608275ms)
Sep 15 16:15:50.083: INFO: (3) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 11.055116ms)
Sep 15 16:15:50.084: INFO: (3) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 10.901285ms)
Sep 15 16:15:50.084: INFO: (3) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 10.882243ms)
Sep 15 16:15:50.084: INFO: (3) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.869695ms)
Sep 15 16:15:50.084: INFO: (3) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 10.986279ms)
Sep 15 16:15:50.084: INFO: (3) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 10.669321ms)
Sep 15 16:15:50.085: INFO: (3) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 12.611177ms)
Sep 15 16:15:50.087: INFO: (3) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 14.466829ms)
Sep 15 16:15:50.087: INFO: (3) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 14.575578ms)
Sep 15 16:15:50.087: INFO: (3) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 14.87294ms)
Sep 15 16:15:50.087: INFO: (3) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 14.638849ms)
Sep 15 16:15:50.090: INFO: (3) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 17.186346ms)
Sep 15 16:15:50.097: INFO: (4) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 7.383265ms)
Sep 15 16:15:50.101: INFO: (4) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 10.420109ms)
Sep 15 16:15:50.101: INFO: (4) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 9.77459ms)
Sep 15 16:15:50.101: INFO: (4) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 10.535953ms)
Sep 15 16:15:50.101: INFO: (4) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.421087ms)
Sep 15 16:15:50.101: INFO: (4) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 10.250281ms)
Sep 15 16:15:50.101: INFO: (4) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 10.390957ms)
Sep 15 16:15:50.101: INFO: (4) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 9.948184ms)
Sep 15 16:15:50.101: INFO: (4) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 10.36411ms)
Sep 15 16:15:50.101: INFO: (4) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 10.235748ms)
Sep 15 16:15:50.104: INFO: (4) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 12.830847ms)
Sep 15 16:15:50.107: INFO: (4) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 16.238988ms)
Sep 15 16:15:50.107: INFO: (4) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 16.458852ms)
Sep 15 16:15:50.107: INFO: (4) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 16.222655ms)
Sep 15 16:15:50.107: INFO: (4) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 16.238721ms)
Sep 15 16:15:50.107: INFO: (4) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 16.898546ms)
Sep 15 16:15:50.113: INFO: (5) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 6.051291ms)
Sep 15 16:15:50.117: INFO: (5) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 9.744879ms)
Sep 15 16:15:50.117: INFO: (5) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.794522ms)
Sep 15 16:15:50.117: INFO: (5) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 9.278095ms)
Sep 15 16:15:50.117: INFO: (5) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 9.420539ms)
Sep 15 16:15:50.117: INFO: (5) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 8.911368ms)
Sep 15 16:15:50.117: INFO: (5) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 9.145606ms)
Sep 15 16:15:50.126: INFO: (5) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 18.896733ms)
Sep 15 16:15:50.126: INFO: (5) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 19.447123ms)
Sep 15 16:15:50.126: INFO: (5) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 18.39311ms)
Sep 15 16:15:50.126: INFO: (5) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 18.809812ms)
Sep 15 16:15:50.126: INFO: (5) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 19.229973ms)
Sep 15 16:15:50.127: INFO: (5) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 20.141931ms)
Sep 15 16:15:50.127: INFO: (5) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 19.49449ms)
Sep 15 16:15:50.127: INFO: (5) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 20.112335ms)
Sep 15 16:15:50.127: INFO: (5) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 20.053275ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 11.010589ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 10.809227ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 11.104736ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 11.090648ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 11.024306ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 11.205056ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.957985ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 10.839758ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 10.950139ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 10.832641ms)
Sep 15 16:15:50.139: INFO: (6) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 11.914435ms)
Sep 15 16:15:50.142: INFO: (6) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 14.186439ms)
Sep 15 16:15:50.142: INFO: (6) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 14.443901ms)
Sep 15 16:15:50.142: INFO: (6) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 14.624455ms)
Sep 15 16:15:50.142: INFO: (6) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 14.405546ms)
Sep 15 16:15:50.142: INFO: (6) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 14.358687ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 13.461472ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 13.615842ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 13.27833ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 13.222003ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 13.431833ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 13.232414ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 13.412167ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 13.398669ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 13.245593ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 13.336332ms)
Sep 15 16:15:50.156: INFO: (7) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 13.251813ms)
Sep 15 16:15:50.158: INFO: (7) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 15.700025ms)
Sep 15 16:15:50.161: INFO: (7) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 18.035908ms)
Sep 15 16:15:50.161: INFO: (7) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 17.999013ms)
Sep 15 16:15:50.161: INFO: (7) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 18.123208ms)
Sep 15 16:15:50.161: INFO: (7) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 18.139287ms)
Sep 15 16:15:50.166: INFO: (8) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 4.709783ms)
Sep 15 16:15:50.169: INFO: (8) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.128349ms)
Sep 15 16:15:50.169: INFO: (8) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 8.030919ms)
Sep 15 16:15:50.169: INFO: (8) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 7.949502ms)
Sep 15 16:15:50.169: INFO: (8) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 7.506274ms)
Sep 15 16:15:50.169: INFO: (8) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 8.16325ms)
Sep 15 16:15:50.169: INFO: (8) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 7.822299ms)
Sep 15 16:15:50.170: INFO: (8) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 7.781749ms)
Sep 15 16:15:50.170: INFO: (8) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 7.902403ms)
Sep 15 16:15:50.170: INFO: (8) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 8.346445ms)
Sep 15 16:15:50.172: INFO: (8) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 10.381942ms)
Sep 15 16:15:50.175: INFO: (8) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 13.349173ms)
Sep 15 16:15:50.175: INFO: (8) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 13.860486ms)
Sep 15 16:15:50.175: INFO: (8) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 13.758182ms)
Sep 15 16:15:50.175: INFO: (8) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 14.234704ms)
Sep 15 16:15:50.175: INFO: (8) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 13.357988ms)
Sep 15 16:15:50.184: INFO: (9) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 9.0156ms)
Sep 15 16:15:50.185: INFO: (9) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 9.051219ms)
Sep 15 16:15:50.185: INFO: (9) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 8.874674ms)
Sep 15 16:15:50.185: INFO: (9) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 9.056444ms)
Sep 15 16:15:50.185: INFO: (9) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 9.271991ms)
Sep 15 16:15:50.185: INFO: (9) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 9.634351ms)
Sep 15 16:15:50.185: INFO: (9) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 9.679635ms)
Sep 15 16:15:50.186: INFO: (9) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 9.551797ms)
Sep 15 16:15:50.186: INFO: (9) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 9.908164ms)
Sep 15 16:15:50.186: INFO: (9) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 9.682658ms)
Sep 15 16:15:50.187: INFO: (9) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 11.039633ms)
Sep 15 16:15:50.189: INFO: (9) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 13.571586ms)
Sep 15 16:15:50.189: INFO: (9) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 13.579647ms)
Sep 15 16:15:50.190: INFO: (9) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 14.208314ms)
Sep 15 16:15:50.190: INFO: (9) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 14.021646ms)
Sep 15 16:15:50.190: INFO: (9) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 13.866073ms)
Sep 15 16:15:50.195: INFO: (10) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 5.000484ms)
Sep 15 16:15:50.199: INFO: (10) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 7.434591ms)
Sep 15 16:15:50.199: INFO: (10) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 9.208299ms)
Sep 15 16:15:50.199: INFO: (10) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 7.43741ms)
Sep 15 16:15:50.199: INFO: (10) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 9.168386ms)
Sep 15 16:15:50.199: INFO: (10) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 7.657599ms)
Sep 15 16:15:50.199: INFO: (10) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 7.538296ms)
Sep 15 16:15:50.199: INFO: (10) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 7.617697ms)
Sep 15 16:15:50.199: INFO: (10) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 8.817447ms)
Sep 15 16:15:50.199: INFO: (10) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 9.137977ms)
Sep 15 16:15:50.202: INFO: (10) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 11.589818ms)
Sep 15 16:15:50.205: INFO: (10) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 14.813675ms)
Sep 15 16:15:50.205: INFO: (10) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 15.12977ms)
Sep 15 16:15:50.205: INFO: (10) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 13.473391ms)
Sep 15 16:15:50.205: INFO: (10) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 14.680441ms)
Sep 15 16:15:50.205: INFO: (10) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 14.78905ms)
Sep 15 16:15:50.213: INFO: (11) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 7.838054ms)
Sep 15 16:15:50.217: INFO: (11) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 11.624005ms)
Sep 15 16:15:50.217: INFO: (11) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 11.617832ms)
Sep 15 16:15:50.217: INFO: (11) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 11.540831ms)
Sep 15 16:15:50.217: INFO: (11) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 11.572588ms)
Sep 15 16:15:50.217: INFO: (11) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 11.568914ms)
Sep 15 16:15:50.217: INFO: (11) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 11.599629ms)
Sep 15 16:15:50.217: INFO: (11) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 11.736108ms)
Sep 15 16:15:50.217: INFO: (11) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 11.547279ms)
Sep 15 16:15:50.219: INFO: (11) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 13.730655ms)
Sep 15 16:15:50.227: INFO: (11) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 21.315689ms)
Sep 15 16:15:50.227: INFO: (11) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 21.916714ms)
Sep 15 16:15:50.227: INFO: (11) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 21.919533ms)
Sep 15 16:15:50.228: INFO: (11) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 22.950761ms)
Sep 15 16:15:50.228: INFO: (11) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 22.857768ms)
Sep 15 16:15:50.228: INFO: (11) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 23.029647ms)
Sep 15 16:15:50.233: INFO: (12) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 4.788611ms)
Sep 15 16:15:50.237: INFO: (12) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 6.869866ms)
Sep 15 16:15:50.237: INFO: (12) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.25302ms)
Sep 15 16:15:50.237: INFO: (12) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.517421ms)
Sep 15 16:15:50.237: INFO: (12) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 8.411809ms)
Sep 15 16:15:50.237: INFO: (12) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 8.616574ms)
Sep 15 16:15:50.237: INFO: (12) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 8.508585ms)
Sep 15 16:15:50.237: INFO: (12) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 7.158041ms)
Sep 15 16:15:50.237: INFO: (12) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 7.375234ms)
Sep 15 16:15:50.237: INFO: (12) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 7.324217ms)
Sep 15 16:15:50.242: INFO: (12) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 12.054065ms)
Sep 15 16:15:50.242: INFO: (12) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 13.214384ms)
Sep 15 16:15:50.242: INFO: (12) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 12.516501ms)
Sep 15 16:15:50.242: INFO: (12) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 12.475595ms)
Sep 15 16:15:50.242: INFO: (12) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 12.656585ms)
Sep 15 16:15:50.242: INFO: (12) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 13.733423ms)
Sep 15 16:15:50.249: INFO: (13) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 6.112143ms)
Sep 15 16:15:50.252: INFO: (13) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 8.628477ms)
Sep 15 16:15:50.252: INFO: (13) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 9.919848ms)
Sep 15 16:15:50.253: INFO: (13) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 10.064698ms)
Sep 15 16:15:50.253: INFO: (13) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 10.141874ms)
Sep 15 16:15:50.253: INFO: (13) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 9.628601ms)
Sep 15 16:15:50.253: INFO: (13) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 9.873798ms)
Sep 15 16:15:50.253: INFO: (13) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.142074ms)
Sep 15 16:15:50.253: INFO: (13) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 9.883899ms)
Sep 15 16:15:50.253: INFO: (13) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 10.013115ms)
Sep 15 16:15:50.256: INFO: (13) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 13.412905ms)
Sep 15 16:15:50.259: INFO: (13) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 15.948816ms)
Sep 15 16:15:50.259: INFO: (13) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 16.515104ms)
Sep 15 16:15:50.259: INFO: (13) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 16.368251ms)
Sep 15 16:15:50.259: INFO: (13) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 15.80223ms)
Sep 15 16:15:50.259: INFO: (13) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 16.494348ms)
Sep 15 16:15:50.264: INFO: (14) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 5.057702ms)
Sep 15 16:15:50.267: INFO: (14) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 7.656697ms)
Sep 15 16:15:50.267: INFO: (14) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 7.454865ms)
Sep 15 16:15:50.267: INFO: (14) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 8.109401ms)
Sep 15 16:15:50.267: INFO: (14) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 7.731489ms)
Sep 15 16:15:50.268: INFO: (14) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 8.497187ms)
Sep 15 16:15:50.268: INFO: (14) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 8.031863ms)
Sep 15 16:15:50.268: INFO: (14) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.842961ms)
Sep 15 16:15:50.268: INFO: (14) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 8.795995ms)
Sep 15 16:15:50.268: INFO: (14) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 8.329302ms)
Sep 15 16:15:50.271: INFO: (14) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 11.521886ms)
Sep 15 16:15:50.275: INFO: (14) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 15.537352ms)
Sep 15 16:15:50.275: INFO: (14) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 15.619214ms)
Sep 15 16:15:50.275: INFO: (14) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 15.690681ms)
Sep 15 16:15:50.275: INFO: (14) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 16.090838ms)
Sep 15 16:15:50.275: INFO: (14) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 15.405621ms)
Sep 15 16:15:50.284: INFO: (15) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 9.331915ms)
Sep 15 16:15:50.284: INFO: (15) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.234358ms)
Sep 15 16:15:50.285: INFO: (15) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 8.413281ms)
Sep 15 16:15:50.285: INFO: (15) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 8.99064ms)
Sep 15 16:15:50.285: INFO: (15) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 9.916058ms)
Sep 15 16:15:50.285: INFO: (15) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 9.537399ms)
Sep 15 16:15:50.285: INFO: (15) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 9.340376ms)
Sep 15 16:15:50.285: INFO: (15) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 9.735997ms)
Sep 15 16:15:50.286: INFO: (15) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 9.898117ms)
Sep 15 16:15:50.286: INFO: (15) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 9.980613ms)
Sep 15 16:15:50.287: INFO: (15) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 11.351088ms)
Sep 15 16:15:50.289: INFO: (15) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 13.836585ms)
Sep 15 16:15:50.289: INFO: (15) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 13.092996ms)
Sep 15 16:15:50.290: INFO: (15) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 14.110471ms)
Sep 15 16:15:50.290: INFO: (15) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 13.848919ms)
Sep 15 16:15:50.290: INFO: (15) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 14.555207ms)
Sep 15 16:15:50.295: INFO: (16) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 4.656459ms)
Sep 15 16:15:50.298: INFO: (16) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 7.411922ms)
Sep 15 16:15:50.298: INFO: (16) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 7.848939ms)
Sep 15 16:15:50.298: INFO: (16) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.113175ms)
Sep 15 16:15:50.298: INFO: (16) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 7.956977ms)
Sep 15 16:15:50.298: INFO: (16) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 6.854948ms)
Sep 15 16:15:50.298: INFO: (16) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 8.126753ms)
Sep 15 16:15:50.298: INFO: (16) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 7.66084ms)
Sep 15 16:15:50.299: INFO: (16) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 8.557011ms)
Sep 15 16:15:50.299: INFO: (16) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 8.184216ms)
Sep 15 16:15:50.301: INFO: (16) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 10.519393ms)
Sep 15 16:15:50.304: INFO: (16) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 13.445017ms)
Sep 15 16:15:50.304: INFO: (16) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 13.820996ms)
Sep 15 16:15:50.304: INFO: (16) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 14.19579ms)
Sep 15 16:15:50.304: INFO: (16) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 13.912506ms)
Sep 15 16:15:50.304: INFO: (16) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 13.735112ms)
Sep 15 16:15:50.309: INFO: (17) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 4.948256ms)
Sep 15 16:15:50.313: INFO: (17) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 7.901041ms)
Sep 15 16:15:50.313: INFO: (17) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.280456ms)
Sep 15 16:15:50.313: INFO: (17) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 8.469348ms)
Sep 15 16:15:50.313: INFO: (17) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 8.676375ms)
Sep 15 16:15:50.314: INFO: (17) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 9.743559ms)
Sep 15 16:15:50.314: INFO: (17) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 9.267177ms)
Sep 15 16:15:50.315: INFO: (17) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 10.076293ms)
Sep 15 16:15:50.316: INFO: (17) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 11.364977ms)
Sep 15 16:15:50.316: INFO: (17) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 11.326983ms)
Sep 15 16:15:50.316: INFO: (17) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 11.338833ms)
Sep 15 16:15:50.319: INFO: (17) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 14.405163ms)
Sep 15 16:15:50.319: INFO: (17) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 14.573441ms)
Sep 15 16:15:50.320: INFO: (17) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 14.805178ms)
Sep 15 16:15:50.326: INFO: (17) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 21.356667ms)
Sep 15 16:15:50.326: INFO: (17) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 21.636448ms)
Sep 15 16:15:50.331: INFO: (18) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 5.166915ms)
Sep 15 16:15:50.335: INFO: (18) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 8.57767ms)
Sep 15 16:15:50.335: INFO: (18) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 8.68087ms)
Sep 15 16:15:50.335: INFO: (18) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 8.768583ms)
Sep 15 16:15:50.335: INFO: (18) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 8.866236ms)
Sep 15 16:15:50.337: INFO: (18) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 10.375423ms)
Sep 15 16:15:50.338: INFO: (18) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.77351ms)
Sep 15 16:15:50.338: INFO: (18) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 10.77177ms)
Sep 15 16:15:50.338: INFO: (18) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 11.221997ms)
Sep 15 16:15:50.338: INFO: (18) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 10.821524ms)
Sep 15 16:15:50.340: INFO: (18) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 13.141901ms)
Sep 15 16:15:50.340: INFO: (18) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 13.526842ms)
Sep 15 16:15:50.342: INFO: (18) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 15.106343ms)
Sep 15 16:15:50.342: INFO: (18) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 15.207001ms)
Sep 15 16:15:50.342: INFO: (18) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 15.285661ms)
Sep 15 16:15:50.343: INFO: (18) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 15.414198ms)
Sep 15 16:15:50.347: INFO: (19) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:162/proxy/: bar (200; 4.643456ms)
Sep 15 16:15:50.350: INFO: (19) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:160/proxy/: foo (200; 7.649626ms)
Sep 15 16:15:50.350: INFO: (19) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:443/proxy/tlsrewritem... (200; 7.101696ms)
Sep 15 16:15:50.354: INFO: (19) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:1080/proxy/rewriteme">test<... (200; 10.432324ms)
Sep 15 16:15:50.354: INFO: (19) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924/proxy/rewriteme">test</a> (200; 10.633115ms)
Sep 15 16:15:50.354: INFO: (19) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:160/proxy/: foo (200; 10.601405ms)
Sep 15 16:15:50.354: INFO: (19) /api/v1/namespaces/proxy-4274/pods/proxy-service-ls8w5-cm924:162/proxy/: bar (200; 10.777532ms)
Sep 15 16:15:50.354: INFO: (19) /api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/: <a href="/api/v1/namespaces/proxy-4274/pods/http:proxy-service-ls8w5-cm924:1080/proxy/rewriteme">... (200; 10.706062ms)
Sep 15 16:15:50.354: INFO: (19) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:462/proxy/: tls qux (200; 10.601465ms)
Sep 15 16:15:50.354: INFO: (19) /api/v1/namespaces/proxy-4274/pods/https:proxy-service-ls8w5-cm924:460/proxy/: tls baz (200; 11.495632ms)
Sep 15 16:15:50.357: INFO: (19) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname1/proxy/: foo (200; 13.288675ms)
Sep 15 16:15:50.357: INFO: (19) /api/v1/namespaces/proxy-4274/services/proxy-service-ls8w5:portname2/proxy/: bar (200; 13.384523ms)
Sep 15 16:15:50.357: INFO: (19) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname2/proxy/: bar (200; 13.393819ms)
Sep 15 16:15:50.357: INFO: (19) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname1/proxy/: tls baz (200; 13.134897ms)
Sep 15 16:15:50.357: INFO: (19) /api/v1/namespaces/proxy-4274/services/https:proxy-service-ls8w5:tlsportname2/proxy/: tls qux (200; 13.300741ms)
Sep 15 16:15:50.357: INFO: (19) /api/v1/namespaces/proxy-4274/services/http:proxy-service-ls8w5:portname1/proxy/: foo (200; 13.195719ms)
STEP: deleting ReplicationController proxy-service-ls8w5 in namespace proxy-4274, will wait for the garbage collector to delete the pods
Sep 15 16:15:50.423: INFO: Deleting ReplicationController proxy-service-ls8w5 took: 11.797975ms
Sep 15 16:15:50.823: INFO: Terminating ReplicationController proxy-service-ls8w5 pods took: 400.257792ms
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:15:53.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4274" for this suite.
Sep 15 16:15:59.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:15:59.199: INFO: namespace proxy-4274 deletion completed in 6.169050617s

• [SLOW TEST:21.405 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:15:59.199: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-48b0fa5c-2afa-45a4-abd1-08398570ffab
STEP: Creating a pod to test consume configMaps
Sep 15 16:15:59.361: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dfa49c81-7f16-444e-b18e-bf700701edc1" in namespace "projected-1477" to be "success or failure"
Sep 15 16:15:59.368: INFO: Pod "pod-projected-configmaps-dfa49c81-7f16-444e-b18e-bf700701edc1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.462725ms
Sep 15 16:16:01.374: INFO: Pod "pod-projected-configmaps-dfa49c81-7f16-444e-b18e-bf700701edc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012078548s
STEP: Saw pod success
Sep 15 16:16:01.374: INFO: Pod "pod-projected-configmaps-dfa49c81-7f16-444e-b18e-bf700701edc1" satisfied condition "success or failure"
Sep 15 16:16:01.378: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-configmaps-dfa49c81-7f16-444e-b18e-bf700701edc1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 16:16:01.412: INFO: Waiting for pod pod-projected-configmaps-dfa49c81-7f16-444e-b18e-bf700701edc1 to disappear
Sep 15 16:16:01.418: INFO: Pod pod-projected-configmaps-dfa49c81-7f16-444e-b18e-bf700701edc1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:16:01.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1477" for this suite.
Sep 15 16:16:07.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:16:07.588: INFO: namespace projected-1477 deletion completed in 6.164010583s

• [SLOW TEST:8.389 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:16:07.588: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 15 16:16:07.749: INFO: Waiting up to 5m0s for pod "pod-84f697e0-d3d1-425d-bd5b-6577cb131849" in namespace "emptydir-5158" to be "success or failure"
Sep 15 16:16:07.755: INFO: Pod "pod-84f697e0-d3d1-425d-bd5b-6577cb131849": Phase="Pending", Reason="", readiness=false. Elapsed: 6.118089ms
Sep 15 16:16:09.760: INFO: Pod "pod-84f697e0-d3d1-425d-bd5b-6577cb131849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010762856s
Sep 15 16:16:11.765: INFO: Pod "pod-84f697e0-d3d1-425d-bd5b-6577cb131849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015807786s
STEP: Saw pod success
Sep 15 16:16:11.765: INFO: Pod "pod-84f697e0-d3d1-425d-bd5b-6577cb131849" satisfied condition "success or failure"
Sep 15 16:16:11.769: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-84f697e0-d3d1-425d-bd5b-6577cb131849 container test-container: <nil>
STEP: delete the pod
Sep 15 16:16:11.799: INFO: Waiting for pod pod-84f697e0-d3d1-425d-bd5b-6577cb131849 to disappear
Sep 15 16:16:11.803: INFO: Pod pod-84f697e0-d3d1-425d-bd5b-6577cb131849 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:16:11.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5158" for this suite.
Sep 15 16:16:17.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:16:17.982: INFO: namespace emptydir-5158 deletion completed in 6.173905137s

• [SLOW TEST:10.394 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:16:17.982: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-3d9f1259-52a4-43cd-bf32-5a418f40018f in namespace container-probe-9408
Sep 15 16:16:20.154: INFO: Started pod busybox-3d9f1259-52a4-43cd-bf32-5a418f40018f in namespace container-probe-9408
STEP: checking the pod's current state and verifying that restartCount is present
Sep 15 16:16:20.158: INFO: Initial restart count of pod busybox-3d9f1259-52a4-43cd-bf32-5a418f40018f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:20:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9408" for this suite.
Sep 15 16:20:26.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:20:27.104: INFO: namespace container-probe-9408 deletion completed in 6.243374498s

• [SLOW TEST:249.122 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:20:27.105: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Sep 15 16:21:07.293: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0915 16:21:07.293507      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:21:07.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5713" for this suite.
Sep 15 16:21:15.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:21:15.468: INFO: namespace gc-5713 deletion completed in 8.169410628s

• [SLOW TEST:48.363 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:21:15.468: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 15 16:21:15.639: INFO: Waiting up to 5m0s for pod "pod-64bcdae2-b7b8-4e04-abbb-f130bc3a1af3" in namespace "emptydir-4514" to be "success or failure"
Sep 15 16:21:15.645: INFO: Pod "pod-64bcdae2-b7b8-4e04-abbb-f130bc3a1af3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.563728ms
Sep 15 16:21:17.651: INFO: Pod "pod-64bcdae2-b7b8-4e04-abbb-f130bc3a1af3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011896433s
STEP: Saw pod success
Sep 15 16:21:17.651: INFO: Pod "pod-64bcdae2-b7b8-4e04-abbb-f130bc3a1af3" satisfied condition "success or failure"
Sep 15 16:21:17.655: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-64bcdae2-b7b8-4e04-abbb-f130bc3a1af3 container test-container: <nil>
STEP: delete the pod
Sep 15 16:21:17.689: INFO: Waiting for pod pod-64bcdae2-b7b8-4e04-abbb-f130bc3a1af3 to disappear
Sep 15 16:21:17.695: INFO: Pod pod-64bcdae2-b7b8-4e04-abbb-f130bc3a1af3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:21:17.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4514" for this suite.
Sep 15 16:21:23.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:21:23.865: INFO: namespace emptydir-4514 deletion completed in 6.16479519s

• [SLOW TEST:8.397 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:21:23.865: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-b29e8560-2141-44bd-b81e-e1fe715a6570
STEP: Creating a pod to test consume secrets
Sep 15 16:21:24.029: INFO: Waiting up to 5m0s for pod "pod-secrets-859b2857-f730-4824-8bd0-bfdb73f06b16" in namespace "secrets-8800" to be "success or failure"
Sep 15 16:21:24.034: INFO: Pod "pod-secrets-859b2857-f730-4824-8bd0-bfdb73f06b16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.931165ms
Sep 15 16:21:26.039: INFO: Pod "pod-secrets-859b2857-f730-4824-8bd0-bfdb73f06b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009955772s
STEP: Saw pod success
Sep 15 16:21:26.039: INFO: Pod "pod-secrets-859b2857-f730-4824-8bd0-bfdb73f06b16" satisfied condition "success or failure"
Sep 15 16:21:26.043: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-secrets-859b2857-f730-4824-8bd0-bfdb73f06b16 container secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:21:26.074: INFO: Waiting for pod pod-secrets-859b2857-f730-4824-8bd0-bfdb73f06b16 to disappear
Sep 15 16:21:26.079: INFO: Pod pod-secrets-859b2857-f730-4824-8bd0-bfdb73f06b16 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:21:26.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8800" for this suite.
Sep 15 16:21:32.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:21:32.256: INFO: namespace secrets-8800 deletion completed in 6.172221551s

• [SLOW TEST:8.391 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:21:32.257: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:21:32.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72bb4400-74ce-40fe-ab75-11975e3a5d9e" in namespace "downward-api-2889" to be "success or failure"
Sep 15 16:21:32.421: INFO: Pod "downwardapi-volume-72bb4400-74ce-40fe-ab75-11975e3a5d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.573723ms
Sep 15 16:21:34.426: INFO: Pod "downwardapi-volume-72bb4400-74ce-40fe-ab75-11975e3a5d9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011710851s
STEP: Saw pod success
Sep 15 16:21:34.426: INFO: Pod "downwardapi-volume-72bb4400-74ce-40fe-ab75-11975e3a5d9e" satisfied condition "success or failure"
Sep 15 16:21:34.431: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-72bb4400-74ce-40fe-ab75-11975e3a5d9e container client-container: <nil>
STEP: delete the pod
Sep 15 16:21:34.462: INFO: Waiting for pod downwardapi-volume-72bb4400-74ce-40fe-ab75-11975e3a5d9e to disappear
Sep 15 16:21:34.467: INFO: Pod downwardapi-volume-72bb4400-74ce-40fe-ab75-11975e3a5d9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:21:34.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2889" for this suite.
Sep 15 16:21:40.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:21:40.635: INFO: namespace downward-api-2889 deletion completed in 6.16216074s

• [SLOW TEST:8.378 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:21:40.635: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3302
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 15 16:21:40.794: INFO: Waiting up to 5m0s for pod "pod-b2a16604-3295-4f49-9c97-e8f559b839ac" in namespace "emptydir-3302" to be "success or failure"
Sep 15 16:21:40.801: INFO: Pod "pod-b2a16604-3295-4f49-9c97-e8f559b839ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.802461ms
Sep 15 16:21:42.806: INFO: Pod "pod-b2a16604-3295-4f49-9c97-e8f559b839ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01145438s
STEP: Saw pod success
Sep 15 16:21:42.806: INFO: Pod "pod-b2a16604-3295-4f49-9c97-e8f559b839ac" satisfied condition "success or failure"
Sep 15 16:21:42.810: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-b2a16604-3295-4f49-9c97-e8f559b839ac container test-container: <nil>
STEP: delete the pod
Sep 15 16:21:42.841: INFO: Waiting for pod pod-b2a16604-3295-4f49-9c97-e8f559b839ac to disappear
Sep 15 16:21:42.845: INFO: Pod pod-b2a16604-3295-4f49-9c97-e8f559b839ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:21:42.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3302" for this suite.
Sep 15 16:21:48.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:21:49.009: INFO: namespace emptydir-3302 deletion completed in 6.158776681s

• [SLOW TEST:8.374 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:21:49.009: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5096
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-a0699706-2b51-4b19-878a-dab929c9843e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:21:51.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5096" for this suite.
Sep 15 16:22:13.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:22:13.385: INFO: namespace configmap-5096 deletion completed in 22.160591841s

• [SLOW TEST:24.375 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:22:13.385: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-3ed22961-2b39-46f4-bf17-b1a80baf990f
STEP: Creating a pod to test consume secrets
Sep 15 16:22:13.549: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-52753b19-f3d6-47fc-878d-da629b00bd93" in namespace "projected-2225" to be "success or failure"
Sep 15 16:22:13.554: INFO: Pod "pod-projected-secrets-52753b19-f3d6-47fc-878d-da629b00bd93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.142506ms
Sep 15 16:22:15.559: INFO: Pod "pod-projected-secrets-52753b19-f3d6-47fc-878d-da629b00bd93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009266288s
STEP: Saw pod success
Sep 15 16:22:15.559: INFO: Pod "pod-projected-secrets-52753b19-f3d6-47fc-878d-da629b00bd93" satisfied condition "success or failure"
Sep 15 16:22:15.563: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-secrets-52753b19-f3d6-47fc-878d-da629b00bd93 container secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:22:15.593: INFO: Waiting for pod pod-projected-secrets-52753b19-f3d6-47fc-878d-da629b00bd93 to disappear
Sep 15 16:22:15.629: INFO: Pod pod-projected-secrets-52753b19-f3d6-47fc-878d-da629b00bd93 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:22:15.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2225" for this suite.
Sep 15 16:22:21.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:22:21.819: INFO: namespace projected-2225 deletion completed in 6.181126768s

• [SLOW TEST:8.434 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:22:21.819: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:22:21.996: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 15 16:22:22.010: INFO: Number of nodes with available pods: 0
Sep 15 16:22:22.010: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 15 16:22:22.035: INFO: Number of nodes with available pods: 0
Sep 15 16:22:22.035: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 16:22:23.040: INFO: Number of nodes with available pods: 1
Sep 15 16:22:23.040: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 15 16:22:23.062: INFO: Number of nodes with available pods: 1
Sep 15 16:22:23.062: INFO: Number of running nodes: 0, number of available pods: 1
Sep 15 16:22:24.067: INFO: Number of nodes with available pods: 0
Sep 15 16:22:24.067: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 15 16:22:24.080: INFO: Number of nodes with available pods: 0
Sep 15 16:22:24.080: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 16:22:25.085: INFO: Number of nodes with available pods: 0
Sep 15 16:22:25.085: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 16:22:26.085: INFO: Number of nodes with available pods: 0
Sep 15 16:22:26.085: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 16:22:27.085: INFO: Number of nodes with available pods: 0
Sep 15 16:22:27.085: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 16:22:28.085: INFO: Number of nodes with available pods: 1
Sep 15 16:22:28.085: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4585, will wait for the garbage collector to delete the pods
Sep 15 16:22:28.169: INFO: Deleting DaemonSet.extensions daemon-set took: 11.972145ms
Sep 15 16:22:28.569: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.265971ms
Sep 15 16:22:42.875: INFO: Number of nodes with available pods: 0
Sep 15 16:22:42.875: INFO: Number of running nodes: 0, number of available pods: 0
Sep 15 16:22:42.879: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4585/daemonsets","resourceVersion":"16009"},"items":null}

Sep 15 16:22:42.883: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4585/pods","resourceVersion":"16009"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:22:42.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4585" for this suite.
Sep 15 16:22:48.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:22:49.084: INFO: namespace daemonsets-4585 deletion completed in 6.164781841s

• [SLOW TEST:27.265 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:22:49.084: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 15 16:22:49.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2765'
Sep 15 16:22:49.364: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 15 16:22:49.364: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep 15 16:22:49.373: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-267n4]
Sep 15 16:22:49.373: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-267n4" in namespace "kubectl-2765" to be "running and ready"
Sep 15 16:22:49.380: INFO: Pod "e2e-test-nginx-rc-267n4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.551866ms
Sep 15 16:22:51.385: INFO: Pod "e2e-test-nginx-rc-267n4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011594461s
Sep 15 16:22:51.385: INFO: Pod "e2e-test-nginx-rc-267n4" satisfied condition "running and ready"
Sep 15 16:22:51.385: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-267n4]
Sep 15 16:22:51.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 logs rc/e2e-test-nginx-rc --namespace=kubectl-2765'
Sep 15 16:22:51.479: INFO: stderr: ""
Sep 15 16:22:51.479: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Sep 15 16:22:51.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete rc e2e-test-nginx-rc --namespace=kubectl-2765'
Sep 15 16:22:51.561: INFO: stderr: ""
Sep 15 16:22:51.561: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:22:51.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2765" for this suite.
Sep 15 16:23:13.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:23:13.737: INFO: namespace kubectl-2765 deletion completed in 22.170596824s

• [SLOW TEST:24.653 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:23:13.737: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6880
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-5ea85191-78c1-413a-aae6-74bc76c3dbb6 in namespace container-probe-6880
Sep 15 16:23:17.917: INFO: Started pod liveness-5ea85191-78c1-413a-aae6-74bc76c3dbb6 in namespace container-probe-6880
STEP: checking the pod's current state and verifying that restartCount is present
Sep 15 16:23:17.921: INFO: Initial restart count of pod liveness-5ea85191-78c1-413a-aae6-74bc76c3dbb6 is 0
Sep 15 16:23:27.953: INFO: Restart count of pod container-probe-6880/liveness-5ea85191-78c1-413a-aae6-74bc76c3dbb6 is now 1 (10.031988564s elapsed)
Sep 15 16:23:48.004: INFO: Restart count of pod container-probe-6880/liveness-5ea85191-78c1-413a-aae6-74bc76c3dbb6 is now 2 (30.08341805s elapsed)
Sep 15 16:24:10.060: INFO: Restart count of pod container-probe-6880/liveness-5ea85191-78c1-413a-aae6-74bc76c3dbb6 is now 3 (52.139309338s elapsed)
Sep 15 16:24:30.111: INFO: Restart count of pod container-probe-6880/liveness-5ea85191-78c1-413a-aae6-74bc76c3dbb6 is now 4 (1m12.190677982s elapsed)
Sep 15 16:25:40.293: INFO: Restart count of pod container-probe-6880/liveness-5ea85191-78c1-413a-aae6-74bc76c3dbb6 is now 5 (2m22.372163041s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:25:40.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6880" for this suite.
Sep 15 16:25:46.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:25:46.557: INFO: namespace container-probe-6880 deletion completed in 6.242238623s

• [SLOW TEST:152.820 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:25:46.558: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:25:46.709: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 15 16:25:48.851: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:25:48.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5468" for this suite.
Sep 15 16:25:54.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:25:55.038: INFO: namespace replication-controller-5468 deletion completed in 6.170834391s

• [SLOW TEST:8.481 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:25:55.038: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 15 16:25:55.194: INFO: Waiting up to 5m0s for pod "pod-91cf6506-63fb-4860-9d62-90e6dfabbc55" in namespace "emptydir-1911" to be "success or failure"
Sep 15 16:25:55.200: INFO: Pod "pod-91cf6506-63fb-4860-9d62-90e6dfabbc55": Phase="Pending", Reason="", readiness=false. Elapsed: 5.904961ms
Sep 15 16:25:57.205: INFO: Pod "pod-91cf6506-63fb-4860-9d62-90e6dfabbc55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011379175s
STEP: Saw pod success
Sep 15 16:25:57.205: INFO: Pod "pod-91cf6506-63fb-4860-9d62-90e6dfabbc55" satisfied condition "success or failure"
Sep 15 16:25:57.210: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-91cf6506-63fb-4860-9d62-90e6dfabbc55 container test-container: <nil>
STEP: delete the pod
Sep 15 16:25:57.240: INFO: Waiting for pod pod-91cf6506-63fb-4860-9d62-90e6dfabbc55 to disappear
Sep 15 16:25:57.245: INFO: Pod pod-91cf6506-63fb-4860-9d62-90e6dfabbc55 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:25:57.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1911" for this suite.
Sep 15 16:26:03.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:26:03.432: INFO: namespace emptydir-1911 deletion completed in 6.180702083s

• [SLOW TEST:8.393 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:26:03.432: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep 15 16:26:03.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-1904'
Sep 15 16:26:04.139: INFO: stderr: ""
Sep 15 16:26:04.139: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 15 16:26:04.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1904'
Sep 15 16:26:04.220: INFO: stderr: ""
Sep 15 16:26:04.220: INFO: stdout: "update-demo-nautilus-mr5qg update-demo-nautilus-v4n8j "
Sep 15 16:26:04.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-mr5qg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:04.286: INFO: stderr: ""
Sep 15 16:26:04.286: INFO: stdout: ""
Sep 15 16:26:04.286: INFO: update-demo-nautilus-mr5qg is created but not running
Sep 15 16:26:09.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1904'
Sep 15 16:26:09.362: INFO: stderr: ""
Sep 15 16:26:09.362: INFO: stdout: "update-demo-nautilus-mr5qg update-demo-nautilus-v4n8j "
Sep 15 16:26:09.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-mr5qg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:09.437: INFO: stderr: ""
Sep 15 16:26:09.437: INFO: stdout: "true"
Sep 15 16:26:09.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-mr5qg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:09.506: INFO: stderr: ""
Sep 15 16:26:09.506: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 15 16:26:09.506: INFO: validating pod update-demo-nautilus-mr5qg
Sep 15 16:26:09.514: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 15 16:26:09.514: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 15 16:26:09.514: INFO: update-demo-nautilus-mr5qg is verified up and running
Sep 15 16:26:09.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-v4n8j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:09.587: INFO: stderr: ""
Sep 15 16:26:09.587: INFO: stdout: "true"
Sep 15 16:26:09.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-v4n8j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:09.659: INFO: stderr: ""
Sep 15 16:26:09.659: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 15 16:26:09.659: INFO: validating pod update-demo-nautilus-v4n8j
Sep 15 16:26:09.667: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 15 16:26:09.667: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 15 16:26:09.667: INFO: update-demo-nautilus-v4n8j is verified up and running
STEP: scaling down the replication controller
Sep 15 16:26:09.669: INFO: scanned /root for discovery docs: <nil>
Sep 15 16:26:09.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1904'
Sep 15 16:26:10.776: INFO: stderr: ""
Sep 15 16:26:10.776: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 15 16:26:10.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1904'
Sep 15 16:26:10.849: INFO: stderr: ""
Sep 15 16:26:10.849: INFO: stdout: "update-demo-nautilus-mr5qg update-demo-nautilus-v4n8j "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 15 16:26:15.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1904'
Sep 15 16:26:15.927: INFO: stderr: ""
Sep 15 16:26:15.927: INFO: stdout: "update-demo-nautilus-v4n8j "
Sep 15 16:26:15.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-v4n8j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:16.001: INFO: stderr: ""
Sep 15 16:26:16.001: INFO: stdout: "true"
Sep 15 16:26:16.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-v4n8j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:16.071: INFO: stderr: ""
Sep 15 16:26:16.071: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 15 16:26:16.071: INFO: validating pod update-demo-nautilus-v4n8j
Sep 15 16:26:16.078: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 15 16:26:16.078: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 15 16:26:16.078: INFO: update-demo-nautilus-v4n8j is verified up and running
STEP: scaling up the replication controller
Sep 15 16:26:16.079: INFO: scanned /root for discovery docs: <nil>
Sep 15 16:26:16.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1904'
Sep 15 16:26:17.179: INFO: stderr: ""
Sep 15 16:26:17.179: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 15 16:26:17.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1904'
Sep 15 16:26:17.258: INFO: stderr: ""
Sep 15 16:26:17.258: INFO: stdout: "update-demo-nautilus-v4n8j update-demo-nautilus-w9wv5 "
Sep 15 16:26:17.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-v4n8j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:17.328: INFO: stderr: ""
Sep 15 16:26:17.329: INFO: stdout: "true"
Sep 15 16:26:17.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-v4n8j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:17.395: INFO: stderr: ""
Sep 15 16:26:17.395: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 15 16:26:17.395: INFO: validating pod update-demo-nautilus-v4n8j
Sep 15 16:26:17.401: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 15 16:26:17.401: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 15 16:26:17.401: INFO: update-demo-nautilus-v4n8j is verified up and running
Sep 15 16:26:17.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-w9wv5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:17.475: INFO: stderr: ""
Sep 15 16:26:17.475: INFO: stdout: "true"
Sep 15 16:26:17.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-w9wv5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Sep 15 16:26:17.545: INFO: stderr: ""
Sep 15 16:26:17.545: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 15 16:26:17.545: INFO: validating pod update-demo-nautilus-w9wv5
Sep 15 16:26:17.553: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 15 16:26:17.553: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 15 16:26:17.553: INFO: update-demo-nautilus-w9wv5 is verified up and running
STEP: using delete to clean up resources
Sep 15 16:26:17.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-1904'
Sep 15 16:26:17.631: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 16:26:17.631: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 15 16:26:17.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1904'
Sep 15 16:26:17.711: INFO: stderr: "No resources found.\n"
Sep 15 16:26:17.711: INFO: stdout: ""
Sep 15 16:26:17.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -l name=update-demo --namespace=kubectl-1904 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 15 16:26:17.788: INFO: stderr: ""
Sep 15 16:26:17.788: INFO: stdout: "update-demo-nautilus-v4n8j\nupdate-demo-nautilus-w9wv5\n"
Sep 15 16:26:18.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1904'
Sep 15 16:26:18.380: INFO: stderr: "No resources found.\n"
Sep 15 16:26:18.380: INFO: stdout: ""
Sep 15 16:26:18.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -l name=update-demo --namespace=kubectl-1904 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 15 16:26:18.458: INFO: stderr: ""
Sep 15 16:26:18.458: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:26:18.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1904" for this suite.
Sep 15 16:26:24.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:26:24.641: INFO: namespace kubectl-1904 deletion completed in 6.17681788s

• [SLOW TEST:21.209 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:26:24.642: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-70903c90-9590-4451-868c-4d7f76396f65 in namespace container-probe-7684
Sep 15 16:26:26.808: INFO: Started pod busybox-70903c90-9590-4451-868c-4d7f76396f65 in namespace container-probe-7684
STEP: checking the pod's current state and verifying that restartCount is present
Sep 15 16:26:26.812: INFO: Initial restart count of pod busybox-70903c90-9590-4451-868c-4d7f76396f65 is 0
Sep 15 16:27:16.947: INFO: Restart count of pod container-probe-7684/busybox-70903c90-9590-4451-868c-4d7f76396f65 is now 1 (50.134228309s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:27:16.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7684" for this suite.
Sep 15 16:27:22.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:27:23.136: INFO: namespace container-probe-7684 deletion completed in 6.166131503s

• [SLOW TEST:58.495 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:27:23.137: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1724.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1724.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1724.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1724.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1724.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1724.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 15 16:27:27.354: INFO: DNS probes using dns-1724/dns-test-b08fce20-8e30-4266-8af1-2c0b043de34e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:27:27.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1724" for this suite.
Sep 15 16:27:33.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:27:33.554: INFO: namespace dns-1724 deletion completed in 6.175760425s

• [SLOW TEST:10.417 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:27:33.554: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2674
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-bhgm
STEP: Creating a pod to test atomic-volume-subpath
Sep 15 16:27:33.722: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bhgm" in namespace "subpath-2674" to be "success or failure"
Sep 15 16:27:33.726: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775473ms
Sep 15 16:27:35.732: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 2.009897928s
Sep 15 16:27:37.737: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 4.014995717s
Sep 15 16:27:39.742: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 6.019884337s
Sep 15 16:27:41.748: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 8.025954582s
Sep 15 16:27:43.753: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 10.030931496s
Sep 15 16:27:45.758: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 12.035862761s
Sep 15 16:27:47.763: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 14.040987897s
Sep 15 16:27:49.768: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 16.04642354s
Sep 15 16:27:51.773: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 18.051715555s
Sep 15 16:27:53.779: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Running", Reason="", readiness=true. Elapsed: 20.057212046s
Sep 15 16:27:55.785: INFO: Pod "pod-subpath-test-secret-bhgm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.063026969s
STEP: Saw pod success
Sep 15 16:27:55.785: INFO: Pod "pod-subpath-test-secret-bhgm" satisfied condition "success or failure"
Sep 15 16:27:55.789: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-subpath-test-secret-bhgm container test-container-subpath-secret-bhgm: <nil>
STEP: delete the pod
Sep 15 16:27:55.823: INFO: Waiting for pod pod-subpath-test-secret-bhgm to disappear
Sep 15 16:27:55.827: INFO: Pod pod-subpath-test-secret-bhgm no longer exists
STEP: Deleting pod pod-subpath-test-secret-bhgm
Sep 15 16:27:55.827: INFO: Deleting pod "pod-subpath-test-secret-bhgm" in namespace "subpath-2674"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:27:55.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2674" for this suite.
Sep 15 16:28:01.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:28:02.020: INFO: namespace subpath-2674 deletion completed in 6.18164146s

• [SLOW TEST:28.466 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:28:02.020: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:28:02.181: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f567e91f-8b52-4804-907a-68e980b1eab8" in namespace "projected-4409" to be "success or failure"
Sep 15 16:28:02.188: INFO: Pod "downwardapi-volume-f567e91f-8b52-4804-907a-68e980b1eab8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.440031ms
Sep 15 16:28:04.193: INFO: Pod "downwardapi-volume-f567e91f-8b52-4804-907a-68e980b1eab8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011026588s
STEP: Saw pod success
Sep 15 16:28:04.193: INFO: Pod "downwardapi-volume-f567e91f-8b52-4804-907a-68e980b1eab8" satisfied condition "success or failure"
Sep 15 16:28:04.197: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-f567e91f-8b52-4804-907a-68e980b1eab8 container client-container: <nil>
STEP: delete the pod
Sep 15 16:28:04.226: INFO: Waiting for pod downwardapi-volume-f567e91f-8b52-4804-907a-68e980b1eab8 to disappear
Sep 15 16:28:04.232: INFO: Pod downwardapi-volume-f567e91f-8b52-4804-907a-68e980b1eab8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:28:04.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4409" for this suite.
Sep 15 16:28:10.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:28:10.402: INFO: namespace projected-4409 deletion completed in 6.164073179s

• [SLOW TEST:8.382 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:28:10.403: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 15 16:28:13.091: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a79fec2d-8044-471f-9e07-101ceba81916"
Sep 15 16:28:13.091: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a79fec2d-8044-471f-9e07-101ceba81916" in namespace "pods-3823" to be "terminated due to deadline exceeded"
Sep 15 16:28:13.095: INFO: Pod "pod-update-activedeadlineseconds-a79fec2d-8044-471f-9e07-101ceba81916": Phase="Running", Reason="", readiness=true. Elapsed: 3.969565ms
Sep 15 16:28:15.100: INFO: Pod "pod-update-activedeadlineseconds-a79fec2d-8044-471f-9e07-101ceba81916": Phase="Running", Reason="", readiness=true. Elapsed: 2.009212571s
Sep 15 16:28:17.105: INFO: Pod "pod-update-activedeadlineseconds-a79fec2d-8044-471f-9e07-101ceba81916": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.0141257s
Sep 15 16:28:17.105: INFO: Pod "pod-update-activedeadlineseconds-a79fec2d-8044-471f-9e07-101ceba81916" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:28:17.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3823" for this suite.
Sep 15 16:28:23.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:28:23.276: INFO: namespace pods-3823 deletion completed in 6.165391829s

• [SLOW TEST:12.873 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:28:23.276: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2422
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-2422
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2422
Sep 15 16:28:23.441: INFO: Found 0 stateful pods, waiting for 1
Sep 15 16:28:33.446: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 15 16:28:33.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-2422 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:28:33.656: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:28:33.656: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:28:33.656: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 15 16:28:33.661: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 15 16:28:43.667: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 15 16:28:43.667: INFO: Waiting for statefulset status.replicas updated to 0
Sep 15 16:28:43.690: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:28:43.690: INFO: ss-0  ip-172-16-34-210.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:23 +0000 UTC  }]
Sep 15 16:28:43.691: INFO: ss-1                                 Pending         []
Sep 15 16:28:43.691: INFO: 
Sep 15 16:28:43.691: INFO: StatefulSet ss has not reached scale 3, at 2
Sep 15 16:28:44.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989942389s
Sep 15 16:28:45.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984988939s
Sep 15 16:28:46.707: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979535575s
Sep 15 16:28:47.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974094102s
Sep 15 16:28:48.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968503248s
Sep 15 16:28:49.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962654889s
Sep 15 16:28:50.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957031248s
Sep 15 16:28:51.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950804598s
Sep 15 16:28:52.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.386581ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2422
Sep 15 16:28:53.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-2422 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 15 16:28:53.932: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 15 16:28:53.932: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 15 16:28:53.932: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 15 16:28:53.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-2422 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 15 16:28:54.108: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 15 16:28:54.108: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 15 16:28:54.108: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 15 16:28:54.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-2422 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 15 16:28:54.309: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 15 16:28:54.309: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 15 16:28:54.309: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 15 16:28:54.315: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep 15 16:29:04.321: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:29:04.321: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:29:04.321: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 15 16:29:04.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-2422 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:29:04.515: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:29:04.515: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:29:04.515: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 15 16:29:04.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-2422 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:29:04.693: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:29:04.693: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:29:04.693: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 15 16:29:04.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-2422 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:29:04.909: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:29:04.909: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:29:04.909: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 15 16:29:04.909: INFO: Waiting for statefulset status.replicas updated to 0
Sep 15 16:29:04.913: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 15 16:29:14.924: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 15 16:29:14.924: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 15 16:29:14.924: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 15 16:29:14.940: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:29:14.940: INFO: ss-0  ip-172-16-34-210.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:23 +0000 UTC  }]
Sep 15 16:29:14.940: INFO: ss-1  ip-172-16-49-162.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:14.940: INFO: ss-2  ip-172-16-74-13.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:14.940: INFO: 
Sep 15 16:29:14.940: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 15 16:29:15.945: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:29:15.945: INFO: ss-0  ip-172-16-34-210.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:23 +0000 UTC  }]
Sep 15 16:29:15.945: INFO: ss-1  ip-172-16-49-162.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:15.945: INFO: ss-2  ip-172-16-74-13.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:15.945: INFO: 
Sep 15 16:29:15.945: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 15 16:29:16.951: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:29:16.951: INFO: ss-0  ip-172-16-34-210.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:23 +0000 UTC  }]
Sep 15 16:29:16.951: INFO: ss-1  ip-172-16-49-162.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:16.951: INFO: ss-2  ip-172-16-74-13.ec2.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:16.951: INFO: 
Sep 15 16:29:16.951: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 15 16:29:17.957: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:29:17.957: INFO: ss-1  ip-172-16-49-162.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:17.957: INFO: 
Sep 15 16:29:17.957: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 15 16:29:18.962: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:29:18.962: INFO: ss-1  ip-172-16-49-162.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:18.962: INFO: 
Sep 15 16:29:18.962: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 15 16:29:19.968: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:29:19.968: INFO: ss-1  ip-172-16-49-162.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:19.968: INFO: 
Sep 15 16:29:19.968: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 15 16:29:20.973: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:29:20.973: INFO: ss-1  ip-172-16-49-162.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:20.974: INFO: 
Sep 15 16:29:20.974: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 15 16:29:21.979: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:29:21.979: INFO: ss-1  ip-172-16-49-162.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:21.979: INFO: 
Sep 15 16:29:21.979: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 15 16:29:22.984: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 15 16:29:22.984: INFO: ss-1  ip-172-16-49-162.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:29:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:28:43 +0000 UTC  }]
Sep 15 16:29:22.984: INFO: 
Sep 15 16:29:22.984: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 15 16:29:23.988: INFO: Verifying statefulset ss doesn't scale past 0 for another 950.684785ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2422
Sep 15 16:29:24.993: INFO: Scaling statefulset ss to 0
Sep 15 16:29:25.007: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 15 16:29:25.011: INFO: Deleting all statefulset in ns statefulset-2422
Sep 15 16:29:25.015: INFO: Scaling statefulset ss to 0
Sep 15 16:29:25.028: INFO: Waiting for statefulset status.replicas updated to 0
Sep 15 16:29:25.032: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:29:25.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2422" for this suite.
Sep 15 16:29:31.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:29:31.317: INFO: namespace statefulset-2422 deletion completed in 6.258853256s

• [SLOW TEST:68.041 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:29:31.317: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-556
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep 15 16:29:34.047: INFO: Successfully updated pod "labelsupdateabcbc9bb-cdd8-4063-89ac-f5b22f9e3823"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:29:38.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-556" for this suite.
Sep 15 16:30:00.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:30:00.275: INFO: namespace downward-api-556 deletion completed in 22.181490527s

• [SLOW TEST:28.958 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:30:00.275: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 15 16:30:00.433: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 15 16:30:05.438: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:30:06.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7008" for this suite.
Sep 15 16:30:12.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:30:12.634: INFO: namespace replication-controller-7008 deletion completed in 6.168782689s

• [SLOW TEST:12.360 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:30:12.635: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:30:12.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2510" for this suite.
Sep 15 16:30:34.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:30:34.973: INFO: namespace pods-2510 deletion completed in 22.168095149s

• [SLOW TEST:22.339 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:30:34.974: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2830
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-413db547-4bb3-4faf-89a2-598456148771
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-413db547-4bb3-4faf-89a2-598456148771
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:30:39.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2830" for this suite.
Sep 15 16:31:01.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:31:01.376: INFO: namespace configmap-2830 deletion completed in 22.171025288s

• [SLOW TEST:26.403 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:31:01.377: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-7e53aa6b-e440-4b8e-bc57-edfc123a1af8
STEP: Creating a pod to test consume configMaps
Sep 15 16:31:01.542: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4e3cc0e-4454-4d35-86dd-150b73bd566e" in namespace "configmap-1608" to be "success or failure"
Sep 15 16:31:01.550: INFO: Pod "pod-configmaps-d4e3cc0e-4454-4d35-86dd-150b73bd566e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.198317ms
Sep 15 16:31:03.556: INFO: Pod "pod-configmaps-d4e3cc0e-4454-4d35-86dd-150b73bd566e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01473962s
STEP: Saw pod success
Sep 15 16:31:03.557: INFO: Pod "pod-configmaps-d4e3cc0e-4454-4d35-86dd-150b73bd566e" satisfied condition "success or failure"
Sep 15 16:31:03.561: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-d4e3cc0e-4454-4d35-86dd-150b73bd566e container configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 16:31:03.597: INFO: Waiting for pod pod-configmaps-d4e3cc0e-4454-4d35-86dd-150b73bd566e to disappear
Sep 15 16:31:03.601: INFO: Pod pod-configmaps-d4e3cc0e-4454-4d35-86dd-150b73bd566e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:31:03.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1608" for this suite.
Sep 15 16:31:09.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:31:09.776: INFO: namespace configmap-1608 deletion completed in 6.168064586s

• [SLOW TEST:8.399 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:31:09.776: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7983
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 15 16:31:12.967: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:31:13.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7983" for this suite.
Sep 15 16:31:36.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:31:36.166: INFO: namespace replicaset-7983 deletion completed in 22.167411721s

• [SLOW TEST:26.390 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:31:36.166: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 15 16:31:36.323: INFO: Waiting up to 5m0s for pod "pod-5b37a371-10a6-47d6-9d97-8caabc9fe659" in namespace "emptydir-7005" to be "success or failure"
Sep 15 16:31:36.329: INFO: Pod "pod-5b37a371-10a6-47d6-9d97-8caabc9fe659": Phase="Pending", Reason="", readiness=false. Elapsed: 5.998227ms
Sep 15 16:31:38.333: INFO: Pod "pod-5b37a371-10a6-47d6-9d97-8caabc9fe659": Phase="Running", Reason="", readiness=true. Elapsed: 2.010545727s
Sep 15 16:31:40.338: INFO: Pod "pod-5b37a371-10a6-47d6-9d97-8caabc9fe659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01552747s
STEP: Saw pod success
Sep 15 16:31:40.338: INFO: Pod "pod-5b37a371-10a6-47d6-9d97-8caabc9fe659" satisfied condition "success or failure"
Sep 15 16:31:40.342: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-5b37a371-10a6-47d6-9d97-8caabc9fe659 container test-container: <nil>
STEP: delete the pod
Sep 15 16:31:40.375: INFO: Waiting for pod pod-5b37a371-10a6-47d6-9d97-8caabc9fe659 to disappear
Sep 15 16:31:40.380: INFO: Pod pod-5b37a371-10a6-47d6-9d97-8caabc9fe659 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:31:40.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7005" for this suite.
Sep 15 16:31:46.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:31:46.559: INFO: namespace emptydir-7005 deletion completed in 6.17330545s

• [SLOW TEST:10.393 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:31:46.560: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 15 16:31:46.717: INFO: Waiting up to 5m0s for pod "pod-4101c5f7-1484-490f-a2ce-e82414417295" in namespace "emptydir-2315" to be "success or failure"
Sep 15 16:31:46.722: INFO: Pod "pod-4101c5f7-1484-490f-a2ce-e82414417295": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304822ms
Sep 15 16:31:48.729: INFO: Pod "pod-4101c5f7-1484-490f-a2ce-e82414417295": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012393102s
STEP: Saw pod success
Sep 15 16:31:48.729: INFO: Pod "pod-4101c5f7-1484-490f-a2ce-e82414417295" satisfied condition "success or failure"
Sep 15 16:31:48.733: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-4101c5f7-1484-490f-a2ce-e82414417295 container test-container: <nil>
STEP: delete the pod
Sep 15 16:31:48.764: INFO: Waiting for pod pod-4101c5f7-1484-490f-a2ce-e82414417295 to disappear
Sep 15 16:31:48.768: INFO: Pod pod-4101c5f7-1484-490f-a2ce-e82414417295 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:31:48.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2315" for this suite.
Sep 15 16:31:54.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:31:54.939: INFO: namespace emptydir-2315 deletion completed in 6.165746619s

• [SLOW TEST:8.380 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:31:54.940: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Sep 15 16:31:55.087: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep 15 16:31:55.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-804'
Sep 15 16:31:55.333: INFO: stderr: ""
Sep 15 16:31:55.333: INFO: stdout: "service/redis-slave created\n"
Sep 15 16:31:55.333: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep 15 16:31:55.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-804'
Sep 15 16:31:55.572: INFO: stderr: ""
Sep 15 16:31:55.572: INFO: stdout: "service/redis-master created\n"
Sep 15 16:31:55.572: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 15 16:31:55.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-804'
Sep 15 16:31:55.808: INFO: stderr: ""
Sep 15 16:31:55.808: INFO: stdout: "service/frontend created\n"
Sep 15 16:31:55.808: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep 15 16:31:55.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-804'
Sep 15 16:31:56.031: INFO: stderr: ""
Sep 15 16:31:56.031: INFO: stdout: "deployment.apps/frontend created\n"
Sep 15 16:31:56.031: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 15 16:31:56.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-804'
Sep 15 16:31:56.276: INFO: stderr: ""
Sep 15 16:31:56.276: INFO: stdout: "deployment.apps/redis-master created\n"
Sep 15 16:31:56.276: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep 15 16:31:56.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-804'
Sep 15 16:31:56.526: INFO: stderr: ""
Sep 15 16:31:56.526: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep 15 16:31:56.526: INFO: Waiting for all frontend pods to be Running.
Sep 15 16:32:11.577: INFO: Waiting for frontend to serve content.
Sep 15 16:32:16.601: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Sep 15 16:32:21.628: INFO: Trying to add a new entry to the guestbook.
Sep 15 16:32:21.651: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 15 16:32:21.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-804'
Sep 15 16:32:21.774: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 16:32:21.774: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 15 16:32:21.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-804'
Sep 15 16:32:21.890: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 16:32:21.890: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 15 16:32:21.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-804'
Sep 15 16:32:22.015: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 16:32:22.015: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 15 16:32:22.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-804'
Sep 15 16:32:22.118: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 16:32:22.118: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 15 16:32:22.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-804'
Sep 15 16:32:22.201: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 16:32:22.201: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 15 16:32:22.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-804'
Sep 15 16:32:22.291: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 16:32:22.291: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:32:22.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-804" for this suite.
Sep 15 16:33:06.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:33:06.458: INFO: namespace kubectl-804 deletion completed in 44.161587765s

• [SLOW TEST:71.518 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:33:06.459: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep 15 16:33:06.607: INFO: namespace kubectl-2127
Sep 15 16:33:06.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-2127'
Sep 15 16:33:06.866: INFO: stderr: ""
Sep 15 16:33:06.866: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 15 16:33:07.871: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 16:33:07.871: INFO: Found 0 / 1
Sep 15 16:33:08.871: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 16:33:08.871: INFO: Found 1 / 1
Sep 15 16:33:08.871: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 15 16:33:08.877: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 16:33:08.877: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 15 16:33:08.877: INFO: wait on redis-master startup in kubectl-2127 
Sep 15 16:33:08.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 logs redis-master-27gpx redis-master --namespace=kubectl-2127'
Sep 15 16:33:08.998: INFO: stderr: ""
Sep 15 16:33:08.998: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 15 Sep 16:33:07.735 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 15 Sep 16:33:07.735 # Server started, Redis version 3.2.12\n1:M 15 Sep 16:33:07.735 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 15 Sep 16:33:07.735 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep 15 16:33:08.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2127'
Sep 15 16:33:09.098: INFO: stderr: ""
Sep 15 16:33:09.098: INFO: stdout: "service/rm2 exposed\n"
Sep 15 16:33:09.102: INFO: Service rm2 in namespace kubectl-2127 found.
STEP: exposing service
Sep 15 16:33:11.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2127'
Sep 15 16:33:11.203: INFO: stderr: ""
Sep 15 16:33:11.203: INFO: stdout: "service/rm3 exposed\n"
Sep 15 16:33:11.208: INFO: Service rm3 in namespace kubectl-2127 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:33:13.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2127" for this suite.
Sep 15 16:33:35.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:33:35.397: INFO: namespace kubectl-2127 deletion completed in 22.173697664s

• [SLOW TEST:28.939 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:33:35.398: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4018
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c2343517-e1fd-4c60-870c-c7407dde2149
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-c2343517-e1fd-4c60-870c-c7407dde2149
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:33:39.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4018" for this suite.
Sep 15 16:34:01.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:34:01.834: INFO: namespace projected-4018 deletion completed in 22.183239546s

• [SLOW TEST:26.436 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:34:01.834: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3825
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep 15 16:34:02.007: INFO: Found 0 stateful pods, waiting for 3
Sep 15 16:34:12.013: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:34:12.013: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:34:12.013: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep 15 16:34:12.047: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 15 16:34:22.091: INFO: Updating stateful set ss2
Sep 15 16:34:22.108: INFO: Waiting for Pod statefulset-3825/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Sep 15 16:34:32.165: INFO: Found 2 stateful pods, waiting for 3
Sep 15 16:34:42.172: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:34:42.172: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:34:42.172: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 15 16:34:42.204: INFO: Updating stateful set ss2
Sep 15 16:34:42.220: INFO: Waiting for Pod statefulset-3825/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep 15 16:34:52.253: INFO: Updating stateful set ss2
Sep 15 16:34:52.263: INFO: Waiting for StatefulSet statefulset-3825/ss2 to complete update
Sep 15 16:34:52.263: INFO: Waiting for Pod statefulset-3825/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep 15 16:35:02.275: INFO: Waiting for StatefulSet statefulset-3825/ss2 to complete update
Sep 15 16:35:02.275: INFO: Waiting for Pod statefulset-3825/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 15 16:35:12.274: INFO: Deleting all statefulset in ns statefulset-3825
Sep 15 16:35:12.279: INFO: Scaling statefulset ss2 to 0
Sep 15 16:35:22.304: INFO: Waiting for statefulset status.replicas updated to 0
Sep 15 16:35:22.309: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:35:22.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3825" for this suite.
Sep 15 16:35:28.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:35:28.508: INFO: namespace statefulset-3825 deletion completed in 6.172717727s

• [SLOW TEST:86.674 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:35:28.509: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-c852defc-2ac6-4f96-8dc4-2334ac6b9964
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:35:28.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1000" for this suite.
Sep 15 16:35:34.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:35:34.835: INFO: namespace configmap-1000 deletion completed in 6.169872906s

• [SLOW TEST:6.327 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:35:34.836: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep 15 16:35:34.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-7806'
Sep 15 16:35:35.155: INFO: stderr: ""
Sep 15 16:35:35.155: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 15 16:35:36.169: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 16:35:36.169: INFO: Found 1 / 1
Sep 15 16:35:36.169: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 15 16:35:36.174: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 16:35:36.174: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 15 16:35:36.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 patch pod redis-master-c5zlc --namespace=kubectl-7806 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 15 16:35:36.269: INFO: stderr: ""
Sep 15 16:35:36.269: INFO: stdout: "pod/redis-master-c5zlc patched\n"
STEP: checking annotations
Sep 15 16:35:36.275: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 16:35:36.275: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:35:36.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7806" for this suite.
Sep 15 16:35:58.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:35:58.446: INFO: namespace kubectl-7806 deletion completed in 22.164856476s

• [SLOW TEST:23.611 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:35:58.447: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep 15 16:35:58.593: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 15 16:35:58.605: INFO: Waiting for terminating namespaces to be deleted...
Sep 15 16:35:58.609: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-34-210.ec2.internal before test
Sep 15 16:35:58.618: INFO: canal-8zmc6 from kube-system started at 2019-09-15 15:36:09 +0000 UTC (4 container statuses recorded)
Sep 15 16:35:58.618: INFO: 	Container calico-node ready: true, restart count 0
Sep 15 16:35:58.618: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 15 16:35:58.618: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Sep 15 16:35:58.618: INFO: 	Container update-network-condition ready: true, restart count 0
Sep 15 16:35:58.618: INFO: kublr-system-shell-bd4545595-l6rcb from kube-system started at 2019-09-15 15:37:03 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.618: INFO: 	Container shell ready: true, restart count 0
Sep 15 16:35:58.618: INFO: sonobuoy-systemd-logs-daemon-set-df7b335343554494-7psrx from heptio-sonobuoy started at 2019-09-15 15:46:17 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.618: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 15 16:35:58.619: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 15 16:35:58.619: INFO: kublr-logging-port-fwd-app-7b6c88769c-ffdkg from kublr started at 2019-09-15 15:37:45 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.619: INFO: 	Container haproxy ready: true, restart count 0
Sep 15 16:35:58.619: INFO: 	Container port-fwd-app ready: true, restart count 0
Sep 15 16:35:58.619: INFO: kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-34-210.ec2.internal from kube-system started at 2019-09-15 15:27:42 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.619: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 15 16:35:58.619: INFO: node-local-dns-gz49t from kube-system started at 2019-09-15 15:36:19 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.619: INFO: 	Container node-cache ready: true, restart count 0
Sep 15 16:35:58.619: INFO: heapster-v1.6.0-beta.1-59cc44564c-zhc8x from kube-system started at 2019-09-15 15:36:30 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.619: INFO: 	Container heapster ready: true, restart count 0
Sep 15 16:35:58.619: INFO: 	Container heapster-nanny ready: true, restart count 0
Sep 15 16:35:58.619: INFO: kublr-logging-kibana-6c5f8fd758-nmztf from kublr started at 2019-09-15 15:37:45 +0000 UTC (4 container statuses recorded)
Sep 15 16:35:58.619: INFO: 	Container keycloak-proxy ready: true, restart count 0
Sep 15 16:35:58.619: INFO: 	Container kibana ready: true, restart count 0
Sep 15 16:35:58.619: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep 15 16:35:58.619: INFO: 	Container sg-auth-proxy ready: true, restart count 0
Sep 15 16:35:58.619: INFO: k8s-api-haproxy-30491611cba52aec18f9e69ca06c97ca1f7bdbbd34703396d270209b01ad231c-ip-172-16-34-210.ec2.internal from kube-system started at 2019-09-15 15:27:42 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.619: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Sep 15 16:35:58.619: INFO: kublr-logging-fluentd-es-bh9xf from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.619: INFO: 	Container fluentd-es ready: true, restart count 0
Sep 15 16:35:58.619: INFO: kublr-logging-elasticsearch-data-0 from kublr started at 2019-09-15 15:37:54 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.619: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 15 16:35:58.619: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-49-162.ec2.internal before test
Sep 15 16:35:58.637: INFO: kublr-logging-fluentd-es-s869l from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container fluentd-es ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kcp-generator-585767ccdf-j8h9s from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container generator ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kcp-terraform-controller-57c4558499-n8479 from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container terraform-controller ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kublr-logging-logstash-8b59d6585-lfv7s from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container logstash ready: true, restart count 1
Sep 15 16:35:58.637: INFO: sonobuoy-systemd-logs-daemon-set-df7b335343554494-4bjgh from heptio-sonobuoy started at 2019-09-15 15:46:17 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 15 16:35:58.637: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 15 16:35:58.637: INFO: canal-rxzvq from kube-system started at 2019-09-15 15:36:09 +0000 UTC (4 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container calico-node ready: true, restart count 0
Sep 15 16:35:58.637: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 15 16:35:58.637: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Sep 15 16:35:58.637: INFO: 	Container update-network-condition ready: true, restart count 0
Sep 15 16:35:58.637: INFO: node-local-dns-br5xd from kube-system started at 2019-09-15 15:36:28 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container node-cache ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kublr-logging-elasticsearch-client-878b8dfc9-bfjr9 from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 15 16:35:58.637: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-15 15:46:12 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kublr-feature-ingress-nginx-ingress-controller-7457b96bd5-v4xbz from kube-system started at 2019-09-15 15:37:06 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kublr-feature-ingress-nginx-ingress-default-backend-76875ffpw75 from kube-system started at 2019-09-15 15:37:06 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kcp-kublr-api-5b65f4d94-ct9tz from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container kublr-api ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-49-162.ec2.internal from kube-system started at 2019-09-15 15:27:34 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kcp-keycloak-1 from kublr started at 2019-09-15 15:40:02 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container init-keycloak ready: true, restart count 0
Sep 15 16:35:58.637: INFO: 	Container kcp-keycloak ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kcp-kublr-ui-f68d5649b-gk9wv from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container kublr-ui ready: true, restart count 0
Sep 15 16:35:58.637: INFO: k8s-api-haproxy-d652df068620165e8e39b6f21664f4bf8f25e18da58de3bbd5b9b30209f04c24-ip-172-16-49-162.ec2.internal from kube-system started at 2019-09-15 15:27:34 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Sep 15 16:35:58.637: INFO: coredns-5c98db65d4-tntcv from kube-system started at 2019-09-15 15:36:32 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container coredns ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kcp-cluster-controller-7df897bcc6-9vxkt from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container cluster-controller ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kcp-backup-controller-6fbdcc7665-26hcv from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container backup-controller ready: true, restart count 0
Sep 15 16:35:58.637: INFO: kublr-monitoring-kube-state-metrics-78575c4f65-vvmgq from kublr started at 2019-09-15 15:38:11 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.637: INFO: 	Container addon-resizer ready: true, restart count 0
Sep 15 16:35:58.637: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 15 16:35:58.637: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-74-13.ec2.internal before test
Sep 15 16:35:58.657: INFO: k8s-api-haproxy-30491611cba52aec18f9e69ca06c97ca1f7bdbbd34703396d270209b01ad231c-ip-172-16-74-13.ec2.internal from kube-system started at 2019-09-15 15:27:39 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-logging-controller-655f75b9fc-m7vws from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container kublr-feature-logging ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kcp-app-mongodb-c88c48bcf-5vgm6 from kublr started at 2019-09-15 15:38:07 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container mongo ready: true, restart count 0
Sep 15 16:35:58.657: INFO: 	Container mongo-exporter ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-monitoring-alertmanager-76479cbbdf-nskwk from kublr started at 2019-09-15 15:37:51 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container alertmanager ready: true, restart count 0
Sep 15 16:35:58.657: INFO: 	Container keycloak-proxy ready: true, restart count 3
Sep 15 16:35:58.657: INFO: kcp-keycloak-0 from kublr started at 2019-09-15 15:37:58 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container init-keycloak ready: true, restart count 0
Sep 15 16:35:58.657: INFO: 	Container kcp-keycloak ready: true, restart count 0
Sep 15 16:35:58.657: INFO: metrics-server-v0.3.1-6f69d6674c-52d69 from kube-system started at 2019-09-15 15:36:36 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container metrics-server ready: true, restart count 0
Sep 15 16:35:58.657: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-monitoring-monitoring-controller-b4f755985-n9zbv from kublr started at 2019-09-15 15:37:52 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container app-monitoring ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-monitoring-grafana-79f56fd86c-hgtxg from kublr started at 2019-09-15 15:37:58 +0000 UTC (3 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container grafana ready: true, restart count 0
Sep 15 16:35:58.657: INFO: 	Container keycloak-proxy ready: true, restart count 3
Sep 15 16:35:58.657: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-logging-rabbitmq-0 from kublr started at 2019-09-15 15:45:27 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container rabbitmq ready: true, restart count 0
Sep 15 16:35:58.657: INFO: sonobuoy-systemd-logs-daemon-set-df7b335343554494-9cvrv from heptio-sonobuoy started at 2019-09-15 15:46:17 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 15 16:35:58.657: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-logging-elasticsearch-exporter-7bbd7f9759-278vb from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container main ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-logging-sg-job-2mhts from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container certgenerator ready: false, restart count 0
Sep 15 16:35:58.657: INFO: kublr-monitoring-prometheus-5fbdf985fd-v8lwr from kublr started at 2019-09-15 15:37:58 +0000 UTC (2 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container keycloak-proxy ready: true, restart count 3
Sep 15 16:35:58.657: INFO: 	Container prometheus ready: true, restart count 6
Sep 15 16:35:58.657: INFO: kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-74-13.ec2.internal from kube-system started at 2019-09-15 15:27:39 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 15 16:35:58.657: INFO: node-local-dns-npqcf from kube-system started at 2019-09-15 15:36:28 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container node-cache ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-logging-fluentd-es-hrgjn from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container fluentd-es ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-logging-elasticsearch-master-0 from kublr started at 2019-09-15 15:37:53 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 15 16:35:58.657: INFO: canal-vkt7q from kube-system started at 2019-09-15 15:36:09 +0000 UTC (4 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container calico-node ready: true, restart count 0
Sep 15 16:35:58.657: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 15 16:35:58.657: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Sep 15 16:35:58.657: INFO: 	Container update-network-condition ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kublr-logging-rabbitmq-exporter-5c97455c8-hjmbj from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container kublr-logging-rabbitmq-exporter ready: true, restart count 0
Sep 15 16:35:58.657: INFO: kcp-app-mysql-9f587666-q8dpt from kublr started at 2019-09-15 15:38:04 +0000 UTC (1 container statuses recorded)
Sep 15 16:35:58.657: INFO: 	Container kcp-app-mysql ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node ip-172-16-34-210.ec2.internal
STEP: verifying the node has the label node ip-172-16-49-162.ec2.internal
STEP: verifying the node has the label node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod sonobuoy-systemd-logs-daemon-set-df7b335343554494-4bjgh requesting resource cpu=0m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod sonobuoy-systemd-logs-daemon-set-df7b335343554494-7psrx requesting resource cpu=0m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod sonobuoy-systemd-logs-daemon-set-df7b335343554494-9cvrv requesting resource cpu=0m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod canal-8zmc6 requesting resource cpu=50m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod canal-rxzvq requesting resource cpu=50m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod canal-vkt7q requesting resource cpu=50m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod coredns-5c98db65d4-tntcv requesting resource cpu=100m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod heapster-v1.6.0-beta.1-59cc44564c-zhc8x requesting resource cpu=138m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod k8s-api-haproxy-30491611cba52aec18f9e69ca06c97ca1f7bdbbd34703396d270209b01ad231c-ip-172-16-34-210.ec2.internal requesting resource cpu=1m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod k8s-api-haproxy-30491611cba52aec18f9e69ca06c97ca1f7bdbbd34703396d270209b01ad231c-ip-172-16-74-13.ec2.internal requesting resource cpu=1m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod k8s-api-haproxy-d652df068620165e8e39b6f21664f4bf8f25e18da58de3bbd5b9b30209f04c24-ip-172-16-49-162.ec2.internal requesting resource cpu=1m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-34-210.ec2.internal requesting resource cpu=5m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-49-162.ec2.internal requesting resource cpu=5m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-74-13.ec2.internal requesting resource cpu=5m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-feature-ingress-nginx-ingress-controller-7457b96bd5-v4xbz requesting resource cpu=100m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-feature-ingress-nginx-ingress-default-backend-76875ffpw75 requesting resource cpu=0m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-system-shell-bd4545595-l6rcb requesting resource cpu=0m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod metrics-server-v0.3.1-6f69d6674c-52d69 requesting resource cpu=98m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod node-local-dns-br5xd requesting resource cpu=25m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod node-local-dns-gz49t requesting resource cpu=25m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod node-local-dns-npqcf requesting resource cpu=25m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-app-mongodb-c88c48bcf-5vgm6 requesting resource cpu=100m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-app-mysql-9f587666-q8dpt requesting resource cpu=100m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-backup-controller-6fbdcc7665-26hcv requesting resource cpu=100m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-cluster-controller-7df897bcc6-9vxkt requesting resource cpu=100m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-generator-585767ccdf-j8h9s requesting resource cpu=100m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-keycloak-0 requesting resource cpu=300m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-keycloak-1 requesting resource cpu=300m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-kublr-api-5b65f4d94-ct9tz requesting resource cpu=100m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-kublr-ui-f68d5649b-gk9wv requesting resource cpu=10m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kcp-terraform-controller-57c4558499-n8479 requesting resource cpu=200m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-controller-655f75b9fc-m7vws requesting resource cpu=100m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-elasticsearch-client-878b8dfc9-bfjr9 requesting resource cpu=25m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-elasticsearch-data-0 requesting resource cpu=25m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-elasticsearch-exporter-7bbd7f9759-278vb requesting resource cpu=5m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-elasticsearch-master-0 requesting resource cpu=25m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-fluentd-es-bh9xf requesting resource cpu=150m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-fluentd-es-hrgjn requesting resource cpu=150m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-fluentd-es-s869l requesting resource cpu=150m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-kibana-6c5f8fd758-nmztf requesting resource cpu=220m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-logstash-8b59d6585-lfv7s requesting resource cpu=500m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-port-fwd-app-7b6c88769c-ffdkg requesting resource cpu=100m on Node ip-172-16-34-210.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-rabbitmq-0 requesting resource cpu=400m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-logging-rabbitmq-exporter-5c97455c8-hjmbj requesting resource cpu=10m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-monitoring-alertmanager-76479cbbdf-nskwk requesting resource cpu=60m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-monitoring-grafana-79f56fd86c-hgtxg requesting resource cpu=320m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-monitoring-kube-state-metrics-78575c4f65-vvmgq requesting resource cpu=116m on Node ip-172-16-49-162.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-monitoring-monitoring-controller-b4f755985-n9zbv requesting resource cpu=100m on Node ip-172-16-74-13.ec2.internal
Sep 15 16:35:58.740: INFO: Pod kublr-monitoring-prometheus-5fbdf985fd-v8lwr requesting resource cpu=710m on Node ip-172-16-74-13.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-17c79785-d8fe-4b85-8673-9d8e4ce5b023.15c4a9e217bfca55], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7119/filler-pod-17c79785-d8fe-4b85-8673-9d8e4ce5b023 to ip-172-16-34-210.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-17c79785-d8fe-4b85-8673-9d8e4ce5b023.15c4a9e23f7a193c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-17c79785-d8fe-4b85-8673-9d8e4ce5b023.15c4a9e241eeb604], Reason = [Created], Message = [Created container filler-pod-17c79785-d8fe-4b85-8673-9d8e4ce5b023]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-17c79785-d8fe-4b85-8673-9d8e4ce5b023.15c4a9e249c661b9], Reason = [Started], Message = [Started container filler-pod-17c79785-d8fe-4b85-8673-9d8e4ce5b023]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28eb6254-4ce5-4beb-83ec-a5e25e81ac36.15c4a9e21845f586], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7119/filler-pod-28eb6254-4ce5-4beb-83ec-a5e25e81ac36 to ip-172-16-49-162.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28eb6254-4ce5-4beb-83ec-a5e25e81ac36.15c4a9e2404739e4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28eb6254-4ce5-4beb-83ec-a5e25e81ac36.15c4a9e2430a173d], Reason = [Created], Message = [Created container filler-pod-28eb6254-4ce5-4beb-83ec-a5e25e81ac36]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28eb6254-4ce5-4beb-83ec-a5e25e81ac36.15c4a9e24ab7a592], Reason = [Started], Message = [Started container filler-pod-28eb6254-4ce5-4beb-83ec-a5e25e81ac36]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e73a86c4-fb5d-4bfa-bea8-d130e9da74da.15c4a9e218a6417a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7119/filler-pod-e73a86c4-fb5d-4bfa-bea8-d130e9da74da to ip-172-16-74-13.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e73a86c4-fb5d-4bfa-bea8-d130e9da74da.15c4a9e2410fd690], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e73a86c4-fb5d-4bfa-bea8-d130e9da74da.15c4a9e2438dbb48], Reason = [Created], Message = [Created container filler-pod-e73a86c4-fb5d-4bfa-bea8-d130e9da74da]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e73a86c4-fb5d-4bfa-bea8-d130e9da74da.15c4a9e24b3cced7], Reason = [Started], Message = [Started container filler-pod-e73a86c4-fb5d-4bfa-bea8-d130e9da74da]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c4a9e291a1fb35], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node ip-172-16-34-210.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-16-49-162.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-16-74-13.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:36:01.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7119" for this suite.
Sep 15 16:36:07.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:36:08.039: INFO: namespace sched-pred-7119 deletion completed in 6.163907153s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:9.592 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:36:08.039: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4013
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 15 16:36:08.197: INFO: Waiting up to 5m0s for pod "pod-31e204e5-455c-479f-88c4-9e1d78b8f02f" in namespace "emptydir-4013" to be "success or failure"
Sep 15 16:36:08.203: INFO: Pod "pod-31e204e5-455c-479f-88c4-9e1d78b8f02f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.75676ms
Sep 15 16:36:10.215: INFO: Pod "pod-31e204e5-455c-479f-88c4-9e1d78b8f02f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017507688s
STEP: Saw pod success
Sep 15 16:36:10.215: INFO: Pod "pod-31e204e5-455c-479f-88c4-9e1d78b8f02f" satisfied condition "success or failure"
Sep 15 16:36:10.219: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-31e204e5-455c-479f-88c4-9e1d78b8f02f container test-container: <nil>
STEP: delete the pod
Sep 15 16:36:10.250: INFO: Waiting for pod pod-31e204e5-455c-479f-88c4-9e1d78b8f02f to disappear
Sep 15 16:36:10.254: INFO: Pod pod-31e204e5-455c-479f-88c4-9e1d78b8f02f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:36:10.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4013" for this suite.
Sep 15 16:36:16.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:36:16.428: INFO: namespace emptydir-4013 deletion completed in 6.167683426s

• [SLOW TEST:8.389 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:36:16.428: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:36:18.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5675" for this suite.
Sep 15 16:36:56.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:36:56.778: INFO: namespace kubelet-test-5675 deletion completed in 38.160631187s

• [SLOW TEST:40.350 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:36:56.778: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 15 16:36:56.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-2364'
Sep 15 16:36:57.243: INFO: stderr: ""
Sep 15 16:36:57.243: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep 15 16:37:02.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pod e2e-test-nginx-pod --namespace=kubectl-2364 -o json'
Sep 15 16:37:02.367: INFO: stderr: ""
Sep 15 16:37:02.367: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.2.176/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-09-15T16:36:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-2364\",\n        \"resourceVersion\": \"19471\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2364/pods/e2e-test-nginx-pod\",\n        \"uid\": \"5d8fe05d-fc55-480f-8e07-c4ab98039215\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-jdfzb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-16-34-210.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-jdfzb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-jdfzb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-15T16:36:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-15T16:36:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-15T16:36:58Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-15T16:36:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://7789e8c3a52c305935cc2bbd4e71c2049e1e8a8d83118077ca82beb1cf60b254\",\n                \"image\": \"docker.io/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-15T16:36:58Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.34.210\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.2.176\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-15T16:36:57Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 15 16:37:02.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 replace -f - --namespace=kubectl-2364'
Sep 15 16:37:02.608: INFO: stderr: ""
Sep 15 16:37:02.608: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Sep 15 16:37:02.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete pods e2e-test-nginx-pod --namespace=kubectl-2364'
Sep 15 16:37:12.868: INFO: stderr: ""
Sep 15 16:37:12.868: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:37:12.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2364" for this suite.
Sep 15 16:37:18.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:37:19.038: INFO: namespace kubectl-2364 deletion completed in 6.162350369s

• [SLOW TEST:22.260 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:37:19.038: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-08ef6733-c7a3-42ad-a786-e4332d66e7d0
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:37:19.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8473" for this suite.
Sep 15 16:37:25.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:37:25.358: INFO: namespace secrets-8473 deletion completed in 6.165683974s

• [SLOW TEST:6.319 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:37:25.358: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 15 16:37:25.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4027'
Sep 15 16:37:25.598: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 15 16:37:25.598: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Sep 15 16:37:27.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4027'
Sep 15 16:37:27.702: INFO: stderr: ""
Sep 15 16:37:27.702: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:37:27.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4027" for this suite.
Sep 15 16:37:49.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:37:49.868: INFO: namespace kubectl-4027 deletion completed in 22.160000964s

• [SLOW TEST:24.510 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:37:49.868: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2851
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 15 16:37:50.048: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2851,SelfLink:/api/v1/namespaces/watch-2851/configmaps/e2e-watch-test-resource-version,UID:068cfb83-7b2d-473e-a230-76e9ded4954a,ResourceVersion:19668,Generation:0,CreationTimestamp:2019-09-15 16:37:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 15 16:37:50.048: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2851,SelfLink:/api/v1/namespaces/watch-2851/configmaps/e2e-watch-test-resource-version,UID:068cfb83-7b2d-473e-a230-76e9ded4954a,ResourceVersion:19669,Generation:0,CreationTimestamp:2019-09-15 16:37:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:37:50.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2851" for this suite.
Sep 15 16:37:56.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:37:56.214: INFO: namespace watch-2851 deletion completed in 6.160566601s

• [SLOW TEST:6.346 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:37:56.214: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Sep 15 16:37:57.419: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0915 16:37:57.419942      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:37:57.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1985" for this suite.
Sep 15 16:38:03.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:38:03.587: INFO: namespace gc-1985 deletion completed in 6.161889887s

• [SLOW TEST:7.373 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:38:03.587: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:38:03.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-226" for this suite.
Sep 15 16:38:09.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:38:09.911: INFO: namespace services-226 deletion completed in 6.165007847s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.324 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:38:09.911: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3923
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:38:12.111: INFO: Waiting up to 5m0s for pod "client-envvars-201301de-a448-4820-ab33-25a00f8c3d5d" in namespace "pods-3923" to be "success or failure"
Sep 15 16:38:12.118: INFO: Pod "client-envvars-201301de-a448-4820-ab33-25a00f8c3d5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.450242ms
Sep 15 16:38:14.122: INFO: Pod "client-envvars-201301de-a448-4820-ab33-25a00f8c3d5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011214569s
STEP: Saw pod success
Sep 15 16:38:14.123: INFO: Pod "client-envvars-201301de-a448-4820-ab33-25a00f8c3d5d" satisfied condition "success or failure"
Sep 15 16:38:14.126: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod client-envvars-201301de-a448-4820-ab33-25a00f8c3d5d container env3cont: <nil>
STEP: delete the pod
Sep 15 16:38:14.161: INFO: Waiting for pod client-envvars-201301de-a448-4820-ab33-25a00f8c3d5d to disappear
Sep 15 16:38:14.167: INFO: Pod client-envvars-201301de-a448-4820-ab33-25a00f8c3d5d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:38:14.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3923" for this suite.
Sep 15 16:38:54.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:38:54.329: INFO: namespace pods-3923 deletion completed in 40.156763562s

• [SLOW TEST:44.418 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:38:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8451/configmap-test-07cd0dc5-ab41-470c-9ffc-704d5694a9ee
STEP: Creating a pod to test consume configMaps
Sep 15 16:38:54.492: INFO: Waiting up to 5m0s for pod "pod-configmaps-725ac2ec-3efc-4e6c-b850-74a92e414f22" in namespace "configmap-8451" to be "success or failure"
Sep 15 16:38:54.498: INFO: Pod "pod-configmaps-725ac2ec-3efc-4e6c-b850-74a92e414f22": Phase="Pending", Reason="", readiness=false. Elapsed: 6.253222ms
Sep 15 16:38:56.504: INFO: Pod "pod-configmaps-725ac2ec-3efc-4e6c-b850-74a92e414f22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012013277s
STEP: Saw pod success
Sep 15 16:38:56.504: INFO: Pod "pod-configmaps-725ac2ec-3efc-4e6c-b850-74a92e414f22" satisfied condition "success or failure"
Sep 15 16:38:56.508: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-725ac2ec-3efc-4e6c-b850-74a92e414f22 container env-test: <nil>
STEP: delete the pod
Sep 15 16:38:56.541: INFO: Waiting for pod pod-configmaps-725ac2ec-3efc-4e6c-b850-74a92e414f22 to disappear
Sep 15 16:38:56.548: INFO: Pod pod-configmaps-725ac2ec-3efc-4e6c-b850-74a92e414f22 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:38:56.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8451" for this suite.
Sep 15 16:39:02.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:39:02.721: INFO: namespace configmap-8451 deletion completed in 6.168489996s

• [SLOW TEST:8.392 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:39:02.722: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8669
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 15 16:39:02.876: INFO: Waiting up to 5m0s for pod "pod-1865a37e-865e-4476-851c-10684ae569f9" in namespace "emptydir-8669" to be "success or failure"
Sep 15 16:39:02.883: INFO: Pod "pod-1865a37e-865e-4476-851c-10684ae569f9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756939ms
Sep 15 16:39:04.888: INFO: Pod "pod-1865a37e-865e-4476-851c-10684ae569f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011882176s
STEP: Saw pod success
Sep 15 16:39:04.888: INFO: Pod "pod-1865a37e-865e-4476-851c-10684ae569f9" satisfied condition "success or failure"
Sep 15 16:39:04.892: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-1865a37e-865e-4476-851c-10684ae569f9 container test-container: <nil>
STEP: delete the pod
Sep 15 16:39:04.924: INFO: Waiting for pod pod-1865a37e-865e-4476-851c-10684ae569f9 to disappear
Sep 15 16:39:04.928: INFO: Pod pod-1865a37e-865e-4476-851c-10684ae569f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:39:04.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8669" for this suite.
Sep 15 16:39:10.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:39:11.111: INFO: namespace emptydir-8669 deletion completed in 6.178195029s

• [SLOW TEST:8.390 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:39:11.112: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Sep 15 16:39:13.297: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-180164986 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 15 16:39:23.376: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:39:23.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7010" for this suite.
Sep 15 16:39:29.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:39:29.542: INFO: namespace pods-7010 deletion completed in 6.156950578s

• [SLOW TEST:18.431 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:39:29.542: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8654
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7446
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:39:36.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5829" for this suite.
Sep 15 16:39:42.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:39:42.173: INFO: namespace namespaces-5829 deletion completed in 6.156414649s
STEP: Destroying namespace "nsdeletetest-8654" for this suite.
Sep 15 16:39:42.177: INFO: Namespace nsdeletetest-8654 was already deleted
STEP: Destroying namespace "nsdeletetest-7446" for this suite.
Sep 15 16:39:48.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:39:48.334: INFO: namespace nsdeletetest-7446 deletion completed in 6.157208004s

• [SLOW TEST:18.792 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:39:48.334: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5937
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 15 16:39:48.481: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 15 16:40:12.601: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.2.184:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5937 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:40:12.601: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:40:12.726: INFO: Found all expected endpoints: [netserver-0]
Sep 15 16:40:12.731: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.3.63:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5937 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:40:12.731: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:40:12.855: INFO: Found all expected endpoints: [netserver-1]
Sep 15 16:40:12.860: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.55:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5937 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:40:12.860: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:40:12.962: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:40:12.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5937" for this suite.
Sep 15 16:40:34.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:40:35.125: INFO: namespace pod-network-test-5937 deletion completed in 22.157758788s

• [SLOW TEST:46.791 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:40:35.126: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:40:35.294: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:40:37.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6893" for this suite.
Sep 15 16:41:25.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:41:25.591: INFO: namespace pods-6893 deletion completed in 48.158730226s

• [SLOW TEST:50.466 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:41:25.591: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:41:25.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 version'
Sep 15 16:41:25.800: INFO: stderr: ""
Sep 15 16:41:25.800: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:41:25.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4086" for this suite.
Sep 15 16:41:31.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:41:32.011: INFO: namespace kubectl-4086 deletion completed in 6.205559234s

• [SLOW TEST:6.420 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:41:32.011: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 15 16:41:32.189: INFO: Waiting up to 5m0s for pod "pod-79769bf9-9e66-4872-ae02-0dca24308048" in namespace "emptydir-4975" to be "success or failure"
Sep 15 16:41:32.195: INFO: Pod "pod-79769bf9-9e66-4872-ae02-0dca24308048": Phase="Pending", Reason="", readiness=false. Elapsed: 5.667305ms
Sep 15 16:41:34.200: INFO: Pod "pod-79769bf9-9e66-4872-ae02-0dca24308048": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010550233s
STEP: Saw pod success
Sep 15 16:41:34.200: INFO: Pod "pod-79769bf9-9e66-4872-ae02-0dca24308048" satisfied condition "success or failure"
Sep 15 16:41:34.204: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-79769bf9-9e66-4872-ae02-0dca24308048 container test-container: <nil>
STEP: delete the pod
Sep 15 16:41:34.234: INFO: Waiting for pod pod-79769bf9-9e66-4872-ae02-0dca24308048 to disappear
Sep 15 16:41:34.238: INFO: Pod pod-79769bf9-9e66-4872-ae02-0dca24308048 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:41:34.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4975" for this suite.
Sep 15 16:41:40.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:41:40.402: INFO: namespace emptydir-4975 deletion completed in 6.158206698s

• [SLOW TEST:8.391 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:41:40.403: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 15 16:41:44.594: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:44.594: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:44.701: INFO: Exec stderr: ""
Sep 15 16:41:44.701: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:44.701: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:44.801: INFO: Exec stderr: ""
Sep 15 16:41:44.801: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:44.802: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:44.897: INFO: Exec stderr: ""
Sep 15 16:41:44.897: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:44.897: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:44.998: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 15 16:41:44.998: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:44.998: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:45.094: INFO: Exec stderr: ""
Sep 15 16:41:45.094: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:45.094: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:45.192: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 15 16:41:45.192: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:45.192: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:45.298: INFO: Exec stderr: ""
Sep 15 16:41:45.298: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:45.298: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:45.389: INFO: Exec stderr: ""
Sep 15 16:41:45.389: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:45.389: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:45.480: INFO: Exec stderr: ""
Sep 15 16:41:45.480: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6392 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:41:45.480: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:41:45.592: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:41:45.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6392" for this suite.
Sep 15 16:42:37.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:42:37.779: INFO: namespace e2e-kubelet-etc-hosts-6392 deletion completed in 52.180679534s

• [SLOW TEST:57.376 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:42:37.779: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-f8efbaf2-7b93-4ffa-b024-2dbad9579ac3 in namespace container-probe-9464
Sep 15 16:42:41.952: INFO: Started pod test-webserver-f8efbaf2-7b93-4ffa-b024-2dbad9579ac3 in namespace container-probe-9464
STEP: checking the pod's current state and verifying that restartCount is present
Sep 15 16:42:41.956: INFO: Initial restart count of pod test-webserver-f8efbaf2-7b93-4ffa-b024-2dbad9579ac3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:46:42.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9464" for this suite.
Sep 15 16:46:48.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:46:48.798: INFO: namespace container-probe-9464 deletion completed in 6.18939981s

• [SLOW TEST:251.020 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:46:48.799: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2532
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 15 16:46:48.974: INFO: Waiting up to 5m0s for pod "pod-948081b5-4127-45af-b22a-e5202cfc9229" in namespace "emptydir-2532" to be "success or failure"
Sep 15 16:46:48.981: INFO: Pod "pod-948081b5-4127-45af-b22a-e5202cfc9229": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506902ms
Sep 15 16:46:50.986: INFO: Pod "pod-948081b5-4127-45af-b22a-e5202cfc9229": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011686598s
STEP: Saw pod success
Sep 15 16:46:50.986: INFO: Pod "pod-948081b5-4127-45af-b22a-e5202cfc9229" satisfied condition "success or failure"
Sep 15 16:46:50.990: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-948081b5-4127-45af-b22a-e5202cfc9229 container test-container: <nil>
STEP: delete the pod
Sep 15 16:46:51.022: INFO: Waiting for pod pod-948081b5-4127-45af-b22a-e5202cfc9229 to disappear
Sep 15 16:46:51.026: INFO: Pod pod-948081b5-4127-45af-b22a-e5202cfc9229 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:46:51.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2532" for this suite.
Sep 15 16:46:57.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:46:57.191: INFO: namespace emptydir-2532 deletion completed in 6.158811717s

• [SLOW TEST:8.393 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:46:57.192: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Sep 15 16:46:57.349: INFO: Waiting up to 5m0s for pod "var-expansion-4aae0ebc-0268-4dfa-b71e-1806b608cb00" in namespace "var-expansion-5751" to be "success or failure"
Sep 15 16:46:57.357: INFO: Pod "var-expansion-4aae0ebc-0268-4dfa-b71e-1806b608cb00": Phase="Pending", Reason="", readiness=false. Elapsed: 7.923711ms
Sep 15 16:46:59.363: INFO: Pod "var-expansion-4aae0ebc-0268-4dfa-b71e-1806b608cb00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013452881s
STEP: Saw pod success
Sep 15 16:46:59.363: INFO: Pod "var-expansion-4aae0ebc-0268-4dfa-b71e-1806b608cb00" satisfied condition "success or failure"
Sep 15 16:46:59.367: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod var-expansion-4aae0ebc-0268-4dfa-b71e-1806b608cb00 container dapi-container: <nil>
STEP: delete the pod
Sep 15 16:46:59.397: INFO: Waiting for pod var-expansion-4aae0ebc-0268-4dfa-b71e-1806b608cb00 to disappear
Sep 15 16:46:59.402: INFO: Pod var-expansion-4aae0ebc-0268-4dfa-b71e-1806b608cb00 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:46:59.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5751" for this suite.
Sep 15 16:47:05.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:47:05.709: INFO: namespace var-expansion-5751 deletion completed in 6.301078571s

• [SLOW TEST:8.517 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:47:05.709: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4642
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4642
STEP: Creating statefulset with conflicting port in namespace statefulset-4642
STEP: Waiting until pod test-pod will start running in namespace statefulset-4642
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4642
Sep 15 16:47:07.899: INFO: Observed stateful pod in namespace: statefulset-4642, name: ss-0, uid: b832baae-9176-4486-9cdd-f34bb5d12955, status phase: Pending. Waiting for statefulset controller to delete.
Sep 15 16:47:12.846: INFO: Observed stateful pod in namespace: statefulset-4642, name: ss-0, uid: b832baae-9176-4486-9cdd-f34bb5d12955, status phase: Failed. Waiting for statefulset controller to delete.
Sep 15 16:47:12.857: INFO: Observed stateful pod in namespace: statefulset-4642, name: ss-0, uid: b832baae-9176-4486-9cdd-f34bb5d12955, status phase: Failed. Waiting for statefulset controller to delete.
Sep 15 16:47:12.865: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4642
STEP: Removing pod with conflicting port in namespace statefulset-4642
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4642 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 15 16:47:14.898: INFO: Deleting all statefulset in ns statefulset-4642
Sep 15 16:47:14.902: INFO: Scaling statefulset ss to 0
Sep 15 16:47:24.922: INFO: Waiting for statefulset status.replicas updated to 0
Sep 15 16:47:24.927: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:47:24.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4642" for this suite.
Sep 15 16:47:30.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:47:31.202: INFO: namespace statefulset-4642 deletion completed in 6.247125578s

• [SLOW TEST:25.493 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:47:31.202: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-ce592ebc-7b8a-4c9f-b9fe-fb5bf180e9c6
STEP: Creating a pod to test consume configMaps
Sep 15 16:47:31.365: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-316c45e7-a52a-43d3-945c-4e259cb6236c" in namespace "projected-4707" to be "success or failure"
Sep 15 16:47:31.370: INFO: Pod "pod-projected-configmaps-316c45e7-a52a-43d3-945c-4e259cb6236c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.692653ms
Sep 15 16:47:33.375: INFO: Pod "pod-projected-configmaps-316c45e7-a52a-43d3-945c-4e259cb6236c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009530756s
STEP: Saw pod success
Sep 15 16:47:33.375: INFO: Pod "pod-projected-configmaps-316c45e7-a52a-43d3-945c-4e259cb6236c" satisfied condition "success or failure"
Sep 15 16:47:33.379: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-configmaps-316c45e7-a52a-43d3-945c-4e259cb6236c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 16:47:33.407: INFO: Waiting for pod pod-projected-configmaps-316c45e7-a52a-43d3-945c-4e259cb6236c to disappear
Sep 15 16:47:33.411: INFO: Pod pod-projected-configmaps-316c45e7-a52a-43d3-945c-4e259cb6236c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:47:33.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4707" for this suite.
Sep 15 16:47:39.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:47:39.581: INFO: namespace projected-4707 deletion completed in 6.165550496s

• [SLOW TEST:8.380 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:47:39.582: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:47:39.761: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8a3598f3-a20c-4703-a43c-c148ccddf7ca", Controller:(*bool)(0xc0021c367e), BlockOwnerDeletion:(*bool)(0xc0021c367f)}}
Sep 15 16:47:39.768: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3a27faf4-6d81-4681-b703-72f67c9268c3", Controller:(*bool)(0xc001e3c83e), BlockOwnerDeletion:(*bool)(0xc001e3c83f)}}
Sep 15 16:47:39.775: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"619a6db3-4bd2-4e29-b115-9c339c157c29", Controller:(*bool)(0xc0021c383e), BlockOwnerDeletion:(*bool)(0xc0021c383f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:47:44.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9201" for this suite.
Sep 15 16:47:50.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:47:50.953: INFO: namespace gc-9201 deletion completed in 6.155493154s

• [SLOW TEST:11.371 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:47:50.953: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep 15 16:47:51.100: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:47:53.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-946" for this suite.
Sep 15 16:47:59.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:47:59.376: INFO: namespace init-container-946 deletion completed in 6.159758064s

• [SLOW TEST:8.422 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:47:59.377: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-8909dd71-229a-4877-917e-3bccf3bb2ab7
STEP: Creating a pod to test consume secrets
Sep 15 16:47:59.542: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-faae2b0e-0d68-466e-929d-316f7456b638" in namespace "projected-5014" to be "success or failure"
Sep 15 16:47:59.547: INFO: Pod "pod-projected-secrets-faae2b0e-0d68-466e-929d-316f7456b638": Phase="Pending", Reason="", readiness=false. Elapsed: 4.718102ms
Sep 15 16:48:01.552: INFO: Pod "pod-projected-secrets-faae2b0e-0d68-466e-929d-316f7456b638": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009867909s
STEP: Saw pod success
Sep 15 16:48:01.552: INFO: Pod "pod-projected-secrets-faae2b0e-0d68-466e-929d-316f7456b638" satisfied condition "success or failure"
Sep 15 16:48:01.557: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-secrets-faae2b0e-0d68-466e-929d-316f7456b638 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:48:01.589: INFO: Waiting for pod pod-projected-secrets-faae2b0e-0d68-466e-929d-316f7456b638 to disappear
Sep 15 16:48:01.595: INFO: Pod pod-projected-secrets-faae2b0e-0d68-466e-929d-316f7456b638 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:48:01.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5014" for this suite.
Sep 15 16:48:07.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:48:07.768: INFO: namespace projected-5014 deletion completed in 6.167555163s

• [SLOW TEST:8.392 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:48:07.768: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep 15 16:48:07.918: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:48:11.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7583" for this suite.
Sep 15 16:48:33.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:48:33.725: INFO: namespace init-container-7583 deletion completed in 22.156539323s

• [SLOW TEST:25.956 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:48:33.725: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5215
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:48:33.890: INFO: (0) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.360549ms)
Sep 15 16:48:33.895: INFO: (1) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.280685ms)
Sep 15 16:48:33.900: INFO: (2) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.36253ms)
Sep 15 16:48:33.906: INFO: (3) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.133901ms)
Sep 15 16:48:33.910: INFO: (4) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.840275ms)
Sep 15 16:48:33.916: INFO: (5) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.117243ms)
Sep 15 16:48:33.920: INFO: (6) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.928186ms)
Sep 15 16:48:33.926: INFO: (7) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.02945ms)
Sep 15 16:48:33.931: INFO: (8) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.17275ms)
Sep 15 16:48:33.936: INFO: (9) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.012158ms)
Sep 15 16:48:33.941: INFO: (10) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.064874ms)
Sep 15 16:48:33.946: INFO: (11) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.936292ms)
Sep 15 16:48:33.951: INFO: (12) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.946484ms)
Sep 15 16:48:33.956: INFO: (13) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.062037ms)
Sep 15 16:48:33.961: INFO: (14) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.21308ms)
Sep 15 16:48:33.966: INFO: (15) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.911492ms)
Sep 15 16:48:33.971: INFO: (16) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.038614ms)
Sep 15 16:48:33.976: INFO: (17) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.026344ms)
Sep 15 16:48:33.981: INFO: (18) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.04211ms)
Sep 15 16:48:33.986: INFO: (19) /api/v1/nodes/ip-172-16-34-210.ec2.internal/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.028667ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:48:33.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5215" for this suite.
Sep 15 16:48:40.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:48:40.153: INFO: namespace proxy-5215 deletion completed in 6.161897201s

• [SLOW TEST:6.429 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:48:40.154: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1953
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 15 16:48:40.302: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 15 16:49:02.430: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.198:8080/dial?request=hostName&protocol=http&host=100.96.3.65&port=8080&tries=1'] Namespace:pod-network-test-1953 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:49:02.430: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:49:02.547: INFO: Waiting for endpoints: map[]
Sep 15 16:49:02.552: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.198:8080/dial?request=hostName&protocol=http&host=100.96.1.56&port=8080&tries=1'] Namespace:pod-network-test-1953 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:49:02.552: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:49:02.685: INFO: Waiting for endpoints: map[]
Sep 15 16:49:02.690: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.198:8080/dial?request=hostName&protocol=http&host=100.96.2.197&port=8080&tries=1'] Namespace:pod-network-test-1953 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 15 16:49:02.690: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
Sep 15 16:49:02.819: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:49:02.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1953" for this suite.
Sep 15 16:49:24.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:49:24.996: INFO: namespace pod-network-test-1953 deletion completed in 22.171052617s

• [SLOW TEST:44.842 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:49:24.996: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:49:25.154: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13af1237-f42d-4ef6-bf5f-5c3b856a21df" in namespace "projected-8064" to be "success or failure"
Sep 15 16:49:25.160: INFO: Pod "downwardapi-volume-13af1237-f42d-4ef6-bf5f-5c3b856a21df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376477ms
Sep 15 16:49:27.165: INFO: Pod "downwardapi-volume-13af1237-f42d-4ef6-bf5f-5c3b856a21df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011450623s
STEP: Saw pod success
Sep 15 16:49:27.165: INFO: Pod "downwardapi-volume-13af1237-f42d-4ef6-bf5f-5c3b856a21df" satisfied condition "success or failure"
Sep 15 16:49:27.169: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-13af1237-f42d-4ef6-bf5f-5c3b856a21df container client-container: <nil>
STEP: delete the pod
Sep 15 16:49:27.200: INFO: Waiting for pod downwardapi-volume-13af1237-f42d-4ef6-bf5f-5c3b856a21df to disappear
Sep 15 16:49:27.205: INFO: Pod downwardapi-volume-13af1237-f42d-4ef6-bf5f-5c3b856a21df no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:49:27.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8064" for this suite.
Sep 15 16:49:33.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:49:33.366: INFO: namespace projected-8064 deletion completed in 6.155379918s

• [SLOW TEST:8.370 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:49:33.366: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:49:37.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9735" for this suite.
Sep 15 16:49:43.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:49:43.706: INFO: namespace kubelet-test-9735 deletion completed in 6.168453782s

• [SLOW TEST:10.340 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:49:43.706: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 15 16:49:47.946: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 15 16:49:47.950: INFO: Pod pod-with-prestop-http-hook still exists
Sep 15 16:49:49.951: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 15 16:49:49.955: INFO: Pod pod-with-prestop-http-hook still exists
Sep 15 16:49:51.951: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 15 16:49:51.955: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:49:51.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1010" for this suite.
Sep 15 16:50:13.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:50:14.234: INFO: namespace container-lifecycle-hook-1010 deletion completed in 22.260836453s

• [SLOW TEST:30.528 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:50:14.235: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8266
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8266
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8266
Sep 15 16:50:14.404: INFO: Found 0 stateful pods, waiting for 1
Sep 15 16:50:24.409: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 15 16:50:24.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-8266 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:50:24.873: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:50:24.873: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:50:24.873: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 15 16:50:24.878: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 15 16:50:34.883: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 15 16:50:34.883: INFO: Waiting for statefulset status.replicas updated to 0
Sep 15 16:50:34.902: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998462s
Sep 15 16:50:35.907: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995427165s
Sep 15 16:50:36.912: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990411941s
Sep 15 16:50:37.918: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985497462s
Sep 15 16:50:38.922: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979577396s
Sep 15 16:50:39.927: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974814491s
Sep 15 16:50:40.932: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.970385851s
Sep 15 16:50:41.937: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.965556177s
Sep 15 16:50:42.941: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960681297s
Sep 15 16:50:43.947: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.886985ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8266
Sep 15 16:50:44.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-8266 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 15 16:50:45.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 15 16:50:45.135: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 15 16:50:45.135: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 15 16:50:45.140: INFO: Found 1 stateful pods, waiting for 3
Sep 15 16:50:55.145: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:50:55.145: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 15 16:50:55.145: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 15 16:50:55.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-8266 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:50:55.318: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:50:55.318: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:50:55.318: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 15 16:50:55.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-8266 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:50:55.485: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:50:55.485: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:50:55.485: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 15 16:50:55.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-8266 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 15 16:50:55.661: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 15 16:50:55.661: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 15 16:50:55.661: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 15 16:50:55.661: INFO: Waiting for statefulset status.replicas updated to 0
Sep 15 16:50:55.665: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 15 16:51:05.675: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 15 16:51:05.675: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 15 16:51:05.675: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 15 16:51:05.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998462s
Sep 15 16:51:06.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992677301s
Sep 15 16:51:07.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987296975s
Sep 15 16:51:08.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981732708s
Sep 15 16:51:09.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976664256s
Sep 15 16:51:10.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971548747s
Sep 15 16:51:11.723: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966386658s
Sep 15 16:51:12.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961292786s
Sep 15 16:51:13.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955629563s
Sep 15 16:51:14.739: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.452403ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8266
Sep 15 16:51:15.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-8266 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 15 16:51:15.949: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 15 16:51:15.949: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 15 16:51:15.949: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 15 16:51:15.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-8266 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 15 16:51:16.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 15 16:51:16.130: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 15 16:51:16.130: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 15 16:51:16.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 exec --namespace=statefulset-8266 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 15 16:51:16.301: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 15 16:51:16.301: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 15 16:51:16.301: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 15 16:51:16.301: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 15 16:51:46.321: INFO: Deleting all statefulset in ns statefulset-8266
Sep 15 16:51:46.325: INFO: Scaling statefulset ss to 0
Sep 15 16:51:46.338: INFO: Waiting for statefulset status.replicas updated to 0
Sep 15 16:51:46.341: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:51:46.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8266" for this suite.
Sep 15 16:51:52.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:51:52.528: INFO: namespace statefulset-8266 deletion completed in 6.161354264s

• [SLOW TEST:98.293 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:51:52.528: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 15 16:51:52.687: INFO: Waiting up to 5m0s for pod "downward-api-ba527fb1-a5a5-43cf-8d16-d2d48919e45f" in namespace "downward-api-2103" to be "success or failure"
Sep 15 16:51:52.692: INFO: Pod "downward-api-ba527fb1-a5a5-43cf-8d16-d2d48919e45f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.612115ms
Sep 15 16:51:54.697: INFO: Pod "downward-api-ba527fb1-a5a5-43cf-8d16-d2d48919e45f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00945147s
STEP: Saw pod success
Sep 15 16:51:54.697: INFO: Pod "downward-api-ba527fb1-a5a5-43cf-8d16-d2d48919e45f" satisfied condition "success or failure"
Sep 15 16:51:54.701: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downward-api-ba527fb1-a5a5-43cf-8d16-d2d48919e45f container dapi-container: <nil>
STEP: delete the pod
Sep 15 16:51:54.734: INFO: Waiting for pod downward-api-ba527fb1-a5a5-43cf-8d16-d2d48919e45f to disappear
Sep 15 16:51:54.740: INFO: Pod downward-api-ba527fb1-a5a5-43cf-8d16-d2d48919e45f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:51:54.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2103" for this suite.
Sep 15 16:52:00.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:52:01.002: INFO: namespace downward-api-2103 deletion completed in 6.257854677s

• [SLOW TEST:8.474 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:52:01.003: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6247
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:52:01.149: INFO: Creating deployment "test-recreate-deployment"
Sep 15 16:52:01.155: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 15 16:52:01.164: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 15 16:52:03.173: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 15 16:52:03.177: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 15 16:52:03.187: INFO: Updating deployment test-recreate-deployment
Sep 15 16:52:03.187: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 15 16:52:03.264: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-6247,SelfLink:/apis/apps/v1/namespaces/deployment-6247/deployments/test-recreate-deployment,UID:9c9f3c20-aac3-4c5f-b690-186e74d98e7c,ResourceVersion:22871,Generation:2,CreationTimestamp:2019-09-15 16:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-15 16:52:03 +0000 UTC 2019-09-15 16:52:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-15 16:52:03 +0000 UTC 2019-09-15 16:52:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep 15 16:52:03.269: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-6247,SelfLink:/apis/apps/v1/namespaces/deployment-6247/replicasets/test-recreate-deployment-5c8c9cc69d,UID:62c3c295-c270-4e89-9378-f760e417adaa,ResourceVersion:22868,Generation:1,CreationTimestamp:2019-09-15 16:52:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9c9f3c20-aac3-4c5f-b690-186e74d98e7c 0xc003194c57 0xc003194c58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 15 16:52:03.269: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 15 16:52:03.269: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-6247,SelfLink:/apis/apps/v1/namespaces/deployment-6247/replicasets/test-recreate-deployment-6df85df6b9,UID:766496f3-4bb6-4f85-8c43-c0e0fe26b9e2,ResourceVersion:22859,Generation:2,CreationTimestamp:2019-09-15 16:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9c9f3c20-aac3-4c5f-b690-186e74d98e7c 0xc003194d27 0xc003194d28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 15 16:52:03.273: INFO: Pod "test-recreate-deployment-5c8c9cc69d-v2442" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-v2442,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-6247,SelfLink:/api/v1/namespaces/deployment-6247/pods/test-recreate-deployment-5c8c9cc69d-v2442,UID:1b464063-eb2f-4cbd-bc62-5c1bd8103dc7,ResourceVersion:22872,Generation:0,CreationTimestamp:2019-09-15 16:52:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 62c3c295-c270-4e89-9378-f760e417adaa 0xc003195617 0xc003195618}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jwh5l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jwh5l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jwh5l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003195680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031956a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:52:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:52:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:52:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:52:03 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:,StartTime:2019-09-15 16:52:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:52:03.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6247" for this suite.
Sep 15 16:52:09.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:52:09.594: INFO: namespace deployment-6247 deletion completed in 6.315790467s

• [SLOW TEST:8.592 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:52:09.594: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 15 16:52:09.750: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-a,UID:b5e69d5e-61b1-4357-963e-4882c82859c0,ResourceVersion:22918,Generation:0,CreationTimestamp:2019-09-15 16:52:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 15 16:52:09.750: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-a,UID:b5e69d5e-61b1-4357-963e-4882c82859c0,ResourceVersion:22918,Generation:0,CreationTimestamp:2019-09-15 16:52:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 15 16:52:19.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-a,UID:b5e69d5e-61b1-4357-963e-4882c82859c0,ResourceVersion:22940,Generation:0,CreationTimestamp:2019-09-15 16:52:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 15 16:52:19.761: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-a,UID:b5e69d5e-61b1-4357-963e-4882c82859c0,ResourceVersion:22940,Generation:0,CreationTimestamp:2019-09-15 16:52:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 15 16:52:29.771: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-a,UID:b5e69d5e-61b1-4357-963e-4882c82859c0,ResourceVersion:22963,Generation:0,CreationTimestamp:2019-09-15 16:52:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 15 16:52:29.771: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-a,UID:b5e69d5e-61b1-4357-963e-4882c82859c0,ResourceVersion:22963,Generation:0,CreationTimestamp:2019-09-15 16:52:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 15 16:52:39.785: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-a,UID:b5e69d5e-61b1-4357-963e-4882c82859c0,ResourceVersion:22984,Generation:0,CreationTimestamp:2019-09-15 16:52:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 15 16:52:39.785: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-a,UID:b5e69d5e-61b1-4357-963e-4882c82859c0,ResourceVersion:22984,Generation:0,CreationTimestamp:2019-09-15 16:52:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 15 16:52:49.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-b,UID:1f4133b7-cef6-477b-aba3-a4f238792a0f,ResourceVersion:23006,Generation:0,CreationTimestamp:2019-09-15 16:52:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 15 16:52:49.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-b,UID:1f4133b7-cef6-477b-aba3-a4f238792a0f,ResourceVersion:23006,Generation:0,CreationTimestamp:2019-09-15 16:52:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 15 16:52:59.809: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-b,UID:1f4133b7-cef6-477b-aba3-a4f238792a0f,ResourceVersion:23029,Generation:0,CreationTimestamp:2019-09-15 16:52:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 15 16:52:59.809: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7255,SelfLink:/api/v1/namespaces/watch-7255/configmaps/e2e-watch-test-configmap-b,UID:1f4133b7-cef6-477b-aba3-a4f238792a0f,ResourceVersion:23029,Generation:0,CreationTimestamp:2019-09-15 16:52:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:53:09.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7255" for this suite.
Sep 15 16:53:15.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:53:15.977: INFO: namespace watch-7255 deletion completed in 6.161289241s

• [SLOW TEST:66.382 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:53:15.977: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 15 16:53:16.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1464'
Sep 15 16:53:16.213: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 15 16:53:16.213: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Sep 15 16:53:16.219: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Sep 15 16:53:16.223: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep 15 16:53:16.231: INFO: scanned /root for discovery docs: <nil>
Sep 15 16:53:16.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1464'
Sep 15 16:53:32.059: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 15 16:53:32.059: INFO: stdout: "Created e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f\nScaling up e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep 15 16:53:32.059: INFO: stdout: "Created e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f\nScaling up e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep 15 16:53:32.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1464'
Sep 15 16:53:32.135: INFO: stderr: ""
Sep 15 16:53:32.135: INFO: stdout: "e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f-cb8gw e2e-test-nginx-rc-4tr7j "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Sep 15 16:53:37.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1464'
Sep 15 16:53:37.210: INFO: stderr: ""
Sep 15 16:53:37.210: INFO: stdout: "e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f-cb8gw "
Sep 15 16:53:37.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f-cb8gw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1464'
Sep 15 16:53:37.278: INFO: stderr: ""
Sep 15 16:53:37.278: INFO: stdout: "true"
Sep 15 16:53:37.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f-cb8gw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1464'
Sep 15 16:53:37.350: INFO: stderr: ""
Sep 15 16:53:37.350: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep 15 16:53:37.350: INFO: e2e-test-nginx-rc-2ef1b84caadb1eb64314f016be21b36f-cb8gw is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Sep 15 16:53:37.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete rc e2e-test-nginx-rc --namespace=kubectl-1464'
Sep 15 16:53:37.433: INFO: stderr: ""
Sep 15 16:53:37.433: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:53:37.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1464" for this suite.
Sep 15 16:53:59.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:53:59.614: INFO: namespace kubectl-1464 deletion completed in 22.168739482s

• [SLOW TEST:43.637 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:53:59.614: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:53:59.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70f46fb1-a1fa-4a7b-bd4b-00c14c1e809c" in namespace "projected-2343" to be "success or failure"
Sep 15 16:53:59.780: INFO: Pod "downwardapi-volume-70f46fb1-a1fa-4a7b-bd4b-00c14c1e809c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.268097ms
Sep 15 16:54:01.785: INFO: Pod "downwardapi-volume-70f46fb1-a1fa-4a7b-bd4b-00c14c1e809c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009162969s
STEP: Saw pod success
Sep 15 16:54:01.785: INFO: Pod "downwardapi-volume-70f46fb1-a1fa-4a7b-bd4b-00c14c1e809c" satisfied condition "success or failure"
Sep 15 16:54:01.789: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-70f46fb1-a1fa-4a7b-bd4b-00c14c1e809c container client-container: <nil>
STEP: delete the pod
Sep 15 16:54:01.822: INFO: Waiting for pod downwardapi-volume-70f46fb1-a1fa-4a7b-bd4b-00c14c1e809c to disappear
Sep 15 16:54:01.826: INFO: Pod downwardapi-volume-70f46fb1-a1fa-4a7b-bd4b-00c14c1e809c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:54:01.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2343" for this suite.
Sep 15 16:54:07.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:54:07.997: INFO: namespace projected-2343 deletion completed in 6.164429866s

• [SLOW TEST:8.383 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:54:07.997: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4519
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 15 16:54:08.155: INFO: Waiting up to 5m0s for pod "downward-api-6c9dafe3-3b85-461e-95b2-4932d8dd0fa9" in namespace "downward-api-4519" to be "success or failure"
Sep 15 16:54:08.162: INFO: Pod "downward-api-6c9dafe3-3b85-461e-95b2-4932d8dd0fa9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.716688ms
Sep 15 16:54:10.167: INFO: Pod "downward-api-6c9dafe3-3b85-461e-95b2-4932d8dd0fa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01172476s
STEP: Saw pod success
Sep 15 16:54:10.167: INFO: Pod "downward-api-6c9dafe3-3b85-461e-95b2-4932d8dd0fa9" satisfied condition "success or failure"
Sep 15 16:54:10.171: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downward-api-6c9dafe3-3b85-461e-95b2-4932d8dd0fa9 container dapi-container: <nil>
STEP: delete the pod
Sep 15 16:54:10.201: INFO: Waiting for pod downward-api-6c9dafe3-3b85-461e-95b2-4932d8dd0fa9 to disappear
Sep 15 16:54:10.209: INFO: Pod downward-api-6c9dafe3-3b85-461e-95b2-4932d8dd0fa9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:54:10.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4519" for this suite.
Sep 15 16:54:16.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:54:16.371: INFO: namespace downward-api-4519 deletion completed in 6.157284761s

• [SLOW TEST:8.374 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:54:16.371: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6078
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 15 16:54:16.533: INFO: Waiting up to 5m0s for pod "downward-api-5f730831-c2e4-4194-9804-f894478d1370" in namespace "downward-api-6078" to be "success or failure"
Sep 15 16:54:16.555: INFO: Pod "downward-api-5f730831-c2e4-4194-9804-f894478d1370": Phase="Pending", Reason="", readiness=false. Elapsed: 21.53027ms
Sep 15 16:54:18.560: INFO: Pod "downward-api-5f730831-c2e4-4194-9804-f894478d1370": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026185156s
STEP: Saw pod success
Sep 15 16:54:18.560: INFO: Pod "downward-api-5f730831-c2e4-4194-9804-f894478d1370" satisfied condition "success or failure"
Sep 15 16:54:18.564: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downward-api-5f730831-c2e4-4194-9804-f894478d1370 container dapi-container: <nil>
STEP: delete the pod
Sep 15 16:54:18.594: INFO: Waiting for pod downward-api-5f730831-c2e4-4194-9804-f894478d1370 to disappear
Sep 15 16:54:18.598: INFO: Pod downward-api-5f730831-c2e4-4194-9804-f894478d1370 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:54:18.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6078" for this suite.
Sep 15 16:54:24.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:54:24.776: INFO: namespace downward-api-6078 deletion completed in 6.172879545s

• [SLOW TEST:8.405 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:54:24.777: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 15 16:54:24.936: INFO: Waiting up to 5m0s for pod "pod-521549c7-ee23-44f0-8693-51da44764da6" in namespace "emptydir-9165" to be "success or failure"
Sep 15 16:54:24.941: INFO: Pod "pod-521549c7-ee23-44f0-8693-51da44764da6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.561512ms
Sep 15 16:54:26.946: INFO: Pod "pod-521549c7-ee23-44f0-8693-51da44764da6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009790423s
STEP: Saw pod success
Sep 15 16:54:26.946: INFO: Pod "pod-521549c7-ee23-44f0-8693-51da44764da6" satisfied condition "success or failure"
Sep 15 16:54:26.950: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-521549c7-ee23-44f0-8693-51da44764da6 container test-container: <nil>
STEP: delete the pod
Sep 15 16:54:26.978: INFO: Waiting for pod pod-521549c7-ee23-44f0-8693-51da44764da6 to disappear
Sep 15 16:54:26.982: INFO: Pod pod-521549c7-ee23-44f0-8693-51da44764da6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:54:26.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9165" for this suite.
Sep 15 16:54:33.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:54:33.143: INFO: namespace emptydir-9165 deletion completed in 6.155523416s

• [SLOW TEST:8.367 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:54:33.144: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7208
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-ddf51811-ca5c-4d81-b8d8-5fcd54c8e423
STEP: Creating configMap with name cm-test-opt-upd-32835dcb-6ea6-470e-a718-19ea9a65a308
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ddf51811-ca5c-4d81-b8d8-5fcd54c8e423
STEP: Updating configmap cm-test-opt-upd-32835dcb-6ea6-470e-a718-19ea9a65a308
STEP: Creating configMap with name cm-test-opt-create-4de59cd5-4c62-4c84-a0c2-02f56008d1a6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:54:39.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7208" for this suite.
Sep 15 16:55:01.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:55:01.640: INFO: namespace configmap-7208 deletion completed in 22.180177175s

• [SLOW TEST:28.496 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:55:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3430
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Sep 15 16:55:01.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-3430'
Sep 15 16:55:02.068: INFO: stderr: ""
Sep 15 16:55:02.068: INFO: stdout: "pod/pause created\n"
Sep 15 16:55:02.068: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 15 16:55:02.068: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3430" to be "running and ready"
Sep 15 16:55:02.073: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458796ms
Sep 15 16:55:04.079: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010551559s
Sep 15 16:55:06.083: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.015326332s
Sep 15 16:55:06.083: INFO: Pod "pause" satisfied condition "running and ready"
Sep 15 16:55:06.083: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 15 16:55:06.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 label pods pause testing-label=testing-label-value --namespace=kubectl-3430'
Sep 15 16:55:06.157: INFO: stderr: ""
Sep 15 16:55:06.157: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 15 16:55:06.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pod pause -L testing-label --namespace=kubectl-3430'
Sep 15 16:55:06.225: INFO: stderr: ""
Sep 15 16:55:06.225: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 15 16:55:06.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 label pods pause testing-label- --namespace=kubectl-3430'
Sep 15 16:55:06.308: INFO: stderr: ""
Sep 15 16:55:06.308: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 15 16:55:06.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pod pause -L testing-label --namespace=kubectl-3430'
Sep 15 16:55:06.380: INFO: stderr: ""
Sep 15 16:55:06.380: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Sep 15 16:55:06.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-3430'
Sep 15 16:55:06.475: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 16:55:06.475: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 15 16:55:06.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get rc,svc -l name=pause --no-headers --namespace=kubectl-3430'
Sep 15 16:55:06.563: INFO: stderr: "No resources found.\n"
Sep 15 16:55:06.563: INFO: stdout: ""
Sep 15 16:55:06.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -l name=pause --namespace=kubectl-3430 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 15 16:55:06.633: INFO: stderr: ""
Sep 15 16:55:06.633: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:55:06.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3430" for this suite.
Sep 15 16:55:12.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:55:12.799: INFO: namespace kubectl-3430 deletion completed in 6.160772475s

• [SLOW TEST:11.159 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:55:12.800: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:55:12.958: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a535514f-b7b3-40f0-b916-b1e1a38390ad" in namespace "downward-api-6064" to be "success or failure"
Sep 15 16:55:12.966: INFO: Pod "downwardapi-volume-a535514f-b7b3-40f0-b916-b1e1a38390ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056226ms
Sep 15 16:55:14.971: INFO: Pod "downwardapi-volume-a535514f-b7b3-40f0-b916-b1e1a38390ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01277566s
STEP: Saw pod success
Sep 15 16:55:14.971: INFO: Pod "downwardapi-volume-a535514f-b7b3-40f0-b916-b1e1a38390ad" satisfied condition "success or failure"
Sep 15 16:55:14.976: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-a535514f-b7b3-40f0-b916-b1e1a38390ad container client-container: <nil>
STEP: delete the pod
Sep 15 16:55:15.009: INFO: Waiting for pod downwardapi-volume-a535514f-b7b3-40f0-b916-b1e1a38390ad to disappear
Sep 15 16:55:15.013: INFO: Pod downwardapi-volume-a535514f-b7b3-40f0-b916-b1e1a38390ad no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:55:15.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6064" for this suite.
Sep 15 16:55:21.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:55:21.176: INFO: namespace downward-api-6064 deletion completed in 6.156171789s

• [SLOW TEST:8.376 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:55:21.176: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7673
STEP: Creating secret with name secret-test-70f5027f-172a-4b6a-9a7a-04e27d78429e
STEP: Creating a pod to test consume secrets
Sep 15 16:55:21.501: INFO: Waiting up to 5m0s for pod "pod-secrets-e312cd77-3519-42c5-93fe-0d960aa38afc" in namespace "secrets-2272" to be "success or failure"
Sep 15 16:55:21.506: INFO: Pod "pod-secrets-e312cd77-3519-42c5-93fe-0d960aa38afc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.670407ms
Sep 15 16:55:23.515: INFO: Pod "pod-secrets-e312cd77-3519-42c5-93fe-0d960aa38afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013848632s
STEP: Saw pod success
Sep 15 16:55:23.515: INFO: Pod "pod-secrets-e312cd77-3519-42c5-93fe-0d960aa38afc" satisfied condition "success or failure"
Sep 15 16:55:23.521: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-secrets-e312cd77-3519-42c5-93fe-0d960aa38afc container secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:55:23.554: INFO: Waiting for pod pod-secrets-e312cd77-3519-42c5-93fe-0d960aa38afc to disappear
Sep 15 16:55:23.559: INFO: Pod pod-secrets-e312cd77-3519-42c5-93fe-0d960aa38afc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:55:23.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2272" for this suite.
Sep 15 16:55:29.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:55:29.722: INFO: namespace secrets-2272 deletion completed in 6.158116796s
STEP: Destroying namespace "secret-namespace-7673" for this suite.
Sep 15 16:55:35.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:55:35.878: INFO: namespace secret-namespace-7673 deletion completed in 6.155261913s

• [SLOW TEST:14.702 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:55:35.878: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-50
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:55:38.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-50" for this suite.
Sep 15 16:56:16.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:56:16.228: INFO: namespace kubelet-test-50 deletion completed in 38.156114505s

• [SLOW TEST:40.350 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:56:16.228: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8042
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:56:21.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8042" for this suite.
Sep 15 16:56:27.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:56:28.136: INFO: namespace watch-8042 deletion completed in 6.247216604s

• [SLOW TEST:11.908 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:56:28.136: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep 15 16:56:28.283: INFO: PodSpec: initContainers in spec.initContainers
Sep 15 16:57:06.350: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-066ff26a-071c-4391-aa31-933c8a4f505c", GenerateName:"", Namespace:"init-container-5330", SelfLink:"/api/v1/namespaces/init-container-5330/pods/pod-init-066ff26a-071c-4391-aa31-933c8a4f505c", UID:"a15cea80-7cfc-470f-ac59-e97175c47d12", ResourceVersion:"24080", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63704163388, loc:(*time.Location)(0x7ec7a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"283755862"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.2.217/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-j2j8d", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00226e840), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j2j8d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j2j8d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j2j8d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003536288), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-16-34-210.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002a44060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003536300)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003536320)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003536328), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00353632c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704163388, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704163388, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704163388, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704163388, loc:(*time.Location)(0x7ec7a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.34.210", PodIP:"100.96.2.217", StartTime:(*v1.Time)(0xc003810160), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000678230)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006782a0)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://356b1bf95848c32ec5e270851459297cbc1e54bbc8a67c4d11c8c3899f92b06e"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038101a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003810180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:57:06.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5330" for this suite.
Sep 15 16:57:28.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:57:28.537: INFO: namespace init-container-5330 deletion completed in 22.180572216s

• [SLOW TEST:60.401 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:57:28.537: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-516
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-516.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-516.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-516.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-516.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-516.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-516.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-516.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-516.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-516.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-516.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-516.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-516.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-516.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 127.231.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.231.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.231.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.231.127_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-516.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-516.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-516.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-516.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-516.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-516.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-516.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-516.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-516.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-516.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-516.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-516.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-516.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 127.231.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.231.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.231.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.231.127_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 15 16:57:30.782: INFO: Unable to read wheezy_udp@dns-test-service.dns-516.svc.cluster.local from pod dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294: the server could not find the requested resource (get pods dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294)
Sep 15 16:57:30.788: INFO: Unable to read wheezy_tcp@dns-test-service.dns-516.svc.cluster.local from pod dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294: the server could not find the requested resource (get pods dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294)
Sep 15 16:57:30.793: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-516.svc.cluster.local from pod dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294: the server could not find the requested resource (get pods dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294)
Sep 15 16:57:30.799: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-516.svc.cluster.local from pod dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294: the server could not find the requested resource (get pods dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294)
Sep 15 16:57:30.857: INFO: Unable to read jessie_udp@dns-test-service.dns-516.svc.cluster.local from pod dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294: the server could not find the requested resource (get pods dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294)
Sep 15 16:57:30.863: INFO: Unable to read jessie_tcp@dns-test-service.dns-516.svc.cluster.local from pod dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294: the server could not find the requested resource (get pods dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294)
Sep 15 16:57:30.869: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-516.svc.cluster.local from pod dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294: the server could not find the requested resource (get pods dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294)
Sep 15 16:57:30.876: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-516.svc.cluster.local from pod dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294: the server could not find the requested resource (get pods dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294)
Sep 15 16:57:30.910: INFO: Lookups using dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294 failed for: [wheezy_udp@dns-test-service.dns-516.svc.cluster.local wheezy_tcp@dns-test-service.dns-516.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-516.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-516.svc.cluster.local jessie_udp@dns-test-service.dns-516.svc.cluster.local jessie_tcp@dns-test-service.dns-516.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-516.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-516.svc.cluster.local]

Sep 15 16:57:36.046: INFO: DNS probes using dns-516/dns-test-da419b7b-6e3c-41cf-ab61-fc53d3acd294 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:57:36.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-516" for this suite.
Sep 15 16:57:42.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:57:42.495: INFO: namespace dns-516 deletion completed in 6.329323082s

• [SLOW TEST:13.959 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:57:42.496: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 16:57:42.655: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd930b05-3c5d-49b2-8167-3f72d2c7ffbe" in namespace "downward-api-8101" to be "success or failure"
Sep 15 16:57:42.661: INFO: Pod "downwardapi-volume-cd930b05-3c5d-49b2-8167-3f72d2c7ffbe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.309229ms
Sep 15 16:57:44.667: INFO: Pod "downwardapi-volume-cd930b05-3c5d-49b2-8167-3f72d2c7ffbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012586462s
STEP: Saw pod success
Sep 15 16:57:44.668: INFO: Pod "downwardapi-volume-cd930b05-3c5d-49b2-8167-3f72d2c7ffbe" satisfied condition "success or failure"
Sep 15 16:57:44.672: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-cd930b05-3c5d-49b2-8167-3f72d2c7ffbe container client-container: <nil>
STEP: delete the pod
Sep 15 16:57:44.711: INFO: Waiting for pod downwardapi-volume-cd930b05-3c5d-49b2-8167-3f72d2c7ffbe to disappear
Sep 15 16:57:44.716: INFO: Pod downwardapi-volume-cd930b05-3c5d-49b2-8167-3f72d2c7ffbe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:57:44.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8101" for this suite.
Sep 15 16:57:50.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:57:50.891: INFO: namespace downward-api-8101 deletion completed in 6.162075684s

• [SLOW TEST:8.395 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:57:50.891: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6203, will wait for the garbage collector to delete the pods
Sep 15 16:57:53.116: INFO: Deleting Job.batch foo took: 12.108595ms
Sep 15 16:57:53.517: INFO: Terminating Job.batch foo pods took: 400.302691ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:58:25.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6203" for this suite.
Sep 15 16:58:31.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:58:31.700: INFO: namespace job-6203 deletion completed in 6.173505393s

• [SLOW TEST:40.809 seconds]
[sig-apps] Job
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:58:31.700: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3536
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-pgvt
STEP: Creating a pod to test atomic-volume-subpath
Sep 15 16:58:31.880: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pgvt" in namespace "subpath-3536" to be "success or failure"
Sep 15 16:58:31.886: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.454224ms
Sep 15 16:58:33.891: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 2.011336568s
Sep 15 16:58:35.896: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 4.0164629s
Sep 15 16:58:37.901: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 6.021684073s
Sep 15 16:58:39.906: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 8.026582671s
Sep 15 16:58:41.911: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 10.031384734s
Sep 15 16:58:43.915: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 12.035846639s
Sep 15 16:58:45.920: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 14.040879327s
Sep 15 16:58:47.925: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 16.045414524s
Sep 15 16:58:49.930: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 18.050305331s
Sep 15 16:58:51.935: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Running", Reason="", readiness=true. Elapsed: 20.05537195s
Sep 15 16:58:53.940: INFO: Pod "pod-subpath-test-configmap-pgvt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.060262666s
STEP: Saw pod success
Sep 15 16:58:53.940: INFO: Pod "pod-subpath-test-configmap-pgvt" satisfied condition "success or failure"
Sep 15 16:58:53.944: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-subpath-test-configmap-pgvt container test-container-subpath-configmap-pgvt: <nil>
STEP: delete the pod
Sep 15 16:58:53.976: INFO: Waiting for pod pod-subpath-test-configmap-pgvt to disappear
Sep 15 16:58:53.981: INFO: Pod pod-subpath-test-configmap-pgvt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pgvt
Sep 15 16:58:53.981: INFO: Deleting pod "pod-subpath-test-configmap-pgvt" in namespace "subpath-3536"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:58:53.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3536" for this suite.
Sep 15 16:59:00.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:59:00.167: INFO: namespace subpath-3536 deletion completed in 6.176697997s

• [SLOW TEST:28.467 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:59:00.167: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-3b5a313c-23ae-4b79-b5a2-82d63f9a47d8
STEP: Creating a pod to test consume secrets
Sep 15 16:59:00.331: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b2489323-beb4-4749-9775-5d6f5ca15ad7" in namespace "projected-2164" to be "success or failure"
Sep 15 16:59:00.335: INFO: Pod "pod-projected-secrets-b2489323-beb4-4749-9775-5d6f5ca15ad7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.177405ms
Sep 15 16:59:02.340: INFO: Pod "pod-projected-secrets-b2489323-beb4-4749-9775-5d6f5ca15ad7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009342951s
STEP: Saw pod success
Sep 15 16:59:02.340: INFO: Pod "pod-projected-secrets-b2489323-beb4-4749-9775-5d6f5ca15ad7" satisfied condition "success or failure"
Sep 15 16:59:02.344: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-secrets-b2489323-beb4-4749-9775-5d6f5ca15ad7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:59:02.378: INFO: Waiting for pod pod-projected-secrets-b2489323-beb4-4749-9775-5d6f5ca15ad7 to disappear
Sep 15 16:59:02.384: INFO: Pod pod-projected-secrets-b2489323-beb4-4749-9775-5d6f5ca15ad7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:59:02.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2164" for this suite.
Sep 15 16:59:08.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:59:08.552: INFO: namespace projected-2164 deletion completed in 6.162084632s

• [SLOW TEST:8.385 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:59:08.552: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8240
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-e4b6f112-4d0d-47c4-8699-0020ced77699
STEP: Creating a pod to test consume configMaps
Sep 15 16:59:08.713: INFO: Waiting up to 5m0s for pod "pod-configmaps-308b807e-4eb3-409c-af1b-a0e199279364" in namespace "configmap-8240" to be "success or failure"
Sep 15 16:59:08.720: INFO: Pod "pod-configmaps-308b807e-4eb3-409c-af1b-a0e199279364": Phase="Pending", Reason="", readiness=false. Elapsed: 6.618068ms
Sep 15 16:59:10.725: INFO: Pod "pod-configmaps-308b807e-4eb3-409c-af1b-a0e199279364": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011402987s
STEP: Saw pod success
Sep 15 16:59:10.725: INFO: Pod "pod-configmaps-308b807e-4eb3-409c-af1b-a0e199279364" satisfied condition "success or failure"
Sep 15 16:59:10.729: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-308b807e-4eb3-409c-af1b-a0e199279364 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 16:59:10.757: INFO: Waiting for pod pod-configmaps-308b807e-4eb3-409c-af1b-a0e199279364 to disappear
Sep 15 16:59:10.762: INFO: Pod pod-configmaps-308b807e-4eb3-409c-af1b-a0e199279364 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:59:10.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8240" for this suite.
Sep 15 16:59:16.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:59:16.920: INFO: namespace configmap-8240 deletion completed in 6.152598733s

• [SLOW TEST:8.367 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:59:16.920: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-dac852bf-ddab-4907-a835-5fb46e5be7d6
STEP: Creating a pod to test consume secrets
Sep 15 16:59:17.083: INFO: Waiting up to 5m0s for pod "pod-secrets-75cb7f70-51c2-43eb-9477-93735c6da217" in namespace "secrets-4036" to be "success or failure"
Sep 15 16:59:17.089: INFO: Pod "pod-secrets-75cb7f70-51c2-43eb-9477-93735c6da217": Phase="Pending", Reason="", readiness=false. Elapsed: 5.979909ms
Sep 15 16:59:19.094: INFO: Pod "pod-secrets-75cb7f70-51c2-43eb-9477-93735c6da217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010815086s
STEP: Saw pod success
Sep 15 16:59:19.094: INFO: Pod "pod-secrets-75cb7f70-51c2-43eb-9477-93735c6da217" satisfied condition "success or failure"
Sep 15 16:59:19.098: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-secrets-75cb7f70-51c2-43eb-9477-93735c6da217 container secret-volume-test: <nil>
STEP: delete the pod
Sep 15 16:59:19.127: INFO: Waiting for pod pod-secrets-75cb7f70-51c2-43eb-9477-93735c6da217 to disappear
Sep 15 16:59:19.131: INFO: Pod pod-secrets-75cb7f70-51c2-43eb-9477-93735c6da217 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:59:19.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4036" for this suite.
Sep 15 16:59:25.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:59:25.314: INFO: namespace secrets-4036 deletion completed in 6.177021445s

• [SLOW TEST:8.394 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:59:25.314: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 16:59:25.478: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 15 16:59:30.483: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 15 16:59:30.483: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 15 16:59:30.511: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-9115,SelfLink:/apis/apps/v1/namespaces/deployment-9115/deployments/test-cleanup-deployment,UID:5b424377-90db-4a21-8938-0c9feaf93eff,ResourceVersion:24676,Generation:1,CreationTimestamp:2019-09-15 16:59:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Sep 15 16:59:30.520: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-9115,SelfLink:/apis/apps/v1/namespaces/deployment-9115/replicasets/test-cleanup-deployment-55bbcbc84c,UID:73eeeee1-75ff-4219-83ba-977dd606bdf0,ResourceVersion:24678,Generation:1,CreationTimestamp:2019-09-15 16:59:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 5b424377-90db-4a21-8938-0c9feaf93eff 0xc003543cf7 0xc003543cf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 15 16:59:30.520: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 15 16:59:30.520: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-9115,SelfLink:/apis/apps/v1/namespaces/deployment-9115/replicasets/test-cleanup-controller,UID:784887fd-4496-4369-bc44-ebe60384dd1c,ResourceVersion:24677,Generation:1,CreationTimestamp:2019-09-15 16:59:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 5b424377-90db-4a21-8938-0c9feaf93eff 0xc003543c27 0xc003543c28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 15 16:59:30.529: INFO: Pod "test-cleanup-controller-vqqhs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-vqqhs,GenerateName:test-cleanup-controller-,Namespace:deployment-9115,SelfLink:/api/v1/namespaces/deployment-9115/pods/test-cleanup-controller-vqqhs,UID:e861d930-636f-4960-b264-a43734f6b613,ResourceVersion:24666,Generation:0,CreationTimestamp:2019-09-15 16:59:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.2.225/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 784887fd-4496-4369-bc44-ebe60384dd1c 0xc000918957 0xc000918958}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sg8kj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sg8kj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sg8kj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-34-210.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0009189c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0009189e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:59:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:59:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:59:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-15 16:59:25 +0000 UTC  }],Message:,Reason:,HostIP:172.16.34.210,PodIP:100.96.2.225,StartTime:2019-09-15 16:59:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-15 16:59:26 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://53ac1e2dc6f9660cb643ebc008f25944b7463ce6a11999315244562347d04b09}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 15 16:59:30.529: INFO: Pod "test-cleanup-deployment-55bbcbc84c-sr7zz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-sr7zz,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-9115,SelfLink:/api/v1/namespaces/deployment-9115/pods/test-cleanup-deployment-55bbcbc84c-sr7zz,UID:e2e6528c-f96c-409d-93b4-e6a3142ec8d4,ResourceVersion:24681,Generation:0,CreationTimestamp:2019-09-15 16:59:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 73eeeee1-75ff-4219-83ba-977dd606bdf0 0xc000918ac7 0xc000918ac8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sg8kj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sg8kj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-sg8kj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000918b30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000918b50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:59:30.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9115" for this suite.
Sep 15 16:59:36.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:59:36.799: INFO: namespace deployment-9115 deletion completed in 6.209766098s

• [SLOW TEST:11.485 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:59:36.799: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 15 16:59:36.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3268'
Sep 15 16:59:37.030: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 15 16:59:37.030: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Sep 15 16:59:37.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete jobs e2e-test-nginx-job --namespace=kubectl-3268'
Sep 15 16:59:37.126: INFO: stderr: ""
Sep 15 16:59:37.126: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:59:37.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3268" for this suite.
Sep 15 16:59:43.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:59:43.300: INFO: namespace kubectl-3268 deletion completed in 6.165316943s

• [SLOW TEST:6.501 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:59:43.300: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep 15 16:59:43.446: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 16:59:47.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2811" for this suite.
Sep 15 16:59:53.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 16:59:53.241: INFO: namespace init-container-2811 deletion completed in 6.154773604s

• [SLOW TEST:9.941 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 16:59:53.241: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 15 16:59:57.461: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 16:59:57.466: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 16:59:59.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 16:59:59.471: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 17:00:01.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 17:00:01.471: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 17:00:03.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 17:00:03.471: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 17:00:05.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 17:00:05.471: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 17:00:07.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 17:00:07.471: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 17:00:09.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 17:00:09.471: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 17:00:11.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 17:00:11.470: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 17:00:13.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 17:00:13.470: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 17:00:15.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 17:00:15.471: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 15 17:00:17.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 15 17:00:17.471: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:00:17.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1921" for this suite.
Sep 15 17:00:39.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:00:39.639: INFO: namespace container-lifecycle-hook-1921 deletion completed in 22.162522075s

• [SLOW TEST:46.398 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:00:39.640: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Sep 15 17:00:40.317: INFO: created pod pod-service-account-defaultsa
Sep 15 17:00:40.317: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 15 17:00:40.326: INFO: created pod pod-service-account-mountsa
Sep 15 17:00:40.326: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 15 17:00:40.334: INFO: created pod pod-service-account-nomountsa
Sep 15 17:00:40.334: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 15 17:00:40.343: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 15 17:00:40.343: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 15 17:00:40.355: INFO: created pod pod-service-account-mountsa-mountspec
Sep 15 17:00:40.356: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 15 17:00:40.364: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 15 17:00:40.364: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 15 17:00:40.375: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 15 17:00:40.375: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 15 17:00:40.390: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 15 17:00:40.390: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 15 17:00:40.399: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 15 17:00:40.399: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:00:40.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4783" for this suite.
Sep 15 17:00:46.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:00:46.564: INFO: namespace svcaccounts-4783 deletion completed in 6.157551162s

• [SLOW TEST:6.925 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:00:46.565: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-50ef87c7-e64b-4bc0-b134-f83ec8b0aacd
STEP: Creating a pod to test consume configMaps
Sep 15 17:00:46.729: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3037e23c-be2e-41f5-b593-370b38202320" in namespace "projected-8054" to be "success or failure"
Sep 15 17:00:46.735: INFO: Pod "pod-projected-configmaps-3037e23c-be2e-41f5-b593-370b38202320": Phase="Pending", Reason="", readiness=false. Elapsed: 6.523879ms
Sep 15 17:00:48.740: INFO: Pod "pod-projected-configmaps-3037e23c-be2e-41f5-b593-370b38202320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011487617s
STEP: Saw pod success
Sep 15 17:00:48.740: INFO: Pod "pod-projected-configmaps-3037e23c-be2e-41f5-b593-370b38202320" satisfied condition "success or failure"
Sep 15 17:00:48.744: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-configmaps-3037e23c-be2e-41f5-b593-370b38202320 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 17:00:48.775: INFO: Waiting for pod pod-projected-configmaps-3037e23c-be2e-41f5-b593-370b38202320 to disappear
Sep 15 17:00:48.780: INFO: Pod pod-projected-configmaps-3037e23c-be2e-41f5-b593-370b38202320 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:00:48.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8054" for this suite.
Sep 15 17:00:54.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:00:54.950: INFO: namespace projected-8054 deletion completed in 6.164122739s

• [SLOW TEST:8.385 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:00:54.950: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 15 17:00:55.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2210'
Sep 15 17:00:55.367: INFO: stderr: ""
Sep 15 17:00:55.367: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Sep 15 17:00:55.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete pods e2e-test-nginx-pod --namespace=kubectl-2210'
Sep 15 17:01:02.865: INFO: stderr: ""
Sep 15 17:01:02.865: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:01:02.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2210" for this suite.
Sep 15 17:01:08.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:01:09.055: INFO: namespace kubectl-2210 deletion completed in 6.183983524s

• [SLOW TEST:14.105 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:01:09.055: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-9066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Sep 15 17:01:09.218: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9066" to be "success or failure"
Sep 15 17:01:09.223: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18296ms
Sep 15 17:01:11.228: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009268359s
STEP: Saw pod success
Sep 15 17:01:11.228: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep 15 17:01:11.232: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 15 17:01:11.261: INFO: Waiting for pod pod-host-path-test to disappear
Sep 15 17:01:11.266: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:01:11.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9066" for this suite.
Sep 15 17:01:17.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:01:17.435: INFO: namespace hostpath-9066 deletion completed in 6.161396933s

• [SLOW TEST:8.380 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:01:17.435: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3186
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-eba338b3-9549-4530-9069-41911191f822
STEP: Creating secret with name s-test-opt-upd-4279e195-91b5-4018-821a-663fad255ffe
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-eba338b3-9549-4530-9069-41911191f822
STEP: Updating secret s-test-opt-upd-4279e195-91b5-4018-821a-663fad255ffe
STEP: Creating secret with name s-test-opt-create-b7dd10f8-6469-4627-8a80-a49b9e5aa425
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:01:21.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3186" for this suite.
Sep 15 17:01:43.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:01:43.908: INFO: namespace secrets-3186 deletion completed in 22.163163937s

• [SLOW TEST:26.473 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:01:43.908: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9685
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep 15 17:02:14.604: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0915 17:02:14.604461      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:02:14.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9685" for this suite.
Sep 15 17:02:20.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:02:20.775: INFO: namespace gc-9685 deletion completed in 6.166431243s

• [SLOW TEST:36.867 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:02:20.776: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2907
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 15 17:02:23.468: INFO: Successfully updated pod "pod-update-e1dfcad1-d2d2-4768-8d4f-03777faee860"
STEP: verifying the updated pod is in kubernetes
Sep 15 17:02:23.477: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:02:23.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2907" for this suite.
Sep 15 17:02:45.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:02:45.788: INFO: namespace pods-2907 deletion completed in 22.305320667s

• [SLOW TEST:25.012 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:02:45.788: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 15 17:02:46.014: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:46.014: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:46.014: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:46.018: INFO: Number of nodes with available pods: 0
Sep 15 17:02:46.019: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:47.025: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:47.025: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:47.025: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:47.029: INFO: Number of nodes with available pods: 0
Sep 15 17:02:47.029: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:48.025: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:48.025: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:48.025: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:48.029: INFO: Number of nodes with available pods: 3
Sep 15 17:02:48.029: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 15 17:02:48.052: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:48.052: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:48.052: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:48.056: INFO: Number of nodes with available pods: 2
Sep 15 17:02:48.056: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:49.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:49.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:49.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:49.067: INFO: Number of nodes with available pods: 2
Sep 15 17:02:49.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:50.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:50.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:50.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:50.067: INFO: Number of nodes with available pods: 2
Sep 15 17:02:50.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:51.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:51.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:51.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:51.067: INFO: Number of nodes with available pods: 2
Sep 15 17:02:51.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:52.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:52.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:52.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:52.069: INFO: Number of nodes with available pods: 2
Sep 15 17:02:52.069: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:53.063: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:53.063: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:53.063: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:53.067: INFO: Number of nodes with available pods: 2
Sep 15 17:02:53.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:54.063: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:54.063: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:54.063: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:54.067: INFO: Number of nodes with available pods: 2
Sep 15 17:02:54.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:55.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:55.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:55.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:55.067: INFO: Number of nodes with available pods: 2
Sep 15 17:02:55.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:56.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:56.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:56.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:56.067: INFO: Number of nodes with available pods: 2
Sep 15 17:02:56.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:57.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:57.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:57.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:57.067: INFO: Number of nodes with available pods: 2
Sep 15 17:02:57.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:58.063: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:58.063: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:58.063: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:58.067: INFO: Number of nodes with available pods: 2
Sep 15 17:02:58.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:02:59.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:59.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:59.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:02:59.066: INFO: Number of nodes with available pods: 2
Sep 15 17:02:59.066: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:03:00.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:00.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:00.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:00.067: INFO: Number of nodes with available pods: 2
Sep 15 17:03:00.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:03:01.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:01.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:01.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:01.066: INFO: Number of nodes with available pods: 2
Sep 15 17:03:01.066: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:03:02.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:02.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:02.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:02.067: INFO: Number of nodes with available pods: 2
Sep 15 17:03:02.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:03:03.063: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:03.063: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:03.063: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:03.067: INFO: Number of nodes with available pods: 2
Sep 15 17:03:03.067: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:03:04.062: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:04.062: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:04.062: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:03:04.066: INFO: Number of nodes with available pods: 3
Sep 15 17:03:04.067: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3105, will wait for the garbage collector to delete the pods
Sep 15 17:03:04.137: INFO: Deleting DaemonSet.extensions daemon-set took: 12.761316ms
Sep 15 17:03:04.538: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.439193ms
Sep 15 17:03:12.943: INFO: Number of nodes with available pods: 0
Sep 15 17:03:12.943: INFO: Number of running nodes: 0, number of available pods: 0
Sep 15 17:03:12.948: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3105/daemonsets","resourceVersion":"25703"},"items":null}

Sep 15 17:03:12.951: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3105/pods","resourceVersion":"25703"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:03:12.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3105" for this suite.
Sep 15 17:03:18.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:03:19.136: INFO: namespace daemonsets-3105 deletion completed in 6.162323651s

• [SLOW TEST:33.348 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:03:19.136: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep 15 17:03:19.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-6080'
Sep 15 17:03:19.472: INFO: stderr: ""
Sep 15 17:03:19.472: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 15 17:03:19.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6080'
Sep 15 17:03:19.555: INFO: stderr: ""
Sep 15 17:03:19.555: INFO: stdout: "update-demo-nautilus-hsqcq update-demo-nautilus-mjcjv "
Sep 15 17:03:19.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-hsqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6080'
Sep 15 17:03:19.628: INFO: stderr: ""
Sep 15 17:03:19.628: INFO: stdout: ""
Sep 15 17:03:19.628: INFO: update-demo-nautilus-hsqcq is created but not running
Sep 15 17:03:24.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6080'
Sep 15 17:03:24.701: INFO: stderr: ""
Sep 15 17:03:24.701: INFO: stdout: "update-demo-nautilus-hsqcq update-demo-nautilus-mjcjv "
Sep 15 17:03:24.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-hsqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6080'
Sep 15 17:03:24.778: INFO: stderr: ""
Sep 15 17:03:24.778: INFO: stdout: "true"
Sep 15 17:03:24.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-hsqcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6080'
Sep 15 17:03:24.845: INFO: stderr: ""
Sep 15 17:03:24.845: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 15 17:03:24.845: INFO: validating pod update-demo-nautilus-hsqcq
Sep 15 17:03:24.853: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 15 17:03:24.853: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 15 17:03:24.853: INFO: update-demo-nautilus-hsqcq is verified up and running
Sep 15 17:03:24.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-mjcjv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6080'
Sep 15 17:03:24.925: INFO: stderr: ""
Sep 15 17:03:24.925: INFO: stdout: "true"
Sep 15 17:03:24.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods update-demo-nautilus-mjcjv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6080'
Sep 15 17:03:24.998: INFO: stderr: ""
Sep 15 17:03:24.998: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 15 17:03:24.998: INFO: validating pod update-demo-nautilus-mjcjv
Sep 15 17:03:25.005: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 15 17:03:25.005: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 15 17:03:25.005: INFO: update-demo-nautilus-mjcjv is verified up and running
STEP: using delete to clean up resources
Sep 15 17:03:25.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-6080'
Sep 15 17:03:25.095: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 17:03:25.095: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 15 17:03:25.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6080'
Sep 15 17:03:25.177: INFO: stderr: "No resources found.\n"
Sep 15 17:03:25.177: INFO: stdout: ""
Sep 15 17:03:25.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -l name=update-demo --namespace=kubectl-6080 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 15 17:03:25.249: INFO: stderr: ""
Sep 15 17:03:25.250: INFO: stdout: "update-demo-nautilus-hsqcq\nupdate-demo-nautilus-mjcjv\n"
Sep 15 17:03:25.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6080'
Sep 15 17:03:25.831: INFO: stderr: "No resources found.\n"
Sep 15 17:03:25.831: INFO: stdout: ""
Sep 15 17:03:25.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -l name=update-demo --namespace=kubectl-6080 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 15 17:03:25.907: INFO: stderr: ""
Sep 15 17:03:25.907: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:03:25.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6080" for this suite.
Sep 15 17:03:47.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:03:48.071: INFO: namespace kubectl-6080 deletion completed in 22.158842276s

• [SLOW TEST:28.935 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:03:48.072: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 15 17:03:52.278: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:03:52.283: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:03:54.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:03:54.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:03:56.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:03:56.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:03:58.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:03:58.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:04:00.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:04:00.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:04:02.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:04:02.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:04:04.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:04:04.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:04:06.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:04:06.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:04:08.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:04:08.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:04:10.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:04:10.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:04:12.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:04:12.288: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 15 17:04:14.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 15 17:04:14.288: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:04:14.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3509" for this suite.
Sep 15 17:04:36.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:04:36.472: INFO: namespace container-lifecycle-hook-3509 deletion completed in 22.163930023s

• [SLOW TEST:48.400 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:04:36.472: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 15 17:04:40.691: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 15 17:04:40.695: INFO: Pod pod-with-poststart-http-hook still exists
Sep 15 17:04:42.695: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 15 17:04:42.701: INFO: Pod pod-with-poststart-http-hook still exists
Sep 15 17:04:44.695: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 15 17:04:44.700: INFO: Pod pod-with-poststart-http-hook still exists
Sep 15 17:04:46.695: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 15 17:04:46.702: INFO: Pod pod-with-poststart-http-hook still exists
Sep 15 17:04:48.695: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 15 17:04:48.700: INFO: Pod pod-with-poststart-http-hook still exists
Sep 15 17:04:50.695: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 15 17:04:50.700: INFO: Pod pod-with-poststart-http-hook still exists
Sep 15 17:04:52.695: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 15 17:04:52.700: INFO: Pod pod-with-poststart-http-hook still exists
Sep 15 17:04:54.695: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 15 17:04:54.700: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:04:54.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9794" for this suite.
Sep 15 17:05:16.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:05:16.872: INFO: namespace container-lifecycle-hook-9794 deletion completed in 22.166422314s

• [SLOW TEST:40.400 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:05:16.873: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5072
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:05:19.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5072" for this suite.
Sep 15 17:05:25.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:05:25.274: INFO: namespace emptydir-wrapper-5072 deletion completed in 6.166596679s

• [SLOW TEST:8.402 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:05:25.275: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 17:05:25.436: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18567d30-809a-427c-91fc-36aa873803c1" in namespace "downward-api-7194" to be "success or failure"
Sep 15 17:05:25.442: INFO: Pod "downwardapi-volume-18567d30-809a-427c-91fc-36aa873803c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.722059ms
Sep 15 17:05:27.448: INFO: Pod "downwardapi-volume-18567d30-809a-427c-91fc-36aa873803c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01201704s
STEP: Saw pod success
Sep 15 17:05:27.448: INFO: Pod "downwardapi-volume-18567d30-809a-427c-91fc-36aa873803c1" satisfied condition "success or failure"
Sep 15 17:05:27.452: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-18567d30-809a-427c-91fc-36aa873803c1 container client-container: <nil>
STEP: delete the pod
Sep 15 17:05:27.483: INFO: Waiting for pod downwardapi-volume-18567d30-809a-427c-91fc-36aa873803c1 to disappear
Sep 15 17:05:27.489: INFO: Pod downwardapi-volume-18567d30-809a-427c-91fc-36aa873803c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:05:27.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7194" for this suite.
Sep 15 17:05:33.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:05:33.651: INFO: namespace downward-api-7194 deletion completed in 6.156545298s

• [SLOW TEST:8.376 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:05:33.651: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-1995
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1995
STEP: Deleting pre-stop pod
Sep 15 17:05:44.854: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:05:44.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1995" for this suite.
Sep 15 17:06:24.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:06:25.034: INFO: namespace prestop-1995 deletion completed in 40.165752581s

• [SLOW TEST:51.384 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:06:25.034: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-3300e03e-c399-4b7d-99f8-cce656c90398
STEP: Creating a pod to test consume secrets
Sep 15 17:06:25.198: INFO: Waiting up to 5m0s for pod "pod-secrets-50b6579a-ef0a-42de-8e5f-9d079920b2c3" in namespace "secrets-5653" to be "success or failure"
Sep 15 17:06:25.206: INFO: Pod "pod-secrets-50b6579a-ef0a-42de-8e5f-9d079920b2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.060874ms
Sep 15 17:06:27.210: INFO: Pod "pod-secrets-50b6579a-ef0a-42de-8e5f-9d079920b2c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011694183s
STEP: Saw pod success
Sep 15 17:06:27.210: INFO: Pod "pod-secrets-50b6579a-ef0a-42de-8e5f-9d079920b2c3" satisfied condition "success or failure"
Sep 15 17:06:27.214: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-secrets-50b6579a-ef0a-42de-8e5f-9d079920b2c3 container secret-env-test: <nil>
STEP: delete the pod
Sep 15 17:06:27.245: INFO: Waiting for pod pod-secrets-50b6579a-ef0a-42de-8e5f-9d079920b2c3 to disappear
Sep 15 17:06:27.250: INFO: Pod pod-secrets-50b6579a-ef0a-42de-8e5f-9d079920b2c3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:06:27.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5653" for this suite.
Sep 15 17:06:33.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:06:33.420: INFO: namespace secrets-5653 deletion completed in 6.164573603s

• [SLOW TEST:8.386 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:06:33.420: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-359ef8ae-5cae-48ad-af54-70a73750cc02
Sep 15 17:06:33.584: INFO: Pod name my-hostname-basic-359ef8ae-5cae-48ad-af54-70a73750cc02: Found 0 pods out of 1
Sep 15 17:06:38.590: INFO: Pod name my-hostname-basic-359ef8ae-5cae-48ad-af54-70a73750cc02: Found 1 pods out of 1
Sep 15 17:06:38.590: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-359ef8ae-5cae-48ad-af54-70a73750cc02" are running
Sep 15 17:06:38.596: INFO: Pod "my-hostname-basic-359ef8ae-5cae-48ad-af54-70a73750cc02-f4shh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-15 17:06:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-15 17:06:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-15 17:06:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-15 17:06:33 +0000 UTC Reason: Message:}])
Sep 15 17:06:38.596: INFO: Trying to dial the pod
Sep 15 17:06:43.618: INFO: Controller my-hostname-basic-359ef8ae-5cae-48ad-af54-70a73750cc02: Got expected result from replica 1 [my-hostname-basic-359ef8ae-5cae-48ad-af54-70a73750cc02-f4shh]: "my-hostname-basic-359ef8ae-5cae-48ad-af54-70a73750cc02-f4shh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:06:43.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4331" for this suite.
Sep 15 17:06:49.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:06:49.794: INFO: namespace replication-controller-4331 deletion completed in 6.170657493s

• [SLOW TEST:16.373 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:06:49.794: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 17:06:49.939: INFO: Creating ReplicaSet my-hostname-basic-d76dd0ef-6b70-4338-ac2b-db3fed7c434d
Sep 15 17:06:49.952: INFO: Pod name my-hostname-basic-d76dd0ef-6b70-4338-ac2b-db3fed7c434d: Found 0 pods out of 1
Sep 15 17:06:54.957: INFO: Pod name my-hostname-basic-d76dd0ef-6b70-4338-ac2b-db3fed7c434d: Found 1 pods out of 1
Sep 15 17:06:54.957: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d76dd0ef-6b70-4338-ac2b-db3fed7c434d" is running
Sep 15 17:06:54.962: INFO: Pod "my-hostname-basic-d76dd0ef-6b70-4338-ac2b-db3fed7c434d-pqjgl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-15 17:06:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-15 17:06:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-15 17:06:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-15 17:06:49 +0000 UTC Reason: Message:}])
Sep 15 17:06:54.962: INFO: Trying to dial the pod
Sep 15 17:06:59.983: INFO: Controller my-hostname-basic-d76dd0ef-6b70-4338-ac2b-db3fed7c434d: Got expected result from replica 1 [my-hostname-basic-d76dd0ef-6b70-4338-ac2b-db3fed7c434d-pqjgl]: "my-hostname-basic-d76dd0ef-6b70-4338-ac2b-db3fed7c434d-pqjgl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:06:59.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6922" for this suite.
Sep 15 17:07:06.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:07:06.153: INFO: namespace replicaset-6922 deletion completed in 6.164289558s

• [SLOW TEST:16.359 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:07:06.153: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 15 17:07:06.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-043cbb85-f5ff-43d3-82ea-c844569a19a7" in namespace "projected-7599" to be "success or failure"
Sep 15 17:07:06.317: INFO: Pod "downwardapi-volume-043cbb85-f5ff-43d3-82ea-c844569a19a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699183ms
Sep 15 17:07:08.322: INFO: Pod "downwardapi-volume-043cbb85-f5ff-43d3-82ea-c844569a19a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009596789s
STEP: Saw pod success
Sep 15 17:07:08.322: INFO: Pod "downwardapi-volume-043cbb85-f5ff-43d3-82ea-c844569a19a7" satisfied condition "success or failure"
Sep 15 17:07:08.327: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod downwardapi-volume-043cbb85-f5ff-43d3-82ea-c844569a19a7 container client-container: <nil>
STEP: delete the pod
Sep 15 17:07:08.366: INFO: Waiting for pod downwardapi-volume-043cbb85-f5ff-43d3-82ea-c844569a19a7 to disappear
Sep 15 17:07:08.372: INFO: Pod downwardapi-volume-043cbb85-f5ff-43d3-82ea-c844569a19a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:07:08.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7599" for this suite.
Sep 15 17:07:14.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:07:14.551: INFO: namespace projected-7599 deletion completed in 6.173863192s

• [SLOW TEST:8.398 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:07:14.552: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 17:07:14.715: INFO: (0) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.723702ms)
Sep 15 17:07:14.721: INFO: (1) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.834626ms)
Sep 15 17:07:14.727: INFO: (2) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.51466ms)
Sep 15 17:07:14.733: INFO: (3) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.699872ms)
Sep 15 17:07:14.738: INFO: (4) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.650953ms)
Sep 15 17:07:14.744: INFO: (5) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.89565ms)
Sep 15 17:07:14.752: INFO: (6) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.660638ms)
Sep 15 17:07:14.759: INFO: (7) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.496588ms)
Sep 15 17:07:14.765: INFO: (8) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.583587ms)
Sep 15 17:07:14.771: INFO: (9) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.659469ms)
Sep 15 17:07:14.776: INFO: (10) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.312001ms)
Sep 15 17:07:14.782: INFO: (11) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.767741ms)
Sep 15 17:07:14.787: INFO: (12) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.512229ms)
Sep 15 17:07:14.793: INFO: (13) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.70057ms)
Sep 15 17:07:14.799: INFO: (14) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.286843ms)
Sep 15 17:07:14.804: INFO: (15) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.602576ms)
Sep 15 17:07:14.810: INFO: (16) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.588116ms)
Sep 15 17:07:14.815: INFO: (17) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.545398ms)
Sep 15 17:07:14.821: INFO: (18) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.573999ms)
Sep 15 17:07:14.826: INFO: (19) /api/v1/nodes/ip-172-16-34-210.ec2.internal:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.333675ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:07:14.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2899" for this suite.
Sep 15 17:07:20.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:07:20.993: INFO: namespace proxy-2899 deletion completed in 6.161875924s

• [SLOW TEST:6.442 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:07:20.994: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 15 17:07:23.173: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:07:23.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8658" for this suite.
Sep 15 17:07:29.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:07:29.359: INFO: namespace container-runtime-8658 deletion completed in 6.157458883s

• [SLOW TEST:8.365 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:07:29.359: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep 15 17:07:29.514: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 15 17:07:29.525: INFO: Waiting for terminating namespaces to be deleted...
Sep 15 17:07:29.529: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-34-210.ec2.internal before test
Sep 15 17:07:29.538: INFO: canal-8zmc6 from kube-system started at 2019-09-15 15:36:09 +0000 UTC (4 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container calico-node ready: true, restart count 0
Sep 15 17:07:29.538: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 15 17:07:29.538: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Sep 15 17:07:29.538: INFO: 	Container update-network-condition ready: true, restart count 0
Sep 15 17:07:29.538: INFO: kublr-system-shell-bd4545595-l6rcb from kube-system started at 2019-09-15 15:37:03 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container shell ready: true, restart count 0
Sep 15 17:07:29.538: INFO: sonobuoy-systemd-logs-daemon-set-df7b335343554494-7psrx from heptio-sonobuoy started at 2019-09-15 15:46:17 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 15 17:07:29.538: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 15 17:07:29.538: INFO: kublr-logging-port-fwd-app-7b6c88769c-ffdkg from kublr started at 2019-09-15 15:37:45 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container haproxy ready: true, restart count 0
Sep 15 17:07:29.538: INFO: 	Container port-fwd-app ready: true, restart count 0
Sep 15 17:07:29.538: INFO: kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-34-210.ec2.internal from kube-system started at 2019-09-15 15:27:42 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 15 17:07:29.538: INFO: node-local-dns-gz49t from kube-system started at 2019-09-15 15:36:19 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container node-cache ready: true, restart count 0
Sep 15 17:07:29.538: INFO: heapster-v1.6.0-beta.1-59cc44564c-zhc8x from kube-system started at 2019-09-15 15:36:30 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container heapster ready: true, restart count 0
Sep 15 17:07:29.538: INFO: 	Container heapster-nanny ready: true, restart count 0
Sep 15 17:07:29.538: INFO: kublr-logging-kibana-6c5f8fd758-nmztf from kublr started at 2019-09-15 15:37:45 +0000 UTC (4 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container keycloak-proxy ready: true, restart count 0
Sep 15 17:07:29.538: INFO: 	Container kibana ready: true, restart count 0
Sep 15 17:07:29.538: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep 15 17:07:29.538: INFO: 	Container sg-auth-proxy ready: true, restart count 0
Sep 15 17:07:29.538: INFO: k8s-api-haproxy-30491611cba52aec18f9e69ca06c97ca1f7bdbbd34703396d270209b01ad231c-ip-172-16-34-210.ec2.internal from kube-system started at 2019-09-15 15:27:42 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Sep 15 17:07:29.538: INFO: kublr-logging-fluentd-es-bh9xf from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container fluentd-es ready: true, restart count 0
Sep 15 17:07:29.538: INFO: kublr-logging-elasticsearch-data-0 from kublr started at 2019-09-15 15:37:54 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.538: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 15 17:07:29.538: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-49-162.ec2.internal before test
Sep 15 17:07:29.553: INFO: canal-rxzvq from kube-system started at 2019-09-15 15:36:09 +0000 UTC (4 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container calico-node ready: true, restart count 0
Sep 15 17:07:29.553: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 15 17:07:29.553: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Sep 15 17:07:29.553: INFO: 	Container update-network-condition ready: true, restart count 0
Sep 15 17:07:29.553: INFO: node-local-dns-br5xd from kube-system started at 2019-09-15 15:36:28 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container node-cache ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kublr-logging-elasticsearch-client-878b8dfc9-bfjr9 from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kublr-logging-logstash-8b59d6585-lfv7s from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container logstash ready: true, restart count 1
Sep 15 17:07:29.553: INFO: sonobuoy-systemd-logs-daemon-set-df7b335343554494-4bjgh from heptio-sonobuoy started at 2019-09-15 15:46:17 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 15 17:07:29.553: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 15 17:07:29.553: INFO: kublr-feature-ingress-nginx-ingress-controller-7457b96bd5-v4xbz from kube-system started at 2019-09-15 15:37:06 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kublr-feature-ingress-nginx-ingress-default-backend-76875ffpw75 from kube-system started at 2019-09-15 15:37:06 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kcp-kublr-api-5b65f4d94-ct9tz from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container kublr-api ready: true, restart count 0
Sep 15 17:07:29.553: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-15 15:46:12 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-49-162.ec2.internal from kube-system started at 2019-09-15 15:27:34 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kcp-keycloak-1 from kublr started at 2019-09-15 15:40:02 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container init-keycloak ready: true, restart count 0
Sep 15 17:07:29.553: INFO: 	Container kcp-keycloak ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kcp-kublr-ui-f68d5649b-gk9wv from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container kublr-ui ready: true, restart count 0
Sep 15 17:07:29.553: INFO: k8s-api-haproxy-d652df068620165e8e39b6f21664f4bf8f25e18da58de3bbd5b9b30209f04c24-ip-172-16-49-162.ec2.internal from kube-system started at 2019-09-15 15:27:34 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Sep 15 17:07:29.553: INFO: coredns-5c98db65d4-tntcv from kube-system started at 2019-09-15 15:36:32 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container coredns ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kcp-cluster-controller-7df897bcc6-9vxkt from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container cluster-controller ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kcp-backup-controller-6fbdcc7665-26hcv from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container backup-controller ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kublr-monitoring-kube-state-metrics-78575c4f65-vvmgq from kublr started at 2019-09-15 15:38:11 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container addon-resizer ready: true, restart count 0
Sep 15 17:07:29.553: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kublr-logging-fluentd-es-s869l from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container fluentd-es ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kcp-generator-585767ccdf-j8h9s from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container generator ready: true, restart count 0
Sep 15 17:07:29.553: INFO: kcp-terraform-controller-57c4558499-n8479 from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.553: INFO: 	Container terraform-controller ready: true, restart count 0
Sep 15 17:07:29.553: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-74-13.ec2.internal before test
Sep 15 17:07:29.567: INFO: kublr-logging-elasticsearch-exporter-7bbd7f9759-278vb from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container main ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-logging-sg-job-2mhts from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container certgenerator ready: false, restart count 0
Sep 15 17:07:29.567: INFO: kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-74-13.ec2.internal from kube-system started at 2019-09-15 15:27:39 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 15 17:07:29.567: INFO: node-local-dns-npqcf from kube-system started at 2019-09-15 15:36:28 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container node-cache ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-logging-fluentd-es-hrgjn from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container fluentd-es ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-logging-elasticsearch-master-0 from kublr started at 2019-09-15 15:37:53 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-monitoring-prometheus-5fbdf985fd-v8lwr from kublr started at 2019-09-15 15:37:58 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container keycloak-proxy ready: true, restart count 3
Sep 15 17:07:29.567: INFO: 	Container prometheus ready: true, restart count 6
Sep 15 17:07:29.567: INFO: canal-vkt7q from kube-system started at 2019-09-15 15:36:09 +0000 UTC (4 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container calico-node ready: true, restart count 0
Sep 15 17:07:29.567: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 15 17:07:29.567: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Sep 15 17:07:29.567: INFO: 	Container update-network-condition ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-logging-rabbitmq-exporter-5c97455c8-hjmbj from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container kublr-logging-rabbitmq-exporter ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kcp-app-mysql-9f587666-q8dpt from kublr started at 2019-09-15 15:38:04 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container kcp-app-mysql ready: true, restart count 0
Sep 15 17:07:29.567: INFO: k8s-api-haproxy-30491611cba52aec18f9e69ca06c97ca1f7bdbbd34703396d270209b01ad231c-ip-172-16-74-13.ec2.internal from kube-system started at 2019-09-15 15:27:39 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-logging-controller-655f75b9fc-m7vws from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container kublr-feature-logging ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kcp-app-mongodb-c88c48bcf-5vgm6 from kublr started at 2019-09-15 15:38:07 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container mongo ready: true, restart count 0
Sep 15 17:07:29.567: INFO: 	Container mongo-exporter ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-monitoring-alertmanager-76479cbbdf-nskwk from kublr started at 2019-09-15 15:37:51 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container alertmanager ready: true, restart count 0
Sep 15 17:07:29.567: INFO: 	Container keycloak-proxy ready: true, restart count 3
Sep 15 17:07:29.567: INFO: kcp-keycloak-0 from kublr started at 2019-09-15 15:37:58 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container init-keycloak ready: true, restart count 0
Sep 15 17:07:29.567: INFO: 	Container kcp-keycloak ready: true, restart count 0
Sep 15 17:07:29.567: INFO: metrics-server-v0.3.1-6f69d6674c-52d69 from kube-system started at 2019-09-15 15:36:36 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container metrics-server ready: true, restart count 0
Sep 15 17:07:29.567: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-monitoring-monitoring-controller-b4f755985-n9zbv from kublr started at 2019-09-15 15:37:52 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container app-monitoring ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-monitoring-grafana-79f56fd86c-hgtxg from kublr started at 2019-09-15 15:37:58 +0000 UTC (3 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container grafana ready: true, restart count 0
Sep 15 17:07:29.567: INFO: 	Container keycloak-proxy ready: true, restart count 3
Sep 15 17:07:29.567: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep 15 17:07:29.567: INFO: kublr-logging-rabbitmq-0 from kublr started at 2019-09-15 15:45:27 +0000 UTC (1 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container rabbitmq ready: true, restart count 0
Sep 15 17:07:29.567: INFO: sonobuoy-systemd-logs-daemon-set-df7b335343554494-9cvrv from heptio-sonobuoy started at 2019-09-15 15:46:17 +0000 UTC (2 container statuses recorded)
Sep 15 17:07:29.567: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 15 17:07:29.567: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c4ab9a56c7f643], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:07:30.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2235" for this suite.
Sep 15 17:07:36.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:07:36.793: INFO: namespace sched-pred-2235 deletion completed in 6.15733245s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.434 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:07:36.794: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-435/secret-test-1318356d-2321-485a-82e2-ecc04f51a991
STEP: Creating a pod to test consume secrets
Sep 15 17:07:36.952: INFO: Waiting up to 5m0s for pod "pod-configmaps-627f4878-24be-4510-8eb5-c0f352bd571f" in namespace "secrets-435" to be "success or failure"
Sep 15 17:07:36.958: INFO: Pod "pod-configmaps-627f4878-24be-4510-8eb5-c0f352bd571f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.147996ms
Sep 15 17:07:38.963: INFO: Pod "pod-configmaps-627f4878-24be-4510-8eb5-c0f352bd571f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010912375s
STEP: Saw pod success
Sep 15 17:07:38.963: INFO: Pod "pod-configmaps-627f4878-24be-4510-8eb5-c0f352bd571f" satisfied condition "success or failure"
Sep 15 17:07:38.968: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-627f4878-24be-4510-8eb5-c0f352bd571f container env-test: <nil>
STEP: delete the pod
Sep 15 17:07:39.006: INFO: Waiting for pod pod-configmaps-627f4878-24be-4510-8eb5-c0f352bd571f to disappear
Sep 15 17:07:39.011: INFO: Pod pod-configmaps-627f4878-24be-4510-8eb5-c0f352bd571f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:07:39.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-435" for this suite.
Sep 15 17:07:45.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:07:45.180: INFO: namespace secrets-435 deletion completed in 6.163627449s

• [SLOW TEST:8.386 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:07:45.180: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:07:47.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2128" for this suite.
Sep 15 17:08:35.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:08:35.543: INFO: namespace kubelet-test-2128 deletion completed in 48.168957217s

• [SLOW TEST:50.363 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:08:35.544: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2189
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-b1e0e129-ce80-4432-b4ac-9155b723d704
STEP: Creating a pod to test consume configMaps
Sep 15 17:08:35.710: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6af90048-0032-4336-a1b9-e6ec511bf226" in namespace "projected-2189" to be "success or failure"
Sep 15 17:08:35.716: INFO: Pod "pod-projected-configmaps-6af90048-0032-4336-a1b9-e6ec511bf226": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056224ms
Sep 15 17:08:37.720: INFO: Pod "pod-projected-configmaps-6af90048-0032-4336-a1b9-e6ec511bf226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010776214s
STEP: Saw pod success
Sep 15 17:08:37.720: INFO: Pod "pod-projected-configmaps-6af90048-0032-4336-a1b9-e6ec511bf226" satisfied condition "success or failure"
Sep 15 17:08:37.724: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-projected-configmaps-6af90048-0032-4336-a1b9-e6ec511bf226 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 17:08:37.754: INFO: Waiting for pod pod-projected-configmaps-6af90048-0032-4336-a1b9-e6ec511bf226 to disappear
Sep 15 17:08:37.763: INFO: Pod pod-projected-configmaps-6af90048-0032-4336-a1b9-e6ec511bf226 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:08:37.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2189" for this suite.
Sep 15 17:08:43.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:08:43.925: INFO: namespace projected-2189 deletion completed in 6.156745212s

• [SLOW TEST:8.382 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:08:43.925: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3198
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Sep 15 17:08:44.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 create -f - --namespace=kubectl-3198'
Sep 15 17:08:44.344: INFO: stderr: ""
Sep 15 17:08:44.344: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Sep 15 17:08:45.349: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 17:08:45.349: INFO: Found 0 / 1
Sep 15 17:08:46.349: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 17:08:46.349: INFO: Found 1 / 1
Sep 15 17:08:46.349: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 15 17:08:46.353: INFO: Selector matched 1 pods for map[app:redis]
Sep 15 17:08:46.353: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep 15 17:08:46.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 logs redis-master-p9wdf redis-master --namespace=kubectl-3198'
Sep 15 17:08:46.447: INFO: stderr: ""
Sep 15 17:08:46.447: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 15 Sep 17:08:45.205 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 15 Sep 17:08:45.205 # Server started, Redis version 3.2.12\n1:M 15 Sep 17:08:45.205 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 15 Sep 17:08:45.205 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep 15 17:08:46.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 log redis-master-p9wdf redis-master --namespace=kubectl-3198 --tail=1'
Sep 15 17:08:46.555: INFO: stderr: ""
Sep 15 17:08:46.555: INFO: stdout: "1:M 15 Sep 17:08:45.205 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep 15 17:08:46.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 log redis-master-p9wdf redis-master --namespace=kubectl-3198 --limit-bytes=1'
Sep 15 17:08:46.645: INFO: stderr: ""
Sep 15 17:08:46.645: INFO: stdout: " "
STEP: exposing timestamps
Sep 15 17:08:46.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 log redis-master-p9wdf redis-master --namespace=kubectl-3198 --tail=1 --timestamps'
Sep 15 17:08:46.733: INFO: stderr: ""
Sep 15 17:08:46.733: INFO: stdout: "2019-09-15T17:08:45.20598105Z 1:M 15 Sep 17:08:45.205 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep 15 17:08:49.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 log redis-master-p9wdf redis-master --namespace=kubectl-3198 --since=1s'
Sep 15 17:08:49.328: INFO: stderr: ""
Sep 15 17:08:49.328: INFO: stdout: ""
Sep 15 17:08:49.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 log redis-master-p9wdf redis-master --namespace=kubectl-3198 --since=24h'
Sep 15 17:08:49.419: INFO: stderr: ""
Sep 15 17:08:49.419: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 15 Sep 17:08:45.205 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 15 Sep 17:08:45.205 # Server started, Redis version 3.2.12\n1:M 15 Sep 17:08:45.205 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 15 Sep 17:08:45.205 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Sep 15 17:08:49.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 delete --grace-period=0 --force -f - --namespace=kubectl-3198'
Sep 15 17:08:49.505: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 15 17:08:49.505: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep 15 17:08:49.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get rc,svc -l name=nginx --no-headers --namespace=kubectl-3198'
Sep 15 17:08:49.590: INFO: stderr: "No resources found.\n"
Sep 15 17:08:49.590: INFO: stdout: ""
Sep 15 17:08:49.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-180164986 get pods -l name=nginx --namespace=kubectl-3198 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 15 17:08:49.657: INFO: stderr: ""
Sep 15 17:08:49.657: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:08:49.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3198" for this suite.
Sep 15 17:09:11.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:09:11.886: INFO: namespace kubectl-3198 deletion completed in 22.223422846s

• [SLOW TEST:27.960 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:09:11.886: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1816
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep 15 17:09:12.036: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Sep 15 17:09:12.665: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 15 17:09:14.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 15 17:09:16.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 15 17:09:18.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 15 17:09:20.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704164152, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 15 17:09:25.956: INFO: Waited 3.216747306s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:09:26.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1816" for this suite.
Sep 15 17:09:32.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:09:32.757: INFO: namespace aggregator-1816 deletion completed in 6.26408969s

• [SLOW TEST:20.872 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:09:32.757: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 17:09:32.938: INFO: Create a RollingUpdate DaemonSet
Sep 15 17:09:32.945: INFO: Check that daemon pods launch on every node of the cluster
Sep 15 17:09:32.951: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:32.951: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:32.951: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:32.955: INFO: Number of nodes with available pods: 0
Sep 15 17:09:32.955: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:09:33.961: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:33.961: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:33.961: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:33.966: INFO: Number of nodes with available pods: 0
Sep 15 17:09:33.967: INFO: Node ip-172-16-34-210.ec2.internal is running more than one daemon pod
Sep 15 17:09:34.961: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:34.961: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:34.961: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:34.965: INFO: Number of nodes with available pods: 3
Sep 15 17:09:34.965: INFO: Number of running nodes: 3, number of available pods: 3
Sep 15 17:09:34.965: INFO: Update the DaemonSet to trigger a rollout
Sep 15 17:09:34.977: INFO: Updating DaemonSet daemon-set
Sep 15 17:09:43.998: INFO: Roll back the DaemonSet before rollout is complete
Sep 15 17:09:44.011: INFO: Updating DaemonSet daemon-set
Sep 15 17:09:44.011: INFO: Make sure DaemonSet rollback is complete
Sep 15 17:09:44.016: INFO: Wrong image for pod: daemon-set-fltmc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 15 17:09:44.016: INFO: Pod daemon-set-fltmc is not available
Sep 15 17:09:44.028: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:44.028: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:44.028: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:45.035: INFO: Wrong image for pod: daemon-set-fltmc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 15 17:09:45.035: INFO: Pod daemon-set-fltmc is not available
Sep 15 17:09:45.040: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:45.040: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:45.040: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:46.035: INFO: Wrong image for pod: daemon-set-fltmc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 15 17:09:46.035: INFO: Pod daemon-set-fltmc is not available
Sep 15 17:09:46.040: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:46.040: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:46.040: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:47.034: INFO: Pod daemon-set-28425 is not available
Sep 15 17:09:47.039: INFO: DaemonSet pods can't tolerate node ip-172-16-5-91.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:47.039: INFO: DaemonSet pods can't tolerate node ip-172-16-6-153.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 15 17:09:47.039: INFO: DaemonSet pods can't tolerate node ip-172-16-8-86.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3336, will wait for the garbage collector to delete the pods
Sep 15 17:09:47.117: INFO: Deleting DaemonSet.extensions daemon-set took: 12.58591ms
Sep 15 17:09:47.517: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.284227ms
Sep 15 17:09:49.921: INFO: Number of nodes with available pods: 0
Sep 15 17:09:49.921: INFO: Number of running nodes: 0, number of available pods: 0
Sep 15 17:09:49.926: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3336/daemonsets","resourceVersion":"27303"},"items":null}

Sep 15 17:09:49.930: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3336/pods","resourceVersion":"27303"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:09:49.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3336" for this suite.
Sep 15 17:09:55.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:09:56.109: INFO: namespace daemonsets-3336 deletion completed in 6.157096041s

• [SLOW TEST:23.352 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:09:56.110: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-681
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Sep 15 17:09:56.265: INFO: Waiting up to 5m0s for pod "client-containers-c118fbcb-65ce-4f57-9bf1-75036722c17b" in namespace "containers-681" to be "success or failure"
Sep 15 17:09:56.270: INFO: Pod "client-containers-c118fbcb-65ce-4f57-9bf1-75036722c17b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.646248ms
Sep 15 17:09:58.275: INFO: Pod "client-containers-c118fbcb-65ce-4f57-9bf1-75036722c17b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009557935s
STEP: Saw pod success
Sep 15 17:09:58.275: INFO: Pod "client-containers-c118fbcb-65ce-4f57-9bf1-75036722c17b" satisfied condition "success or failure"
Sep 15 17:09:58.279: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod client-containers-c118fbcb-65ce-4f57-9bf1-75036722c17b container test-container: <nil>
STEP: delete the pod
Sep 15 17:09:58.311: INFO: Waiting for pod client-containers-c118fbcb-65ce-4f57-9bf1-75036722c17b to disappear
Sep 15 17:09:58.315: INFO: Pod client-containers-c118fbcb-65ce-4f57-9bf1-75036722c17b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:09:58.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-681" for this suite.
Sep 15 17:10:04.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:10:04.481: INFO: namespace containers-681 deletion completed in 6.161314151s

• [SLOW TEST:8.372 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:10:04.481: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-facbaff9-63c9-41b7-bd43-1612b10bed76
STEP: Creating a pod to test consume configMaps
Sep 15 17:10:04.647: INFO: Waiting up to 5m0s for pod "pod-configmaps-97eb9eb9-e502-48a2-9174-eed319de98ad" in namespace "configmap-4839" to be "success or failure"
Sep 15 17:10:04.651: INFO: Pod "pod-configmaps-97eb9eb9-e502-48a2-9174-eed319de98ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.507116ms
Sep 15 17:10:06.657: INFO: Pod "pod-configmaps-97eb9eb9-e502-48a2-9174-eed319de98ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009847304s
STEP: Saw pod success
Sep 15 17:10:06.657: INFO: Pod "pod-configmaps-97eb9eb9-e502-48a2-9174-eed319de98ad" satisfied condition "success or failure"
Sep 15 17:10:06.661: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-97eb9eb9-e502-48a2-9174-eed319de98ad container configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 17:10:06.693: INFO: Waiting for pod pod-configmaps-97eb9eb9-e502-48a2-9174-eed319de98ad to disappear
Sep 15 17:10:06.699: INFO: Pod pod-configmaps-97eb9eb9-e502-48a2-9174-eed319de98ad no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:10:06.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4839" for this suite.
Sep 15 17:10:12.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:10:12.889: INFO: namespace configmap-4839 deletion completed in 6.183601954s

• [SLOW TEST:8.407 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:10:12.889: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 15 17:10:13.047: INFO: Waiting up to 5m0s for pod "pod-0f0e2c92-2747-447a-9a5d-080b5b27d7e4" in namespace "emptydir-555" to be "success or failure"
Sep 15 17:10:13.055: INFO: Pod "pod-0f0e2c92-2747-447a-9a5d-080b5b27d7e4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.579567ms
Sep 15 17:10:15.062: INFO: Pod "pod-0f0e2c92-2747-447a-9a5d-080b5b27d7e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014454602s
STEP: Saw pod success
Sep 15 17:10:15.062: INFO: Pod "pod-0f0e2c92-2747-447a-9a5d-080b5b27d7e4" satisfied condition "success or failure"
Sep 15 17:10:15.066: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-0f0e2c92-2747-447a-9a5d-080b5b27d7e4 container test-container: <nil>
STEP: delete the pod
Sep 15 17:10:15.098: INFO: Waiting for pod pod-0f0e2c92-2747-447a-9a5d-080b5b27d7e4 to disappear
Sep 15 17:10:15.102: INFO: Pod pod-0f0e2c92-2747-447a-9a5d-080b5b27d7e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:10:15.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-555" for this suite.
Sep 15 17:10:21.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:10:21.268: INFO: namespace emptydir-555 deletion completed in 6.159771066s

• [SLOW TEST:8.380 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:10:21.268: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-1b4e4a70-f013-4471-89fa-dfeacd2aa1a3 in namespace container-probe-6189
Sep 15 17:10:23.435: INFO: Started pod liveness-1b4e4a70-f013-4471-89fa-dfeacd2aa1a3 in namespace container-probe-6189
STEP: checking the pod's current state and verifying that restartCount is present
Sep 15 17:10:23.440: INFO: Initial restart count of pod liveness-1b4e4a70-f013-4471-89fa-dfeacd2aa1a3 is 0
Sep 15 17:10:47.505: INFO: Restart count of pod container-probe-6189/liveness-1b4e4a70-f013-4471-89fa-dfeacd2aa1a3 is now 1 (24.065062065s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:10:47.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6189" for this suite.
Sep 15 17:10:53.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:10:53.689: INFO: namespace container-probe-6189 deletion completed in 6.160018583s

• [SLOW TEST:32.420 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:10:53.689: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-0c08367b-58c7-4d03-8dea-f0a430fbbda1
STEP: Creating a pod to test consume secrets
Sep 15 17:10:53.852: INFO: Waiting up to 5m0s for pod "pod-secrets-64f7c4d4-3a59-47cf-869c-eee5da54e060" in namespace "secrets-6440" to be "success or failure"
Sep 15 17:10:53.859: INFO: Pod "pod-secrets-64f7c4d4-3a59-47cf-869c-eee5da54e060": Phase="Pending", Reason="", readiness=false. Elapsed: 6.793067ms
Sep 15 17:10:55.864: INFO: Pod "pod-secrets-64f7c4d4-3a59-47cf-869c-eee5da54e060": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011693175s
STEP: Saw pod success
Sep 15 17:10:55.864: INFO: Pod "pod-secrets-64f7c4d4-3a59-47cf-869c-eee5da54e060" satisfied condition "success or failure"
Sep 15 17:10:55.868: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-secrets-64f7c4d4-3a59-47cf-869c-eee5da54e060 container secret-volume-test: <nil>
STEP: delete the pod
Sep 15 17:10:55.898: INFO: Waiting for pod pod-secrets-64f7c4d4-3a59-47cf-869c-eee5da54e060 to disappear
Sep 15 17:10:55.902: INFO: Pod pod-secrets-64f7c4d4-3a59-47cf-869c-eee5da54e060 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:10:55.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6440" for this suite.
Sep 15 17:11:01.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:11:02.077: INFO: namespace secrets-6440 deletion completed in 6.169532471s

• [SLOW TEST:8.388 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:11:02.077: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:11:23.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-502" for this suite.
Sep 15 17:11:29.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:11:29.805: INFO: namespace container-runtime-502 deletion completed in 6.295898952s

• [SLOW TEST:27.727 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:11:29.805: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep 15 17:11:29.952: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 15 17:11:29.962: INFO: Waiting for terminating namespaces to be deleted...
Sep 15 17:11:29.966: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-34-210.ec2.internal before test
Sep 15 17:11:29.976: INFO: kublr-logging-port-fwd-app-7b6c88769c-ffdkg from kublr started at 2019-09-15 15:37:45 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container haproxy ready: true, restart count 0
Sep 15 17:11:29.976: INFO: 	Container port-fwd-app ready: true, restart count 0
Sep 15 17:11:29.976: INFO: kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-34-210.ec2.internal from kube-system started at 2019-09-15 15:27:42 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 15 17:11:29.976: INFO: node-local-dns-gz49t from kube-system started at 2019-09-15 15:36:19 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container node-cache ready: true, restart count 0
Sep 15 17:11:29.976: INFO: heapster-v1.6.0-beta.1-59cc44564c-zhc8x from kube-system started at 2019-09-15 15:36:30 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container heapster ready: true, restart count 0
Sep 15 17:11:29.976: INFO: 	Container heapster-nanny ready: true, restart count 0
Sep 15 17:11:29.976: INFO: kublr-logging-kibana-6c5f8fd758-nmztf from kublr started at 2019-09-15 15:37:45 +0000 UTC (4 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container keycloak-proxy ready: true, restart count 0
Sep 15 17:11:29.976: INFO: 	Container kibana ready: true, restart count 0
Sep 15 17:11:29.976: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep 15 17:11:29.976: INFO: 	Container sg-auth-proxy ready: true, restart count 0
Sep 15 17:11:29.976: INFO: k8s-api-haproxy-30491611cba52aec18f9e69ca06c97ca1f7bdbbd34703396d270209b01ad231c-ip-172-16-34-210.ec2.internal from kube-system started at 2019-09-15 15:27:42 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Sep 15 17:11:29.976: INFO: kublr-logging-fluentd-es-bh9xf from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container fluentd-es ready: true, restart count 0
Sep 15 17:11:29.976: INFO: kublr-logging-elasticsearch-data-0 from kublr started at 2019-09-15 15:37:54 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 15 17:11:29.976: INFO: canal-8zmc6 from kube-system started at 2019-09-15 15:36:09 +0000 UTC (4 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container calico-node ready: true, restart count 0
Sep 15 17:11:29.976: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 15 17:11:29.976: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Sep 15 17:11:29.976: INFO: 	Container update-network-condition ready: true, restart count 0
Sep 15 17:11:29.976: INFO: kublr-system-shell-bd4545595-l6rcb from kube-system started at 2019-09-15 15:37:03 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container shell ready: true, restart count 0
Sep 15 17:11:29.976: INFO: sonobuoy-systemd-logs-daemon-set-df7b335343554494-7psrx from heptio-sonobuoy started at 2019-09-15 15:46:17 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:29.976: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 15 17:11:29.976: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 15 17:11:29.976: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-49-162.ec2.internal before test
Sep 15 17:11:29.991: INFO: kublr-logging-fluentd-es-s869l from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container fluentd-es ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kcp-generator-585767ccdf-j8h9s from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container generator ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kcp-terraform-controller-57c4558499-n8479 from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container terraform-controller ready: true, restart count 0
Sep 15 17:11:29.991: INFO: canal-rxzvq from kube-system started at 2019-09-15 15:36:09 +0000 UTC (4 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container calico-node ready: true, restart count 0
Sep 15 17:11:29.991: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 15 17:11:29.991: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Sep 15 17:11:29.991: INFO: 	Container update-network-condition ready: true, restart count 0
Sep 15 17:11:29.991: INFO: node-local-dns-br5xd from kube-system started at 2019-09-15 15:36:28 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container node-cache ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kublr-logging-elasticsearch-client-878b8dfc9-bfjr9 from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kublr-logging-logstash-8b59d6585-lfv7s from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container logstash ready: true, restart count 1
Sep 15 17:11:29.991: INFO: sonobuoy-systemd-logs-daemon-set-df7b335343554494-4bjgh from heptio-sonobuoy started at 2019-09-15 15:46:17 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 15 17:11:29.991: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 15 17:11:29.991: INFO: kublr-feature-ingress-nginx-ingress-controller-7457b96bd5-v4xbz from kube-system started at 2019-09-15 15:37:06 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kublr-feature-ingress-nginx-ingress-default-backend-76875ffpw75 from kube-system started at 2019-09-15 15:37:06 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kcp-kublr-api-5b65f4d94-ct9tz from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container kublr-api ready: true, restart count 0
Sep 15 17:11:29.991: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-15 15:46:12 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-49-162.ec2.internal from kube-system started at 2019-09-15 15:27:34 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kcp-keycloak-1 from kublr started at 2019-09-15 15:40:02 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container init-keycloak ready: true, restart count 0
Sep 15 17:11:29.991: INFO: 	Container kcp-keycloak ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kcp-kublr-ui-f68d5649b-gk9wv from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container kublr-ui ready: true, restart count 0
Sep 15 17:11:29.991: INFO: k8s-api-haproxy-d652df068620165e8e39b6f21664f4bf8f25e18da58de3bbd5b9b30209f04c24-ip-172-16-49-162.ec2.internal from kube-system started at 2019-09-15 15:27:34 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Sep 15 17:11:29.991: INFO: coredns-5c98db65d4-tntcv from kube-system started at 2019-09-15 15:36:32 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container coredns ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kcp-cluster-controller-7df897bcc6-9vxkt from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container cluster-controller ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kcp-backup-controller-6fbdcc7665-26hcv from kublr started at 2019-09-15 15:37:58 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container backup-controller ready: true, restart count 0
Sep 15 17:11:29.991: INFO: kublr-monitoring-kube-state-metrics-78575c4f65-vvmgq from kublr started at 2019-09-15 15:38:11 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:29.991: INFO: 	Container addon-resizer ready: true, restart count 0
Sep 15 17:11:29.991: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 15 17:11:29.991: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-74-13.ec2.internal before test
Sep 15 17:11:30.018: INFO: kublr-logging-elasticsearch-exporter-7bbd7f9759-278vb from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container main ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kublr-logging-sg-job-2mhts from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container certgenerator ready: false, restart count 0
Sep 15 17:11:30.018: INFO: kublr-monitoring-prometheus-5fbdf985fd-v8lwr from kublr started at 2019-09-15 15:37:58 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container keycloak-proxy ready: true, restart count 3
Sep 15 17:11:30.018: INFO: 	Container prometheus ready: true, restart count 6
Sep 15 17:11:30.018: INFO: kube-proxy-9dc02808ceb56aa06a768fa08239472035b5b59d70105cbe4f95160456a71a7c-ip-172-16-74-13.ec2.internal from kube-system started at 2019-09-15 15:27:39 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 15 17:11:30.018: INFO: node-local-dns-npqcf from kube-system started at 2019-09-15 15:36:28 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container node-cache ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kublr-logging-fluentd-es-hrgjn from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container fluentd-es ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kublr-logging-elasticsearch-master-0 from kublr started at 2019-09-15 15:37:53 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 15 17:11:30.018: INFO: canal-vkt7q from kube-system started at 2019-09-15 15:36:09 +0000 UTC (4 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container calico-node ready: true, restart count 0
Sep 15 17:11:30.018: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 15 17:11:30.018: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Sep 15 17:11:30.018: INFO: 	Container update-network-condition ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kublr-logging-rabbitmq-exporter-5c97455c8-hjmbj from kublr started at 2019-09-15 15:37:45 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container kublr-logging-rabbitmq-exporter ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kcp-app-mysql-9f587666-q8dpt from kublr started at 2019-09-15 15:38:04 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container kcp-app-mysql ready: true, restart count 0
Sep 15 17:11:30.018: INFO: k8s-api-haproxy-30491611cba52aec18f9e69ca06c97ca1f7bdbbd34703396d270209b01ad231c-ip-172-16-74-13.ec2.internal from kube-system started at 2019-09-15 15:27:39 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kublr-logging-controller-655f75b9fc-m7vws from kublr started at 2019-09-15 15:37:46 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container kublr-feature-logging ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kcp-app-mongodb-c88c48bcf-5vgm6 from kublr started at 2019-09-15 15:38:07 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container mongo ready: true, restart count 0
Sep 15 17:11:30.018: INFO: 	Container mongo-exporter ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kublr-monitoring-alertmanager-76479cbbdf-nskwk from kublr started at 2019-09-15 15:37:51 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container alertmanager ready: true, restart count 0
Sep 15 17:11:30.018: INFO: 	Container keycloak-proxy ready: true, restart count 3
Sep 15 17:11:30.018: INFO: kcp-keycloak-0 from kublr started at 2019-09-15 15:37:58 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container init-keycloak ready: true, restart count 0
Sep 15 17:11:30.018: INFO: 	Container kcp-keycloak ready: true, restart count 0
Sep 15 17:11:30.018: INFO: sonobuoy-systemd-logs-daemon-set-df7b335343554494-9cvrv from heptio-sonobuoy started at 2019-09-15 15:46:17 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 15 17:11:30.018: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 15 17:11:30.018: INFO: metrics-server-v0.3.1-6f69d6674c-52d69 from kube-system started at 2019-09-15 15:36:36 +0000 UTC (2 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container metrics-server ready: true, restart count 0
Sep 15 17:11:30.018: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kublr-monitoring-monitoring-controller-b4f755985-n9zbv from kublr started at 2019-09-15 15:37:52 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container app-monitoring ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kublr-monitoring-grafana-79f56fd86c-hgtxg from kublr started at 2019-09-15 15:37:58 +0000 UTC (3 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container grafana ready: true, restart count 0
Sep 15 17:11:30.018: INFO: 	Container keycloak-proxy ready: true, restart count 3
Sep 15 17:11:30.018: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep 15 17:11:30.018: INFO: kublr-logging-rabbitmq-0 from kublr started at 2019-09-15 15:45:27 +0000 UTC (1 container statuses recorded)
Sep 15 17:11:30.018: INFO: 	Container rabbitmq ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-07c2fc32-bd44-43ca-9213-b8ce739576bb 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-07c2fc32-bd44-43ca-9213-b8ce739576bb off the node ip-172-16-34-210.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-07c2fc32-bd44-43ca-9213-b8ce739576bb
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:11:34.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1575" for this suite.
Sep 15 17:11:42.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:11:42.282: INFO: namespace sched-pred-1575 deletion completed in 8.156590789s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:12.477 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:11:42.282: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 15 17:11:44.966: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4316 pod-service-account-4ef73854-6e65-4d25-bd15-0193f4e56aac -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 15 17:11:45.450: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4316 pod-service-account-4ef73854-6e65-4d25-bd15-0193f4e56aac -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 15 17:11:45.636: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4316 pod-service-account-4ef73854-6e65-4d25-bd15-0193f4e56aac -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:11:45.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4316" for this suite.
Sep 15 17:11:51.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:11:51.981: INFO: namespace svcaccounts-4316 deletion completed in 6.163752684s

• [SLOW TEST:9.698 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:11:51.981: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-a820242f-e7d4-4b65-b264-a6436691eb27
STEP: Creating a pod to test consume configMaps
Sep 15 17:11:52.157: INFO: Waiting up to 5m0s for pod "pod-configmaps-fbda213e-8ea9-4f3b-9526-8bb801b9483b" in namespace "configmap-6922" to be "success or failure"
Sep 15 17:11:52.162: INFO: Pod "pod-configmaps-fbda213e-8ea9-4f3b-9526-8bb801b9483b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138166ms
Sep 15 17:11:54.167: INFO: Pod "pod-configmaps-fbda213e-8ea9-4f3b-9526-8bb801b9483b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010077466s
STEP: Saw pod success
Sep 15 17:11:54.167: INFO: Pod "pod-configmaps-fbda213e-8ea9-4f3b-9526-8bb801b9483b" satisfied condition "success or failure"
Sep 15 17:11:54.171: INFO: Trying to get logs from node ip-172-16-34-210.ec2.internal pod pod-configmaps-fbda213e-8ea9-4f3b-9526-8bb801b9483b container configmap-volume-test: <nil>
STEP: delete the pod
Sep 15 17:11:54.200: INFO: Waiting for pod pod-configmaps-fbda213e-8ea9-4f3b-9526-8bb801b9483b to disappear
Sep 15 17:11:54.204: INFO: Pod pod-configmaps-fbda213e-8ea9-4f3b-9526-8bb801b9483b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:11:54.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6922" for this suite.
Sep 15 17:12:00.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:12:00.367: INFO: namespace configmap-6922 deletion completed in 6.15794674s

• [SLOW TEST:8.386 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 15 17:12:00.367: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5608
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 15 17:12:00.592: INFO: >>> kubeConfig: /tmp/kubeconfig-180164986
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 15 17:12:06.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5608" for this suite.
Sep 15 17:12:12.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 15 17:12:12.336: INFO: namespace custom-resource-definition-5608 deletion completed in 6.158453529s

• [SLOW TEST:11.969 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSep 15 17:12:12.336: INFO: Running AfterSuite actions on all nodes
Sep 15 17:12:12.336: INFO: Running AfterSuite actions on node 1
Sep 15 17:12:12.336: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5139.107 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h25m40.434418141s
Test Suite Passed
