I0716 13:06:19.021150      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-981533691
I0716 13:06:19.021539      15 e2e.go:241] Starting e2e run "022f6f13-70a3-4a75-b2a8-0ac8d2825466" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1563282375 - Will randomize all specs
Will run 215 of 4411 specs

Jul 16 13:06:19.459: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:06:19.462: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 16 13:06:19.493: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 16 13:06:19.595: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 16 13:06:19.595: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jul 16 13:06:19.596: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 16 13:06:19.645: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Jul 16 13:06:19.646: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 16 13:06:19.646: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jul 16 13:06:19.646: INFO: e2e test version: v1.15.0
Jul 16 13:06:19.650: INFO: kube-apiserver version: v1.15.0
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:06:19.651: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
Jul 16 13:06:19.739: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-8ab5d515-73db-4b9a-af1e-6c08e897db76
STEP: Creating a pod to test consume configMaps
Jul 16 13:06:19.778: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18d31a92-356b-4ad8-ae4d-64573ff38e1f" in namespace "projected-8736" to be "success or failure"
Jul 16 13:06:19.783: INFO: Pod "pod-projected-configmaps-18d31a92-356b-4ad8-ae4d-64573ff38e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078262ms
Jul 16 13:06:21.788: INFO: Pod "pod-projected-configmaps-18d31a92-356b-4ad8-ae4d-64573ff38e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010693832s
Jul 16 13:06:23.794: INFO: Pod "pod-projected-configmaps-18d31a92-356b-4ad8-ae4d-64573ff38e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016641044s
Jul 16 13:06:25.802: INFO: Pod "pod-projected-configmaps-18d31a92-356b-4ad8-ae4d-64573ff38e1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024822332s
STEP: Saw pod success
Jul 16 13:06:25.802: INFO: Pod "pod-projected-configmaps-18d31a92-356b-4ad8-ae4d-64573ff38e1f" satisfied condition "success or failure"
Jul 16 13:06:25.809: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-configmaps-18d31a92-356b-4ad8-ae4d-64573ff38e1f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:06:26.046: INFO: Waiting for pod pod-projected-configmaps-18d31a92-356b-4ad8-ae4d-64573ff38e1f to disappear
Jul 16 13:06:26.050: INFO: Pod pod-projected-configmaps-18d31a92-356b-4ad8-ae4d-64573ff38e1f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:06:26.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8736" for this suite.
Jul 16 13:06:32.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:06:32.446: INFO: namespace projected-8736 deletion completed in 6.389380696s

• [SLOW TEST:12.795 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:06:32.449: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:06:32.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb159659-525c-435f-a712-62dd7d3dd0cf" in namespace "downward-api-5663" to be "success or failure"
Jul 16 13:06:32.699: INFO: Pod "downwardapi-volume-eb159659-525c-435f-a712-62dd7d3dd0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.049688ms
Jul 16 13:06:34.705: INFO: Pod "downwardapi-volume-eb159659-525c-435f-a712-62dd7d3dd0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011556037s
Jul 16 13:06:36.712: INFO: Pod "downwardapi-volume-eb159659-525c-435f-a712-62dd7d3dd0cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018664125s
STEP: Saw pod success
Jul 16 13:06:36.713: INFO: Pod "downwardapi-volume-eb159659-525c-435f-a712-62dd7d3dd0cf" satisfied condition "success or failure"
Jul 16 13:06:36.717: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-eb159659-525c-435f-a712-62dd7d3dd0cf container client-container: <nil>
STEP: delete the pod
Jul 16 13:06:36.782: INFO: Waiting for pod downwardapi-volume-eb159659-525c-435f-a712-62dd7d3dd0cf to disappear
Jul 16 13:06:36.787: INFO: Pod downwardapi-volume-eb159659-525c-435f-a712-62dd7d3dd0cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:06:36.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5663" for this suite.
Jul 16 13:06:42.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:06:43.172: INFO: namespace downward-api-5663 deletion completed in 6.379038391s

• [SLOW TEST:10.723 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:06:43.174: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-195815c3-006c-460f-93be-16b1bf92c89a
STEP: Creating a pod to test consume secrets
Jul 16 13:06:43.289: INFO: Waiting up to 5m0s for pod "pod-secrets-b67b30cf-9040-47be-ae43-0b298c666150" in namespace "secrets-7366" to be "success or failure"
Jul 16 13:06:43.294: INFO: Pod "pod-secrets-b67b30cf-9040-47be-ae43-0b298c666150": Phase="Pending", Reason="", readiness=false. Elapsed: 4.471441ms
Jul 16 13:06:45.300: INFO: Pod "pod-secrets-b67b30cf-9040-47be-ae43-0b298c666150": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009970984s
Jul 16 13:06:47.306: INFO: Pod "pod-secrets-b67b30cf-9040-47be-ae43-0b298c666150": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015952043s
STEP: Saw pod success
Jul 16 13:06:47.306: INFO: Pod "pod-secrets-b67b30cf-9040-47be-ae43-0b298c666150" satisfied condition "success or failure"
Jul 16 13:06:47.310: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-secrets-b67b30cf-9040-47be-ae43-0b298c666150 container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:06:47.398: INFO: Waiting for pod pod-secrets-b67b30cf-9040-47be-ae43-0b298c666150 to disappear
Jul 16 13:06:47.403: INFO: Pod pod-secrets-b67b30cf-9040-47be-ae43-0b298c666150 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:06:47.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7366" for this suite.
Jul 16 13:06:53.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:06:54.159: INFO: namespace secrets-7366 deletion completed in 6.749454119s

• [SLOW TEST:10.986 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:06:54.160: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 16 13:06:54.265: INFO: Waiting up to 5m0s for pod "downward-api-8e1e774d-3194-480e-a392-22c4c359112e" in namespace "downward-api-8384" to be "success or failure"
Jul 16 13:06:54.270: INFO: Pod "downward-api-8e1e774d-3194-480e-a392-22c4c359112e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.887892ms
Jul 16 13:06:56.289: INFO: Pod "downward-api-8e1e774d-3194-480e-a392-22c4c359112e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024232295s
Jul 16 13:06:58.300: INFO: Pod "downward-api-8e1e774d-3194-480e-a392-22c4c359112e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034893348s
Jul 16 13:07:00.316: INFO: Pod "downward-api-8e1e774d-3194-480e-a392-22c4c359112e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050839984s
STEP: Saw pod success
Jul 16 13:07:00.316: INFO: Pod "downward-api-8e1e774d-3194-480e-a392-22c4c359112e" satisfied condition "success or failure"
Jul 16 13:07:00.322: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downward-api-8e1e774d-3194-480e-a392-22c4c359112e container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:07:00.746: INFO: Waiting for pod downward-api-8e1e774d-3194-480e-a392-22c4c359112e to disappear
Jul 16 13:07:00.755: INFO: Pod downward-api-8e1e774d-3194-480e-a392-22c4c359112e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:07:00.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8384" for this suite.
Jul 16 13:07:06.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:07:07.047: INFO: namespace downward-api-8384 deletion completed in 6.284391895s

• [SLOW TEST:12.888 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:07:07.050: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0716 13:07:37.812774      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 16 13:07:37.813: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:07:37.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2869" for this suite.
Jul 16 13:07:43.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:07:44.242: INFO: namespace gc-2869 deletion completed in 6.420534226s

• [SLOW TEST:37.192 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:07:44.245: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-369
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-369
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-369
Jul 16 13:07:44.414: INFO: Found 0 stateful pods, waiting for 1
Jul 16 13:07:54.429: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 16 13:07:54.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 13:07:55.831: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 13:07:55.831: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 13:07:55.831: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 13:07:55.841: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 16 13:08:05.851: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 13:08:05.852: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 13:08:05.880: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:05.880: INFO: ss-0  conformance-worker-54b54f4f98-f9trz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:44 +0000 UTC  }]
Jul 16 13:08:05.880: INFO: 
Jul 16 13:08:05.880: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 16 13:08:06.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994157565s
Jul 16 13:08:07.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98241053s
Jul 16 13:08:08.911: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975957236s
Jul 16 13:08:09.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963195776s
Jul 16 13:08:10.938: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.945151332s
Jul 16 13:08:12.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936170713s
Jul 16 13:08:13.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.872848392s
Jul 16 13:08:14.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.865963367s
Jul 16 13:08:15.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 849.930746ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-369
Jul 16 13:08:16.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:08:16.768: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 16 13:08:16.768: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 13:08:16.768: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 13:08:16.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:08:17.610: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 16 13:08:17.610: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 13:08:17.610: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 13:08:17.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:08:18.449: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 16 13:08:18.450: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 13:08:18.450: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 13:08:18.459: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:08:18.459: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 13:08:18.459: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 16 13:08:18.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 13:08:19.161: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 13:08:19.161: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 13:08:19.161: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 13:08:19.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 13:08:20.143: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 13:08:20.143: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 13:08:20.143: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 13:08:20.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 13:08:21.105: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 13:08:21.105: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 13:08:21.105: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 13:08:21.105: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 13:08:21.112: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 16 13:08:31.130: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 13:08:31.131: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 13:08:31.131: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 13:08:31.154: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:31.154: INFO: ss-0  conformance-worker-54b54f4f98-f9trz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:44 +0000 UTC  }]
Jul 16 13:08:31.155: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:31.155: INFO: ss-2  conformance-worker-54b54f4f98-nqplr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:31.155: INFO: 
Jul 16 13:08:31.155: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 16 13:08:32.171: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:32.171: INFO: ss-0  conformance-worker-54b54f4f98-f9trz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:44 +0000 UTC  }]
Jul 16 13:08:32.171: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:32.171: INFO: ss-2  conformance-worker-54b54f4f98-nqplr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:32.171: INFO: 
Jul 16 13:08:32.171: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 16 13:08:33.241: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:33.242: INFO: ss-0  conformance-worker-54b54f4f98-f9trz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:07:44 +0000 UTC  }]
Jul 16 13:08:33.242: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:33.242: INFO: ss-2  conformance-worker-54b54f4f98-nqplr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:33.242: INFO: 
Jul 16 13:08:33.242: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 16 13:08:34.252: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:34.252: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:34.252: INFO: ss-2  conformance-worker-54b54f4f98-nqplr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:34.252: INFO: 
Jul 16 13:08:34.252: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 13:08:35.313: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:35.313: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:35.313: INFO: ss-2  conformance-worker-54b54f4f98-nqplr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:35.313: INFO: 
Jul 16 13:08:35.313: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 13:08:36.320: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:36.321: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:36.321: INFO: ss-2  conformance-worker-54b54f4f98-nqplr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:36.321: INFO: 
Jul 16 13:08:36.321: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 13:08:37.334: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:37.334: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:37.335: INFO: ss-2  conformance-worker-54b54f4f98-nqplr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:37.335: INFO: 
Jul 16 13:08:37.335: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 13:08:38.342: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:38.342: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:38.343: INFO: ss-2  conformance-worker-54b54f4f98-nqplr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:38.343: INFO: 
Jul 16 13:08:38.343: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 16 13:08:39.349: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:39.349: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:39.349: INFO: 
Jul 16 13:08:39.349: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 16 13:08:40.356: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jul 16 13:08:40.356: INFO: ss-1  conformance-worker-54b54f4f98-m9svg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:08:05 +0000 UTC  }]
Jul 16 13:08:40.356: INFO: 
Jul 16 13:08:40.356: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-369
Jul 16 13:08:41.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:08:41.741: INFO: rc: 1
Jul 16 13:08:41.741: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002e961b0 exit status 1 <nil> <nil> true [0xc002f56260 0xc002f562b8 0xc002f562d0] [0xc002f56260 0xc002f562b8 0xc002f562d0] [0xc002f562a0 0xc002f562c8] [0x9d17b0 0x9d17b0] 0xc0034b6de0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jul 16 13:08:51.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:08:51.876: INFO: rc: 1
Jul 16 13:08:51.877: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef6b40 exit status 1 <nil> <nil> true [0xc002d5a348 0xc002d5a390 0xc002d5a3d0] [0xc002d5a348 0xc002d5a390 0xc002d5a3d0] [0xc002d5a370 0xc002d5a3c8] [0x9d17b0 0x9d17b0] 0xc00303b800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:09:01.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:09:02.028: INFO: rc: 1
Jul 16 13:09:02.029: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef6f30 exit status 1 <nil> <nil> true [0xc002d5a3d8 0xc002d5a400 0xc002d5a418] [0xc002d5a3d8 0xc002d5a400 0xc002d5a418] [0xc002d5a3e8 0xc002d5a410] [0x9d17b0 0x9d17b0] 0xc00303bb60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:09:12.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:09:12.166: INFO: rc: 1
Jul 16 13:09:12.166: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef72c0 exit status 1 <nil> <nil> true [0xc002d5a420 0xc002d5a438 0xc002d5a478] [0xc002d5a420 0xc002d5a438 0xc002d5a478] [0xc002d5a430 0xc002d5a458] [0x9d17b0 0x9d17b0] 0xc00303bec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:09:22.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:09:22.318: INFO: rc: 1
Jul 16 13:09:22.318: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef76e0 exit status 1 <nil> <nil> true [0xc002d5a490 0xc002d5a4d8 0xc002d5a500] [0xc002d5a490 0xc002d5a4d8 0xc002d5a500] [0xc002d5a4b8 0xc002d5a4f8] [0x9d17b0 0x9d17b0] 0xc0034e2300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:09:32.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:09:32.468: INFO: rc: 1
Jul 16 13:09:32.469: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef7b00 exit status 1 <nil> <nil> true [0xc002d5a508 0xc002d5a538 0xc002d5a578] [0xc002d5a508 0xc002d5a538 0xc002d5a578] [0xc002d5a520 0xc002d5a558] [0x9d17b0 0x9d17b0] 0xc0034e2660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:09:42.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:09:42.615: INFO: rc: 1
Jul 16 13:09:42.615: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002e96540 exit status 1 <nil> <nil> true [0xc002f562d8 0xc002f56310 0xc002f56348] [0xc002f562d8 0xc002f56310 0xc002f56348] [0xc002f562f8 0xc002f56328] [0x9d17b0 0x9d17b0] 0xc0034b7140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:09:52.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:09:52.763: INFO: rc: 1
Jul 16 13:09:52.764: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002e968d0 exit status 1 <nil> <nil> true [0xc002f56368 0xc002f563a8 0xc002f563c0] [0xc002f56368 0xc002f563a8 0xc002f563c0] [0xc002f563a0 0xc002f563b8] [0x9d17b0 0x9d17b0] 0xc0034b74a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:10:02.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:10:02.897: INFO: rc: 1
Jul 16 13:10:02.898: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef7ec0 exit status 1 <nil> <nil> true [0xc002d5a580 0xc002d5a5c0 0xc002d5a5f0] [0xc002d5a580 0xc002d5a5c0 0xc002d5a5f0] [0xc002d5a5a8 0xc002d5a5d0] [0x9d17b0 0x9d17b0] 0xc0034e29c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:10:12.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:10:13.040: INFO: rc: 1
Jul 16 13:10:13.041: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d5c2d0 exit status 1 <nil> <nil> true [0xc002d5a618 0xc002d5a640 0xc002d5a658] [0xc002d5a618 0xc002d5a640 0xc002d5a658] [0xc002d5a638 0xc002d5a650] [0x9d17b0 0x9d17b0] 0xc0034e2de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:10:23.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:10:23.181: INFO: rc: 1
Jul 16 13:10:23.182: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d5c660 exit status 1 <nil> <nil> true [0xc002d5a668 0xc002d5a688 0xc002d5a6d8] [0xc002d5a668 0xc002d5a688 0xc002d5a6d8] [0xc002d5a678 0xc002d5a6d0] [0x9d17b0 0x9d17b0] 0xc0034e3140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:10:33.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:10:33.303: INFO: rc: 1
Jul 16 13:10:33.303: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff4330 exit status 1 <nil> <nil> true [0xc002f56000 0xc002f56048 0xc002f56060] [0xc002f56000 0xc002f56048 0xc002f56060] [0xc002f56030 0xc002f56058] [0x9d17b0 0x9d17b0] 0xc00303a360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:10:43.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:10:43.449: INFO: rc: 1
Jul 16 13:10:43.449: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef63c0 exit status 1 <nil> <nil> true [0xc002d5a008 0xc002d5a040 0xc002d5a070] [0xc002d5a008 0xc002d5a040 0xc002d5a070] [0xc002d5a038 0xc002d5a068] [0x9d17b0 0x9d17b0] 0xc0034b6360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:10:53.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:10:53.575: INFO: rc: 1
Jul 16 13:10:53.575: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef67b0 exit status 1 <nil> <nil> true [0xc002d5a078 0xc002d5a0b0 0xc002d5a0e8] [0xc002d5a078 0xc002d5a0b0 0xc002d5a0e8] [0xc002d5a0a8 0xc002d5a0c8] [0x9d17b0 0x9d17b0] 0xc0034b6720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:11:03.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:11:03.702: INFO: rc: 1
Jul 16 13:11:03.702: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef6bd0 exit status 1 <nil> <nil> true [0xc002d5a0f0 0xc002d5a120 0xc002d5a150] [0xc002d5a0f0 0xc002d5a120 0xc002d5a150] [0xc002d5a110 0xc002d5a148] [0x9d17b0 0x9d17b0] 0xc0034b6c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:11:13.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:11:13.833: INFO: rc: 1
Jul 16 13:11:13.833: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff4690 exit status 1 <nil> <nil> true [0xc002f56068 0xc002f56080 0xc002f560b8] [0xc002f56068 0xc002f56080 0xc002f560b8] [0xc002f56078 0xc002f560a0] [0x9d17b0 0x9d17b0] 0xc00303a900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:11:23.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:11:23.972: INFO: rc: 1
Jul 16 13:11:23.972: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff4a20 exit status 1 <nil> <nil> true [0xc002f560c0 0xc002f560d8 0xc002f560f0] [0xc002f560c0 0xc002f560d8 0xc002f560f0] [0xc002f560d0 0xc002f560e8] [0x9d17b0 0x9d17b0] 0xc00303ad20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:11:33.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:11:34.092: INFO: rc: 1
Jul 16 13:11:34.092: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff4db0 exit status 1 <nil> <nil> true [0xc002f560f8 0xc002f56110 0xc002f56140] [0xc002f560f8 0xc002f56110 0xc002f56140] [0xc002f56108 0xc002f56128] [0x9d17b0 0x9d17b0] 0xc00303b0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:11:44.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:11:44.217: INFO: rc: 1
Jul 16 13:11:44.217: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff5140 exit status 1 <nil> <nil> true [0xc002f56148 0xc002f56180 0xc002f561c8] [0xc002f56148 0xc002f56180 0xc002f561c8] [0xc002f56160 0xc002f561b0] [0x9d17b0 0x9d17b0] 0xc00303b440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:11:54.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:11:54.308: INFO: rc: 1
Jul 16 13:11:54.308: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff54d0 exit status 1 <nil> <nil> true [0xc002f561d0 0xc002f56210 0xc002f56238] [0xc002f561d0 0xc002f56210 0xc002f56238] [0xc002f561f0 0xc002f56230] [0x9d17b0 0x9d17b0] 0xc00303b7a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:12:04.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:12:04.435: INFO: rc: 1
Jul 16 13:12:04.436: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef7050 exit status 1 <nil> <nil> true [0xc002d5a178 0xc002d5a1c8 0xc002d5a200] [0xc002d5a178 0xc002d5a1c8 0xc002d5a200] [0xc002d5a1a8 0xc002d5a1e8] [0x9d17b0 0x9d17b0] 0xc0034b6f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:12:14.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:12:14.569: INFO: rc: 1
Jul 16 13:12:14.569: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff5830 exit status 1 <nil> <nil> true [0xc002f56240 0xc002f562a0 0xc002f562c8] [0xc002f56240 0xc002f562a0 0xc002f562c8] [0xc002f56280 0xc002f562c0] [0x9d17b0 0x9d17b0] 0xc00303bb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:12:24.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:12:24.698: INFO: rc: 1
Jul 16 13:12:24.699: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef6360 exit status 1 <nil> <nil> true [0xc002d5a010 0xc002d5a050 0xc002d5a078] [0xc002d5a010 0xc002d5a050 0xc002d5a078] [0xc002d5a040 0xc002d5a070] [0x9d17b0 0x9d17b0] 0xc0034b6360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:12:34.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:12:39.292: INFO: rc: 1
Jul 16 13:12:39.293: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef6780 exit status 1 <nil> <nil> true [0xc002d5a090 0xc002d5a0b8 0xc002d5a0f0] [0xc002d5a090 0xc002d5a0b8 0xc002d5a0f0] [0xc002d5a0b0 0xc002d5a0e8] [0x9d17b0 0x9d17b0] 0xc0034b6720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:12:49.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:12:49.427: INFO: rc: 1
Jul 16 13:12:49.427: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff4360 exit status 1 <nil> <nil> true [0xc002f56000 0xc002f56048 0xc002f56060] [0xc002f56000 0xc002f56048 0xc002f56060] [0xc002f56030 0xc002f56058] [0x9d17b0 0x9d17b0] 0xc00303a360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:12:59.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:12:59.592: INFO: rc: 1
Jul 16 13:12:59.592: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff46f0 exit status 1 <nil> <nil> true [0xc002f56068 0xc002f56080 0xc002f560b8] [0xc002f56068 0xc002f56080 0xc002f560b8] [0xc002f56078 0xc002f560a0] [0x9d17b0 0x9d17b0] 0xc00303a900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:13:09.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:13:09.748: INFO: rc: 1
Jul 16 13:13:09.749: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff4ab0 exit status 1 <nil> <nil> true [0xc002f560c0 0xc002f560d8 0xc002f560f0] [0xc002f560c0 0xc002f560d8 0xc002f560f0] [0xc002f560d0 0xc002f560e8] [0x9d17b0 0x9d17b0] 0xc00303ad20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:13:19.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:13:19.886: INFO: rc: 1
Jul 16 13:13:19.886: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef6c30 exit status 1 <nil> <nil> true [0xc002d5a108 0xc002d5a138 0xc002d5a178] [0xc002d5a108 0xc002d5a138 0xc002d5a178] [0xc002d5a120 0xc002d5a150] [0x9d17b0 0x9d17b0] 0xc0034b6c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:13:29.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:13:30.046: INFO: rc: 1
Jul 16 13:13:30.047: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ef7020 exit status 1 <nil> <nil> true [0xc002d5a190 0xc002d5a1e0 0xc002d5a208] [0xc002d5a190 0xc002d5a1e0 0xc002d5a208] [0xc002d5a1c8 0xc002d5a200] [0x9d17b0 0x9d17b0] 0xc0034b6f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:13:40.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:13:40.202: INFO: rc: 1
Jul 16 13:13:40.203: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002ff4e70 exit status 1 <nil> <nil> true [0xc002f560f8 0xc002f56110 0xc002f56140] [0xc002f560f8 0xc002f56110 0xc002f56140] [0xc002f56108 0xc002f56128] [0x9d17b0 0x9d17b0] 0xc00303b0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 16 13:13:50.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-369 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 13:13:50.338: INFO: rc: 1
Jul 16 13:13:50.339: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Jul 16 13:13:50.339: INFO: Scaling statefulset ss to 0
Jul 16 13:13:50.369: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 16 13:13:50.375: INFO: Deleting all statefulset in ns statefulset-369
Jul 16 13:13:50.382: INFO: Scaling statefulset ss to 0
Jul 16 13:13:50.412: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 13:13:50.418: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:13:50.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-369" for this suite.
Jul 16 13:13:58.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:13:58.961: INFO: namespace statefulset-369 deletion completed in 8.491179063s

• [SLOW TEST:374.717 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:13:58.966: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:13:59.117: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:14:03.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-365" for this suite.
Jul 16 13:14:43.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:14:43.700: INFO: namespace pods-365 deletion completed in 40.441792067s

• [SLOW TEST:44.734 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:14:43.705: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:14:47.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7613" for this suite.
Jul 16 13:15:36.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:15:36.355: INFO: namespace kubelet-test-7613 deletion completed in 48.474401178s

• [SLOW TEST:52.651 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:15:36.361: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 16 13:15:36.607: INFO: Waiting up to 5m0s for pod "downward-api-f0fd447d-1621-42b9-b70f-efead94313a8" in namespace "downward-api-8619" to be "success or failure"
Jul 16 13:15:36.615: INFO: Pod "downward-api-f0fd447d-1621-42b9-b70f-efead94313a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.523472ms
Jul 16 13:15:38.622: INFO: Pod "downward-api-f0fd447d-1621-42b9-b70f-efead94313a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015384791s
Jul 16 13:15:40.630: INFO: Pod "downward-api-f0fd447d-1621-42b9-b70f-efead94313a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023788483s
Jul 16 13:15:42.639: INFO: Pod "downward-api-f0fd447d-1621-42b9-b70f-efead94313a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032658942s
Jul 16 13:15:44.658: INFO: Pod "downward-api-f0fd447d-1621-42b9-b70f-efead94313a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.05149899s
Jul 16 13:15:46.697: INFO: Pod "downward-api-f0fd447d-1621-42b9-b70f-efead94313a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.089946684s
STEP: Saw pod success
Jul 16 13:15:46.697: INFO: Pod "downward-api-f0fd447d-1621-42b9-b70f-efead94313a8" satisfied condition "success or failure"
Jul 16 13:15:46.709: INFO: Trying to get logs from node conformance-worker-54b54f4f98-m9svg pod downward-api-f0fd447d-1621-42b9-b70f-efead94313a8 container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:15:46.785: INFO: Waiting for pod downward-api-f0fd447d-1621-42b9-b70f-efead94313a8 to disappear
Jul 16 13:15:46.789: INFO: Pod downward-api-f0fd447d-1621-42b9-b70f-efead94313a8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:15:46.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8619" for this suite.
Jul 16 13:15:52.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:15:53.214: INFO: namespace downward-api-8619 deletion completed in 6.416072516s

• [SLOW TEST:16.854 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:15:53.225: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 13:15:53.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8227'
Jul 16 13:15:53.529: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 13:15:53.530: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 16 13:15:53.595: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-km92s]
Jul 16 13:15:53.595: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-km92s" in namespace "kubectl-8227" to be "running and ready"
Jul 16 13:15:53.635: INFO: Pod "e2e-test-nginx-rc-km92s": Phase="Pending", Reason="", readiness=false. Elapsed: 39.647046ms
Jul 16 13:15:55.655: INFO: Pod "e2e-test-nginx-rc-km92s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060443433s
Jul 16 13:15:57.664: INFO: Pod "e2e-test-nginx-rc-km92s": Phase="Running", Reason="", readiness=true. Elapsed: 4.069306507s
Jul 16 13:15:57.664: INFO: Pod "e2e-test-nginx-rc-km92s" satisfied condition "running and ready"
Jul 16 13:15:57.664: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-km92s]
Jul 16 13:15:57.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 logs rc/e2e-test-nginx-rc --namespace=kubectl-8227'
Jul 16 13:15:57.872: INFO: stderr: ""
Jul 16 13:15:57.872: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jul 16 13:15:57.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete rc e2e-test-nginx-rc --namespace=kubectl-8227'
Jul 16 13:15:58.016: INFO: stderr: ""
Jul 16 13:15:58.016: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:15:58.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8227" for this suite.
Jul 16 13:16:06.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:16:06.568: INFO: namespace kubectl-8227 deletion completed in 8.543738241s

• [SLOW TEST:13.344 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:16:06.572: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jul 16 13:16:06.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 cluster-info'
Jul 16 13:16:06.833: INFO: stderr: ""
Jul 16 13:16:06.833: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:16:06.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7602" for this suite.
Jul 16 13:16:12.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:16:13.320: INFO: namespace kubectl-7602 deletion completed in 6.465016381s

• [SLOW TEST:6.749 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:16:13.327: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-41cf52f3-7e31-44da-b9d5-9871c4444734
STEP: Creating a pod to test consume configMaps
Jul 16 13:16:13.662: INFO: Waiting up to 5m0s for pod "pod-configmaps-78568d1f-b1b6-4bfa-8e3c-e7ff4be03e46" in namespace "configmap-4520" to be "success or failure"
Jul 16 13:16:13.668: INFO: Pod "pod-configmaps-78568d1f-b1b6-4bfa-8e3c-e7ff4be03e46": Phase="Pending", Reason="", readiness=false. Elapsed: 5.745922ms
Jul 16 13:16:15.678: INFO: Pod "pod-configmaps-78568d1f-b1b6-4bfa-8e3c-e7ff4be03e46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016367895s
Jul 16 13:16:17.688: INFO: Pod "pod-configmaps-78568d1f-b1b6-4bfa-8e3c-e7ff4be03e46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026099397s
STEP: Saw pod success
Jul 16 13:16:17.688: INFO: Pod "pod-configmaps-78568d1f-b1b6-4bfa-8e3c-e7ff4be03e46" satisfied condition "success or failure"
Jul 16 13:16:17.695: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-78568d1f-b1b6-4bfa-8e3c-e7ff4be03e46 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:16:17.783: INFO: Waiting for pod pod-configmaps-78568d1f-b1b6-4bfa-8e3c-e7ff4be03e46 to disappear
Jul 16 13:16:17.790: INFO: Pod pod-configmaps-78568d1f-b1b6-4bfa-8e3c-e7ff4be03e46 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:16:17.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4520" for this suite.
Jul 16 13:16:23.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:16:24.181: INFO: namespace configmap-4520 deletion completed in 6.382480343s

• [SLOW TEST:10.854 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:16:24.186: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:16:24.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b48442d-9d53-4757-8b2d-d12081f89615" in namespace "projected-623" to be "success or failure"
Jul 16 13:16:24.506: INFO: Pod "downwardapi-volume-0b48442d-9d53-4757-8b2d-d12081f89615": Phase="Pending", Reason="", readiness=false. Elapsed: 66.571311ms
Jul 16 13:16:26.515: INFO: Pod "downwardapi-volume-0b48442d-9d53-4757-8b2d-d12081f89615": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074938942s
Jul 16 13:16:28.522: INFO: Pod "downwardapi-volume-0b48442d-9d53-4757-8b2d-d12081f89615": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082387171s
STEP: Saw pod success
Jul 16 13:16:28.522: INFO: Pod "downwardapi-volume-0b48442d-9d53-4757-8b2d-d12081f89615" satisfied condition "success or failure"
Jul 16 13:16:28.530: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-0b48442d-9d53-4757-8b2d-d12081f89615 container client-container: <nil>
STEP: delete the pod
Jul 16 13:16:28.588: INFO: Waiting for pod downwardapi-volume-0b48442d-9d53-4757-8b2d-d12081f89615 to disappear
Jul 16 13:16:28.594: INFO: Pod downwardapi-volume-0b48442d-9d53-4757-8b2d-d12081f89615 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:16:28.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-623" for this suite.
Jul 16 13:16:34.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:16:34.936: INFO: namespace projected-623 deletion completed in 6.333643178s

• [SLOW TEST:10.750 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:16:34.937: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-d7b976f8-f134-4c9a-bda1-5e02449ef84c
STEP: Creating a pod to test consume configMaps
Jul 16 13:16:35.089: INFO: Waiting up to 5m0s for pod "pod-configmaps-a763320b-d5be-4736-86e6-933a9ef24b21" in namespace "configmap-1896" to be "success or failure"
Jul 16 13:16:35.104: INFO: Pod "pod-configmaps-a763320b-d5be-4736-86e6-933a9ef24b21": Phase="Pending", Reason="", readiness=false. Elapsed: 14.693581ms
Jul 16 13:16:37.111: INFO: Pod "pod-configmaps-a763320b-d5be-4736-86e6-933a9ef24b21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022008959s
Jul 16 13:16:39.120: INFO: Pod "pod-configmaps-a763320b-d5be-4736-86e6-933a9ef24b21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030869005s
STEP: Saw pod success
Jul 16 13:16:39.120: INFO: Pod "pod-configmaps-a763320b-d5be-4736-86e6-933a9ef24b21" satisfied condition "success or failure"
Jul 16 13:16:39.126: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-a763320b-d5be-4736-86e6-933a9ef24b21 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:16:39.203: INFO: Waiting for pod pod-configmaps-a763320b-d5be-4736-86e6-933a9ef24b21 to disappear
Jul 16 13:16:39.209: INFO: Pod pod-configmaps-a763320b-d5be-4736-86e6-933a9ef24b21 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:16:39.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1896" for this suite.
Jul 16 13:16:47.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:16:48.024: INFO: namespace configmap-1896 deletion completed in 8.805622967s

• [SLOW TEST:13.087 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:16:48.027: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:16:48.123: INFO: Creating ReplicaSet my-hostname-basic-67cd6ba4-b256-450b-aabb-ce4d3cc7a39d
Jul 16 13:16:48.355: INFO: Pod name my-hostname-basic-67cd6ba4-b256-450b-aabb-ce4d3cc7a39d: Found 1 pods out of 1
Jul 16 13:16:48.355: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-67cd6ba4-b256-450b-aabb-ce4d3cc7a39d" is running
Jul 16 13:16:54.392: INFO: Pod "my-hostname-basic-67cd6ba4-b256-450b-aabb-ce4d3cc7a39d-c5f8f" is running (conditions: [])
Jul 16 13:16:54.393: INFO: Trying to dial the pod
Jul 16 13:16:59.572: INFO: Controller my-hostname-basic-67cd6ba4-b256-450b-aabb-ce4d3cc7a39d: Got expected result from replica 1 [my-hostname-basic-67cd6ba4-b256-450b-aabb-ce4d3cc7a39d-c5f8f]: "my-hostname-basic-67cd6ba4-b256-450b-aabb-ce4d3cc7a39d-c5f8f", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:16:59.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4330" for this suite.
Jul 16 13:17:05.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:17:05.994: INFO: namespace replicaset-4330 deletion completed in 6.40855704s

• [SLOW TEST:17.967 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:17:05.995: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 16 13:17:06.132: INFO: Waiting up to 5m0s for pod "downward-api-719aba11-1a71-4e2f-aadb-22352c31de90" in namespace "downward-api-4638" to be "success or failure"
Jul 16 13:17:06.141: INFO: Pod "downward-api-719aba11-1a71-4e2f-aadb-22352c31de90": Phase="Pending", Reason="", readiness=false. Elapsed: 8.897125ms
Jul 16 13:17:08.147: INFO: Pod "downward-api-719aba11-1a71-4e2f-aadb-22352c31de90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01523218s
Jul 16 13:17:10.201: INFO: Pod "downward-api-719aba11-1a71-4e2f-aadb-22352c31de90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068782121s
STEP: Saw pod success
Jul 16 13:17:10.201: INFO: Pod "downward-api-719aba11-1a71-4e2f-aadb-22352c31de90" satisfied condition "success or failure"
Jul 16 13:17:10.207: INFO: Trying to get logs from node conformance-worker-54b54f4f98-m9svg pod downward-api-719aba11-1a71-4e2f-aadb-22352c31de90 container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:17:10.320: INFO: Waiting for pod downward-api-719aba11-1a71-4e2f-aadb-22352c31de90 to disappear
Jul 16 13:17:10.332: INFO: Pod downward-api-719aba11-1a71-4e2f-aadb-22352c31de90 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:17:10.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4638" for this suite.
Jul 16 13:17:16.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:17:16.709: INFO: namespace downward-api-4638 deletion completed in 6.357712094s

• [SLOW TEST:10.715 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:17:16.715: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jul 16 13:17:17.466: INFO: created pod pod-service-account-defaultsa
Jul 16 13:17:17.466: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 16 13:17:17.501: INFO: created pod pod-service-account-mountsa
Jul 16 13:17:17.501: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 16 13:17:17.514: INFO: created pod pod-service-account-nomountsa
Jul 16 13:17:17.514: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 16 13:17:17.522: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 16 13:17:17.522: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 16 13:17:17.577: INFO: created pod pod-service-account-mountsa-mountspec
Jul 16 13:17:17.577: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 16 13:17:17.589: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 16 13:17:17.589: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 16 13:17:17.639: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 16 13:17:17.639: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 16 13:17:17.660: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 16 13:17:17.660: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 16 13:17:17.681: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 16 13:17:17.681: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:17:17.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8128" for this suite.
Jul 16 13:17:41.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:17:42.157: INFO: namespace svcaccounts-8128 deletion completed in 24.443544941s

• [SLOW TEST:25.443 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:17:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:17:46.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4197" for this suite.
Jul 16 13:17:52.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:17:52.655: INFO: namespace kubelet-test-4197 deletion completed in 6.356577324s

• [SLOW TEST:10.493 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:17:52.659: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:17:52.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-401827c4-c0f6-4590-b892-8dc24b00d447" in namespace "downward-api-5754" to be "success or failure"
Jul 16 13:17:52.802: INFO: Pod "downwardapi-volume-401827c4-c0f6-4590-b892-8dc24b00d447": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059465ms
Jul 16 13:17:54.816: INFO: Pod "downwardapi-volume-401827c4-c0f6-4590-b892-8dc24b00d447": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020219128s
Jul 16 13:17:56.829: INFO: Pod "downwardapi-volume-401827c4-c0f6-4590-b892-8dc24b00d447": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032777086s
STEP: Saw pod success
Jul 16 13:17:56.829: INFO: Pod "downwardapi-volume-401827c4-c0f6-4590-b892-8dc24b00d447" satisfied condition "success or failure"
Jul 16 13:17:56.835: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-401827c4-c0f6-4590-b892-8dc24b00d447 container client-container: <nil>
STEP: delete the pod
Jul 16 13:17:56.971: INFO: Waiting for pod downwardapi-volume-401827c4-c0f6-4590-b892-8dc24b00d447 to disappear
Jul 16 13:17:56.977: INFO: Pod downwardapi-volume-401827c4-c0f6-4590-b892-8dc24b00d447 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:17:56.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5754" for this suite.
Jul 16 13:18:03.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:18:03.384: INFO: namespace downward-api-5754 deletion completed in 6.395388808s

• [SLOW TEST:10.726 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:18:03.389: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-dc88ecda-bfbc-4719-b29d-5a592c035304
STEP: Creating configMap with name cm-test-opt-upd-84c6a393-bc92-45ac-bd56-6b4338d06be9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-dc88ecda-bfbc-4719-b29d-5a592c035304
STEP: Updating configmap cm-test-opt-upd-84c6a393-bc92-45ac-bd56-6b4338d06be9
STEP: Creating configMap with name cm-test-opt-create-f81a75ad-0ccb-4db1-9953-753d42b5124e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:19:13.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8600" for this suite.
Jul 16 13:19:37.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:19:38.150: INFO: namespace configmap-8600 deletion completed in 24.535657399s

• [SLOW TEST:94.762 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:19:38.151: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3444
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 16 13:19:38.248: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 16 13:20:02.618: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.12:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3444 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:20:02.619: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:20:03.460: INFO: Found all expected endpoints: [netserver-0]
Jul 16 13:20:03.467: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.25:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3444 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:20:03.467: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:20:04.026: INFO: Found all expected endpoints: [netserver-1]
Jul 16 13:20:04.034: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.14:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3444 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:20:04.034: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:20:04.754: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:20:04.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3444" for this suite.
Jul 16 13:20:28.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:20:29.457: INFO: namespace pod-network-test-3444 deletion completed in 24.590308217s

• [SLOW TEST:51.306 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:20:29.459: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:20:29.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ac1f2c3-a87b-4805-b960-0c7c6d980f0d" in namespace "downward-api-5809" to be "success or failure"
Jul 16 13:20:29.745: INFO: Pod "downwardapi-volume-5ac1f2c3-a87b-4805-b960-0c7c6d980f0d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874448ms
Jul 16 13:20:31.751: INFO: Pod "downwardapi-volume-5ac1f2c3-a87b-4805-b960-0c7c6d980f0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012668326s
Jul 16 13:20:33.758: INFO: Pod "downwardapi-volume-5ac1f2c3-a87b-4805-b960-0c7c6d980f0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019817776s
STEP: Saw pod success
Jul 16 13:20:33.759: INFO: Pod "downwardapi-volume-5ac1f2c3-a87b-4805-b960-0c7c6d980f0d" satisfied condition "success or failure"
Jul 16 13:20:33.768: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-5ac1f2c3-a87b-4805-b960-0c7c6d980f0d container client-container: <nil>
STEP: delete the pod
Jul 16 13:20:34.072: INFO: Waiting for pod downwardapi-volume-5ac1f2c3-a87b-4805-b960-0c7c6d980f0d to disappear
Jul 16 13:20:34.078: INFO: Pod downwardapi-volume-5ac1f2c3-a87b-4805-b960-0c7c6d980f0d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:20:34.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5809" for this suite.
Jul 16 13:20:40.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:20:40.525: INFO: namespace downward-api-5809 deletion completed in 6.438376976s

• [SLOW TEST:11.068 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:20:40.535: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 16 13:20:51.782: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 13:20:51.793: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 13:20:53.793: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 13:20:53.809: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 13:20:55.793: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 13:20:55.808: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 13:20:57.793: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 13:20:57.954: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 13:20:59.793: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 13:20:59.803: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 13:21:01.793: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 13:21:01.803: INFO: Pod pod-with-prestop-http-hook still exists
Jul 16 13:21:03.793: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 16 13:21:03.803: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:21:03.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5807" for this suite.
Jul 16 13:21:31.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:21:32.161: INFO: namespace container-lifecycle-hook-5807 deletion completed in 28.294053087s

• [SLOW TEST:51.626 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:21:32.164: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:22:32.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6187" for this suite.
Jul 16 13:22:56.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:22:56.780: INFO: namespace container-probe-6187 deletion completed in 24.514229855s

• [SLOW TEST:84.617 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:22:56.783: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jul 16 13:22:56.969: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3255" to be "success or failure"
Jul 16 13:22:56.975: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.498243ms
Jul 16 13:22:58.989: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019560403s
Jul 16 13:23:00.996: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027235335s
STEP: Saw pod success
Jul 16 13:23:00.996: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 16 13:23:01.002: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 16 13:23:01.066: INFO: Waiting for pod pod-host-path-test to disappear
Jul 16 13:23:01.076: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:23:01.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3255" for this suite.
Jul 16 13:23:07.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:23:07.419: INFO: namespace hostpath-3255 deletion completed in 6.334155457s

• [SLOW TEST:10.636 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:23:07.425: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 16 13:23:07.619: INFO: Waiting up to 5m0s for pod "downward-api-6cc083c6-279d-499e-89c6-2a16032efe1b" in namespace "downward-api-2988" to be "success or failure"
Jul 16 13:23:07.625: INFO: Pod "downward-api-6cc083c6-279d-499e-89c6-2a16032efe1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.151439ms
Jul 16 13:23:09.642: INFO: Pod "downward-api-6cc083c6-279d-499e-89c6-2a16032efe1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022952685s
Jul 16 13:23:11.648: INFO: Pod "downward-api-6cc083c6-279d-499e-89c6-2a16032efe1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028649832s
STEP: Saw pod success
Jul 16 13:23:11.648: INFO: Pod "downward-api-6cc083c6-279d-499e-89c6-2a16032efe1b" satisfied condition "success or failure"
Jul 16 13:23:11.657: INFO: Trying to get logs from node conformance-worker-54b54f4f98-m9svg pod downward-api-6cc083c6-279d-499e-89c6-2a16032efe1b container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:23:11.779: INFO: Waiting for pod downward-api-6cc083c6-279d-499e-89c6-2a16032efe1b to disappear
Jul 16 13:23:11.784: INFO: Pod downward-api-6cc083c6-279d-499e-89c6-2a16032efe1b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:23:11.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2988" for this suite.
Jul 16 13:23:17.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:23:18.145: INFO: namespace downward-api-2988 deletion completed in 6.354372254s

• [SLOW TEST:10.720 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:23:18.153: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e4204a6c-7326-45e2-9a66-7737d19e27de
STEP: Creating a pod to test consume secrets
Jul 16 13:23:18.286: INFO: Waiting up to 5m0s for pod "pod-secrets-d6838327-4313-44c0-b380-dab25f78a15c" in namespace "secrets-6090" to be "success or failure"
Jul 16 13:23:18.319: INFO: Pod "pod-secrets-d6838327-4313-44c0-b380-dab25f78a15c": Phase="Pending", Reason="", readiness=false. Elapsed: 32.201909ms
Jul 16 13:23:20.338: INFO: Pod "pod-secrets-d6838327-4313-44c0-b380-dab25f78a15c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051702507s
Jul 16 13:23:22.345: INFO: Pod "pod-secrets-d6838327-4313-44c0-b380-dab25f78a15c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057977262s
STEP: Saw pod success
Jul 16 13:23:22.345: INFO: Pod "pod-secrets-d6838327-4313-44c0-b380-dab25f78a15c" satisfied condition "success or failure"
Jul 16 13:23:22.349: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-secrets-d6838327-4313-44c0-b380-dab25f78a15c container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:23:22.406: INFO: Waiting for pod pod-secrets-d6838327-4313-44c0-b380-dab25f78a15c to disappear
Jul 16 13:23:22.413: INFO: Pod pod-secrets-d6838327-4313-44c0-b380-dab25f78a15c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:23:22.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6090" for this suite.
Jul 16 13:23:28.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:23:28.780: INFO: namespace secrets-6090 deletion completed in 6.360055432s

• [SLOW TEST:10.627 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:23:28.783: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 16 13:23:29.044: INFO: Waiting up to 5m0s for pod "pod-968feb5a-e013-4b12-b97c-ae4ba5a6ed36" in namespace "emptydir-9640" to be "success or failure"
Jul 16 13:23:29.049: INFO: Pod "pod-968feb5a-e013-4b12-b97c-ae4ba5a6ed36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525195ms
Jul 16 13:23:31.059: INFO: Pod "pod-968feb5a-e013-4b12-b97c-ae4ba5a6ed36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014429321s
Jul 16 13:23:33.155: INFO: Pod "pod-968feb5a-e013-4b12-b97c-ae4ba5a6ed36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.110941696s
STEP: Saw pod success
Jul 16 13:23:33.155: INFO: Pod "pod-968feb5a-e013-4b12-b97c-ae4ba5a6ed36" satisfied condition "success or failure"
Jul 16 13:23:33.160: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-968feb5a-e013-4b12-b97c-ae4ba5a6ed36 container test-container: <nil>
STEP: delete the pod
Jul 16 13:23:33.302: INFO: Waiting for pod pod-968feb5a-e013-4b12-b97c-ae4ba5a6ed36 to disappear
Jul 16 13:23:33.306: INFO: Pod pod-968feb5a-e013-4b12-b97c-ae4ba5a6ed36 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:23:33.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9640" for this suite.
Jul 16 13:23:39.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:23:39.686: INFO: namespace emptydir-9640 deletion completed in 6.373110243s

• [SLOW TEST:10.905 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:23:39.702: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:23:39.833: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 16 13:23:39.853: INFO: Number of nodes with available pods: 0
Jul 16 13:23:39.853: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 16 13:23:39.909: INFO: Number of nodes with available pods: 0
Jul 16 13:23:39.909: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:40.916: INFO: Number of nodes with available pods: 0
Jul 16 13:23:40.916: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:41.916: INFO: Number of nodes with available pods: 0
Jul 16 13:23:41.916: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:42.916: INFO: Number of nodes with available pods: 1
Jul 16 13:23:42.916: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 16 13:23:43.051: INFO: Number of nodes with available pods: 1
Jul 16 13:23:43.051: INFO: Number of running nodes: 0, number of available pods: 1
Jul 16 13:23:44.058: INFO: Number of nodes with available pods: 0
Jul 16 13:23:44.058: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 16 13:23:44.075: INFO: Number of nodes with available pods: 0
Jul 16 13:23:44.075: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:45.081: INFO: Number of nodes with available pods: 0
Jul 16 13:23:45.081: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:46.081: INFO: Number of nodes with available pods: 0
Jul 16 13:23:46.081: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:47.089: INFO: Number of nodes with available pods: 0
Jul 16 13:23:47.089: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:48.085: INFO: Number of nodes with available pods: 0
Jul 16 13:23:48.085: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:49.097: INFO: Number of nodes with available pods: 0
Jul 16 13:23:49.097: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:50.088: INFO: Number of nodes with available pods: 0
Jul 16 13:23:50.088: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:51.082: INFO: Number of nodes with available pods: 0
Jul 16 13:23:51.082: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:52.085: INFO: Number of nodes with available pods: 0
Jul 16 13:23:52.086: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:53.085: INFO: Number of nodes with available pods: 0
Jul 16 13:23:53.086: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:54.083: INFO: Number of nodes with available pods: 0
Jul 16 13:23:54.083: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:55.083: INFO: Number of nodes with available pods: 0
Jul 16 13:23:55.084: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:23:56.084: INFO: Number of nodes with available pods: 1
Jul 16 13:23:56.084: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5448, will wait for the garbage collector to delete the pods
Jul 16 13:23:56.171: INFO: Deleting DaemonSet.extensions daemon-set took: 20.432757ms
Jul 16 13:23:56.780: INFO: Terminating DaemonSet.extensions daemon-set pods took: 609.280926ms
Jul 16 13:23:59.386: INFO: Number of nodes with available pods: 0
Jul 16 13:23:59.387: INFO: Number of running nodes: 0, number of available pods: 0
Jul 16 13:23:59.412: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5448/daemonsets","resourceVersion":"7631"},"items":null}

Jul 16 13:23:59.418: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5448/pods","resourceVersion":"7631"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:23:59.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5448" for this suite.
Jul 16 13:24:05.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:24:05.985: INFO: namespace daemonsets-5448 deletion completed in 6.456485843s

• [SLOW TEST:26.283 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:24:05.987: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jul 16 13:24:06.057: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 16 13:24:06.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-686'
Jul 16 13:24:07.220: INFO: stderr: ""
Jul 16 13:24:07.220: INFO: stdout: "service/redis-slave created\n"
Jul 16 13:24:07.220: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 16 13:24:07.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-686'
Jul 16 13:24:07.996: INFO: stderr: ""
Jul 16 13:24:07.996: INFO: stdout: "service/redis-master created\n"
Jul 16 13:24:07.996: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 16 13:24:07.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-686'
Jul 16 13:24:08.439: INFO: stderr: ""
Jul 16 13:24:08.439: INFO: stdout: "service/frontend created\n"
Jul 16 13:24:08.439: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 16 13:24:08.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-686'
Jul 16 13:24:08.861: INFO: stderr: ""
Jul 16 13:24:08.861: INFO: stdout: "deployment.apps/frontend created\n"
Jul 16 13:24:08.862: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 16 13:24:08.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-686'
Jul 16 13:24:09.283: INFO: stderr: ""
Jul 16 13:24:09.283: INFO: stdout: "deployment.apps/redis-master created\n"
Jul 16 13:24:09.285: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 16 13:24:09.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-686'
Jul 16 13:24:09.935: INFO: stderr: ""
Jul 16 13:24:09.935: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul 16 13:24:09.935: INFO: Waiting for all frontend pods to be Running.
Jul 16 13:25:08.799: INFO: Waiting for frontend to serve content.
Jul 16 13:25:13.901: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul 16 13:25:24.028: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul 16 13:25:34.150: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul 16 13:25:39.296: INFO: Trying to add a new entry to the guestbook.
Jul 16 13:25:39.317: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 16 13:25:39.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-686'
Jul 16 13:25:39.534: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:25:39.534: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:25:39.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-686'
Jul 16 13:25:39.721: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:25:39.721: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:25:39.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-686'
Jul 16 13:25:39.986: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:25:39.986: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:25:39.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-686'
Jul 16 13:25:40.163: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:25:40.163: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:25:40.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-686'
Jul 16 13:25:40.328: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:25:40.328: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 16 13:25:40.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-686'
Jul 16 13:25:40.474: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:25:40.474: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:25:40.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-686" for this suite.
Jul 16 13:26:20.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:26:20.824: INFO: namespace kubectl-686 deletion completed in 40.340254114s

• [SLOW TEST:134.837 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:26:20.826: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-nsjc
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 13:26:20.969: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nsjc" in namespace "subpath-4689" to be "success or failure"
Jul 16 13:26:20.974: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277955ms
Jul 16 13:26:22.998: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029165683s
Jul 16 13:26:25.004: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 4.034829924s
Jul 16 13:26:27.013: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 6.043841504s
Jul 16 13:26:29.050: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 8.080695422s
Jul 16 13:26:31.057: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 10.088462732s
Jul 16 13:26:33.064: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 12.095200343s
Jul 16 13:26:35.069: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 14.100484541s
Jul 16 13:26:37.077: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 16.107797344s
Jul 16 13:26:39.094: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 18.125393085s
Jul 16 13:26:41.101: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 20.132207322s
Jul 16 13:26:43.137: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Running", Reason="", readiness=true. Elapsed: 22.167805967s
Jul 16 13:26:45.150: INFO: Pod "pod-subpath-test-configmap-nsjc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.181544635s
STEP: Saw pod success
Jul 16 13:26:45.152: INFO: Pod "pod-subpath-test-configmap-nsjc" satisfied condition "success or failure"
Jul 16 13:26:45.157: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-subpath-test-configmap-nsjc container test-container-subpath-configmap-nsjc: <nil>
STEP: delete the pod
Jul 16 13:26:45.278: INFO: Waiting for pod pod-subpath-test-configmap-nsjc to disappear
Jul 16 13:26:45.284: INFO: Pod pod-subpath-test-configmap-nsjc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nsjc
Jul 16 13:26:45.285: INFO: Deleting pod "pod-subpath-test-configmap-nsjc" in namespace "subpath-4689"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:26:45.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4689" for this suite.
Jul 16 13:26:51.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:26:51.665: INFO: namespace subpath-4689 deletion completed in 6.366813186s

• [SLOW TEST:30.840 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:26:51.667: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9708
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 16 13:26:51.807: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 16 13:27:31.322: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.41:8080/dial?request=hostName&protocol=http&host=172.25.1.40&port=8080&tries=1'] Namespace:pod-network-test-9708 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:27:31.322: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:27:31.983: INFO: Waiting for endpoints: map[]
Jul 16 13:27:31.989: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.41:8080/dial?request=hostName&protocol=http&host=172.25.2.19&port=8080&tries=1'] Namespace:pod-network-test-9708 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:27:31.989: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:27:32.632: INFO: Waiting for endpoints: map[]
Jul 16 13:27:32.638: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.41:8080/dial?request=hostName&protocol=http&host=172.25.0.13&port=8080&tries=1'] Namespace:pod-network-test-9708 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:27:32.638: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:27:33.351: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:27:33.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9708" for this suite.
Jul 16 13:27:57.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:27:57.751: INFO: namespace pod-network-test-9708 deletion completed in 24.3926685s

• [SLOW TEST:66.085 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:27:57.758: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 16 13:27:57.928: INFO: Number of nodes with available pods: 0
Jul 16 13:27:57.928: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:27:58.941: INFO: Number of nodes with available pods: 0
Jul 16 13:27:58.941: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:27:59.941: INFO: Number of nodes with available pods: 0
Jul 16 13:27:59.941: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:28:00.968: INFO: Number of nodes with available pods: 2
Jul 16 13:28:00.969: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:01.944: INFO: Number of nodes with available pods: 3
Jul 16 13:28:01.945: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 16 13:28:02.002: INFO: Number of nodes with available pods: 2
Jul 16 13:28:02.002: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:03.017: INFO: Number of nodes with available pods: 2
Jul 16 13:28:03.017: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:04.022: INFO: Number of nodes with available pods: 2
Jul 16 13:28:04.022: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:05.017: INFO: Number of nodes with available pods: 2
Jul 16 13:28:05.017: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:06.019: INFO: Number of nodes with available pods: 2
Jul 16 13:28:06.019: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:07.016: INFO: Number of nodes with available pods: 2
Jul 16 13:28:07.016: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:08.017: INFO: Number of nodes with available pods: 2
Jul 16 13:28:08.017: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:09.016: INFO: Number of nodes with available pods: 2
Jul 16 13:28:09.016: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:10.019: INFO: Number of nodes with available pods: 2
Jul 16 13:28:10.019: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:11.019: INFO: Number of nodes with available pods: 2
Jul 16 13:28:11.019: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:12.017: INFO: Number of nodes with available pods: 2
Jul 16 13:28:12.018: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:13.017: INFO: Number of nodes with available pods: 2
Jul 16 13:28:13.017: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:14.018: INFO: Number of nodes with available pods: 2
Jul 16 13:28:14.018: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:15.041: INFO: Number of nodes with available pods: 2
Jul 16 13:28:15.041: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:16.026: INFO: Number of nodes with available pods: 2
Jul 16 13:28:16.026: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:17.023: INFO: Number of nodes with available pods: 2
Jul 16 13:28:17.023: INFO: Node conformance-worker-54b54f4f98-m9svg is running more than one daemon pod
Jul 16 13:28:18.018: INFO: Number of nodes with available pods: 3
Jul 16 13:28:18.018: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-364, will wait for the garbage collector to delete the pods
Jul 16 13:28:18.110: INFO: Deleting DaemonSet.extensions daemon-set took: 19.430323ms
Jul 16 13:28:18.710: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.374297ms
Jul 16 13:28:25.022: INFO: Number of nodes with available pods: 0
Jul 16 13:28:25.023: INFO: Number of running nodes: 0, number of available pods: 0
Jul 16 13:28:25.032: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-364/daemonsets","resourceVersion":"8886"},"items":null}

Jul 16 13:28:25.036: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-364/pods","resourceVersion":"8886"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:28:25.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-364" for this suite.
Jul 16 13:28:31.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:28:31.419: INFO: namespace daemonsets-364 deletion completed in 6.350766829s

• [SLOW TEST:33.661 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:28:31.420: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-jmc5j in namespace proxy-1680
I0716 13:28:31.612097      15 runners.go:180] Created replication controller with name: proxy-service-jmc5j, namespace: proxy-1680, replica count: 1
I0716 13:28:32.662993      15 runners.go:180] proxy-service-jmc5j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 13:28:33.663371      15 runners.go:180] proxy-service-jmc5j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 13:28:34.663934      15 runners.go:180] proxy-service-jmc5j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 13:28:35.664243      15 runners.go:180] proxy-service-jmc5j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 13:28:36.664495      15 runners.go:180] proxy-service-jmc5j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 13:28:37.664886      15 runners.go:180] proxy-service-jmc5j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0716 13:28:38.665251      15 runners.go:180] proxy-service-jmc5j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0716 13:28:39.665818      15 runners.go:180] proxy-service-jmc5j Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 16 13:28:39.676: INFO: setup took 8.162475801s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 16 13:28:39.927: INFO: (0) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 250.271272ms)
Jul 16 13:28:39.928: INFO: (0) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 251.263245ms)
Jul 16 13:28:39.928: INFO: (0) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 250.530848ms)
Jul 16 13:28:39.929: INFO: (0) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 251.412109ms)
Jul 16 13:28:39.929: INFO: (0) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 251.210767ms)
Jul 16 13:28:39.929: INFO: (0) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 251.14484ms)
Jul 16 13:28:39.929: INFO: (0) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 252.227007ms)
Jul 16 13:28:39.939: INFO: (0) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 260.790008ms)
Jul 16 13:28:39.940: INFO: (0) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 262.47447ms)
Jul 16 13:28:39.940: INFO: (0) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 263.329443ms)
Jul 16 13:28:39.940: INFO: (0) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 263.049351ms)
Jul 16 13:28:39.951: INFO: (0) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 273.686057ms)
Jul 16 13:28:39.951: INFO: (0) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 273.26907ms)
Jul 16 13:28:39.954: INFO: (0) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 277.416255ms)
Jul 16 13:28:39.954: INFO: (0) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 277.207894ms)
Jul 16 13:28:39.955: INFO: (0) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 277.4626ms)
Jul 16 13:28:39.973: INFO: (1) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 18.186425ms)
Jul 16 13:28:39.974: INFO: (1) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 17.957033ms)
Jul 16 13:28:39.973: INFO: (1) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 18.179457ms)
Jul 16 13:28:39.974: INFO: (1) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 18.603253ms)
Jul 16 13:28:39.974: INFO: (1) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 18.673045ms)
Jul 16 13:28:39.977: INFO: (1) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 20.564876ms)
Jul 16 13:28:39.977: INFO: (1) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 21.525141ms)
Jul 16 13:28:39.977: INFO: (1) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 21.106633ms)
Jul 16 13:28:39.977: INFO: (1) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 21.024453ms)
Jul 16 13:28:39.977: INFO: (1) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 20.804722ms)
Jul 16 13:28:39.977: INFO: (1) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 21.947329ms)
Jul 16 13:28:39.977: INFO: (1) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 20.758597ms)
Jul 16 13:28:39.982: INFO: (1) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 25.16331ms)
Jul 16 13:28:39.982: INFO: (1) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 26.050344ms)
Jul 16 13:28:39.982: INFO: (1) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 26.16025ms)
Jul 16 13:28:39.984: INFO: (1) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 27.458004ms)
Jul 16 13:28:39.996: INFO: (2) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 10.808472ms)
Jul 16 13:28:39.997: INFO: (2) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 12.22811ms)
Jul 16 13:28:39.997: INFO: (2) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 12.723865ms)
Jul 16 13:28:39.998: INFO: (2) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 12.808761ms)
Jul 16 13:28:39.998: INFO: (2) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 13.522813ms)
Jul 16 13:28:39.999: INFO: (2) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 13.069981ms)
Jul 16 13:28:39.999: INFO: (2) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 14.532779ms)
Jul 16 13:28:39.999: INFO: (2) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 14.292849ms)
Jul 16 13:28:39.999: INFO: (2) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 13.964115ms)
Jul 16 13:28:39.999: INFO: (2) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 14.042413ms)
Jul 16 13:28:39.999: INFO: (2) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 13.996789ms)
Jul 16 13:28:40.007: INFO: (2) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 22.135263ms)
Jul 16 13:28:40.007: INFO: (2) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 22.634298ms)
Jul 16 13:28:40.008: INFO: (2) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 23.239313ms)
Jul 16 13:28:40.008: INFO: (2) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 23.436853ms)
Jul 16 13:28:40.008: INFO: (2) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 23.78747ms)
Jul 16 13:28:40.038: INFO: (3) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 29.565203ms)
Jul 16 13:28:40.038: INFO: (3) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 29.404387ms)
Jul 16 13:28:40.039: INFO: (3) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 30.14562ms)
Jul 16 13:28:40.039: INFO: (3) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 30.066195ms)
Jul 16 13:28:40.039: INFO: (3) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 29.937463ms)
Jul 16 13:28:40.039: INFO: (3) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 29.659034ms)
Jul 16 13:28:40.040: INFO: (3) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 30.683348ms)
Jul 16 13:28:40.040: INFO: (3) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 30.987587ms)
Jul 16 13:28:40.040: INFO: (3) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 30.475051ms)
Jul 16 13:28:40.040: INFO: (3) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 30.404908ms)
Jul 16 13:28:40.040: INFO: (3) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 30.962315ms)
Jul 16 13:28:40.040: INFO: (3) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 30.719519ms)
Jul 16 13:28:40.043: INFO: (3) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 33.318609ms)
Jul 16 13:28:40.043: INFO: (3) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 33.572344ms)
Jul 16 13:28:40.054: INFO: (3) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 45.769621ms)
Jul 16 13:28:40.056: INFO: (3) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 47.618462ms)
Jul 16 13:28:40.066: INFO: (4) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 8.716621ms)
Jul 16 13:28:40.066: INFO: (4) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 9.071634ms)
Jul 16 13:28:40.067: INFO: (4) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 10.438635ms)
Jul 16 13:28:40.067: INFO: (4) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 10.328495ms)
Jul 16 13:28:40.067: INFO: (4) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 9.127079ms)
Jul 16 13:28:40.068: INFO: (4) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 10.715277ms)
Jul 16 13:28:40.068: INFO: (4) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 10.734044ms)
Jul 16 13:28:40.069: INFO: (4) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 11.760035ms)
Jul 16 13:28:40.069: INFO: (4) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 11.861908ms)
Jul 16 13:28:40.070: INFO: (4) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 12.707121ms)
Jul 16 13:28:40.069: INFO: (4) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 12.118739ms)
Jul 16 13:28:40.069: INFO: (4) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 12.57242ms)
Jul 16 13:28:40.071: INFO: (4) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 13.360625ms)
Jul 16 13:28:40.071: INFO: (4) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 14.29686ms)
Jul 16 13:28:40.071: INFO: (4) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 13.783911ms)
Jul 16 13:28:40.071: INFO: (4) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 13.744009ms)
Jul 16 13:28:40.079: INFO: (5) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 7.940841ms)
Jul 16 13:28:40.082: INFO: (5) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 10.182892ms)
Jul 16 13:28:40.082: INFO: (5) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 10.17963ms)
Jul 16 13:28:40.082: INFO: (5) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 10.399517ms)
Jul 16 13:28:40.085: INFO: (5) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 10.684782ms)
Jul 16 13:28:40.085: INFO: (5) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 11.540843ms)
Jul 16 13:28:40.085: INFO: (5) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 12.039274ms)
Jul 16 13:28:40.085: INFO: (5) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 11.965708ms)
Jul 16 13:28:40.085: INFO: (5) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 12.267825ms)
Jul 16 13:28:40.086: INFO: (5) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 13.09473ms)
Jul 16 13:28:40.087: INFO: (5) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 13.534357ms)
Jul 16 13:28:40.087: INFO: (5) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 15.141362ms)
Jul 16 13:28:40.088: INFO: (5) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 14.653141ms)
Jul 16 13:28:40.089: INFO: (5) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 16.643277ms)
Jul 16 13:28:40.089: INFO: (5) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 15.380069ms)
Jul 16 13:28:40.089: INFO: (5) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 17.120082ms)
Jul 16 13:28:40.104: INFO: (6) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 13.853478ms)
Jul 16 13:28:40.104: INFO: (6) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 14.133446ms)
Jul 16 13:28:40.104: INFO: (6) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 14.128027ms)
Jul 16 13:28:40.104: INFO: (6) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 14.549013ms)
Jul 16 13:28:40.105: INFO: (6) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 15.269034ms)
Jul 16 13:28:40.105: INFO: (6) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 15.213998ms)
Jul 16 13:28:40.105: INFO: (6) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 15.544788ms)
Jul 16 13:28:40.105: INFO: (6) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 15.023928ms)
Jul 16 13:28:40.105: INFO: (6) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 15.470525ms)
Jul 16 13:28:40.105: INFO: (6) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 15.350569ms)
Jul 16 13:28:40.106: INFO: (6) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 16.528495ms)
Jul 16 13:28:40.106: INFO: (6) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 16.455515ms)
Jul 16 13:28:40.107: INFO: (6) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 17.346594ms)
Jul 16 13:28:40.150: INFO: (6) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 60.201602ms)
Jul 16 13:28:40.151: INFO: (6) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 61.083853ms)
Jul 16 13:28:40.150: INFO: (6) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 60.849447ms)
Jul 16 13:28:40.162: INFO: (7) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 10.756971ms)
Jul 16 13:28:40.163: INFO: (7) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 10.542519ms)
Jul 16 13:28:40.163: INFO: (7) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 11.105654ms)
Jul 16 13:28:40.165: INFO: (7) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 12.114116ms)
Jul 16 13:28:40.165: INFO: (7) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 12.272962ms)
Jul 16 13:28:40.166: INFO: (7) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 13.548126ms)
Jul 16 13:28:40.167: INFO: (7) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 14.270783ms)
Jul 16 13:28:40.167: INFO: (7) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 14.848911ms)
Jul 16 13:28:40.167: INFO: (7) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 15.026093ms)
Jul 16 13:28:40.168: INFO: (7) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 14.899822ms)
Jul 16 13:28:40.168: INFO: (7) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 15.155775ms)
Jul 16 13:28:40.168: INFO: (7) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 16.312834ms)
Jul 16 13:28:40.169: INFO: (7) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 16.899403ms)
Jul 16 13:28:40.169: INFO: (7) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 16.532448ms)
Jul 16 13:28:40.170: INFO: (7) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 17.683924ms)
Jul 16 13:28:40.173: INFO: (7) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 20.325001ms)
Jul 16 13:28:40.186: INFO: (8) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 13.526635ms)
Jul 16 13:28:40.197: INFO: (8) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 23.39925ms)
Jul 16 13:28:40.198: INFO: (8) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 23.633553ms)
Jul 16 13:28:40.198: INFO: (8) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 24.377291ms)
Jul 16 13:28:40.198: INFO: (8) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 24.69747ms)
Jul 16 13:28:40.200: INFO: (8) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 25.391253ms)
Jul 16 13:28:40.201: INFO: (8) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 25.531227ms)
Jul 16 13:28:40.202: INFO: (8) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 26.661256ms)
Jul 16 13:28:40.204: INFO: (8) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 30.25867ms)
Jul 16 13:28:40.204: INFO: (8) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 30.114924ms)
Jul 16 13:28:40.204: INFO: (8) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 29.744824ms)
Jul 16 13:28:40.204: INFO: (8) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 29.580428ms)
Jul 16 13:28:40.204: INFO: (8) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 30.496544ms)
Jul 16 13:28:40.204: INFO: (8) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 29.923809ms)
Jul 16 13:28:40.204: INFO: (8) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 29.887589ms)
Jul 16 13:28:40.251: INFO: (8) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 75.847097ms)
Jul 16 13:28:40.263: INFO: (9) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 11.918763ms)
Jul 16 13:28:40.265: INFO: (9) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 13.013741ms)
Jul 16 13:28:40.266: INFO: (9) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 14.488928ms)
Jul 16 13:28:40.266: INFO: (9) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 14.169093ms)
Jul 16 13:28:40.266: INFO: (9) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 14.623666ms)
Jul 16 13:28:40.266: INFO: (9) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 14.169554ms)
Jul 16 13:28:40.266: INFO: (9) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 15.432296ms)
Jul 16 13:28:40.267: INFO: (9) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 15.386095ms)
Jul 16 13:28:40.267: INFO: (9) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 15.99039ms)
Jul 16 13:28:40.267: INFO: (9) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 15.947723ms)
Jul 16 13:28:40.268: INFO: (9) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 15.224232ms)
Jul 16 13:28:40.269: INFO: (9) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 16.426632ms)
Jul 16 13:28:40.269: INFO: (9) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 17.664642ms)
Jul 16 13:28:40.269: INFO: (9) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 16.946597ms)
Jul 16 13:28:40.272: INFO: (9) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 20.291582ms)
Jul 16 13:28:40.272: INFO: (9) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 20.42537ms)
Jul 16 13:28:40.284: INFO: (10) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 12.032116ms)
Jul 16 13:28:40.285: INFO: (10) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 12.546727ms)
Jul 16 13:28:40.286: INFO: (10) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 13.414849ms)
Jul 16 13:28:40.287: INFO: (10) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 13.967933ms)
Jul 16 13:28:40.287: INFO: (10) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 14.430024ms)
Jul 16 13:28:40.287: INFO: (10) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 14.767644ms)
Jul 16 13:28:40.288: INFO: (10) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 15.673689ms)
Jul 16 13:28:40.289: INFO: (10) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 15.52899ms)
Jul 16 13:28:40.289: INFO: (10) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 16.196479ms)
Jul 16 13:28:40.289: INFO: (10) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 16.959548ms)
Jul 16 13:28:40.290: INFO: (10) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 17.462801ms)
Jul 16 13:28:40.291: INFO: (10) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 17.484221ms)
Jul 16 13:28:40.292: INFO: (10) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 18.489627ms)
Jul 16 13:28:40.292: INFO: (10) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 19.094385ms)
Jul 16 13:28:40.292: INFO: (10) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 18.952797ms)
Jul 16 13:28:40.292: INFO: (10) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 19.133813ms)
Jul 16 13:28:40.302: INFO: (11) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 9.469644ms)
Jul 16 13:28:40.302: INFO: (11) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 9.730548ms)
Jul 16 13:28:40.303: INFO: (11) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 10.34458ms)
Jul 16 13:28:40.303: INFO: (11) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 10.501066ms)
Jul 16 13:28:40.303: INFO: (11) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 10.011408ms)
Jul 16 13:28:40.303: INFO: (11) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 10.417865ms)
Jul 16 13:28:40.303: INFO: (11) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 10.662826ms)
Jul 16 13:28:40.303: INFO: (11) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 10.073262ms)
Jul 16 13:28:40.303: INFO: (11) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 10.505337ms)
Jul 16 13:28:40.306: INFO: (11) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 13.613298ms)
Jul 16 13:28:40.306: INFO: (11) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 13.524586ms)
Jul 16 13:28:40.306: INFO: (11) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 13.695742ms)
Jul 16 13:28:40.306: INFO: (11) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 13.908176ms)
Jul 16 13:28:40.307: INFO: (11) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 13.553243ms)
Jul 16 13:28:40.310: INFO: (11) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 17.262298ms)
Jul 16 13:28:40.351: INFO: (11) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 58.413658ms)
Jul 16 13:28:40.364: INFO: (12) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 12.251019ms)
Jul 16 13:28:40.364: INFO: (12) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 12.406308ms)
Jul 16 13:28:40.380: INFO: (12) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 28.596857ms)
Jul 16 13:28:40.381: INFO: (12) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 29.029672ms)
Jul 16 13:28:40.381: INFO: (12) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 28.994872ms)
Jul 16 13:28:40.381: INFO: (12) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 29.326515ms)
Jul 16 13:28:40.381: INFO: (12) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 29.211759ms)
Jul 16 13:28:40.381: INFO: (12) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 29.362061ms)
Jul 16 13:28:40.381: INFO: (12) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 29.032129ms)
Jul 16 13:28:40.381: INFO: (12) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 29.717872ms)
Jul 16 13:28:40.384: INFO: (12) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 32.473884ms)
Jul 16 13:28:40.428: INFO: (12) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 75.80314ms)
Jul 16 13:28:40.428: INFO: (12) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 75.8877ms)
Jul 16 13:28:40.428: INFO: (12) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 76.039241ms)
Jul 16 13:28:40.428: INFO: (12) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 76.351859ms)
Jul 16 13:28:40.428: INFO: (12) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 76.32702ms)
Jul 16 13:28:40.438: INFO: (13) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 10.057878ms)
Jul 16 13:28:40.442: INFO: (13) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 13.277605ms)
Jul 16 13:28:40.443: INFO: (13) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 13.981833ms)
Jul 16 13:28:40.444: INFO: (13) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 14.541112ms)
Jul 16 13:28:40.444: INFO: (13) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 14.953952ms)
Jul 16 13:28:40.444: INFO: (13) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 14.835021ms)
Jul 16 13:28:40.444: INFO: (13) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 15.477504ms)
Jul 16 13:28:40.444: INFO: (13) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 16.101925ms)
Jul 16 13:28:40.444: INFO: (13) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 15.516787ms)
Jul 16 13:28:40.445: INFO: (13) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 14.921035ms)
Jul 16 13:28:40.446: INFO: (13) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 16.367705ms)
Jul 16 13:28:40.446: INFO: (13) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 16.816868ms)
Jul 16 13:28:40.446: INFO: (13) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 16.782512ms)
Jul 16 13:28:40.446: INFO: (13) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 17.595952ms)
Jul 16 13:28:40.447: INFO: (13) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 17.893152ms)
Jul 16 13:28:40.447: INFO: (13) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 17.567745ms)
Jul 16 13:28:40.458: INFO: (14) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 10.749952ms)
Jul 16 13:28:40.459: INFO: (14) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 11.095265ms)
Jul 16 13:28:40.459: INFO: (14) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 10.850839ms)
Jul 16 13:28:40.459: INFO: (14) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 10.845954ms)
Jul 16 13:28:40.461: INFO: (14) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 12.908264ms)
Jul 16 13:28:40.461: INFO: (14) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 13.827042ms)
Jul 16 13:28:40.462: INFO: (14) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 13.863733ms)
Jul 16 13:28:40.462: INFO: (14) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 13.902189ms)
Jul 16 13:28:40.462: INFO: (14) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 14.905325ms)
Jul 16 13:28:40.467: INFO: (14) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 18.216605ms)
Jul 16 13:28:40.467: INFO: (14) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 18.778404ms)
Jul 16 13:28:40.467: INFO: (14) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 18.932629ms)
Jul 16 13:28:40.467: INFO: (14) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 18.940285ms)
Jul 16 13:28:40.467: INFO: (14) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 19.003456ms)
Jul 16 13:28:40.467: INFO: (14) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 19.310707ms)
Jul 16 13:28:40.468: INFO: (14) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 19.501821ms)
Jul 16 13:28:40.478: INFO: (15) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 10.339082ms)
Jul 16 13:28:40.482: INFO: (15) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 13.79244ms)
Jul 16 13:28:40.483: INFO: (15) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 14.510206ms)
Jul 16 13:28:40.483: INFO: (15) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 14.589997ms)
Jul 16 13:28:40.483: INFO: (15) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 14.720647ms)
Jul 16 13:28:40.483: INFO: (15) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 15.120667ms)
Jul 16 13:28:40.483: INFO: (15) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 15.495668ms)
Jul 16 13:28:40.484: INFO: (15) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 15.325144ms)
Jul 16 13:28:40.484: INFO: (15) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 15.513253ms)
Jul 16 13:28:40.484: INFO: (15) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 15.656802ms)
Jul 16 13:28:40.484: INFO: (15) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 16.36367ms)
Jul 16 13:28:40.484: INFO: (15) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 16.033131ms)
Jul 16 13:28:40.486: INFO: (15) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 17.975997ms)
Jul 16 13:28:40.519: INFO: (15) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 51.370323ms)
Jul 16 13:28:40.519: INFO: (15) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 51.620609ms)
Jul 16 13:28:40.519: INFO: (15) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 51.350256ms)
Jul 16 13:28:40.531: INFO: (16) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 11.033522ms)
Jul 16 13:28:40.531: INFO: (16) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 10.582511ms)
Jul 16 13:28:40.531: INFO: (16) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 10.968975ms)
Jul 16 13:28:40.531: INFO: (16) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 10.808479ms)
Jul 16 13:28:40.531: INFO: (16) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 11.317023ms)
Jul 16 13:28:40.532: INFO: (16) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 11.267032ms)
Jul 16 13:28:40.532: INFO: (16) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 11.969385ms)
Jul 16 13:28:40.533: INFO: (16) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 12.76088ms)
Jul 16 13:28:40.533: INFO: (16) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 13.123622ms)
Jul 16 13:28:40.533: INFO: (16) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 13.518312ms)
Jul 16 13:28:40.533: INFO: (16) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 13.054215ms)
Jul 16 13:28:40.534: INFO: (16) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 13.93149ms)
Jul 16 13:28:40.567: INFO: (16) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 46.758607ms)
Jul 16 13:28:40.567: INFO: (16) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 46.665512ms)
Jul 16 13:28:40.567: INFO: (16) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 46.996213ms)
Jul 16 13:28:40.567: INFO: (16) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 46.843852ms)
Jul 16 13:28:40.579: INFO: (17) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 10.966686ms)
Jul 16 13:28:40.580: INFO: (17) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 12.089068ms)
Jul 16 13:28:40.580: INFO: (17) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 12.127169ms)
Jul 16 13:28:40.580: INFO: (17) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 12.213579ms)
Jul 16 13:28:40.580: INFO: (17) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 12.920159ms)
Jul 16 13:28:40.581: INFO: (17) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 12.891297ms)
Jul 16 13:28:40.581: INFO: (17) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 13.471012ms)
Jul 16 13:28:40.581: INFO: (17) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 13.363489ms)
Jul 16 13:28:40.581: INFO: (17) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 14.08906ms)
Jul 16 13:28:40.581: INFO: (17) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 13.854316ms)
Jul 16 13:28:40.582: INFO: (17) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 14.080652ms)
Jul 16 13:28:40.582: INFO: (17) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 14.339184ms)
Jul 16 13:28:40.582: INFO: (17) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 14.233142ms)
Jul 16 13:28:40.583: INFO: (17) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 14.955966ms)
Jul 16 13:28:40.585: INFO: (17) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 17.04577ms)
Jul 16 13:28:40.585: INFO: (17) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 17.016307ms)
Jul 16 13:28:40.599: INFO: (18) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 13.499672ms)
Jul 16 13:28:40.599: INFO: (18) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 13.011919ms)
Jul 16 13:28:40.599: INFO: (18) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 13.549411ms)
Jul 16 13:28:40.599: INFO: (18) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 13.657271ms)
Jul 16 13:28:40.599: INFO: (18) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 13.644889ms)
Jul 16 13:28:40.600: INFO: (18) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 14.359752ms)
Jul 16 13:28:40.600: INFO: (18) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 14.466689ms)
Jul 16 13:28:40.600: INFO: (18) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 14.851535ms)
Jul 16 13:28:40.600: INFO: (18) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 14.447425ms)
Jul 16 13:28:40.601: INFO: (18) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 14.295169ms)
Jul 16 13:28:40.607: INFO: (18) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 20.913166ms)
Jul 16 13:28:40.607: INFO: (18) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 20.538737ms)
Jul 16 13:28:40.607: INFO: (18) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 20.741056ms)
Jul 16 13:28:40.610: INFO: (18) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 23.670589ms)
Jul 16 13:28:40.610: INFO: (18) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 25.036417ms)
Jul 16 13:28:40.610: INFO: (18) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 24.03515ms)
Jul 16 13:28:40.631: INFO: (19) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 20.189276ms)
Jul 16 13:28:40.632: INFO: (19) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:460/proxy/: tls baz (200; 21.726344ms)
Jul 16 13:28:40.632: INFO: (19) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname2/proxy/: tls qux (200; 21.781083ms)
Jul 16 13:28:40.633: INFO: (19) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:462/proxy/: tls qux (200; 21.679933ms)
Jul 16 13:28:40.633: INFO: (19) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx/proxy/rewriteme">test</a> (200; 22.120628ms)
Jul 16 13:28:40.633: INFO: (19) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:162/proxy/: bar (200; 22.101638ms)
Jul 16 13:28:40.633: INFO: (19) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 22.623908ms)
Jul 16 13:28:40.633: INFO: (19) /api/v1/namespaces/proxy-1680/services/https:proxy-service-jmc5j:tlsportname1/proxy/: tls baz (200; 22.85349ms)
Jul 16 13:28:40.634: INFO: (19) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname2/proxy/: bar (200; 22.600527ms)
Jul 16 13:28:40.634: INFO: (19) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:160/proxy/: foo (200; 22.717495ms)
Jul 16 13:28:40.634: INFO: (19) /api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">test<... (200; 23.143264ms)
Jul 16 13:28:40.634: INFO: (19) /api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/https:proxy-service-jmc5j-4jfmx:443/proxy/tlsrewritem... (200; 23.003249ms)
Jul 16 13:28:40.634: INFO: (19) /api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1680/pods/http:proxy-service-jmc5j-4jfmx:1080/proxy/rewriteme">... (200; 23.156379ms)
Jul 16 13:28:40.638: INFO: (19) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname2/proxy/: bar (200; 27.540891ms)
Jul 16 13:28:40.638: INFO: (19) /api/v1/namespaces/proxy-1680/services/http:proxy-service-jmc5j:portname1/proxy/: foo (200; 27.141095ms)
Jul 16 13:28:40.675: INFO: (19) /api/v1/namespaces/proxy-1680/services/proxy-service-jmc5j:portname1/proxy/: foo (200; 64.698294ms)
STEP: deleting ReplicationController proxy-service-jmc5j in namespace proxy-1680, will wait for the garbage collector to delete the pods
Jul 16 13:28:40.745: INFO: Deleting ReplicationController proxy-service-jmc5j took: 14.025015ms
Jul 16 13:28:41.245: INFO: Terminating ReplicationController proxy-service-jmc5j pods took: 500.733498ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:28:43.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1680" for this suite.
Jul 16 13:28:49.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:28:49.894: INFO: namespace proxy-1680 deletion completed in 6.436393855s

• [SLOW TEST:18.474 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:28:49.903: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 16 13:28:50.021: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-a,UID:11444a1e-1e91-43a3-9883-13e7a09e09cd,ResourceVersion:9043,Generation:0,CreationTimestamp:2019-07-16 13:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 13:28:50.021: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-a,UID:11444a1e-1e91-43a3-9883-13e7a09e09cd,ResourceVersion:9043,Generation:0,CreationTimestamp:2019-07-16 13:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 16 13:29:00.095: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-a,UID:11444a1e-1e91-43a3-9883-13e7a09e09cd,ResourceVersion:9074,Generation:0,CreationTimestamp:2019-07-16 13:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 16 13:29:00.096: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-a,UID:11444a1e-1e91-43a3-9883-13e7a09e09cd,ResourceVersion:9074,Generation:0,CreationTimestamp:2019-07-16 13:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 16 13:29:10.141: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-a,UID:11444a1e-1e91-43a3-9883-13e7a09e09cd,ResourceVersion:9106,Generation:0,CreationTimestamp:2019-07-16 13:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 13:29:10.141: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-a,UID:11444a1e-1e91-43a3-9883-13e7a09e09cd,ResourceVersion:9106,Generation:0,CreationTimestamp:2019-07-16 13:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 16 13:29:20.179: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-a,UID:11444a1e-1e91-43a3-9883-13e7a09e09cd,ResourceVersion:9140,Generation:0,CreationTimestamp:2019-07-16 13:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 13:29:20.180: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-a,UID:11444a1e-1e91-43a3-9883-13e7a09e09cd,ResourceVersion:9140,Generation:0,CreationTimestamp:2019-07-16 13:28:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 16 13:29:30.202: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-b,UID:e2c8c423-2a41-4e60-a493-0c340f0262cc,ResourceVersion:9172,Generation:0,CreationTimestamp:2019-07-16 13:29:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 13:29:30.203: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-b,UID:e2c8c423-2a41-4e60-a493-0c340f0262cc,ResourceVersion:9172,Generation:0,CreationTimestamp:2019-07-16 13:29:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 16 13:29:40.236: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-b,UID:e2c8c423-2a41-4e60-a493-0c340f0262cc,ResourceVersion:9205,Generation:0,CreationTimestamp:2019-07-16 13:29:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 13:29:40.237: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2373,SelfLink:/api/v1/namespaces/watch-2373/configmaps/e2e-watch-test-configmap-b,UID:e2c8c423-2a41-4e60-a493-0c340f0262cc,ResourceVersion:9205,Generation:0,CreationTimestamp:2019-07-16 13:29:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:29:50.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2373" for this suite.
Jul 16 13:29:56.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:29:56.637: INFO: namespace watch-2373 deletion completed in 6.389305607s

• [SLOW TEST:66.734 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:29:56.638: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 16 13:30:01.404: INFO: Successfully updated pod "labelsupdate51d97951-2d7f-4d92-a5a2-3b46b230b2bd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:30:03.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5650" for this suite.
Jul 16 13:30:31.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:30:31.939: INFO: namespace projected-5650 deletion completed in 28.375523885s

• [SLOW TEST:35.301 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:30:31.942: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 16 13:30:32.064: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:30:38.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7500" for this suite.
Jul 16 13:31:02.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:31:02.577: INFO: namespace init-container-7500 deletion completed in 24.347264388s

• [SLOW TEST:30.636 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:31:02.579: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-1fce0226-fc0f-4a2d-ac8e-8ec4e971480f
STEP: Creating a pod to test consume configMaps
Jul 16 13:31:02.729: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7d2e6cf-25d7-477b-9a2c-da2f03d559b3" in namespace "projected-4838" to be "success or failure"
Jul 16 13:31:02.740: INFO: Pod "pod-projected-configmaps-d7d2e6cf-25d7-477b-9a2c-da2f03d559b3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.685764ms
Jul 16 13:31:04.747: INFO: Pod "pod-projected-configmaps-d7d2e6cf-25d7-477b-9a2c-da2f03d559b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017600409s
Jul 16 13:31:06.823: INFO: Pod "pod-projected-configmaps-d7d2e6cf-25d7-477b-9a2c-da2f03d559b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.093894675s
STEP: Saw pod success
Jul 16 13:31:06.823: INFO: Pod "pod-projected-configmaps-d7d2e6cf-25d7-477b-9a2c-da2f03d559b3" satisfied condition "success or failure"
Jul 16 13:31:06.827: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-configmaps-d7d2e6cf-25d7-477b-9a2c-da2f03d559b3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:31:06.875: INFO: Waiting for pod pod-projected-configmaps-d7d2e6cf-25d7-477b-9a2c-da2f03d559b3 to disappear
Jul 16 13:31:06.879: INFO: Pod pod-projected-configmaps-d7d2e6cf-25d7-477b-9a2c-da2f03d559b3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:31:06.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4838" for this suite.
Jul 16 13:31:14.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:31:15.313: INFO: namespace projected-4838 deletion completed in 8.424656621s

• [SLOW TEST:12.735 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:31:15.314: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-f38b3699-5630-4ece-9882-f10209a09dd8 in namespace container-probe-4589
Jul 16 13:31:21.493: INFO: Started pod liveness-f38b3699-5630-4ece-9882-f10209a09dd8 in namespace container-probe-4589
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 13:31:21.499: INFO: Initial restart count of pod liveness-f38b3699-5630-4ece-9882-f10209a09dd8 is 0
Jul 16 13:31:33.585: INFO: Restart count of pod container-probe-4589/liveness-f38b3699-5630-4ece-9882-f10209a09dd8 is now 1 (12.085832153s elapsed)
Jul 16 13:31:51.953: INFO: Restart count of pod container-probe-4589/liveness-f38b3699-5630-4ece-9882-f10209a09dd8 is now 2 (30.453355675s elapsed)
Jul 16 13:32:12.528: INFO: Restart count of pod container-probe-4589/liveness-f38b3699-5630-4ece-9882-f10209a09dd8 is now 3 (51.029000358s elapsed)
Jul 16 13:32:32.745: INFO: Restart count of pod container-probe-4589/liveness-f38b3699-5630-4ece-9882-f10209a09dd8 is now 4 (1m11.246030086s elapsed)
Jul 16 13:33:44.129: INFO: Restart count of pod container-probe-4589/liveness-f38b3699-5630-4ece-9882-f10209a09dd8 is now 5 (2m22.63012077s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:33:44.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4589" for this suite.
Jul 16 13:33:50.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:33:50.554: INFO: namespace container-probe-4589 deletion completed in 6.345893128s

• [SLOW TEST:155.241 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:33:50.558: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-129e85d4-5631-4380-bdc6-e2f70656cfa1
STEP: Creating a pod to test consume secrets
Jul 16 13:33:50.661: INFO: Waiting up to 5m0s for pod "pod-secrets-6be94971-036a-4476-a0a6-b72f6c816758" in namespace "secrets-3434" to be "success or failure"
Jul 16 13:33:50.671: INFO: Pod "pod-secrets-6be94971-036a-4476-a0a6-b72f6c816758": Phase="Pending", Reason="", readiness=false. Elapsed: 9.251899ms
Jul 16 13:33:52.678: INFO: Pod "pod-secrets-6be94971-036a-4476-a0a6-b72f6c816758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016511962s
Jul 16 13:33:54.686: INFO: Pod "pod-secrets-6be94971-036a-4476-a0a6-b72f6c816758": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024335759s
STEP: Saw pod success
Jul 16 13:33:54.686: INFO: Pod "pod-secrets-6be94971-036a-4476-a0a6-b72f6c816758" satisfied condition "success or failure"
Jul 16 13:33:54.693: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-secrets-6be94971-036a-4476-a0a6-b72f6c816758 container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:33:54.797: INFO: Waiting for pod pod-secrets-6be94971-036a-4476-a0a6-b72f6c816758 to disappear
Jul 16 13:33:54.804: INFO: Pod pod-secrets-6be94971-036a-4476-a0a6-b72f6c816758 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:33:54.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3434" for this suite.
Jul 16 13:34:00.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:34:01.123: INFO: namespace secrets-3434 deletion completed in 6.31145269s

• [SLOW TEST:10.565 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:34:01.124: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 16 13:34:01.204: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jul 16 13:34:01.678: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jul 16 13:34:03.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:34:05.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:34:07.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:34:09.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:34:11.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:34:13.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:34:15.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:34:17.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:34:19.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698880841, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:34:23.168: INFO: Waited 1.337150543s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:34:25.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8817" for this suite.
Jul 16 13:34:33.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:34:33.866: INFO: namespace aggregator-8817 deletion completed in 8.666103439s

• [SLOW TEST:32.742 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:34:33.870: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 16 13:34:34.031: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:34:39.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3010" for this suite.
Jul 16 13:34:45.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:34:46.271: INFO: namespace init-container-3010 deletion completed in 6.40302563s

• [SLOW TEST:12.402 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:34:46.274: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 16 13:34:46.390: INFO: Waiting up to 5m0s for pod "pod-3a63da22-a65d-47b1-8a2a-3c854ae506f9" in namespace "emptydir-7896" to be "success or failure"
Jul 16 13:34:46.423: INFO: Pod "pod-3a63da22-a65d-47b1-8a2a-3c854ae506f9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.797826ms
Jul 16 13:34:48.431: INFO: Pod "pod-3a63da22-a65d-47b1-8a2a-3c854ae506f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041197768s
Jul 16 13:34:50.439: INFO: Pod "pod-3a63da22-a65d-47b1-8a2a-3c854ae506f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049079315s
STEP: Saw pod success
Jul 16 13:34:50.439: INFO: Pod "pod-3a63da22-a65d-47b1-8a2a-3c854ae506f9" satisfied condition "success or failure"
Jul 16 13:34:50.444: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-3a63da22-a65d-47b1-8a2a-3c854ae506f9 container test-container: <nil>
STEP: delete the pod
Jul 16 13:34:50.504: INFO: Waiting for pod pod-3a63da22-a65d-47b1-8a2a-3c854ae506f9 to disappear
Jul 16 13:34:50.510: INFO: Pod pod-3a63da22-a65d-47b1-8a2a-3c854ae506f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:34:50.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7896" for this suite.
Jul 16 13:34:58.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:34:58.972: INFO: namespace emptydir-7896 deletion completed in 8.451326531s

• [SLOW TEST:12.698 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:34:58.973: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:34:59.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 version'
Jul 16 13:34:59.315: INFO: stderr: ""
Jul 16 13:34:59.315: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:34:59.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2487" for this suite.
Jul 16 13:35:05.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:35:05.694: INFO: namespace kubectl-2487 deletion completed in 6.3613336s

• [SLOW TEST:6.721 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:35:05.697: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-17d64812-57d8-4fa9-90b2-6cc97e6dfac1
STEP: Creating a pod to test consume configMaps
Jul 16 13:35:05.972: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc5dc96b-5c89-4f88-bd6e-aa5655751067" in namespace "configmap-3139" to be "success or failure"
Jul 16 13:35:06.028: INFO: Pod "pod-configmaps-cc5dc96b-5c89-4f88-bd6e-aa5655751067": Phase="Pending", Reason="", readiness=false. Elapsed: 55.070224ms
Jul 16 13:35:08.582: INFO: Pod "pod-configmaps-cc5dc96b-5c89-4f88-bd6e-aa5655751067": Phase="Pending", Reason="", readiness=false. Elapsed: 2.609073338s
Jul 16 13:35:10.590: INFO: Pod "pod-configmaps-cc5dc96b-5c89-4f88-bd6e-aa5655751067": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.617453247s
STEP: Saw pod success
Jul 16 13:35:10.590: INFO: Pod "pod-configmaps-cc5dc96b-5c89-4f88-bd6e-aa5655751067" satisfied condition "success or failure"
Jul 16 13:35:10.597: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-cc5dc96b-5c89-4f88-bd6e-aa5655751067 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:35:10.962: INFO: Waiting for pod pod-configmaps-cc5dc96b-5c89-4f88-bd6e-aa5655751067 to disappear
Jul 16 13:35:10.976: INFO: Pod pod-configmaps-cc5dc96b-5c89-4f88-bd6e-aa5655751067 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:35:10.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3139" for this suite.
Jul 16 13:35:17.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:35:17.327: INFO: namespace configmap-3139 deletion completed in 6.342947884s

• [SLOW TEST:11.630 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:35:17.328: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jul 16 13:35:23.456: INFO: Pod pod-hostip-8aacdf53-5dd4-448e-8e02-73ec6e703cb0 has hostIP: 192.168.1.2
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:35:23.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6502" for this suite.
Jul 16 13:35:47.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:35:47.852: INFO: namespace pods-6502 deletion completed in 24.385667697s

• [SLOW TEST:30.524 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:35:47.856: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-7fc5889c-21c8-4022-ad78-4ebd08a349f9
STEP: Creating a pod to test consume configMaps
Jul 16 13:35:48.016: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-962ca24d-a358-479d-ba63-60804589a471" in namespace "projected-6513" to be "success or failure"
Jul 16 13:35:48.030: INFO: Pod "pod-projected-configmaps-962ca24d-a358-479d-ba63-60804589a471": Phase="Pending", Reason="", readiness=false. Elapsed: 13.8195ms
Jul 16 13:35:50.045: INFO: Pod "pod-projected-configmaps-962ca24d-a358-479d-ba63-60804589a471": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028753572s
Jul 16 13:35:52.076: INFO: Pod "pod-projected-configmaps-962ca24d-a358-479d-ba63-60804589a471": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060530383s
STEP: Saw pod success
Jul 16 13:35:52.077: INFO: Pod "pod-projected-configmaps-962ca24d-a358-479d-ba63-60804589a471" satisfied condition "success or failure"
Jul 16 13:35:52.083: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-configmaps-962ca24d-a358-479d-ba63-60804589a471 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:35:55.295: INFO: Waiting for pod pod-projected-configmaps-962ca24d-a358-479d-ba63-60804589a471 to disappear
Jul 16 13:35:55.306: INFO: Pod pod-projected-configmaps-962ca24d-a358-479d-ba63-60804589a471 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:35:55.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6513" for this suite.
Jul 16 13:36:01.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:36:01.873: INFO: namespace projected-6513 deletion completed in 6.558227643s

• [SLOW TEST:14.017 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:36:01.874: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-6338
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6338
STEP: Deleting pre-stop pod
Jul 16 13:36:17.475: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:36:17.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6338" for this suite.
Jul 16 13:36:57.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:36:58.008: INFO: namespace prestop-6338 deletion completed in 40.491121285s

• [SLOW TEST:56.134 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:36:58.013: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-51eb6a06-0066-4907-a974-cda4e4890c72
STEP: Creating a pod to test consume configMaps
Jul 16 13:36:58.321: INFO: Waiting up to 5m0s for pod "pod-configmaps-9a499a3f-c364-4266-a308-2d14abe474e5" in namespace "configmap-9686" to be "success or failure"
Jul 16 13:36:58.328: INFO: Pod "pod-configmaps-9a499a3f-c364-4266-a308-2d14abe474e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.323221ms
Jul 16 13:37:00.335: INFO: Pod "pod-configmaps-9a499a3f-c364-4266-a308-2d14abe474e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01372814s
Jul 16 13:37:02.344: INFO: Pod "pod-configmaps-9a499a3f-c364-4266-a308-2d14abe474e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022850008s
STEP: Saw pod success
Jul 16 13:37:02.344: INFO: Pod "pod-configmaps-9a499a3f-c364-4266-a308-2d14abe474e5" satisfied condition "success or failure"
Jul 16 13:37:02.350: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-9a499a3f-c364-4266-a308-2d14abe474e5 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:37:02.416: INFO: Waiting for pod pod-configmaps-9a499a3f-c364-4266-a308-2d14abe474e5 to disappear
Jul 16 13:37:02.435: INFO: Pod pod-configmaps-9a499a3f-c364-4266-a308-2d14abe474e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:37:02.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9686" for this suite.
Jul 16 13:37:08.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:37:08.892: INFO: namespace configmap-9686 deletion completed in 6.44822081s

• [SLOW TEST:10.880 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:37:08.897: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-91e05ef7-91e1-40b3-88c3-370b6dec5f1e
Jul 16 13:37:09.111: INFO: Pod name my-hostname-basic-91e05ef7-91e1-40b3-88c3-370b6dec5f1e: Found 0 pods out of 1
Jul 16 13:37:14.120: INFO: Pod name my-hostname-basic-91e05ef7-91e1-40b3-88c3-370b6dec5f1e: Found 1 pods out of 1
Jul 16 13:37:14.120: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-91e05ef7-91e1-40b3-88c3-370b6dec5f1e" are running
Jul 16 13:37:14.127: INFO: Pod "my-hostname-basic-91e05ef7-91e1-40b3-88c3-370b6dec5f1e-k7mzb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 13:37:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 13:37:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 13:37:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-16 13:37:09 +0000 UTC Reason: Message:}])
Jul 16 13:37:14.127: INFO: Trying to dial the pod
Jul 16 13:37:19.258: INFO: Controller my-hostname-basic-91e05ef7-91e1-40b3-88c3-370b6dec5f1e: Got expected result from replica 1 [my-hostname-basic-91e05ef7-91e1-40b3-88c3-370b6dec5f1e-k7mzb]: "my-hostname-basic-91e05ef7-91e1-40b3-88c3-370b6dec5f1e-k7mzb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:37:19.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1529" for this suite.
Jul 16 13:37:27.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:37:29.153: INFO: namespace replication-controller-1529 deletion completed in 9.88598833s

• [SLOW TEST:20.256 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:37:29.156: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-e5595399-552a-4922-bcae-2f11880641dc
STEP: Creating a pod to test consume secrets
Jul 16 13:37:29.316: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d10a22d5-4211-406a-aac5-8edc3e1dcb17" in namespace "projected-6405" to be "success or failure"
Jul 16 13:37:29.326: INFO: Pod "pod-projected-secrets-d10a22d5-4211-406a-aac5-8edc3e1dcb17": Phase="Pending", Reason="", readiness=false. Elapsed: 9.697518ms
Jul 16 13:37:31.341: INFO: Pod "pod-projected-secrets-d10a22d5-4211-406a-aac5-8edc3e1dcb17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024739113s
Jul 16 13:37:33.356: INFO: Pod "pod-projected-secrets-d10a22d5-4211-406a-aac5-8edc3e1dcb17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040032674s
STEP: Saw pod success
Jul 16 13:37:33.356: INFO: Pod "pod-projected-secrets-d10a22d5-4211-406a-aac5-8edc3e1dcb17" satisfied condition "success or failure"
Jul 16 13:37:33.370: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-secrets-d10a22d5-4211-406a-aac5-8edc3e1dcb17 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 13:37:33.583: INFO: Waiting for pod pod-projected-secrets-d10a22d5-4211-406a-aac5-8edc3e1dcb17 to disappear
Jul 16 13:37:33.591: INFO: Pod pod-projected-secrets-d10a22d5-4211-406a-aac5-8edc3e1dcb17 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:37:33.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6405" for this suite.
Jul 16 13:37:41.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:37:41.965: INFO: namespace projected-6405 deletion completed in 8.365415594s

• [SLOW TEST:12.809 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:37:41.971: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:37:42.144: INFO: Creating deployment "test-recreate-deployment"
Jul 16 13:37:42.182: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 16 13:37:42.206: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 16 13:37:44.241: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 16 13:37:44.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881062, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881062, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881062, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881062, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:37:46.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881062, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881062, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881062, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881062, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:37:48.262: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 16 13:37:48.314: INFO: Updating deployment test-recreate-deployment
Jul 16 13:37:48.315: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 16 13:37:48.676: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3700,SelfLink:/apis/apps/v1/namespaces/deployment-3700/deployments/test-recreate-deployment,UID:9e9809ff-aa7c-497f-a4e2-789f0505a849,ResourceVersion:11285,Generation:2,CreationTimestamp:2019-07-16 13:37:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-16 13:37:48 +0000 UTC 2019-07-16 13:37:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-16 13:37:48 +0000 UTC 2019-07-16 13:37:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul 16 13:37:48.683: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-3700,SelfLink:/apis/apps/v1/namespaces/deployment-3700/replicasets/test-recreate-deployment-5c8c9cc69d,UID:fdacd25d-7cd1-400c-82c9-f12d7f772240,ResourceVersion:11283,Generation:1,CreationTimestamp:2019-07-16 13:37:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9e9809ff-aa7c-497f-a4e2-789f0505a849 0xc00292e8f7 0xc00292e8f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 13:37:48.683: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 16 13:37:48.683: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-3700,SelfLink:/apis/apps/v1/namespaces/deployment-3700/replicasets/test-recreate-deployment-6df85df6b9,UID:63d8f610-e5af-4f10-b30e-e4656ed53fa7,ResourceVersion:11274,Generation:2,CreationTimestamp:2019-07-16 13:37:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9e9809ff-aa7c-497f-a4e2-789f0505a849 0xc00292e9c7 0xc00292e9c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 13:37:48.692: INFO: Pod "test-recreate-deployment-5c8c9cc69d-9vpw8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-9vpw8,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-3700,SelfLink:/api/v1/namespaces/deployment-3700/pods/test-recreate-deployment-5c8c9cc69d-9vpw8,UID:305d0113-cf8a-4347-81cd-44defe56ef2a,ResourceVersion:11286,Generation:0,CreationTimestamp:2019-07-16 13:37:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d fdacd25d-7cd1-400c-82c9-f12d7f772240 0xc002cc24c7 0xc002cc24c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ls722 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ls722,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ls722 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002cc2530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002cc2550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:37:48 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:,StartTime:2019-07-16 13:37:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:37:48.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3700" for this suite.
Jul 16 13:37:56.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:37:57.003: INFO: namespace deployment-3700 deletion completed in 8.287635934s

• [SLOW TEST:15.032 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:37:57.004: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:37:57.108: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 16 13:37:57.127: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 16 13:38:02.138: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 16 13:38:02.139: INFO: Creating deployment "test-rolling-update-deployment"
Jul 16 13:38:02.161: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 16 13:38:02.176: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 16 13:38:04.189: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 16 13:38:04.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881082, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881082, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881082, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698881082, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 13:38:06.202: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 16 13:38:06.229: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-5861,SelfLink:/apis/apps/v1/namespaces/deployment-5861/deployments/test-rolling-update-deployment,UID:2c4d0afb-287f-4a96-acd6-4a3b192bdb1c,ResourceVersion:11410,Generation:1,CreationTimestamp:2019-07-16 13:38:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-16 13:38:02 +0000 UTC 2019-07-16 13:38:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-16 13:38:04 +0000 UTC 2019-07-16 13:38:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 16 13:38:06.235: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-5861,SelfLink:/apis/apps/v1/namespaces/deployment-5861/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:bebaf226-310d-400e-9b6f-bf6b52b5836c,ResourceVersion:11400,Generation:1,CreationTimestamp:2019-07-16 13:38:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2c4d0afb-287f-4a96-acd6-4a3b192bdb1c 0xc0009f1ef7 0xc0009f1ef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 16 13:38:06.235: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 16 13:38:06.236: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-5861,SelfLink:/apis/apps/v1/namespaces/deployment-5861/replicasets/test-rolling-update-controller,UID:415a23a0-d2dc-476c-97b7-690153bf85a7,ResourceVersion:11407,Generation:2,CreationTimestamp:2019-07-16 13:37:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2c4d0afb-287f-4a96-acd6-4a3b192bdb1c 0xc0009f1e27 0xc0009f1e28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 13:38:06.243: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-8frwp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-8frwp,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-5861,SelfLink:/api/v1/namespaces/deployment-5861/pods/test-rolling-update-deployment-79f6b9d75c-8frwp,UID:9ddb6d67-0d3d-4763-81e8-4345bc60ef51,ResourceVersion:11399,Generation:0,CreationTimestamp:2019-07-16 13:38:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c bebaf226-310d-400e-9b6f-bf6b52b5836c 0xc000a4e7d7 0xc000a4e7d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pvkn9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pvkn9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pvkn9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a4e840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a4e860}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:38:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:38:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:38:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 13:38:02 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.23,StartTime:2019-07-16 13:38:02 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-16 13:38:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://82542b4f8790d244a29cb04fb1e78b17b83325914db660cf2f7313014af0112b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:38:06.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5861" for this suite.
Jul 16 13:38:14.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:38:14.690: INFO: namespace deployment-5861 deletion completed in 8.438467272s

• [SLOW TEST:17.686 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:38:14.691: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:38:14.830: INFO: (0) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 18.196589ms)
Jul 16 13:38:14.874: INFO: (1) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 43.400225ms)
Jul 16 13:38:14.946: INFO: (2) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 72.548753ms)
Jul 16 13:38:14.959: INFO: (3) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.054175ms)
Jul 16 13:38:14.969: INFO: (4) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.045764ms)
Jul 16 13:38:15.049: INFO: (5) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 80.112015ms)
Jul 16 13:38:15.071: INFO: (6) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 21.496818ms)
Jul 16 13:38:15.123: INFO: (7) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 51.931145ms)
Jul 16 13:38:15.135: INFO: (8) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.319168ms)
Jul 16 13:38:15.145: INFO: (9) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.576352ms)
Jul 16 13:38:15.155: INFO: (10) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.016324ms)
Jul 16 13:38:15.165: INFO: (11) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.32295ms)
Jul 16 13:38:15.176: INFO: (12) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.97265ms)
Jul 16 13:38:15.189: INFO: (13) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.602325ms)
Jul 16 13:38:15.199: INFO: (14) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.704308ms)
Jul 16 13:38:15.208: INFO: (15) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.917703ms)
Jul 16 13:38:15.219: INFO: (16) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.891753ms)
Jul 16 13:38:15.230: INFO: (17) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.611526ms)
Jul 16 13:38:15.240: INFO: (18) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.456021ms)
Jul 16 13:38:15.252: INFO: (19) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.927401ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:38:15.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-828" for this suite.
Jul 16 13:38:21.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:38:21.668: INFO: namespace proxy-828 deletion completed in 6.406486545s

• [SLOW TEST:6.978 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:38:21.677: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 16 13:38:21.816: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:38:22.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5571" for this suite.
Jul 16 13:38:28.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:38:29.266: INFO: namespace replication-controller-5571 deletion completed in 6.321231924s

• [SLOW TEST:7.590 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:38:29.267: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:38:29.895: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"de4ffe09-f03a-46b8-9b81-0fb94d42f68f", Controller:(*bool)(0xc00309241a), BlockOwnerDeletion:(*bool)(0xc00309241b)}}
Jul 16 13:38:29.955: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"7bfee15f-b6e2-4e47-92af-639f2ec0ef0b", Controller:(*bool)(0xc0030b57aa), BlockOwnerDeletion:(*bool)(0xc0030b57ab)}}
Jul 16 13:38:30.187: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5e47b581-41ef-48b6-9ac2-84dbb7f7daa2", Controller:(*bool)(0xc0030925d6), BlockOwnerDeletion:(*bool)(0xc0030925d7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:38:35.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6663" for this suite.
Jul 16 13:38:41.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:38:41.646: INFO: namespace gc-6663 deletion completed in 6.350464729s

• [SLOW TEST:12.380 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:38:41.648: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 16 13:38:41.931: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3780,SelfLink:/api/v1/namespaces/watch-3780/configmaps/e2e-watch-test-label-changed,UID:f6528672-1195-493b-870a-321af467a4f6,ResourceVersion:11651,Generation:0,CreationTimestamp:2019-07-16 13:38:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 13:38:41.932: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3780,SelfLink:/api/v1/namespaces/watch-3780/configmaps/e2e-watch-test-label-changed,UID:f6528672-1195-493b-870a-321af467a4f6,ResourceVersion:11652,Generation:0,CreationTimestamp:2019-07-16 13:38:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 16 13:38:41.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3780,SelfLink:/api/v1/namespaces/watch-3780/configmaps/e2e-watch-test-label-changed,UID:f6528672-1195-493b-870a-321af467a4f6,ResourceVersion:11653,Generation:0,CreationTimestamp:2019-07-16 13:38:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 16 13:38:52.023: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3780,SelfLink:/api/v1/namespaces/watch-3780/configmaps/e2e-watch-test-label-changed,UID:f6528672-1195-493b-870a-321af467a4f6,ResourceVersion:11686,Generation:0,CreationTimestamp:2019-07-16 13:38:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 13:38:52.024: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3780,SelfLink:/api/v1/namespaces/watch-3780/configmaps/e2e-watch-test-label-changed,UID:f6528672-1195-493b-870a-321af467a4f6,ResourceVersion:11687,Generation:0,CreationTimestamp:2019-07-16 13:38:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 16 13:38:52.024: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3780,SelfLink:/api/v1/namespaces/watch-3780/configmaps/e2e-watch-test-label-changed,UID:f6528672-1195-493b-870a-321af467a4f6,ResourceVersion:11688,Generation:0,CreationTimestamp:2019-07-16 13:38:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:38:52.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3780" for this suite.
Jul 16 13:38:58.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:38:58.399: INFO: namespace watch-3780 deletion completed in 6.365597627s

• [SLOW TEST:16.751 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:38:58.399: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jul 16 13:38:58.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 api-versions'
Jul 16 13:38:58.647: INFO: stderr: ""
Jul 16 13:38:58.647: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:38:58.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1754" for this suite.
Jul 16 13:39:04.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:39:05.027: INFO: namespace kubectl-1754 deletion completed in 6.366570339s

• [SLOW TEST:6.628 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:39:05.031: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-7868/configmap-test-fd61bc2e-d892-4488-833b-6490e40d6496
STEP: Creating a pod to test consume configMaps
Jul 16 13:39:05.197: INFO: Waiting up to 5m0s for pod "pod-configmaps-6bd10960-1cdb-457d-b0c2-ff8fb7a12d41" in namespace "configmap-7868" to be "success or failure"
Jul 16 13:39:05.223: INFO: Pod "pod-configmaps-6bd10960-1cdb-457d-b0c2-ff8fb7a12d41": Phase="Pending", Reason="", readiness=false. Elapsed: 25.22706ms
Jul 16 13:39:07.229: INFO: Pod "pod-configmaps-6bd10960-1cdb-457d-b0c2-ff8fb7a12d41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032092479s
Jul 16 13:39:09.239: INFO: Pod "pod-configmaps-6bd10960-1cdb-457d-b0c2-ff8fb7a12d41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041220912s
STEP: Saw pod success
Jul 16 13:39:09.239: INFO: Pod "pod-configmaps-6bd10960-1cdb-457d-b0c2-ff8fb7a12d41" satisfied condition "success or failure"
Jul 16 13:39:09.247: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-6bd10960-1cdb-457d-b0c2-ff8fb7a12d41 container env-test: <nil>
STEP: delete the pod
Jul 16 13:39:09.396: INFO: Waiting for pod pod-configmaps-6bd10960-1cdb-457d-b0c2-ff8fb7a12d41 to disappear
Jul 16 13:39:09.411: INFO: Pod pod-configmaps-6bd10960-1cdb-457d-b0c2-ff8fb7a12d41 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:39:09.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7868" for this suite.
Jul 16 13:39:15.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:39:15.752: INFO: namespace configmap-7868 deletion completed in 6.332084874s

• [SLOW TEST:10.722 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:39:15.755: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:39:15.887: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bab7c152-d2c3-49a2-93cb-8b41153fa16a" in namespace "projected-2577" to be "success or failure"
Jul 16 13:39:15.893: INFO: Pod "downwardapi-volume-bab7c152-d2c3-49a2-93cb-8b41153fa16a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050057ms
Jul 16 13:39:17.903: INFO: Pod "downwardapi-volume-bab7c152-d2c3-49a2-93cb-8b41153fa16a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015390179s
Jul 16 13:39:19.911: INFO: Pod "downwardapi-volume-bab7c152-d2c3-49a2-93cb-8b41153fa16a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023795945s
STEP: Saw pod success
Jul 16 13:39:19.911: INFO: Pod "downwardapi-volume-bab7c152-d2c3-49a2-93cb-8b41153fa16a" satisfied condition "success or failure"
Jul 16 13:39:19.917: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-bab7c152-d2c3-49a2-93cb-8b41153fa16a container client-container: <nil>
STEP: delete the pod
Jul 16 13:39:19.994: INFO: Waiting for pod downwardapi-volume-bab7c152-d2c3-49a2-93cb-8b41153fa16a to disappear
Jul 16 13:39:20.000: INFO: Pod downwardapi-volume-bab7c152-d2c3-49a2-93cb-8b41153fa16a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:39:20.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2577" for this suite.
Jul 16 13:39:26.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:39:26.373: INFO: namespace projected-2577 deletion completed in 6.364332295s

• [SLOW TEST:10.618 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:39:26.374: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 16 13:39:26.478: INFO: Waiting up to 5m0s for pod "pod-78533400-68ed-49e9-9ed8-0ee9e1af4555" in namespace "emptydir-3965" to be "success or failure"
Jul 16 13:39:26.492: INFO: Pod "pod-78533400-68ed-49e9-9ed8-0ee9e1af4555": Phase="Pending", Reason="", readiness=false. Elapsed: 13.939993ms
Jul 16 13:39:28.508: INFO: Pod "pod-78533400-68ed-49e9-9ed8-0ee9e1af4555": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02934155s
Jul 16 13:39:30.515: INFO: Pod "pod-78533400-68ed-49e9-9ed8-0ee9e1af4555": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036367303s
Jul 16 13:39:32.523: INFO: Pod "pod-78533400-68ed-49e9-9ed8-0ee9e1af4555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044730377s
STEP: Saw pod success
Jul 16 13:39:32.523: INFO: Pod "pod-78533400-68ed-49e9-9ed8-0ee9e1af4555" satisfied condition "success or failure"
Jul 16 13:39:32.529: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-78533400-68ed-49e9-9ed8-0ee9e1af4555 container test-container: <nil>
STEP: delete the pod
Jul 16 13:39:32.751: INFO: Waiting for pod pod-78533400-68ed-49e9-9ed8-0ee9e1af4555 to disappear
Jul 16 13:39:32.757: INFO: Pod pod-78533400-68ed-49e9-9ed8-0ee9e1af4555 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:39:32.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3965" for this suite.
Jul 16 13:39:38.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:39:39.203: INFO: namespace emptydir-3965 deletion completed in 6.436751311s

• [SLOW TEST:12.829 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:39:39.204: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 16 13:39:39.405: INFO: Waiting up to 5m0s for pod "pod-88a277f2-b8b0-4abc-a822-1ebeb63d978c" in namespace "emptydir-7618" to be "success or failure"
Jul 16 13:39:39.487: INFO: Pod "pod-88a277f2-b8b0-4abc-a822-1ebeb63d978c": Phase="Pending", Reason="", readiness=false. Elapsed: 82.199913ms
Jul 16 13:39:41.495: INFO: Pod "pod-88a277f2-b8b0-4abc-a822-1ebeb63d978c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090250849s
Jul 16 13:39:43.503: INFO: Pod "pod-88a277f2-b8b0-4abc-a822-1ebeb63d978c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098535986s
STEP: Saw pod success
Jul 16 13:39:43.503: INFO: Pod "pod-88a277f2-b8b0-4abc-a822-1ebeb63d978c" satisfied condition "success or failure"
Jul 16 13:39:43.509: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-88a277f2-b8b0-4abc-a822-1ebeb63d978c container test-container: <nil>
STEP: delete the pod
Jul 16 13:39:43.660: INFO: Waiting for pod pod-88a277f2-b8b0-4abc-a822-1ebeb63d978c to disappear
Jul 16 13:39:43.666: INFO: Pod pod-88a277f2-b8b0-4abc-a822-1ebeb63d978c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:39:43.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7618" for this suite.
Jul 16 13:39:49.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:39:50.215: INFO: namespace emptydir-7618 deletion completed in 6.541676227s

• [SLOW TEST:11.011 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:39:50.217: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-3eb72dd9-666b-419c-b55c-183d554688d3
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-3eb72dd9-666b-419c-b55c-183d554688d3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:40:57.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5462" for this suite.
Jul 16 13:41:21.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:41:21.876: INFO: namespace configmap-5462 deletion completed in 24.421389203s

• [SLOW TEST:91.660 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:41:21.878: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 16 13:41:22.001: INFO: namespace kubectl-8886
Jul 16 13:41:22.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-8886'
Jul 16 13:41:23.837: INFO: stderr: ""
Jul 16 13:41:23.837: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 16 13:41:24.847: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 13:41:24.847: INFO: Found 0 / 1
Jul 16 13:41:25.850: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 13:41:25.850: INFO: Found 0 / 1
Jul 16 13:41:26.852: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 13:41:26.852: INFO: Found 1 / 1
Jul 16 13:41:26.852: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 16 13:41:26.862: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 13:41:26.862: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 16 13:41:26.862: INFO: wait on redis-master startup in kubectl-8886 
Jul 16 13:41:26.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 logs redis-master-qg48g redis-master --namespace=kubectl-8886'
Jul 16 13:41:27.052: INFO: stderr: ""
Jul 16 13:41:27.052: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 Jul 13:41:26.047 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jul 13:41:26.047 # Server started, Redis version 3.2.12\n1:M 16 Jul 13:41:26.048 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jul 13:41:26.048 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 16 13:41:27.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8886'
Jul 16 13:41:27.203: INFO: stderr: ""
Jul 16 13:41:27.203: INFO: stdout: "service/rm2 exposed\n"
Jul 16 13:41:27.270: INFO: Service rm2 in namespace kubectl-8886 found.
STEP: exposing service
Jul 16 13:41:29.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8886'
Jul 16 13:41:29.482: INFO: stderr: ""
Jul 16 13:41:29.482: INFO: stdout: "service/rm3 exposed\n"
Jul 16 13:41:29.490: INFO: Service rm3 in namespace kubectl-8886 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:41:31.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8886" for this suite.
Jul 16 13:41:55.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:41:55.852: INFO: namespace kubectl-8886 deletion completed in 24.333249588s

• [SLOW TEST:33.975 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:41:55.862: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2036
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 16 13:41:56.028: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 16 13:42:22.188: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.25 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2036 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:42:22.188: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:42:23.845: INFO: Found all expected endpoints: [netserver-0]
Jul 16 13:42:23.855: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.69 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2036 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:42:23.855: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:42:25.623: INFO: Found all expected endpoints: [netserver-1]
Jul 16 13:42:25.632: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.15 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2036 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 13:42:25.632: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 13:42:27.374: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:42:27.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2036" for this suite.
Jul 16 13:42:51.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:42:51.744: INFO: namespace pod-network-test-2036 deletion completed in 24.358996077s

• [SLOW TEST:55.883 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:42:51.745: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jul 16 13:42:51.915: INFO: Waiting up to 5m0s for pod "client-containers-e2f3e40b-212b-4369-81f8-7ece2a662e54" in namespace "containers-1928" to be "success or failure"
Jul 16 13:42:51.921: INFO: Pod "client-containers-e2f3e40b-212b-4369-81f8-7ece2a662e54": Phase="Pending", Reason="", readiness=false. Elapsed: 5.892981ms
Jul 16 13:42:53.949: INFO: Pod "client-containers-e2f3e40b-212b-4369-81f8-7ece2a662e54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03402676s
Jul 16 13:42:55.964: INFO: Pod "client-containers-e2f3e40b-212b-4369-81f8-7ece2a662e54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048868276s
Jul 16 13:42:57.973: INFO: Pod "client-containers-e2f3e40b-212b-4369-81f8-7ece2a662e54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057458809s
STEP: Saw pod success
Jul 16 13:42:57.973: INFO: Pod "client-containers-e2f3e40b-212b-4369-81f8-7ece2a662e54" satisfied condition "success or failure"
Jul 16 13:42:57.978: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod client-containers-e2f3e40b-212b-4369-81f8-7ece2a662e54 container test-container: <nil>
STEP: delete the pod
Jul 16 13:42:58.169: INFO: Waiting for pod client-containers-e2f3e40b-212b-4369-81f8-7ece2a662e54 to disappear
Jul 16 13:42:58.187: INFO: Pod client-containers-e2f3e40b-212b-4369-81f8-7ece2a662e54 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:42:58.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1928" for this suite.
Jul 16 13:43:04.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:43:04.638: INFO: namespace containers-1928 deletion completed in 6.43962915s

• [SLOW TEST:12.894 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:43:04.649: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-47d041b3-77c0-46bb-a6f2-1295d33163bb
STEP: Creating secret with name secret-projected-all-test-volume-584c23f3-9eea-4d5b-a5be-31da7ee96cc3
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 16 13:43:04.873: INFO: Waiting up to 5m0s for pod "projected-volume-c133f45c-036d-4a45-8628-ab662872a745" in namespace "projected-4682" to be "success or failure"
Jul 16 13:43:04.901: INFO: Pod "projected-volume-c133f45c-036d-4a45-8628-ab662872a745": Phase="Pending", Reason="", readiness=false. Elapsed: 27.764413ms
Jul 16 13:43:06.912: INFO: Pod "projected-volume-c133f45c-036d-4a45-8628-ab662872a745": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039151622s
Jul 16 13:43:08.923: INFO: Pod "projected-volume-c133f45c-036d-4a45-8628-ab662872a745": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049874782s
STEP: Saw pod success
Jul 16 13:43:08.923: INFO: Pod "projected-volume-c133f45c-036d-4a45-8628-ab662872a745" satisfied condition "success or failure"
Jul 16 13:43:08.929: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod projected-volume-c133f45c-036d-4a45-8628-ab662872a745 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 16 13:43:09.078: INFO: Waiting for pod projected-volume-c133f45c-036d-4a45-8628-ab662872a745 to disappear
Jul 16 13:43:09.084: INFO: Pod projected-volume-c133f45c-036d-4a45-8628-ab662872a745 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:43:09.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4682" for this suite.
Jul 16 13:43:15.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:43:15.428: INFO: namespace projected-4682 deletion completed in 6.335264046s

• [SLOW TEST:10.779 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:43:15.430: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 16 13:43:15.654: INFO: Waiting up to 5m0s for pod "pod-abe5f46a-2264-4525-aa1e-4a5e51501baf" in namespace "emptydir-9029" to be "success or failure"
Jul 16 13:43:15.661: INFO: Pod "pod-abe5f46a-2264-4525-aa1e-4a5e51501baf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.100258ms
Jul 16 13:43:17.671: INFO: Pod "pod-abe5f46a-2264-4525-aa1e-4a5e51501baf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017356657s
Jul 16 13:43:19.680: INFO: Pod "pod-abe5f46a-2264-4525-aa1e-4a5e51501baf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025805288s
STEP: Saw pod success
Jul 16 13:43:19.680: INFO: Pod "pod-abe5f46a-2264-4525-aa1e-4a5e51501baf" satisfied condition "success or failure"
Jul 16 13:43:19.685: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-abe5f46a-2264-4525-aa1e-4a5e51501baf container test-container: <nil>
STEP: delete the pod
Jul 16 13:43:19.759: INFO: Waiting for pod pod-abe5f46a-2264-4525-aa1e-4a5e51501baf to disappear
Jul 16 13:43:19.766: INFO: Pod pod-abe5f46a-2264-4525-aa1e-4a5e51501baf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:43:19.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9029" for this suite.
Jul 16 13:43:25.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:43:26.163: INFO: namespace emptydir-9029 deletion completed in 6.378489187s

• [SLOW TEST:10.734 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:43:26.168: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 16 13:43:26.300: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3006,SelfLink:/api/v1/namespaces/watch-3006/configmaps/e2e-watch-test-watch-closed,UID:0a580653-36b7-4158-8027-ce07dbf7f4bf,ResourceVersion:12891,Generation:0,CreationTimestamp:2019-07-16 13:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 16 13:43:26.301: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3006,SelfLink:/api/v1/namespaces/watch-3006/configmaps/e2e-watch-test-watch-closed,UID:0a580653-36b7-4158-8027-ce07dbf7f4bf,ResourceVersion:12893,Generation:0,CreationTimestamp:2019-07-16 13:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 16 13:43:26.332: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3006,SelfLink:/api/v1/namespaces/watch-3006/configmaps/e2e-watch-test-watch-closed,UID:0a580653-36b7-4158-8027-ce07dbf7f4bf,ResourceVersion:12894,Generation:0,CreationTimestamp:2019-07-16 13:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 13:43:26.332: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3006,SelfLink:/api/v1/namespaces/watch-3006/configmaps/e2e-watch-test-watch-closed,UID:0a580653-36b7-4158-8027-ce07dbf7f4bf,ResourceVersion:12895,Generation:0,CreationTimestamp:2019-07-16 13:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:43:26.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3006" for this suite.
Jul 16 13:43:32.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:43:32.731: INFO: namespace watch-3006 deletion completed in 6.382034647s

• [SLOW TEST:6.564 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:43:32.733: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jul 16 13:43:33.605: INFO: Waiting up to 5m0s for pod "var-expansion-3b3255ef-20f4-4d1e-8bab-24ee06a26ed8" in namespace "var-expansion-6460" to be "success or failure"
Jul 16 13:43:33.611: INFO: Pod "var-expansion-3b3255ef-20f4-4d1e-8bab-24ee06a26ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.973861ms
Jul 16 13:43:35.619: INFO: Pod "var-expansion-3b3255ef-20f4-4d1e-8bab-24ee06a26ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013721227s
Jul 16 13:43:37.633: INFO: Pod "var-expansion-3b3255ef-20f4-4d1e-8bab-24ee06a26ed8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028457048s
STEP: Saw pod success
Jul 16 13:43:37.633: INFO: Pod "var-expansion-3b3255ef-20f4-4d1e-8bab-24ee06a26ed8" satisfied condition "success or failure"
Jul 16 13:43:37.640: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod var-expansion-3b3255ef-20f4-4d1e-8bab-24ee06a26ed8 container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:43:37.770: INFO: Waiting for pod var-expansion-3b3255ef-20f4-4d1e-8bab-24ee06a26ed8 to disappear
Jul 16 13:43:37.776: INFO: Pod var-expansion-3b3255ef-20f4-4d1e-8bab-24ee06a26ed8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:43:37.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6460" for this suite.
Jul 16 13:43:43.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:43:44.208: INFO: namespace var-expansion-6460 deletion completed in 6.410472164s

• [SLOW TEST:11.476 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:43:44.210: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 16 13:43:44.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-82'
Jul 16 13:43:44.996: INFO: stderr: ""
Jul 16 13:43:44.996: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:43:44.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-82'
Jul 16 13:43:45.160: INFO: stderr: ""
Jul 16 13:43:45.160: INFO: stdout: "update-demo-nautilus-cd5p4 update-demo-nautilus-lhjdl "
Jul 16 13:43:45.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cd5p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:45.264: INFO: stderr: ""
Jul 16 13:43:45.264: INFO: stdout: ""
Jul 16 13:43:45.264: INFO: update-demo-nautilus-cd5p4 is created but not running
Jul 16 13:43:50.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-82'
Jul 16 13:43:50.418: INFO: stderr: ""
Jul 16 13:43:50.418: INFO: stdout: "update-demo-nautilus-cd5p4 update-demo-nautilus-lhjdl "
Jul 16 13:43:50.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cd5p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:50.538: INFO: stderr: ""
Jul 16 13:43:50.538: INFO: stdout: "true"
Jul 16 13:43:50.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cd5p4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:50.684: INFO: stderr: ""
Jul 16 13:43:50.684: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:43:50.684: INFO: validating pod update-demo-nautilus-cd5p4
Jul 16 13:43:50.787: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:43:50.788: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:43:50.788: INFO: update-demo-nautilus-cd5p4 is verified up and running
Jul 16 13:43:50.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-lhjdl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:50.910: INFO: stderr: ""
Jul 16 13:43:50.910: INFO: stdout: "true"
Jul 16 13:43:50.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-lhjdl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:51.024: INFO: stderr: ""
Jul 16 13:43:51.024: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:43:51.024: INFO: validating pod update-demo-nautilus-lhjdl
Jul 16 13:43:51.122: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:43:51.122: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:43:51.122: INFO: update-demo-nautilus-lhjdl is verified up and running
STEP: scaling down the replication controller
Jul 16 13:43:51.128: INFO: scanned /root for discovery docs: <nil>
Jul 16 13:43:51.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-82'
Jul 16 13:43:52.363: INFO: stderr: ""
Jul 16 13:43:52.363: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:43:52.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-82'
Jul 16 13:43:52.500: INFO: stderr: ""
Jul 16 13:43:52.500: INFO: stdout: "update-demo-nautilus-cd5p4 update-demo-nautilus-lhjdl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 16 13:43:57.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-82'
Jul 16 13:43:57.712: INFO: stderr: ""
Jul 16 13:43:57.712: INFO: stdout: "update-demo-nautilus-cd5p4 "
Jul 16 13:43:57.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cd5p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:57.845: INFO: stderr: ""
Jul 16 13:43:57.845: INFO: stdout: "true"
Jul 16 13:43:57.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cd5p4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:57.978: INFO: stderr: ""
Jul 16 13:43:57.978: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:43:57.978: INFO: validating pod update-demo-nautilus-cd5p4
Jul 16 13:43:58.005: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:43:58.005: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:43:58.005: INFO: update-demo-nautilus-cd5p4 is verified up and running
STEP: scaling up the replication controller
Jul 16 13:43:58.009: INFO: scanned /root for discovery docs: <nil>
Jul 16 13:43:58.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-82'
Jul 16 13:43:59.214: INFO: stderr: ""
Jul 16 13:43:59.214: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:43:59.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-82'
Jul 16 13:43:59.351: INFO: stderr: ""
Jul 16 13:43:59.351: INFO: stdout: "update-demo-nautilus-cd5p4 update-demo-nautilus-r8cbn "
Jul 16 13:43:59.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cd5p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:59.482: INFO: stderr: ""
Jul 16 13:43:59.482: INFO: stdout: "true"
Jul 16 13:43:59.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cd5p4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:59.637: INFO: stderr: ""
Jul 16 13:43:59.637: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:43:59.637: INFO: validating pod update-demo-nautilus-cd5p4
Jul 16 13:43:59.746: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:43:59.746: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:43:59.746: INFO: update-demo-nautilus-cd5p4 is verified up and running
Jul 16 13:43:59.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-r8cbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:43:59.868: INFO: stderr: ""
Jul 16 13:43:59.868: INFO: stdout: ""
Jul 16 13:43:59.868: INFO: update-demo-nautilus-r8cbn is created but not running
Jul 16 13:44:04.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-82'
Jul 16 13:44:05.031: INFO: stderr: ""
Jul 16 13:44:05.031: INFO: stdout: "update-demo-nautilus-cd5p4 update-demo-nautilus-r8cbn "
Jul 16 13:44:05.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cd5p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:44:05.153: INFO: stderr: ""
Jul 16 13:44:05.153: INFO: stdout: "true"
Jul 16 13:44:05.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cd5p4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:44:05.300: INFO: stderr: ""
Jul 16 13:44:05.300: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:44:05.300: INFO: validating pod update-demo-nautilus-cd5p4
Jul 16 13:44:05.315: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:44:05.315: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:44:05.315: INFO: update-demo-nautilus-cd5p4 is verified up and running
Jul 16 13:44:05.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-r8cbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:44:05.445: INFO: stderr: ""
Jul 16 13:44:05.445: INFO: stdout: "true"
Jul 16 13:44:05.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-r8cbn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-82'
Jul 16 13:44:05.564: INFO: stderr: ""
Jul 16 13:44:05.564: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:44:05.564: INFO: validating pod update-demo-nautilus-r8cbn
Jul 16 13:44:05.686: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:44:05.686: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:44:05.686: INFO: update-demo-nautilus-r8cbn is verified up and running
STEP: using delete to clean up resources
Jul 16 13:44:05.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-82'
Jul 16 13:44:05.863: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:44:05.863: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 16 13:44:05.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-82'
Jul 16 13:44:06.020: INFO: stderr: "No resources found.\n"
Jul 16 13:44:06.020: INFO: stdout: ""
Jul 16 13:44:06.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -l name=update-demo --namespace=kubectl-82 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 13:44:06.160: INFO: stderr: ""
Jul 16 13:44:06.160: INFO: stdout: "update-demo-nautilus-cd5p4\nupdate-demo-nautilus-r8cbn\n"
Jul 16 13:44:06.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-82'
Jul 16 13:44:06.834: INFO: stderr: "No resources found.\n"
Jul 16 13:44:06.834: INFO: stdout: ""
Jul 16 13:44:06.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -l name=update-demo --namespace=kubectl-82 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 13:44:06.999: INFO: stderr: ""
Jul 16 13:44:06.999: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:44:06.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-82" for this suite.
Jul 16 13:44:15.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:44:15.365: INFO: namespace kubectl-82 deletion completed in 8.357797813s

• [SLOW TEST:31.156 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:44:15.368: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 16 13:44:15.557: INFO: Waiting up to 5m0s for pod "pod-9b3bb69d-542a-47b1-892f-8f86fbd9a4ef" in namespace "emptydir-7352" to be "success or failure"
Jul 16 13:44:15.662: INFO: Pod "pod-9b3bb69d-542a-47b1-892f-8f86fbd9a4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 105.055187ms
Jul 16 13:44:17.721: INFO: Pod "pod-9b3bb69d-542a-47b1-892f-8f86fbd9a4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.16467731s
Jul 16 13:44:19.732: INFO: Pod "pod-9b3bb69d-542a-47b1-892f-8f86fbd9a4ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.174923822s
STEP: Saw pod success
Jul 16 13:44:19.732: INFO: Pod "pod-9b3bb69d-542a-47b1-892f-8f86fbd9a4ef" satisfied condition "success or failure"
Jul 16 13:44:19.738: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-9b3bb69d-542a-47b1-892f-8f86fbd9a4ef container test-container: <nil>
STEP: delete the pod
Jul 16 13:44:19.920: INFO: Waiting for pod pod-9b3bb69d-542a-47b1-892f-8f86fbd9a4ef to disappear
Jul 16 13:44:19.926: INFO: Pod pod-9b3bb69d-542a-47b1-892f-8f86fbd9a4ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:44:19.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7352" for this suite.
Jul 16 13:44:25.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:44:26.482: INFO: namespace emptydir-7352 deletion completed in 6.547175159s

• [SLOW TEST:11.114 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:44:26.482: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:44:26.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4659" for this suite.
Jul 16 13:44:50.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:44:51.173: INFO: namespace kubelet-test-4659 deletion completed in 24.505523397s

• [SLOW TEST:24.691 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:44:51.176: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6411
I0716 13:44:51.306748      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6411, replica count: 1
I0716 13:44:52.357657      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 13:44:53.358021      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0716 13:44:54.358381      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 16 13:44:54.590: INFO: Created: latency-svc-bfhl9
Jul 16 13:44:54.609: INFO: Got endpoints: latency-svc-bfhl9 [50.514966ms]
Jul 16 13:44:54.647: INFO: Created: latency-svc-gdrx7
Jul 16 13:44:54.658: INFO: Created: latency-svc-7txl2
Jul 16 13:44:54.670: INFO: Got endpoints: latency-svc-gdrx7 [59.097122ms]
Jul 16 13:44:54.678: INFO: Got endpoints: latency-svc-7txl2 [67.5712ms]
Jul 16 13:44:54.693: INFO: Created: latency-svc-6dgn2
Jul 16 13:44:54.706: INFO: Created: latency-svc-mp5r6
Jul 16 13:44:54.732: INFO: Got endpoints: latency-svc-6dgn2 [120.402876ms]
Jul 16 13:44:54.732: INFO: Created: latency-svc-7hvmw
Jul 16 13:44:54.748: INFO: Got endpoints: latency-svc-mp5r6 [136.719804ms]
Jul 16 13:44:54.754: INFO: Got endpoints: latency-svc-7hvmw [142.565625ms]
Jul 16 13:44:54.770: INFO: Created: latency-svc-6mggg
Jul 16 13:44:54.777: INFO: Created: latency-svc-hjt8p
Jul 16 13:44:54.802: INFO: Created: latency-svc-5clq8
Jul 16 13:44:54.815: INFO: Got endpoints: latency-svc-6mggg [203.640884ms]
Jul 16 13:44:54.817: INFO: Got endpoints: latency-svc-hjt8p [205.240879ms]
Jul 16 13:44:54.825: INFO: Created: latency-svc-9kwh8
Jul 16 13:44:54.825: INFO: Got endpoints: latency-svc-5clq8 [213.433936ms]
Jul 16 13:44:54.833: INFO: Got endpoints: latency-svc-9kwh8 [221.207629ms]
Jul 16 13:44:54.834: INFO: Created: latency-svc-dbsv6
Jul 16 13:44:54.850: INFO: Created: latency-svc-h6llp
Jul 16 13:44:54.887: INFO: Got endpoints: latency-svc-h6llp [275.152712ms]
Jul 16 13:44:54.887: INFO: Got endpoints: latency-svc-dbsv6 [275.715832ms]
Jul 16 13:44:54.932: INFO: Created: latency-svc-kljth
Jul 16 13:44:54.939: INFO: Created: latency-svc-mf9bp
Jul 16 13:44:54.950: INFO: Created: latency-svc-kqkjb
Jul 16 13:44:54.957: INFO: Got endpoints: latency-svc-mf9bp [345.328575ms]
Jul 16 13:44:54.957: INFO: Got endpoints: latency-svc-kljth [345.724651ms]
Jul 16 13:44:54.965: INFO: Got endpoints: latency-svc-kqkjb [352.686543ms]
Jul 16 13:44:54.967: INFO: Created: latency-svc-mfphj
Jul 16 13:44:55.212: INFO: Created: latency-svc-gwvqz
Jul 16 13:44:55.230: INFO: Got endpoints: latency-svc-mfphj [618.294049ms]
Jul 16 13:44:55.295: INFO: Got endpoints: latency-svc-gwvqz [625.352793ms]
Jul 16 13:44:55.315: INFO: Created: latency-svc-qtpzh
Jul 16 13:44:55.324: INFO: Got endpoints: latency-svc-qtpzh [645.593376ms]
Jul 16 13:44:55.326: INFO: Created: latency-svc-j6dq5
Jul 16 13:44:55.340: INFO: Got endpoints: latency-svc-j6dq5 [607.001899ms]
Jul 16 13:44:55.365: INFO: Created: latency-svc-dndw2
Jul 16 13:44:55.383: INFO: Created: latency-svc-xftld
Jul 16 13:44:55.385: INFO: Got endpoints: latency-svc-dndw2 [636.4914ms]
Jul 16 13:44:55.386: INFO: Created: latency-svc-r8mr6
Jul 16 13:44:55.397: INFO: Got endpoints: latency-svc-r8mr6 [580.889137ms]
Jul 16 13:44:55.397: INFO: Got endpoints: latency-svc-xftld [642.979987ms]
Jul 16 13:44:55.413: INFO: Created: latency-svc-zcrf4
Jul 16 13:44:55.426: INFO: Got endpoints: latency-svc-zcrf4 [609.186539ms]
Jul 16 13:44:55.430: INFO: Created: latency-svc-v7ptp
Jul 16 13:44:55.446: INFO: Got endpoints: latency-svc-v7ptp [620.178821ms]
Jul 16 13:44:55.452: INFO: Created: latency-svc-sfdpg
Jul 16 13:44:55.465: INFO: Got endpoints: latency-svc-sfdpg [632.141727ms]
Jul 16 13:44:55.482: INFO: Created: latency-svc-vn2zq
Jul 16 13:44:55.483: INFO: Created: latency-svc-7vtcz
Jul 16 13:44:55.490: INFO: Created: latency-svc-knbfq
Jul 16 13:44:55.497: INFO: Got endpoints: latency-svc-7vtcz [610.332759ms]
Jul 16 13:44:55.498: INFO: Got endpoints: latency-svc-vn2zq [610.976259ms]
Jul 16 13:44:55.499: INFO: Created: latency-svc-sj884
Jul 16 13:44:55.529: INFO: Got endpoints: latency-svc-knbfq [571.923274ms]
Jul 16 13:44:55.529: INFO: Got endpoints: latency-svc-sj884 [571.846688ms]
Jul 16 13:44:55.539: INFO: Created: latency-svc-6lztg
Jul 16 13:44:55.570: INFO: Created: latency-svc-vhft2
Jul 16 13:44:55.574: INFO: Got endpoints: latency-svc-6lztg [609.307783ms]
Jul 16 13:44:55.610: INFO: Got endpoints: latency-svc-vhft2 [379.627043ms]
Jul 16 13:44:55.648: INFO: Created: latency-svc-msj59
Jul 16 13:44:55.648: INFO: Created: latency-svc-hgtbq
Jul 16 13:44:55.666: INFO: Got endpoints: latency-svc-hgtbq [369.619715ms]
Jul 16 13:44:55.683: INFO: Created: latency-svc-mvs7s
Jul 16 13:44:55.683: INFO: Created: latency-svc-5sgwc
Jul 16 13:44:55.684: INFO: Created: latency-svc-cwsps
Jul 16 13:44:55.690: INFO: Got endpoints: latency-svc-cwsps [350.085832ms]
Jul 16 13:44:55.690: INFO: Got endpoints: latency-svc-msj59 [365.825424ms]
Jul 16 13:44:55.692: INFO: Got endpoints: latency-svc-mvs7s [305.586851ms]
Jul 16 13:44:55.709: INFO: Created: latency-svc-4c5xg
Jul 16 13:44:55.717: INFO: Got endpoints: latency-svc-5sgwc [319.730235ms]
Jul 16 13:44:55.727: INFO: Created: latency-svc-vpcjp
Jul 16 13:44:55.744: INFO: Got endpoints: latency-svc-4c5xg [347.589049ms]
Jul 16 13:44:55.748: INFO: Got endpoints: latency-svc-vpcjp [321.366583ms]
Jul 16 13:44:55.764: INFO: Created: latency-svc-hgvk4
Jul 16 13:44:55.775: INFO: Created: latency-svc-f94sw
Jul 16 13:44:55.783: INFO: Got endpoints: latency-svc-hgvk4 [337.629886ms]
Jul 16 13:44:55.787: INFO: Created: latency-svc-bpjln
Jul 16 13:44:55.809: INFO: Created: latency-svc-j5sl8
Jul 16 13:44:55.810: INFO: Got endpoints: latency-svc-bpjln [312.203221ms]
Jul 16 13:44:55.810: INFO: Got endpoints: latency-svc-f94sw [343.775151ms]
Jul 16 13:44:55.818: INFO: Created: latency-svc-jd27t
Jul 16 13:44:55.825: INFO: Got endpoints: latency-svc-j5sl8 [325.569416ms]
Jul 16 13:44:55.837: INFO: Got endpoints: latency-svc-jd27t [307.099837ms]
Jul 16 13:44:55.872: INFO: Created: latency-svc-nnq4c
Jul 16 13:44:55.878: INFO: Created: latency-svc-8k77h
Jul 16 13:44:55.887: INFO: Created: latency-svc-p8rjd
Jul 16 13:44:55.888: INFO: Got endpoints: latency-svc-nnq4c [358.010525ms]
Jul 16 13:44:56.302: INFO: Got endpoints: latency-svc-p8rjd [690.426728ms]
Jul 16 13:44:56.302: INFO: Got endpoints: latency-svc-8k77h [727.745257ms]
Jul 16 13:44:56.333: INFO: Created: latency-svc-6phbz
Jul 16 13:44:56.612: INFO: Got endpoints: latency-svc-6phbz [946.367405ms]
Jul 16 13:44:56.891: INFO: Created: latency-svc-4llf6
Jul 16 13:44:57.191: INFO: Got endpoints: latency-svc-4llf6 [1.500667488s]
Jul 16 13:44:57.191: INFO: Created: latency-svc-kmddg
Jul 16 13:44:57.204: INFO: Got endpoints: latency-svc-kmddg [1.51380951s]
Jul 16 13:44:57.225: INFO: Created: latency-svc-s4nk4
Jul 16 13:44:57.237: INFO: Created: latency-svc-xh566
Jul 16 13:44:57.239: INFO: Created: latency-svc-cxnqv
Jul 16 13:44:57.254: INFO: Created: latency-svc-mkr84
Jul 16 13:44:57.259: INFO: Got endpoints: latency-svc-s4nk4 [1.56749952s]
Jul 16 13:44:57.260: INFO: Got endpoints: latency-svc-xh566 [1.542484759s]
Jul 16 13:44:57.265: INFO: Got endpoints: latency-svc-cxnqv [1.520161621s]
Jul 16 13:44:57.268: INFO: Got endpoints: latency-svc-mkr84 [1.519920347s]
Jul 16 13:44:57.278: INFO: Created: latency-svc-dgbt7
Jul 16 13:44:57.311: INFO: Got endpoints: latency-svc-dgbt7 [1.52750544s]
Jul 16 13:44:57.312: INFO: Created: latency-svc-gpdzp
Jul 16 13:44:57.326: INFO: Got endpoints: latency-svc-gpdzp [1.516094524s]
Jul 16 13:44:57.334: INFO: Created: latency-svc-tnx5k
Jul 16 13:44:57.353: INFO: Created: latency-svc-lmpl7
Jul 16 13:44:57.357: INFO: Created: latency-svc-8w56w
Jul 16 13:44:57.382: INFO: Created: latency-svc-fl4jg
Jul 16 13:44:57.386: INFO: Got endpoints: latency-svc-tnx5k [1.575787858s]
Jul 16 13:44:57.391: INFO: Got endpoints: latency-svc-8w56w [1.553745164s]
Jul 16 13:44:57.391: INFO: Got endpoints: latency-svc-lmpl7 [1.566473016s]
Jul 16 13:44:57.396: INFO: Got endpoints: latency-svc-fl4jg [1.508710926s]
Jul 16 13:44:57.399: INFO: Created: latency-svc-7jnfj
Jul 16 13:44:57.421: INFO: Created: latency-svc-82qx5
Jul 16 13:44:57.430: INFO: Created: latency-svc-6r4l2
Jul 16 13:44:57.434: INFO: Got endpoints: latency-svc-7jnfj [1.131623526s]
Jul 16 13:44:57.466: INFO: Got endpoints: latency-svc-82qx5 [1.163611573s]
Jul 16 13:44:57.470: INFO: Got endpoints: latency-svc-6r4l2 [857.281975ms]
Jul 16 13:44:57.472: INFO: Created: latency-svc-46q49
Jul 16 13:44:57.494: INFO: Created: latency-svc-gwqrk
Jul 16 13:44:57.495: INFO: Got endpoints: latency-svc-46q49 [303.442352ms]
Jul 16 13:44:57.514: INFO: Got endpoints: latency-svc-gwqrk [309.793546ms]
Jul 16 13:44:57.530: INFO: Created: latency-svc-4fxlh
Jul 16 13:44:57.547: INFO: Created: latency-svc-nh5b8
Jul 16 13:44:57.547: INFO: Got endpoints: latency-svc-4fxlh [287.698254ms]
Jul 16 13:44:57.559: INFO: Created: latency-svc-m77vk
Jul 16 13:44:57.565: INFO: Got endpoints: latency-svc-nh5b8 [304.997631ms]
Jul 16 13:44:57.579: INFO: Got endpoints: latency-svc-m77vk [314.429914ms]
Jul 16 13:44:57.598: INFO: Created: latency-svc-qh9mx
Jul 16 13:44:57.615: INFO: Got endpoints: latency-svc-qh9mx [347.123059ms]
Jul 16 13:44:57.616: INFO: Created: latency-svc-bv82h
Jul 16 13:44:57.647: INFO: Got endpoints: latency-svc-bv82h [335.00478ms]
Jul 16 13:44:57.660: INFO: Created: latency-svc-gstqm
Jul 16 13:44:57.662: INFO: Created: latency-svc-2qq5b
Jul 16 13:44:57.681: INFO: Got endpoints: latency-svc-2qq5b [295.263478ms]
Jul 16 13:44:57.681: INFO: Got endpoints: latency-svc-gstqm [355.386654ms]
Jul 16 13:44:57.696: INFO: Created: latency-svc-jpg5c
Jul 16 13:44:57.728: INFO: Created: latency-svc-ltlvh
Jul 16 13:44:57.781: INFO: Got endpoints: latency-svc-ltlvh [390.201827ms]
Jul 16 13:44:57.781: INFO: Got endpoints: latency-svc-jpg5c [389.952648ms]
Jul 16 13:44:57.791: INFO: Created: latency-svc-h6wvz
Jul 16 13:44:57.806: INFO: Got endpoints: latency-svc-h6wvz [409.422913ms]
Jul 16 13:44:57.807: INFO: Created: latency-svc-mjpwq
Jul 16 13:44:57.830: INFO: Got endpoints: latency-svc-mjpwq [395.652254ms]
Jul 16 13:44:57.831: INFO: Created: latency-svc-9csjd
Jul 16 13:44:57.853: INFO: Got endpoints: latency-svc-9csjd [387.154341ms]
Jul 16 13:44:57.865: INFO: Created: latency-svc-9xmtl
Jul 16 13:44:57.879: INFO: Created: latency-svc-qjlbx
Jul 16 13:44:57.895: INFO: Created: latency-svc-tdgwc
Jul 16 13:44:57.899: INFO: Got endpoints: latency-svc-9xmtl [428.876003ms]
Jul 16 13:44:57.913: INFO: Got endpoints: latency-svc-qjlbx [417.796643ms]
Jul 16 13:44:57.914: INFO: Created: latency-svc-799wz
Jul 16 13:44:57.930: INFO: Created: latency-svc-4bj56
Jul 16 13:44:57.937: INFO: Got endpoints: latency-svc-tdgwc [421.980097ms]
Jul 16 13:44:57.939: INFO: Got endpoints: latency-svc-799wz [391.92431ms]
Jul 16 13:44:57.943: INFO: Got endpoints: latency-svc-4bj56 [378.337598ms]
Jul 16 13:44:57.990: INFO: Created: latency-svc-mz4fj
Jul 16 13:44:57.991: INFO: Created: latency-svc-wmk7m
Jul 16 13:44:57.991: INFO: Created: latency-svc-tmpn2
Jul 16 13:44:58.003: INFO: Created: latency-svc-rvjh8
Jul 16 13:44:58.017: INFO: Got endpoints: latency-svc-tmpn2 [401.448546ms]
Jul 16 13:44:58.017: INFO: Got endpoints: latency-svc-mz4fj [370.070269ms]
Jul 16 13:44:58.017: INFO: Got endpoints: latency-svc-wmk7m [437.47827ms]
Jul 16 13:44:58.037: INFO: Got endpoints: latency-svc-rvjh8 [356.135876ms]
Jul 16 13:44:58.038: INFO: Created: latency-svc-rwxg7
Jul 16 13:44:58.055: INFO: Created: latency-svc-jv7nn
Jul 16 13:44:58.059: INFO: Got endpoints: latency-svc-rwxg7 [377.58526ms]
Jul 16 13:44:58.076: INFO: Got endpoints: latency-svc-jv7nn [294.932181ms]
Jul 16 13:44:58.128: INFO: Created: latency-svc-ntcmx
Jul 16 13:44:58.131: INFO: Created: latency-svc-mxvts
Jul 16 13:44:58.146: INFO: Created: latency-svc-dxhwh
Jul 16 13:44:58.161: INFO: Created: latency-svc-2xf68
Jul 16 13:44:58.161: INFO: Got endpoints: latency-svc-ntcmx [379.49526ms]
Jul 16 13:44:58.163: INFO: Got endpoints: latency-svc-mxvts [356.596395ms]
Jul 16 13:44:58.164: INFO: Got endpoints: latency-svc-dxhwh [334.252239ms]
Jul 16 13:44:58.171: INFO: Got endpoints: latency-svc-2xf68 [318.337317ms]
Jul 16 13:44:58.186: INFO: Created: latency-svc-jv5vv
Jul 16 13:44:58.239: INFO: Got endpoints: latency-svc-jv5vv [339.643327ms]
Jul 16 13:44:58.315: INFO: Created: latency-svc-hmns9
Jul 16 13:44:58.333: INFO: Got endpoints: latency-svc-hmns9 [419.821854ms]
Jul 16 13:44:58.340: INFO: Created: latency-svc-cdzpp
Jul 16 13:44:58.340: INFO: Created: latency-svc-gz6tl
Jul 16 13:44:58.341: INFO: Created: latency-svc-l8s8s
Jul 16 13:44:58.341: INFO: Created: latency-svc-5jstn
Jul 16 13:44:58.341: INFO: Created: latency-svc-pqthv
Jul 16 13:44:58.355: INFO: Created: latency-svc-8xg78
Jul 16 13:44:58.356: INFO: Created: latency-svc-c857j
Jul 16 13:44:58.370: INFO: Created: latency-svc-zslbh
Jul 16 13:44:58.379: INFO: Got endpoints: latency-svc-cdzpp [439.733155ms]
Jul 16 13:44:58.379: INFO: Got endpoints: latency-svc-l8s8s [442.561104ms]
Jul 16 13:44:58.380: INFO: Got endpoints: latency-svc-pqthv [363.087137ms]
Jul 16 13:44:58.380: INFO: Got endpoints: latency-svc-gz6tl [140.867492ms]
Jul 16 13:44:58.380: INFO: Got endpoints: latency-svc-5jstn [436.373868ms]
Jul 16 13:44:58.401: INFO: Got endpoints: latency-svc-8xg78 [383.9115ms]
Jul 16 13:44:58.401: INFO: Got endpoints: latency-svc-zslbh [363.852592ms]
Jul 16 13:44:58.401: INFO: Got endpoints: latency-svc-c857j [384.256517ms]
Jul 16 13:44:58.407: INFO: Created: latency-svc-xwmxh
Jul 16 13:44:58.413: INFO: Created: latency-svc-kwdm6
Jul 16 13:44:58.429: INFO: Got endpoints: latency-svc-xwmxh [369.394376ms]
Jul 16 13:44:58.441: INFO: Created: latency-svc-jmf7k
Jul 16 13:44:58.472: INFO: Got endpoints: latency-svc-kwdm6 [395.630173ms]
Jul 16 13:44:58.484: INFO: Created: latency-svc-whtw7
Jul 16 13:44:58.516: INFO: Got endpoints: latency-svc-jmf7k [353.488784ms]
Jul 16 13:44:58.516: INFO: Created: latency-svc-df6gh
Jul 16 13:44:58.532: INFO: Created: latency-svc-8vzlh
Jul 16 13:44:58.546: INFO: Created: latency-svc-nmfmv
Jul 16 13:44:58.593: INFO: Created: latency-svc-rr22j
Jul 16 13:44:58.596: INFO: Got endpoints: latency-svc-whtw7 [434.85107ms]
Jul 16 13:44:58.610: INFO: Created: latency-svc-nlptb
Jul 16 13:44:58.610: INFO: Got endpoints: latency-svc-df6gh [445.698191ms]
Jul 16 13:44:58.627: INFO: Created: latency-svc-5m4k6
Jul 16 13:44:58.641: INFO: Created: latency-svc-77w6w
Jul 16 13:44:58.652: INFO: Created: latency-svc-z8g22
Jul 16 13:44:58.657: INFO: Got endpoints: latency-svc-8vzlh [485.756805ms]
Jul 16 13:44:58.689: INFO: Created: latency-svc-hsl2g
Jul 16 13:44:58.699: INFO: Created: latency-svc-w57bm
Jul 16 13:44:58.700: INFO: Created: latency-svc-gd857
Jul 16 13:44:58.730: INFO: Created: latency-svc-7rtsp
Jul 16 13:44:58.737: INFO: Got endpoints: latency-svc-nmfmv [404.283244ms]
Jul 16 13:44:58.751: INFO: Created: latency-svc-lfmft
Jul 16 13:44:58.755: INFO: Got endpoints: latency-svc-rr22j [375.49076ms]
Jul 16 13:44:58.783: INFO: Created: latency-svc-kn872
Jul 16 13:44:58.796: INFO: Created: latency-svc-xc8hl
Jul 16 13:44:58.818: INFO: Got endpoints: latency-svc-nlptb [438.366066ms]
Jul 16 13:44:58.819: INFO: Created: latency-svc-v8nkg
Jul 16 13:44:58.838: INFO: Created: latency-svc-84hm7
Jul 16 13:44:58.851: INFO: Created: latency-svc-v65b5
Jul 16 13:44:58.856: INFO: Created: latency-svc-dkbqp
Jul 16 13:44:58.879: INFO: Created: latency-svc-xqnzv
Jul 16 13:44:58.890: INFO: Got endpoints: latency-svc-5m4k6 [510.360478ms]
Jul 16 13:44:58.916: INFO: Got endpoints: latency-svc-77w6w [535.929339ms]
Jul 16 13:44:58.917: INFO: Created: latency-svc-8qq8s
Jul 16 13:44:58.933: INFO: Created: latency-svc-458vj
Jul 16 13:44:58.951: INFO: Got endpoints: latency-svc-z8g22 [571.411724ms]
Jul 16 13:44:58.987: INFO: Created: latency-svc-v9cwq
Jul 16 13:44:59.003: INFO: Got endpoints: latency-svc-hsl2g [602.440837ms]
Jul 16 13:44:59.049: INFO: Created: latency-svc-xsxjf
Jul 16 13:44:59.050: INFO: Got endpoints: latency-svc-gd857 [648.513502ms]
Jul 16 13:44:59.105: INFO: Created: latency-svc-mz6w5
Jul 16 13:44:59.110: INFO: Got endpoints: latency-svc-w57bm [708.947758ms]
Jul 16 13:44:59.139: INFO: Created: latency-svc-ncbxz
Jul 16 13:44:59.150: INFO: Got endpoints: latency-svc-7rtsp [720.959298ms]
Jul 16 13:44:59.203: INFO: Got endpoints: latency-svc-lfmft [730.724636ms]
Jul 16 13:44:59.204: INFO: Created: latency-svc-xr7t9
Jul 16 13:44:59.239: INFO: Created: latency-svc-mx5l9
Jul 16 13:44:59.253: INFO: Got endpoints: latency-svc-kn872 [736.939268ms]
Jul 16 13:44:59.279: INFO: Created: latency-svc-sz2m4
Jul 16 13:44:59.309: INFO: Got endpoints: latency-svc-xc8hl [713.293331ms]
Jul 16 13:44:59.349: INFO: Got endpoints: latency-svc-v8nkg [739.007451ms]
Jul 16 13:44:59.355: INFO: Created: latency-svc-r8mxh
Jul 16 13:44:59.382: INFO: Created: latency-svc-9fsk5
Jul 16 13:44:59.406: INFO: Got endpoints: latency-svc-84hm7 [748.762187ms]
Jul 16 13:44:59.450: INFO: Got endpoints: latency-svc-v65b5 [712.508962ms]
Jul 16 13:44:59.455: INFO: Created: latency-svc-cnfc7
Jul 16 13:44:59.470: INFO: Created: latency-svc-bmtbf
Jul 16 13:44:59.547: INFO: Got endpoints: latency-svc-dkbqp [792.309877ms]
Jul 16 13:44:59.557: INFO: Got endpoints: latency-svc-xqnzv [738.878564ms]
Jul 16 13:44:59.593: INFO: Created: latency-svc-k2pr6
Jul 16 13:44:59.597: INFO: Created: latency-svc-z77kb
Jul 16 13:44:59.599: INFO: Got endpoints: latency-svc-8qq8s [709.02265ms]
Jul 16 13:44:59.630: INFO: Created: latency-svc-n24mf
Jul 16 13:44:59.662: INFO: Got endpoints: latency-svc-458vj [745.461891ms]
Jul 16 13:44:59.702: INFO: Created: latency-svc-552tx
Jul 16 13:44:59.708: INFO: Got endpoints: latency-svc-v9cwq [756.958687ms]
Jul 16 13:44:59.753: INFO: Created: latency-svc-cx8wx
Jul 16 13:44:59.773: INFO: Got endpoints: latency-svc-xsxjf [770.076991ms]
Jul 16 13:44:59.810: INFO: Created: latency-svc-dtftw
Jul 16 13:44:59.824: INFO: Got endpoints: latency-svc-mz6w5 [774.116095ms]
Jul 16 13:44:59.849: INFO: Got endpoints: latency-svc-ncbxz [738.940038ms]
Jul 16 13:44:59.883: INFO: Created: latency-svc-56fhr
Jul 16 13:44:59.891: INFO: Created: latency-svc-h5f82
Jul 16 13:44:59.906: INFO: Got endpoints: latency-svc-xr7t9 [756.174045ms]
Jul 16 13:44:59.940: INFO: Created: latency-svc-w6xlq
Jul 16 13:44:59.961: INFO: Got endpoints: latency-svc-mx5l9 [758.742379ms]
Jul 16 13:44:59.992: INFO: Created: latency-svc-8t7zb
Jul 16 13:45:00.023: INFO: Got endpoints: latency-svc-sz2m4 [769.463317ms]
Jul 16 13:45:00.054: INFO: Created: latency-svc-dnhwg
Jul 16 13:45:00.057: INFO: Got endpoints: latency-svc-r8mxh [747.261243ms]
Jul 16 13:45:00.091: INFO: Created: latency-svc-rwwr2
Jul 16 13:45:00.106: INFO: Got endpoints: latency-svc-9fsk5 [756.975842ms]
Jul 16 13:45:00.144: INFO: Created: latency-svc-tr5nv
Jul 16 13:45:00.155: INFO: Got endpoints: latency-svc-cnfc7 [748.903139ms]
Jul 16 13:45:00.178: INFO: Created: latency-svc-s5cbc
Jul 16 13:45:00.197: INFO: Got endpoints: latency-svc-bmtbf [747.791727ms]
Jul 16 13:45:00.260: INFO: Created: latency-svc-ltbnw
Jul 16 13:45:00.266: INFO: Got endpoints: latency-svc-k2pr6 [718.542895ms]
Jul 16 13:45:00.299: INFO: Got endpoints: latency-svc-z77kb [741.136047ms]
Jul 16 13:45:00.328: INFO: Created: latency-svc-jvkll
Jul 16 13:45:00.328: INFO: Created: latency-svc-xkcmt
Jul 16 13:45:00.366: INFO: Got endpoints: latency-svc-n24mf [766.329923ms]
Jul 16 13:45:00.425: INFO: Got endpoints: latency-svc-552tx [763.08353ms]
Jul 16 13:45:00.455: INFO: Got endpoints: latency-svc-cx8wx [747.233845ms]
Jul 16 13:45:00.468: INFO: Created: latency-svc-x9c8t
Jul 16 13:45:00.493: INFO: Created: latency-svc-cs52w
Jul 16 13:45:00.496: INFO: Created: latency-svc-fxbtv
Jul 16 13:45:00.513: INFO: Got endpoints: latency-svc-dtftw [739.853814ms]
Jul 16 13:45:00.531: INFO: Created: latency-svc-d4gqv
Jul 16 13:45:00.556: INFO: Got endpoints: latency-svc-56fhr [732.209049ms]
Jul 16 13:45:00.581: INFO: Created: latency-svc-gss2z
Jul 16 13:45:00.601: INFO: Got endpoints: latency-svc-h5f82 [751.786307ms]
Jul 16 13:45:00.635: INFO: Created: latency-svc-9h7zw
Jul 16 13:45:00.661: INFO: Got endpoints: latency-svc-w6xlq [754.225406ms]
Jul 16 13:45:00.681: INFO: Created: latency-svc-2kdmv
Jul 16 13:45:00.718: INFO: Got endpoints: latency-svc-8t7zb [755.949455ms]
Jul 16 13:45:00.739: INFO: Created: latency-svc-z7ntv
Jul 16 13:45:00.760: INFO: Got endpoints: latency-svc-dnhwg [736.372755ms]
Jul 16 13:45:00.823: INFO: Created: latency-svc-ffr7f
Jul 16 13:45:00.858: INFO: Got endpoints: latency-svc-rwwr2 [801.112488ms]
Jul 16 13:45:00.874: INFO: Got endpoints: latency-svc-tr5nv [767.367621ms]
Jul 16 13:45:00.921: INFO: Created: latency-svc-kkbhn
Jul 16 13:45:00.928: INFO: Got endpoints: latency-svc-s5cbc [773.272731ms]
Jul 16 13:45:00.949: INFO: Created: latency-svc-x2nqt
Jul 16 13:45:00.954: INFO: Got endpoints: latency-svc-ltbnw [756.827472ms]
Jul 16 13:45:00.978: INFO: Created: latency-svc-8bh4f
Jul 16 13:45:00.991: INFO: Created: latency-svc-rddf9
Jul 16 13:45:01.009: INFO: Got endpoints: latency-svc-xkcmt [742.681016ms]
Jul 16 13:45:01.035: INFO: Created: latency-svc-d6htg
Jul 16 13:45:01.068: INFO: Got endpoints: latency-svc-jvkll [768.774402ms]
Jul 16 13:45:01.090: INFO: Created: latency-svc-tl6qs
Jul 16 13:45:01.106: INFO: Got endpoints: latency-svc-x9c8t [740.158632ms]
Jul 16 13:45:01.146: INFO: Created: latency-svc-5wnlb
Jul 16 13:45:01.153: INFO: Got endpoints: latency-svc-cs52w [727.900643ms]
Jul 16 13:45:01.176: INFO: Created: latency-svc-fqw4f
Jul 16 13:45:01.201: INFO: Got endpoints: latency-svc-fxbtv [745.889531ms]
Jul 16 13:45:01.239: INFO: Created: latency-svc-76qq8
Jul 16 13:45:01.262: INFO: Got endpoints: latency-svc-d4gqv [748.602606ms]
Jul 16 13:45:01.287: INFO: Created: latency-svc-dhdb5
Jul 16 13:45:01.313: INFO: Got endpoints: latency-svc-gss2z [756.274192ms]
Jul 16 13:45:01.361: INFO: Got endpoints: latency-svc-9h7zw [759.44662ms]
Jul 16 13:45:01.366: INFO: Created: latency-svc-dj522
Jul 16 13:45:01.407: INFO: Got endpoints: latency-svc-2kdmv [746.374622ms]
Jul 16 13:45:01.415: INFO: Created: latency-svc-pg2rz
Jul 16 13:45:01.463: INFO: Created: latency-svc-6lswf
Jul 16 13:45:01.466: INFO: Got endpoints: latency-svc-z7ntv [748.58116ms]
Jul 16 13:45:01.521: INFO: Got endpoints: latency-svc-ffr7f [761.021588ms]
Jul 16 13:45:01.527: INFO: Created: latency-svc-fkfnj
Jul 16 13:45:01.550: INFO: Created: latency-svc-7t4zb
Jul 16 13:45:01.587: INFO: Got endpoints: latency-svc-kkbhn [728.551173ms]
Jul 16 13:45:01.602: INFO: Got endpoints: latency-svc-x2nqt [727.97022ms]
Jul 16 13:45:01.622: INFO: Created: latency-svc-k687l
Jul 16 13:45:01.653: INFO: Created: latency-svc-8zsr6
Jul 16 13:45:01.657: INFO: Got endpoints: latency-svc-8bh4f [728.898015ms]
Jul 16 13:45:01.688: INFO: Created: latency-svc-8dgvc
Jul 16 13:45:01.752: INFO: Got endpoints: latency-svc-rddf9 [797.426007ms]
Jul 16 13:45:01.759: INFO: Got endpoints: latency-svc-d6htg [749.920956ms]
Jul 16 13:45:01.806: INFO: Got endpoints: latency-svc-tl6qs [738.141851ms]
Jul 16 13:45:01.808: INFO: Created: latency-svc-48jw9
Jul 16 13:45:01.820: INFO: Created: latency-svc-j4vdg
Jul 16 13:45:01.862: INFO: Created: latency-svc-wdxnx
Jul 16 13:45:01.870: INFO: Got endpoints: latency-svc-5wnlb [763.524113ms]
Jul 16 13:45:01.910: INFO: Got endpoints: latency-svc-fqw4f [756.445083ms]
Jul 16 13:45:01.917: INFO: Created: latency-svc-fsgql
Jul 16 13:45:01.943: INFO: Created: latency-svc-n4xfv
Jul 16 13:45:01.956: INFO: Got endpoints: latency-svc-76qq8 [754.900743ms]
Jul 16 13:45:02.023: INFO: Created: latency-svc-qd872
Jul 16 13:45:02.052: INFO: Got endpoints: latency-svc-dhdb5 [789.744606ms]
Jul 16 13:45:02.056: INFO: Got endpoints: latency-svc-dj522 [743.042651ms]
Jul 16 13:45:02.090: INFO: Created: latency-svc-m7mmz
Jul 16 13:45:02.102: INFO: Got endpoints: latency-svc-pg2rz [741.168037ms]
Jul 16 13:45:02.111: INFO: Created: latency-svc-bgrkh
Jul 16 13:45:02.147: INFO: Created: latency-svc-6b76j
Jul 16 13:45:02.170: INFO: Got endpoints: latency-svc-6lswf [762.377122ms]
Jul 16 13:45:02.223: INFO: Created: latency-svc-74c59
Jul 16 13:45:02.223: INFO: Got endpoints: latency-svc-fkfnj [756.882284ms]
Jul 16 13:45:02.258: INFO: Got endpoints: latency-svc-7t4zb [737.338674ms]
Jul 16 13:45:02.307: INFO: Created: latency-svc-4vl2t
Jul 16 13:45:02.316: INFO: Created: latency-svc-d5m47
Jul 16 13:45:02.331: INFO: Got endpoints: latency-svc-k687l [743.256999ms]
Jul 16 13:45:02.368: INFO: Got endpoints: latency-svc-8zsr6 [765.726382ms]
Jul 16 13:45:02.404: INFO: Got endpoints: latency-svc-8dgvc [746.316721ms]
Jul 16 13:45:02.416: INFO: Created: latency-svc-wj8ns
Jul 16 13:45:02.430: INFO: Created: latency-svc-hq8qb
Jul 16 13:45:02.455: INFO: Got endpoints: latency-svc-48jw9 [702.755464ms]
Jul 16 13:45:02.456: INFO: Created: latency-svc-ps74q
Jul 16 13:45:02.506: INFO: Created: latency-svc-jxr7w
Jul 16 13:45:02.530: INFO: Got endpoints: latency-svc-j4vdg [770.69508ms]
Jul 16 13:45:02.586: INFO: Created: latency-svc-nmqzr
Jul 16 13:45:02.591: INFO: Got endpoints: latency-svc-wdxnx [784.320327ms]
Jul 16 13:45:02.605: INFO: Got endpoints: latency-svc-fsgql [730.176424ms]
Jul 16 13:45:02.663: INFO: Got endpoints: latency-svc-n4xfv [752.512025ms]
Jul 16 13:45:02.701: INFO: Created: latency-svc-kl6n4
Jul 16 13:45:02.702: INFO: Created: latency-svc-wjtch
Jul 16 13:45:02.721: INFO: Got endpoints: latency-svc-qd872 [765.057921ms]
Jul 16 13:45:02.760: INFO: Got endpoints: latency-svc-m7mmz [708.076527ms]
Jul 16 13:45:02.812: INFO: Got endpoints: latency-svc-bgrkh [755.729389ms]
Jul 16 13:45:02.867: INFO: Got endpoints: latency-svc-6b76j [764.335816ms]
Jul 16 13:45:02.910: INFO: Got endpoints: latency-svc-74c59 [740.245658ms]
Jul 16 13:45:02.963: INFO: Got endpoints: latency-svc-4vl2t [739.201351ms]
Jul 16 13:45:03.017: INFO: Got endpoints: latency-svc-d5m47 [758.062989ms]
Jul 16 13:45:03.060: INFO: Got endpoints: latency-svc-wj8ns [728.861523ms]
Jul 16 13:45:03.100: INFO: Got endpoints: latency-svc-hq8qb [732.593397ms]
Jul 16 13:45:03.153: INFO: Got endpoints: latency-svc-ps74q [749.014556ms]
Jul 16 13:45:03.214: INFO: Got endpoints: latency-svc-jxr7w [758.256804ms]
Jul 16 13:45:03.262: INFO: Got endpoints: latency-svc-nmqzr [731.286608ms]
Jul 16 13:45:03.329: INFO: Got endpoints: latency-svc-kl6n4 [737.071725ms]
Jul 16 13:45:03.359: INFO: Got endpoints: latency-svc-wjtch [754.074084ms]
Jul 16 13:45:03.360: INFO: Latencies: [59.097122ms 67.5712ms 120.402876ms 136.719804ms 140.867492ms 142.565625ms 203.640884ms 205.240879ms 213.433936ms 221.207629ms 275.152712ms 275.715832ms 287.698254ms 294.932181ms 295.263478ms 303.442352ms 304.997631ms 305.586851ms 307.099837ms 309.793546ms 312.203221ms 314.429914ms 318.337317ms 319.730235ms 321.366583ms 325.569416ms 334.252239ms 335.00478ms 337.629886ms 339.643327ms 343.775151ms 345.328575ms 345.724651ms 347.123059ms 347.589049ms 350.085832ms 352.686543ms 353.488784ms 355.386654ms 356.135876ms 356.596395ms 358.010525ms 363.087137ms 363.852592ms 365.825424ms 369.394376ms 369.619715ms 370.070269ms 375.49076ms 377.58526ms 378.337598ms 379.49526ms 379.627043ms 383.9115ms 384.256517ms 387.154341ms 389.952648ms 390.201827ms 391.92431ms 395.630173ms 395.652254ms 401.448546ms 404.283244ms 409.422913ms 417.796643ms 419.821854ms 421.980097ms 428.876003ms 434.85107ms 436.373868ms 437.47827ms 438.366066ms 439.733155ms 442.561104ms 445.698191ms 485.756805ms 510.360478ms 535.929339ms 571.411724ms 571.846688ms 571.923274ms 580.889137ms 602.440837ms 607.001899ms 609.186539ms 609.307783ms 610.332759ms 610.976259ms 618.294049ms 620.178821ms 625.352793ms 632.141727ms 636.4914ms 642.979987ms 645.593376ms 648.513502ms 690.426728ms 702.755464ms 708.076527ms 708.947758ms 709.02265ms 712.508962ms 713.293331ms 718.542895ms 720.959298ms 727.745257ms 727.900643ms 727.97022ms 728.551173ms 728.861523ms 728.898015ms 730.176424ms 730.724636ms 731.286608ms 732.209049ms 732.593397ms 736.372755ms 736.939268ms 737.071725ms 737.338674ms 738.141851ms 738.878564ms 738.940038ms 739.007451ms 739.201351ms 739.853814ms 740.158632ms 740.245658ms 741.136047ms 741.168037ms 742.681016ms 743.042651ms 743.256999ms 745.461891ms 745.889531ms 746.316721ms 746.374622ms 747.233845ms 747.261243ms 747.791727ms 748.58116ms 748.602606ms 748.762187ms 748.903139ms 749.014556ms 749.920956ms 751.786307ms 752.512025ms 754.074084ms 754.225406ms 754.900743ms 755.729389ms 755.949455ms 756.174045ms 756.274192ms 756.445083ms 756.827472ms 756.882284ms 756.958687ms 756.975842ms 758.062989ms 758.256804ms 758.742379ms 759.44662ms 761.021588ms 762.377122ms 763.08353ms 763.524113ms 764.335816ms 765.057921ms 765.726382ms 766.329923ms 767.367621ms 768.774402ms 769.463317ms 770.076991ms 770.69508ms 773.272731ms 774.116095ms 784.320327ms 789.744606ms 792.309877ms 797.426007ms 801.112488ms 857.281975ms 946.367405ms 1.131623526s 1.163611573s 1.500667488s 1.508710926s 1.51380951s 1.516094524s 1.519920347s 1.520161621s 1.52750544s 1.542484759s 1.553745164s 1.566473016s 1.56749952s 1.575787858s]
Jul 16 13:45:03.360: INFO: 50 %ile: 709.02265ms
Jul 16 13:45:03.360: INFO: 90 %ile: 789.744606ms
Jul 16 13:45:03.360: INFO: 99 %ile: 1.56749952s
Jul 16 13:45:03.360: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:45:03.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6411" for this suite.
Jul 16 13:45:23.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:45:23.826: INFO: namespace svc-latency-6411 deletion completed in 20.458551834s

• [SLOW TEST:32.650 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:45:23.827: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jul 16 13:45:23.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 --namespace=kubectl-2817 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 16 13:45:27.666: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 16 13:45:27.666: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:45:29.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2817" for this suite.
Jul 16 13:45:37.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:45:38.397: INFO: namespace kubectl-2817 deletion completed in 8.702507855s

• [SLOW TEST:14.570 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:45:38.397: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:45:42.629: INFO: Waiting up to 5m0s for pod "client-envvars-a7d2c317-ccb3-420c-bbe5-213bd53fab38" in namespace "pods-3161" to be "success or failure"
Jul 16 13:45:42.643: INFO: Pod "client-envvars-a7d2c317-ccb3-420c-bbe5-213bd53fab38": Phase="Pending", Reason="", readiness=false. Elapsed: 13.726383ms
Jul 16 13:45:44.656: INFO: Pod "client-envvars-a7d2c317-ccb3-420c-bbe5-213bd53fab38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027457366s
Jul 16 13:45:46.744: INFO: Pod "client-envvars-a7d2c317-ccb3-420c-bbe5-213bd53fab38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.115543362s
STEP: Saw pod success
Jul 16 13:45:46.744: INFO: Pod "client-envvars-a7d2c317-ccb3-420c-bbe5-213bd53fab38" satisfied condition "success or failure"
Jul 16 13:45:46.751: INFO: Trying to get logs from node conformance-worker-54b54f4f98-m9svg pod client-envvars-a7d2c317-ccb3-420c-bbe5-213bd53fab38 container env3cont: <nil>
STEP: delete the pod
Jul 16 13:45:46.927: INFO: Waiting for pod client-envvars-a7d2c317-ccb3-420c-bbe5-213bd53fab38 to disappear
Jul 16 13:45:46.934: INFO: Pod client-envvars-a7d2c317-ccb3-420c-bbe5-213bd53fab38 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:45:46.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3161" for this suite.
Jul 16 13:46:36.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:46:37.481: INFO: namespace pods-3161 deletion completed in 50.536611511s

• [SLOW TEST:59.084 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:46:37.482: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0716 13:46:39.394572      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 16 13:46:39.395: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:46:39.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4988" for this suite.
Jul 16 13:46:45.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:46:45.748: INFO: namespace gc-4988 deletion completed in 6.345761812s

• [SLOW TEST:8.266 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:46:45.749: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:46:45.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-337e9fb1-7f32-4743-94a6-81312a08d0f9" in namespace "downward-api-8395" to be "success or failure"
Jul 16 13:46:45.905: INFO: Pod "downwardapi-volume-337e9fb1-7f32-4743-94a6-81312a08d0f9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.033057ms
Jul 16 13:46:47.914: INFO: Pod "downwardapi-volume-337e9fb1-7f32-4743-94a6-81312a08d0f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020651082s
Jul 16 13:46:49.921: INFO: Pod "downwardapi-volume-337e9fb1-7f32-4743-94a6-81312a08d0f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027858152s
STEP: Saw pod success
Jul 16 13:46:49.922: INFO: Pod "downwardapi-volume-337e9fb1-7f32-4743-94a6-81312a08d0f9" satisfied condition "success or failure"
Jul 16 13:46:49.927: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-337e9fb1-7f32-4743-94a6-81312a08d0f9 container client-container: <nil>
STEP: delete the pod
Jul 16 13:46:49.998: INFO: Waiting for pod downwardapi-volume-337e9fb1-7f32-4743-94a6-81312a08d0f9 to disappear
Jul 16 13:46:50.004: INFO: Pod downwardapi-volume-337e9fb1-7f32-4743-94a6-81312a08d0f9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:46:50.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8395" for this suite.
Jul 16 13:46:56.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:46:56.349: INFO: namespace downward-api-8395 deletion completed in 6.337035175s

• [SLOW TEST:10.600 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:46:56.352: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:46:56.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-248d770b-da03-4896-b221-e0b4bfbb99bc" in namespace "projected-6537" to be "success or failure"
Jul 16 13:46:56.511: INFO: Pod "downwardapi-volume-248d770b-da03-4896-b221-e0b4bfbb99bc": Phase="Pending", Reason="", readiness=false. Elapsed: 27.067485ms
Jul 16 13:46:58.521: INFO: Pod "downwardapi-volume-248d770b-da03-4896-b221-e0b4bfbb99bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037249047s
Jul 16 13:47:00.548: INFO: Pod "downwardapi-volume-248d770b-da03-4896-b221-e0b4bfbb99bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064462911s
STEP: Saw pod success
Jul 16 13:47:00.548: INFO: Pod "downwardapi-volume-248d770b-da03-4896-b221-e0b4bfbb99bc" satisfied condition "success or failure"
Jul 16 13:47:00.556: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-248d770b-da03-4896-b221-e0b4bfbb99bc container client-container: <nil>
STEP: delete the pod
Jul 16 13:47:00.602: INFO: Waiting for pod downwardapi-volume-248d770b-da03-4896-b221-e0b4bfbb99bc to disappear
Jul 16 13:47:00.607: INFO: Pod downwardapi-volume-248d770b-da03-4896-b221-e0b4bfbb99bc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:47:00.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6537" for this suite.
Jul 16 13:47:06.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:47:07.039: INFO: namespace projected-6537 deletion completed in 6.422212539s

• [SLOW TEST:10.687 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:47:07.042: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jul 16 13:47:07.219: INFO: Waiting up to 5m0s for pod "var-expansion-dd5f1005-f8b7-4bd7-90a1-db15e925fcb8" in namespace "var-expansion-8130" to be "success or failure"
Jul 16 13:47:07.443: INFO: Pod "var-expansion-dd5f1005-f8b7-4bd7-90a1-db15e925fcb8": Phase="Pending", Reason="", readiness=false. Elapsed: 223.108252ms
Jul 16 13:47:09.454: INFO: Pod "var-expansion-dd5f1005-f8b7-4bd7-90a1-db15e925fcb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.23428736s
Jul 16 13:47:11.461: INFO: Pod "var-expansion-dd5f1005-f8b7-4bd7-90a1-db15e925fcb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.240840085s
STEP: Saw pod success
Jul 16 13:47:11.461: INFO: Pod "var-expansion-dd5f1005-f8b7-4bd7-90a1-db15e925fcb8" satisfied condition "success or failure"
Jul 16 13:47:11.467: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod var-expansion-dd5f1005-f8b7-4bd7-90a1-db15e925fcb8 container dapi-container: <nil>
STEP: delete the pod
Jul 16 13:47:11.521: INFO: Waiting for pod var-expansion-dd5f1005-f8b7-4bd7-90a1-db15e925fcb8 to disappear
Jul 16 13:47:11.527: INFO: Pod var-expansion-dd5f1005-f8b7-4bd7-90a1-db15e925fcb8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:47:11.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8130" for this suite.
Jul 16 13:47:17.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:47:17.911: INFO: namespace var-expansion-8130 deletion completed in 6.376981174s

• [SLOW TEST:10.870 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:47:17.916: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 16 13:47:22.565: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2482 pod-service-account-28bc8365-0580-4cea-ac50-a5321049078f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 16 13:47:23.394: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2482 pod-service-account-28bc8365-0580-4cea-ac50-a5321049078f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 16 13:47:24.130: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2482 pod-service-account-28bc8365-0580-4cea-ac50-a5321049078f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:47:24.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2482" for this suite.
Jul 16 13:47:30.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:47:31.241: INFO: namespace svcaccounts-2482 deletion completed in 6.318440344s

• [SLOW TEST:13.325 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:47:31.256: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 16 13:47:31.339: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:47:49.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7250" for this suite.
Jul 16 13:47:55.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:47:55.956: INFO: namespace pods-7250 deletion completed in 6.440002845s

• [SLOW TEST:24.701 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:47:55.959: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 16 13:48:02.300: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0716 13:48:02.299842      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 16 13:48:02.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1312" for this suite.
Jul 16 13:48:10.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:48:10.674: INFO: namespace gc-1312 deletion completed in 8.367627169s

• [SLOW TEST:14.716 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:48:10.680: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:48:11.058: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c0365e6-eefd-4d63-ac6a-a71ff82f4d85" in namespace "downward-api-7025" to be "success or failure"
Jul 16 13:48:11.107: INFO: Pod "downwardapi-volume-7c0365e6-eefd-4d63-ac6a-a71ff82f4d85": Phase="Pending", Reason="", readiness=false. Elapsed: 49.505286ms
Jul 16 13:48:13.118: INFO: Pod "downwardapi-volume-7c0365e6-eefd-4d63-ac6a-a71ff82f4d85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059970617s
Jul 16 13:48:15.138: INFO: Pod "downwardapi-volume-7c0365e6-eefd-4d63-ac6a-a71ff82f4d85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080232415s
STEP: Saw pod success
Jul 16 13:48:15.138: INFO: Pod "downwardapi-volume-7c0365e6-eefd-4d63-ac6a-a71ff82f4d85" satisfied condition "success or failure"
Jul 16 13:48:15.145: INFO: Trying to get logs from node conformance-worker-54b54f4f98-m9svg pod downwardapi-volume-7c0365e6-eefd-4d63-ac6a-a71ff82f4d85 container client-container: <nil>
STEP: delete the pod
Jul 16 13:48:15.244: INFO: Waiting for pod downwardapi-volume-7c0365e6-eefd-4d63-ac6a-a71ff82f4d85 to disappear
Jul 16 13:48:15.257: INFO: Pod downwardapi-volume-7c0365e6-eefd-4d63-ac6a-a71ff82f4d85 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:48:15.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7025" for this suite.
Jul 16 13:48:21.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:48:21.610: INFO: namespace downward-api-7025 deletion completed in 6.325401864s

• [SLOW TEST:10.930 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:48:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:48:21.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51ad0cc1-39d4-4472-89d8-85fccffcab75" in namespace "projected-791" to be "success or failure"
Jul 16 13:48:21.753: INFO: Pod "downwardapi-volume-51ad0cc1-39d4-4472-89d8-85fccffcab75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.909848ms
Jul 16 13:48:23.762: INFO: Pod "downwardapi-volume-51ad0cc1-39d4-4472-89d8-85fccffcab75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013927328s
Jul 16 13:48:25.772: INFO: Pod "downwardapi-volume-51ad0cc1-39d4-4472-89d8-85fccffcab75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024460393s
STEP: Saw pod success
Jul 16 13:48:25.773: INFO: Pod "downwardapi-volume-51ad0cc1-39d4-4472-89d8-85fccffcab75" satisfied condition "success or failure"
Jul 16 13:48:25.779: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-51ad0cc1-39d4-4472-89d8-85fccffcab75 container client-container: <nil>
STEP: delete the pod
Jul 16 13:48:25.974: INFO: Waiting for pod downwardapi-volume-51ad0cc1-39d4-4472-89d8-85fccffcab75 to disappear
Jul 16 13:48:25.980: INFO: Pod downwardapi-volume-51ad0cc1-39d4-4472-89d8-85fccffcab75 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:48:25.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-791" for this suite.
Jul 16 13:48:32.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:48:32.368: INFO: namespace projected-791 deletion completed in 6.328399624s

• [SLOW TEST:10.756 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:48:32.377: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 13:48:32.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1030'
Jul 16 13:48:32.637: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 13:48:32.637: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul 16 13:48:32.668: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul 16 13:48:32.722: INFO: scanned /root for discovery docs: <nil>
Jul 16 13:48:32.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1030'
Jul 16 13:48:48.788: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 16 13:48:48.788: INFO: stdout: "Created e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71\nScaling up e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul 16 13:48:48.788: INFO: stdout: "Created e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71\nScaling up e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 16 13:48:48.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1030'
Jul 16 13:48:48.971: INFO: stderr: ""
Jul 16 13:48:48.971: INFO: stdout: "e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71-dkd4m "
Jul 16 13:48:48.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71-dkd4m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1030'
Jul 16 13:48:49.099: INFO: stderr: ""
Jul 16 13:48:49.099: INFO: stdout: "true"
Jul 16 13:48:49.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71-dkd4m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1030'
Jul 16 13:48:49.251: INFO: stderr: ""
Jul 16 13:48:49.251: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul 16 13:48:49.251: INFO: e2e-test-nginx-rc-dda0bb0ff6b51abfb25b56d4f9673a71-dkd4m is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jul 16 13:48:49.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete rc e2e-test-nginx-rc --namespace=kubectl-1030'
Jul 16 13:48:49.399: INFO: stderr: ""
Jul 16 13:48:49.399: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:48:49.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1030" for this suite.
Jul 16 13:48:57.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:48:58.568: INFO: namespace kubectl-1030 deletion completed in 9.148398501s

• [SLOW TEST:26.192 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:48:58.569: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4526.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4526.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4526.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4526.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4526.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4526.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 16 13:49:33.818: INFO: DNS probes using dns-4526/dns-test-4e250362-213b-40d8-a56d-4af4795186fe succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:49:33.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4526" for this suite.
Jul 16 13:49:41.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:49:42.222: INFO: namespace dns-4526 deletion completed in 8.323945768s

• [SLOW TEST:43.653 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:49:42.227: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-24035c27-844d-46f4-8630-f9d5d0422cc6
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:49:42.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5925" for this suite.
Jul 16 13:49:48.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:49:48.620: INFO: namespace configmap-5925 deletion completed in 6.30162152s

• [SLOW TEST:6.394 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:49:48.623: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 16 13:49:56.979: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 16 13:49:56.985: INFO: Pod pod-with-poststart-http-hook still exists
Jul 16 13:49:58.986: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 16 13:49:58.993: INFO: Pod pod-with-poststart-http-hook still exists
Jul 16 13:50:00.986: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 16 13:50:01.023: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:50:01.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5761" for this suite.
Jul 16 13:50:25.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:50:25.419: INFO: namespace container-lifecycle-hook-5761 deletion completed in 24.386023099s

• [SLOW TEST:36.796 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:50:25.423: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jul 16 13:50:25.679: INFO: Waiting up to 5m0s for pod "client-containers-1904ae7a-98d4-46d3-9ade-4a6b1bd03f2b" in namespace "containers-6701" to be "success or failure"
Jul 16 13:50:25.685: INFO: Pod "client-containers-1904ae7a-98d4-46d3-9ade-4a6b1bd03f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.254757ms
Jul 16 13:50:27.694: INFO: Pod "client-containers-1904ae7a-98d4-46d3-9ade-4a6b1bd03f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015025896s
Jul 16 13:50:29.702: INFO: Pod "client-containers-1904ae7a-98d4-46d3-9ade-4a6b1bd03f2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023612309s
STEP: Saw pod success
Jul 16 13:50:29.703: INFO: Pod "client-containers-1904ae7a-98d4-46d3-9ade-4a6b1bd03f2b" satisfied condition "success or failure"
Jul 16 13:50:29.709: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod client-containers-1904ae7a-98d4-46d3-9ade-4a6b1bd03f2b container test-container: <nil>
STEP: delete the pod
Jul 16 13:50:29.779: INFO: Waiting for pod client-containers-1904ae7a-98d4-46d3-9ade-4a6b1bd03f2b to disappear
Jul 16 13:50:29.786: INFO: Pod client-containers-1904ae7a-98d4-46d3-9ade-4a6b1bd03f2b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:50:29.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6701" for this suite.
Jul 16 13:50:35.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:50:36.118: INFO: namespace containers-6701 deletion completed in 6.319088982s

• [SLOW TEST:10.695 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:50:36.119: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 16 13:50:36.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-8619'
Jul 16 13:50:36.891: INFO: stderr: ""
Jul 16 13:50:36.891: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 13:50:36.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8619'
Jul 16 13:50:37.104: INFO: stderr: ""
Jul 16 13:50:37.104: INFO: stdout: "update-demo-nautilus-fmt5s update-demo-nautilus-gp25m "
Jul 16 13:50:37.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-fmt5s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8619'
Jul 16 13:50:37.253: INFO: stderr: ""
Jul 16 13:50:37.253: INFO: stdout: ""
Jul 16 13:50:37.253: INFO: update-demo-nautilus-fmt5s is created but not running
Jul 16 13:50:42.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8619'
Jul 16 13:50:42.367: INFO: stderr: ""
Jul 16 13:50:42.368: INFO: stdout: "update-demo-nautilus-fmt5s update-demo-nautilus-gp25m "
Jul 16 13:50:42.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-fmt5s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8619'
Jul 16 13:50:42.478: INFO: stderr: ""
Jul 16 13:50:42.478: INFO: stdout: "true"
Jul 16 13:50:42.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-fmt5s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8619'
Jul 16 13:50:42.601: INFO: stderr: ""
Jul 16 13:50:42.601: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:50:42.601: INFO: validating pod update-demo-nautilus-fmt5s
Jul 16 13:50:42.717: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:50:42.717: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:50:42.717: INFO: update-demo-nautilus-fmt5s is verified up and running
Jul 16 13:50:42.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-gp25m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8619'
Jul 16 13:50:42.843: INFO: stderr: ""
Jul 16 13:50:42.843: INFO: stdout: "true"
Jul 16 13:50:42.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-gp25m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8619'
Jul 16 13:50:42.988: INFO: stderr: ""
Jul 16 13:50:42.988: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 13:50:42.988: INFO: validating pod update-demo-nautilus-gp25m
Jul 16 13:50:43.091: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 13:50:43.091: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 13:50:43.092: INFO: update-demo-nautilus-gp25m is verified up and running
STEP: using delete to clean up resources
Jul 16 13:50:43.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-8619'
Jul 16 13:50:43.232: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 13:50:43.232: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 16 13:50:43.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8619'
Jul 16 13:50:43.353: INFO: stderr: "No resources found.\n"
Jul 16 13:50:43.353: INFO: stdout: ""
Jul 16 13:50:43.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -l name=update-demo --namespace=kubectl-8619 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 13:50:43.479: INFO: stderr: ""
Jul 16 13:50:43.479: INFO: stdout: "update-demo-nautilus-fmt5s\nupdate-demo-nautilus-gp25m\n"
Jul 16 13:50:43.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8619'
Jul 16 13:50:44.114: INFO: stderr: "No resources found.\n"
Jul 16 13:50:44.114: INFO: stdout: ""
Jul 16 13:50:44.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -l name=update-demo --namespace=kubectl-8619 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 13:50:44.259: INFO: stderr: ""
Jul 16 13:50:44.259: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:50:44.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8619" for this suite.
Jul 16 13:51:08.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:51:08.655: INFO: namespace kubectl-8619 deletion completed in 24.385498729s

• [SLOW TEST:32.536 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:51:08.660: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 16 13:51:08.810: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 16 13:51:08.833: INFO: Waiting for terminating namespaces to be deleted...
Jul 16 13:51:08.839: INFO: 
Logging pods the kubelet thinks is on node conformance-worker-54b54f4f98-f9trz before test
Jul 16 13:51:13.030: INFO: node-exporter-dk6wg from kube-system started at 2019-07-16 12:58:27 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.031: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 13:51:13.031: INFO: restic-dqv4x from velero started at 2019-07-16 12:58:27 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.031: INFO: 	Container velero ready: true, restart count 0
Jul 16 13:51:13.031: INFO: canal-4r5mb from kube-system started at 2019-07-16 12:58:27 +0000 UTC (3 container statuses recorded)
Jul 16 13:51:13.032: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 13:51:13.032: INFO: 	Container install-cni ready: true, restart count 0
Jul 16 13:51:13.032: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 16 13:51:13.032: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-16 13:04:36 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.032: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 16 13:51:13.032: INFO: kube-proxy-9l5xc from kube-system started at 2019-07-16 12:58:27 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.032: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 13:51:13.032: INFO: sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-mgtcd from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 13:51:13.033: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 13:51:13.033: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 16 13:51:13.033: INFO: 
Logging pods the kubelet thinks is on node conformance-worker-54b54f4f98-m9svg before test
Jul 16 13:51:13.088: INFO: restic-vssb6 from velero started at 2019-07-16 12:58:37 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.088: INFO: 	Container velero ready: true, restart count 0
Jul 16 13:51:13.088: INFO: canal-mtsjt from kube-system started at 2019-07-16 12:58:37 +0000 UTC (3 container statuses recorded)
Jul 16 13:51:13.088: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 13:51:13.088: INFO: 	Container install-cni ready: true, restart count 0
Jul 16 13:51:13.088: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 16 13:51:13.088: INFO: sonobuoy-e2e-job-352202216adf4593 from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 13:51:13.088: INFO: 	Container e2e ready: true, restart count 0
Jul 16 13:51:13.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 13:51:13.088: INFO: kube-proxy-nwtx4 from kube-system started at 2019-07-16 12:58:37 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.088: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 13:51:13.088: INFO: node-exporter-qrl7j from kube-system started at 2019-07-16 12:58:37 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.088: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 13:51:13.088: INFO: sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-t84ms from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 13:51:13.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 13:51:13.088: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 16 13:51:13.088: INFO: 
Logging pods the kubelet thinks is on node conformance-worker-54b54f4f98-nqplr before test
Jul 16 13:51:13.143: INFO: coredns-6f56645db5-86hnn from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container coredns ready: true, restart count 0
Jul 16 13:51:13.143: INFO: canal-qhfws from kube-system started at 2019-07-16 12:58:10 +0000 UTC (3 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 13:51:13.143: INFO: 	Container install-cni ready: true, restart count 0
Jul 16 13:51:13.143: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 16 13:51:13.143: INFO: velero-79764c8c47-l746r from velero started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container velero ready: true, restart count 0
Jul 16 13:51:13.143: INFO: restic-pbp8t from velero started at 2019-07-16 12:58:09 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container velero ready: true, restart count 0
Jul 16 13:51:13.143: INFO: kube-proxy-vwpcz from kube-system started at 2019-07-16 12:58:10 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 13:51:13.143: INFO: node-exporter-p6bc4 from kube-system started at 2019-07-16 12:58:10 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 13:51:13.143: INFO: coredns-6f56645db5-hxmmb from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container coredns ready: true, restart count 0
Jul 16 13:51:13.143: INFO: cluster-autoscaler-7b894d5874-6bxbm from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Jul 16 13:51:13.143: INFO: webterminal-64d57d745b-z8m6c from webterminal started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container webterminal ready: true, restart count 0
Jul 16 13:51:13.143: INFO: openvpn-client-95bcd78-6vrxz from kube-system started at 2019-07-16 12:59:02 +0000 UTC (2 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container dnat-controller ready: true, restart count 0
Jul 16 13:51:13.143: INFO: 	Container openvpn-client ready: true, restart count 0
Jul 16 13:51:13.143: INFO: tiller-deploy-7c7d69dd75-kxhk8 from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container tiller ready: true, restart count 0
Jul 16 13:51:13.143: INFO: sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-v2cx8 from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 13:51:13.143: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 13:51:13.143: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6c0a8605-d397-456d-994f-2c96990cb97e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-6c0a8605-d397-456d-994f-2c96990cb97e off the node conformance-worker-54b54f4f98-f9trz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6c0a8605-d397-456d-994f-2c96990cb97e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:51:21.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1795" for this suite.
Jul 16 13:51:31.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:51:32.154: INFO: namespace sched-pred-1795 deletion completed in 10.400768551s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:23.495 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:51:32.155: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:51:32.215: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:51:36.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4550" for this suite.
Jul 16 13:52:26.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:52:27.037: INFO: namespace pods-4550 deletion completed in 50.316082519s

• [SLOW TEST:54.883 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:52:27.040: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-26ff75a9-faaa-4f71-99b4-c4c361ea38ab
STEP: Creating a pod to test consume secrets
Jul 16 13:52:27.232: INFO: Waiting up to 5m0s for pod "pod-secrets-9e971eef-26ea-41c5-bbaf-5e0722612356" in namespace "secrets-1137" to be "success or failure"
Jul 16 13:52:27.238: INFO: Pod "pod-secrets-9e971eef-26ea-41c5-bbaf-5e0722612356": Phase="Pending", Reason="", readiness=false. Elapsed: 5.542335ms
Jul 16 13:52:29.265: INFO: Pod "pod-secrets-9e971eef-26ea-41c5-bbaf-5e0722612356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032605712s
Jul 16 13:52:31.273: INFO: Pod "pod-secrets-9e971eef-26ea-41c5-bbaf-5e0722612356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040215582s
STEP: Saw pod success
Jul 16 13:52:31.273: INFO: Pod "pod-secrets-9e971eef-26ea-41c5-bbaf-5e0722612356" satisfied condition "success or failure"
Jul 16 13:52:31.278: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-secrets-9e971eef-26ea-41c5-bbaf-5e0722612356 container secret-env-test: <nil>
STEP: delete the pod
Jul 16 13:52:31.386: INFO: Waiting for pod pod-secrets-9e971eef-26ea-41c5-bbaf-5e0722612356 to disappear
Jul 16 13:52:31.391: INFO: Pod pod-secrets-9e971eef-26ea-41c5-bbaf-5e0722612356 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:52:31.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1137" for this suite.
Jul 16 13:52:37.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:52:37.806: INFO: namespace secrets-1137 deletion completed in 6.395929536s

• [SLOW TEST:10.766 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:52:37.812: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 13:52:37.973: INFO: Waiting up to 5m0s for pod "downwardapi-volume-560524ca-2ce0-48ea-8307-be515689f13a" in namespace "projected-7087" to be "success or failure"
Jul 16 13:52:37.989: INFO: Pod "downwardapi-volume-560524ca-2ce0-48ea-8307-be515689f13a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.87827ms
Jul 16 13:52:39.997: INFO: Pod "downwardapi-volume-560524ca-2ce0-48ea-8307-be515689f13a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023723782s
Jul 16 13:52:42.009: INFO: Pod "downwardapi-volume-560524ca-2ce0-48ea-8307-be515689f13a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035276803s
STEP: Saw pod success
Jul 16 13:52:42.009: INFO: Pod "downwardapi-volume-560524ca-2ce0-48ea-8307-be515689f13a" satisfied condition "success or failure"
Jul 16 13:52:42.028: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-560524ca-2ce0-48ea-8307-be515689f13a container client-container: <nil>
STEP: delete the pod
Jul 16 13:52:42.211: INFO: Waiting for pod downwardapi-volume-560524ca-2ce0-48ea-8307-be515689f13a to disappear
Jul 16 13:52:42.218: INFO: Pod downwardapi-volume-560524ca-2ce0-48ea-8307-be515689f13a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:52:42.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7087" for this suite.
Jul 16 13:52:48.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:52:48.607: INFO: namespace projected-7087 deletion completed in 6.381307064s

• [SLOW TEST:10.795 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:52:48.609: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-f542bb4c-6e92-45b4-8cb6-071e9237093a
STEP: Creating a pod to test consume configMaps
Jul 16 13:52:48.717: INFO: Waiting up to 5m0s for pod "pod-configmaps-6d05dc8d-2e09-481e-bfa7-81d29a360c18" in namespace "configmap-2244" to be "success or failure"
Jul 16 13:52:48.724: INFO: Pod "pod-configmaps-6d05dc8d-2e09-481e-bfa7-81d29a360c18": Phase="Pending", Reason="", readiness=false. Elapsed: 7.364839ms
Jul 16 13:52:50.734: INFO: Pod "pod-configmaps-6d05dc8d-2e09-481e-bfa7-81d29a360c18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017283233s
Jul 16 13:52:52.750: INFO: Pod "pod-configmaps-6d05dc8d-2e09-481e-bfa7-81d29a360c18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033011267s
STEP: Saw pod success
Jul 16 13:52:52.750: INFO: Pod "pod-configmaps-6d05dc8d-2e09-481e-bfa7-81d29a360c18" satisfied condition "success or failure"
Jul 16 13:52:52.757: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-6d05dc8d-2e09-481e-bfa7-81d29a360c18 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 13:52:52.907: INFO: Waiting for pod pod-configmaps-6d05dc8d-2e09-481e-bfa7-81d29a360c18 to disappear
Jul 16 13:52:52.914: INFO: Pod pod-configmaps-6d05dc8d-2e09-481e-bfa7-81d29a360c18 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:52:52.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2244" for this suite.
Jul 16 13:52:58.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:52:59.356: INFO: namespace configmap-2244 deletion completed in 6.43415759s

• [SLOW TEST:10.747 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:52:59.359: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 16 13:53:04.040: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f712c49c-7173-433e-88ee-e2ecace3e45c"
Jul 16 13:53:04.040: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f712c49c-7173-433e-88ee-e2ecace3e45c" in namespace "pods-540" to be "terminated due to deadline exceeded"
Jul 16 13:53:04.045: INFO: Pod "pod-update-activedeadlineseconds-f712c49c-7173-433e-88ee-e2ecace3e45c": Phase="Running", Reason="", readiness=true. Elapsed: 4.643917ms
Jul 16 13:53:06.058: INFO: Pod "pod-update-activedeadlineseconds-f712c49c-7173-433e-88ee-e2ecace3e45c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.01697031s
Jul 16 13:53:06.058: INFO: Pod "pod-update-activedeadlineseconds-f712c49c-7173-433e-88ee-e2ecace3e45c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:53:06.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-540" for this suite.
Jul 16 13:53:12.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:53:12.389: INFO: namespace pods-540 deletion completed in 6.319647805s

• [SLOW TEST:13.030 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:53:12.391: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:53:12.579: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 16 13:53:12.621: INFO: Number of nodes with available pods: 0
Jul 16 13:53:12.621: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:53:13.639: INFO: Number of nodes with available pods: 0
Jul 16 13:53:13.639: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:53:14.634: INFO: Number of nodes with available pods: 0
Jul 16 13:53:14.634: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:53:15.648: INFO: Number of nodes with available pods: 3
Jul 16 13:53:15.648: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 16 13:53:15.739: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:15.739: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:15.739: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:16.777: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:16.777: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:16.777: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:17.770: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:17.770: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:17.770: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:18.771: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:18.771: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:18.772: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:19.768: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:19.768: INFO: Pod daemon-set-8m9cp is not available
Jul 16 13:53:19.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:19.768: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:20.766: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:20.766: INFO: Pod daemon-set-8m9cp is not available
Jul 16 13:53:20.766: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:20.766: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:21.769: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:21.769: INFO: Pod daemon-set-8m9cp is not available
Jul 16 13:53:21.769: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:21.769: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:22.775: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:22.775: INFO: Pod daemon-set-8m9cp is not available
Jul 16 13:53:22.775: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:22.775: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:23.773: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:23.773: INFO: Pod daemon-set-8m9cp is not available
Jul 16 13:53:23.773: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:23.773: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:24.768: INFO: Wrong image for pod: daemon-set-8m9cp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:24.768: INFO: Pod daemon-set-8m9cp is not available
Jul 16 13:53:24.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:24.768: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:25.767: INFO: Pod daemon-set-4mwjs is not available
Jul 16 13:53:25.767: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:25.767: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:26.772: INFO: Pod daemon-set-4mwjs is not available
Jul 16 13:53:26.772: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:26.772: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:27.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:27.768: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:28.770: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:28.770: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:29.767: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:29.767: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:29.767: INFO: Pod daemon-set-zx9rc is not available
Jul 16 13:53:30.767: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:30.767: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:30.767: INFO: Pod daemon-set-zx9rc is not available
Jul 16 13:53:31.767: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:31.767: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:31.767: INFO: Pod daemon-set-zx9rc is not available
Jul 16 13:53:32.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:32.768: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:32.768: INFO: Pod daemon-set-zx9rc is not available
Jul 16 13:53:33.792: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:33.792: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:33.792: INFO: Pod daemon-set-zx9rc is not available
Jul 16 13:53:34.766: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:34.767: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:34.767: INFO: Pod daemon-set-zx9rc is not available
Jul 16 13:53:35.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:35.768: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:35.768: INFO: Pod daemon-set-zx9rc is not available
Jul 16 13:53:36.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:36.768: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:36.768: INFO: Pod daemon-set-zx9rc is not available
Jul 16 13:53:37.770: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:37.770: INFO: Wrong image for pod: daemon-set-zx9rc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:37.770: INFO: Pod daemon-set-zx9rc is not available
Jul 16 13:53:38.771: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:38.771: INFO: Pod daemon-set-t448c is not available
Jul 16 13:53:39.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:39.768: INFO: Pod daemon-set-t448c is not available
Jul 16 13:53:40.856: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:40.856: INFO: Pod daemon-set-t448c is not available
Jul 16 13:53:41.772: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:41.772: INFO: Pod daemon-set-t448c is not available
Jul 16 13:53:42.769: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:42.769: INFO: Pod daemon-set-t448c is not available
Jul 16 13:53:43.769: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:43.769: INFO: Pod daemon-set-t448c is not available
Jul 16 13:53:44.766: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:45.769: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:45.769: INFO: Pod daemon-set-9kbpl is not available
Jul 16 13:53:46.767: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:46.767: INFO: Pod daemon-set-9kbpl is not available
Jul 16 13:53:47.769: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:47.769: INFO: Pod daemon-set-9kbpl is not available
Jul 16 13:53:48.767: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:48.767: INFO: Pod daemon-set-9kbpl is not available
Jul 16 13:53:49.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:49.768: INFO: Pod daemon-set-9kbpl is not available
Jul 16 13:53:50.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:50.768: INFO: Pod daemon-set-9kbpl is not available
Jul 16 13:53:51.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:51.768: INFO: Pod daemon-set-9kbpl is not available
Jul 16 13:53:52.770: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:52.770: INFO: Pod daemon-set-9kbpl is not available
Jul 16 13:53:53.768: INFO: Wrong image for pod: daemon-set-9kbpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 16 13:53:53.768: INFO: Pod daemon-set-9kbpl is not available
Jul 16 13:53:54.766: INFO: Pod daemon-set-d74kc is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 16 13:53:54.783: INFO: Number of nodes with available pods: 2
Jul 16 13:53:54.783: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:53:55.801: INFO: Number of nodes with available pods: 2
Jul 16 13:53:55.801: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:53:56.801: INFO: Number of nodes with available pods: 2
Jul 16 13:53:56.801: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 13:53:57.802: INFO: Number of nodes with available pods: 3
Jul 16 13:53:57.803: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5580, will wait for the garbage collector to delete the pods
Jul 16 13:53:57.905: INFO: Deleting DaemonSet.extensions daemon-set took: 15.556895ms
Jul 16 13:53:58.506: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.390241ms
Jul 16 13:54:01.729: INFO: Number of nodes with available pods: 0
Jul 16 13:54:01.729: INFO: Number of running nodes: 0, number of available pods: 0
Jul 16 13:54:01.734: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5580/daemonsets","resourceVersion":"17479"},"items":null}

Jul 16 13:54:01.740: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5580/pods","resourceVersion":"17479"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:54:01.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5580" for this suite.
Jul 16 13:54:09.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:54:10.150: INFO: namespace daemonsets-5580 deletion completed in 8.366946735s

• [SLOW TEST:57.759 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:54:10.151: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 13:54:10.339: INFO: (0) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.897453ms)
Jul 16 13:54:10.388: INFO: (1) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 49.136501ms)
Jul 16 13:54:10.443: INFO: (2) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 54.722767ms)
Jul 16 13:54:10.541: INFO: (3) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 98.578448ms)
Jul 16 13:54:10.557: INFO: (4) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 16.059546ms)
Jul 16 13:54:10.568: INFO: (5) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.113295ms)
Jul 16 13:54:10.643: INFO: (6) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 75.144964ms)
Jul 16 13:54:10.659: INFO: (7) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 16.472974ms)
Jul 16 13:54:10.669: INFO: (8) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.980729ms)
Jul 16 13:54:10.719: INFO: (9) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 49.860939ms)
Jul 16 13:54:10.734: INFO: (10) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.513946ms)
Jul 16 13:54:10.763: INFO: (11) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 28.385946ms)
Jul 16 13:54:10.789: INFO: (12) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 26.711276ms)
Jul 16 13:54:10.803: INFO: (13) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.414608ms)
Jul 16 13:54:10.820: INFO: (14) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 16.789478ms)
Jul 16 13:54:10.830: INFO: (15) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.793645ms)
Jul 16 13:54:10.838: INFO: (16) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.64313ms)
Jul 16 13:54:10.852: INFO: (17) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.153039ms)
Jul 16 13:54:10.860: INFO: (18) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.563392ms)
Jul 16 13:54:10.869: INFO: (19) /api/v1/nodes/conformance-worker-54b54f4f98-f9trz:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.596399ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:54:10.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8412" for this suite.
Jul 16 13:54:16.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:54:17.188: INFO: namespace proxy-8412 deletion completed in 6.303320765s

• [SLOW TEST:7.037 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:54:17.189: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-a4930542-31d2-4118-b63d-87da066d5fb3 in namespace container-probe-4278
Jul 16 13:54:21.302: INFO: Started pod busybox-a4930542-31d2-4118-b63d-87da066d5fb3 in namespace container-probe-4278
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 13:54:21.342: INFO: Initial restart count of pod busybox-a4930542-31d2-4118-b63d-87da066d5fb3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:58:22.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4278" for this suite.
Jul 16 13:58:28.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:58:28.926: INFO: namespace container-probe-4278 deletion completed in 6.439724463s

• [SLOW TEST:251.737 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:58:28.929: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8200.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8200.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8200.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8200.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8200.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8200.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8200.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8200.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8200.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8200.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8200.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.98_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8200.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8200.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8200.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8200.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8200.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8200.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8200.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8200.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8200.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8200.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8200.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.98_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 16 13:58:35.358: INFO: Unable to read wheezy_udp@dns-test-service.dns-8200.svc.cluster.local from pod dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4: the server could not find the requested resource (get pods dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4)
Jul 16 13:58:35.400: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8200.svc.cluster.local from pod dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4: the server could not find the requested resource (get pods dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4)
Jul 16 13:58:35.410: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local from pod dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4: the server could not find the requested resource (get pods dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4)
Jul 16 13:58:35.419: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local from pod dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4: the server could not find the requested resource (get pods dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4)
Jul 16 13:58:35.865: INFO: Unable to read jessie_udp@dns-test-service.dns-8200.svc.cluster.local from pod dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4: the server could not find the requested resource (get pods dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4)
Jul 16 13:58:35.876: INFO: Unable to read jessie_tcp@dns-test-service.dns-8200.svc.cluster.local from pod dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4: the server could not find the requested resource (get pods dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4)
Jul 16 13:58:35.887: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local from pod dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4: the server could not find the requested resource (get pods dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4)
Jul 16 13:58:35.896: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local from pod dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4: the server could not find the requested resource (get pods dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4)
Jul 16 13:58:36.357: INFO: Lookups using dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4 failed for: [wheezy_udp@dns-test-service.dns-8200.svc.cluster.local wheezy_tcp@dns-test-service.dns-8200.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local jessie_udp@dns-test-service.dns-8200.svc.cluster.local jessie_tcp@dns-test-service.dns-8200.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8200.svc.cluster.local]

Jul 16 13:58:42.973: INFO: DNS probes using dns-8200/dns-test-4243da4e-7b92-4e8f-aad9-1e7410c508c4 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:58:43.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8200" for this suite.
Jul 16 13:58:51.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:58:51.861: INFO: namespace dns-8200 deletion completed in 8.34797015s

• [SLOW TEST:22.933 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:58:51.863: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-cb6d7e45-c9f1-4c4b-95c4-5ad037d627e6
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:58:56.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2278" for this suite.
Jul 16 13:59:20.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:59:20.588: INFO: namespace configmap-2278 deletion completed in 24.317517584s

• [SLOW TEST:28.725 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:59:20.588: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jul 16 13:59:20.670: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-981533691 proxy --unix-socket=/tmp/kubectl-proxy-unix581583154/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 13:59:20.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6463" for this suite.
Jul 16 13:59:26.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 13:59:27.122: INFO: namespace kubectl-6463 deletion completed in 6.336075003s

• [SLOW TEST:6.534 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 13:59:27.123: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5603, will wait for the garbage collector to delete the pods
Jul 16 13:59:31.326: INFO: Deleting Job.batch foo took: 13.339882ms
Jul 16 13:59:31.826: INFO: Terminating Job.batch foo pods took: 500.20437ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:00:15.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5603" for this suite.
Jul 16 14:00:25.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:00:25.480: INFO: namespace job-5603 deletion completed in 10.424951839s

• [SLOW TEST:58.357 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:00:25.482: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 16 14:00:36.553: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:36.553: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:37.250: INFO: Exec stderr: ""
Jul 16 14:00:37.250: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:37.251: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:37.913: INFO: Exec stderr: ""
Jul 16 14:00:37.913: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:37.913: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:38.548: INFO: Exec stderr: ""
Jul 16 14:00:38.550: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:38.550: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:39.236: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 16 14:00:39.236: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:39.236: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:39.885: INFO: Exec stderr: ""
Jul 16 14:00:39.885: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:39.885: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:40.638: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 16 14:00:40.638: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:40.638: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:41.407: INFO: Exec stderr: ""
Jul 16 14:00:41.407: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:41.407: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:42.173: INFO: Exec stderr: ""
Jul 16 14:00:42.173: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:42.176: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:42.835: INFO: Exec stderr: ""
Jul 16 14:00:42.835: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2919 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:00:42.835: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:00:43.604: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:00:43.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2919" for this suite.
Jul 16 14:01:33.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:01:34.155: INFO: namespace e2e-kubelet-etc-hosts-2919 deletion completed in 50.505045507s

• [SLOW TEST:68.673 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:01:34.166: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-47a8d760-da9f-42b0-b72d-e05132ca3d4f
STEP: Creating a pod to test consume configMaps
Jul 16 14:01:34.302: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-604edfe2-e23c-4208-b135-9d6b8f5908b7" in namespace "projected-8997" to be "success or failure"
Jul 16 14:01:34.308: INFO: Pod "pod-projected-configmaps-604edfe2-e23c-4208-b135-9d6b8f5908b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.877403ms
Jul 16 14:01:36.383: INFO: Pod "pod-projected-configmaps-604edfe2-e23c-4208-b135-9d6b8f5908b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080441662s
Jul 16 14:01:38.391: INFO: Pod "pod-projected-configmaps-604edfe2-e23c-4208-b135-9d6b8f5908b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088204109s
STEP: Saw pod success
Jul 16 14:01:38.391: INFO: Pod "pod-projected-configmaps-604edfe2-e23c-4208-b135-9d6b8f5908b7" satisfied condition "success or failure"
Jul 16 14:01:38.397: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-configmaps-604edfe2-e23c-4208-b135-9d6b8f5908b7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:01:38.504: INFO: Waiting for pod pod-projected-configmaps-604edfe2-e23c-4208-b135-9d6b8f5908b7 to disappear
Jul 16 14:01:38.514: INFO: Pod pod-projected-configmaps-604edfe2-e23c-4208-b135-9d6b8f5908b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:01:38.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8997" for this suite.
Jul 16 14:01:44.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:01:44.891: INFO: namespace projected-8997 deletion completed in 6.369099363s

• [SLOW TEST:10.726 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:01:44.892: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 16 14:01:44.981: INFO: Waiting up to 5m0s for pod "downward-api-5d9a76c9-5a10-46a0-9972-b570a5b9c4bb" in namespace "downward-api-5457" to be "success or failure"
Jul 16 14:01:44.991: INFO: Pod "downward-api-5d9a76c9-5a10-46a0-9972-b570a5b9c4bb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.622766ms
Jul 16 14:01:46.998: INFO: Pod "downward-api-5d9a76c9-5a10-46a0-9972-b570a5b9c4bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016647184s
Jul 16 14:01:49.006: INFO: Pod "downward-api-5d9a76c9-5a10-46a0-9972-b570a5b9c4bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025175817s
STEP: Saw pod success
Jul 16 14:01:49.007: INFO: Pod "downward-api-5d9a76c9-5a10-46a0-9972-b570a5b9c4bb" satisfied condition "success or failure"
Jul 16 14:01:49.014: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downward-api-5d9a76c9-5a10-46a0-9972-b570a5b9c4bb container dapi-container: <nil>
STEP: delete the pod
Jul 16 14:01:49.169: INFO: Waiting for pod downward-api-5d9a76c9-5a10-46a0-9972-b570a5b9c4bb to disappear
Jul 16 14:01:49.181: INFO: Pod downward-api-5d9a76c9-5a10-46a0-9972-b570a5b9c4bb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:01:49.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5457" for this suite.
Jul 16 14:01:55.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:01:55.982: INFO: namespace downward-api-5457 deletion completed in 6.792805506s

• [SLOW TEST:11.091 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:01:55.983: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:02:01.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-12" for this suite.
Jul 16 14:02:07.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:02:08.031: INFO: namespace watch-12 deletion completed in 6.482683349s

• [SLOW TEST:12.049 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:02:08.037: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 16 14:02:08.141: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 16 14:02:08.156: INFO: Waiting for terminating namespaces to be deleted...
Jul 16 14:02:08.170: INFO: 
Logging pods the kubelet thinks is on node conformance-worker-54b54f4f98-f9trz before test
Jul 16 14:02:08.445: INFO: sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-mgtcd from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 14:02:08.445: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 14:02:08.445: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 16 14:02:08.445: INFO: kube-proxy-9l5xc from kube-system started at 2019-07-16 12:58:27 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.445: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 14:02:08.445: INFO: restic-dqv4x from velero started at 2019-07-16 12:58:27 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.445: INFO: 	Container velero ready: true, restart count 0
Jul 16 14:02:08.445: INFO: canal-4r5mb from kube-system started at 2019-07-16 12:58:27 +0000 UTC (3 container statuses recorded)
Jul 16 14:02:08.445: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 14:02:08.445: INFO: 	Container install-cni ready: true, restart count 0
Jul 16 14:02:08.445: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 16 14:02:08.445: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-16 13:04:36 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.445: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 16 14:02:08.445: INFO: node-exporter-dk6wg from kube-system started at 2019-07-16 12:58:27 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.445: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 14:02:08.445: INFO: 
Logging pods the kubelet thinks is on node conformance-worker-54b54f4f98-m9svg before test
Jul 16 14:02:08.544: INFO: kube-proxy-nwtx4 from kube-system started at 2019-07-16 12:58:37 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.544: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 14:02:08.544: INFO: node-exporter-qrl7j from kube-system started at 2019-07-16 12:58:37 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.544: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 14:02:08.544: INFO: sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-t84ms from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 14:02:08.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 14:02:08.544: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 16 14:02:08.544: INFO: restic-vssb6 from velero started at 2019-07-16 12:58:37 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.544: INFO: 	Container velero ready: true, restart count 0
Jul 16 14:02:08.544: INFO: canal-mtsjt from kube-system started at 2019-07-16 12:58:37 +0000 UTC (3 container statuses recorded)
Jul 16 14:02:08.544: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 14:02:08.544: INFO: 	Container install-cni ready: true, restart count 0
Jul 16 14:02:08.544: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 16 14:02:08.545: INFO: sonobuoy-e2e-job-352202216adf4593 from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 14:02:08.545: INFO: 	Container e2e ready: true, restart count 0
Jul 16 14:02:08.545: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 14:02:08.545: INFO: 
Logging pods the kubelet thinks is on node conformance-worker-54b54f4f98-nqplr before test
Jul 16 14:02:08.646: INFO: cluster-autoscaler-7b894d5874-6bxbm from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.647: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Jul 16 14:02:08.647: INFO: webterminal-64d57d745b-z8m6c from webterminal started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.647: INFO: 	Container webterminal ready: true, restart count 0
Jul 16 14:02:08.647: INFO: coredns-6f56645db5-hxmmb from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.647: INFO: 	Container coredns ready: true, restart count 0
Jul 16 14:02:08.647: INFO: tiller-deploy-7c7d69dd75-kxhk8 from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.647: INFO: 	Container tiller ready: true, restart count 0
Jul 16 14:02:08.647: INFO: sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-v2cx8 from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 14:02:08.647: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 14:02:08.647: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 16 14:02:08.647: INFO: openvpn-client-95bcd78-6vrxz from kube-system started at 2019-07-16 12:59:02 +0000 UTC (2 container statuses recorded)
Jul 16 14:02:08.647: INFO: 	Container dnat-controller ready: true, restart count 0
Jul 16 14:02:08.647: INFO: 	Container openvpn-client ready: true, restart count 0
Jul 16 14:02:08.648: INFO: coredns-6f56645db5-86hnn from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.648: INFO: 	Container coredns ready: true, restart count 0
Jul 16 14:02:08.648: INFO: kube-proxy-vwpcz from kube-system started at 2019-07-16 12:58:10 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.648: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 14:02:08.648: INFO: node-exporter-p6bc4 from kube-system started at 2019-07-16 12:58:10 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.648: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 14:02:08.648: INFO: canal-qhfws from kube-system started at 2019-07-16 12:58:10 +0000 UTC (3 container statuses recorded)
Jul 16 14:02:08.648: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 14:02:08.648: INFO: 	Container install-cni ready: true, restart count 0
Jul 16 14:02:08.648: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 16 14:02:08.648: INFO: velero-79764c8c47-l746r from velero started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.648: INFO: 	Container velero ready: true, restart count 0
Jul 16 14:02:08.648: INFO: restic-pbp8t from velero started at 2019-07-16 12:58:09 +0000 UTC (1 container statuses recorded)
Jul 16 14:02:08.648: INFO: 	Container velero ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b1e8167217c9e3], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:02:09.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9967" for this suite.
Jul 16 14:02:15.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:02:16.097: INFO: namespace sched-pred-9967 deletion completed in 6.33520143s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:8.061 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:02:16.098: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-4a7ba07c-b246-45fb-8fb0-13c3230bc490
STEP: Creating secret with name s-test-opt-upd-929c3491-40c4-4020-b30d-ec200c0ecfec
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4a7ba07c-b246-45fb-8fb0-13c3230bc490
STEP: Updating secret s-test-opt-upd-929c3491-40c4-4020-b30d-ec200c0ecfec
STEP: Creating secret with name s-test-opt-create-23fb6dfb-84d2-4ce9-81c3-a2c34342d077
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:02:22.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4741" for this suite.
Jul 16 14:02:47.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:02:47.448: INFO: namespace projected-4741 deletion completed in 24.574358409s

• [SLOW TEST:31.350 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:02:47.450: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 14:02:47.523: INFO: Creating deployment "nginx-deployment"
Jul 16 14:02:47.545: INFO: Waiting for observed generation 1
Jul 16 14:02:49.559: INFO: Waiting for all required pods to come up
Jul 16 14:02:49.568: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 16 14:02:53.594: INFO: Waiting for deployment "nginx-deployment" to complete
Jul 16 14:02:53.612: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul 16 14:02:53.638: INFO: Updating deployment nginx-deployment
Jul 16 14:02:53.638: INFO: Waiting for observed generation 2
Jul 16 14:02:55.665: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 16 14:02:55.677: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 16 14:02:55.684: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 16 14:02:55.702: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 16 14:02:55.703: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 16 14:02:55.711: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 16 14:02:55.722: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul 16 14:02:55.722: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul 16 14:02:55.751: INFO: Updating deployment nginx-deployment
Jul 16 14:02:55.751: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul 16 14:02:55.771: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 16 14:02:55.794: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 16 14:02:55.932: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-9941,SelfLink:/apis/apps/v1/namespaces/deployment-9941/deployments/nginx-deployment,UID:abf8f1cc-96dc-421a-96cb-f361a457e747,ResourceVersion:19846,Generation:3,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-07-16 14:02:54 +0000 UTC 2019-07-16 14:02:47 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-07-16 14:02:55 +0000 UTC 2019-07-16 14:02:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul 16 14:02:55.965: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-9941,SelfLink:/apis/apps/v1/namespaces/deployment-9941/replicasets/nginx-deployment-55fb7cb77f,UID:b271b48e-b150-44d5-989f-498933a0c20c,ResourceVersion:19834,Generation:3,CreationTimestamp:2019-07-16 14:02:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment abf8f1cc-96dc-421a-96cb-f361a457e747 0xc0029d5017 0xc0029d5018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 14:02:55.965: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul 16 14:02:55.965: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-9941,SelfLink:/apis/apps/v1/namespaces/deployment-9941/replicasets/nginx-deployment-7b8c6f4498,UID:d7bf89a2-bb82-4cae-b050-a4f3074375b3,ResourceVersion:19831,Generation:3,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment abf8f1cc-96dc-421a-96cb-f361a457e747 0xc0029d50e7 0xc0029d50e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul 16 14:02:56.038: INFO: Pod "nginx-deployment-55fb7cb77f-57qw9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-57qw9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-57qw9,UID:4b499e21-7b68-4790-a4f5-2a3e59309c08,ResourceVersion:19824,Generation:0,CreationTimestamp:2019-07-16 14:02:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc0029d5a87 0xc0029d5a88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029d5af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029d5b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.39,StartTime:2019-07-16 14:02:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.039: INFO: Pod "nginx-deployment-55fb7cb77f-6ljlw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6ljlw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-6ljlw,UID:12fa71df-a63e-4423-98c8-6a1c8ffd3708,ResourceVersion:19874,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc0029d5bf0 0xc0029d5bf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029d5c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029d5c80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.039: INFO: Pod "nginx-deployment-55fb7cb77f-6xgbd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6xgbd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-6xgbd,UID:37b0960d-5083-46a1-9896-c44ebb169595,ResourceVersion:19818,Generation:0,CreationTimestamp:2019-07-16 14:02:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc0029d5d00 0xc0029d5d01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029d5d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029d5d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.9,PodIP:172.25.0.22,StartTime:2019-07-16 14:02:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.040: INFO: Pod "nginx-deployment-55fb7cb77f-fbz5l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-fbz5l,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-fbz5l,UID:861575fc-eef5-4f45-9d6d-915b6dcc109c,ResourceVersion:19826,Generation:0,CreationTimestamp:2019-07-16 14:02:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc0029d5e70 0xc0029d5e71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029d5ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029d5f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:172.25.1.123,StartTime:2019-07-16 14:02:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.040: INFO: Pod "nginx-deployment-55fb7cb77f-k82fg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-k82fg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-k82fg,UID:3d384bc2-5dfe-40ee-a95f-731ca4e9d078,ResourceVersion:19857,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc0029d5fe0 0xc0029d5fe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2c050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2c070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.040: INFO: Pod "nginx-deployment-55fb7cb77f-m6mgm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-m6mgm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-m6mgm,UID:cb1428b6-3fc9-4d2c-ac68-d7bb08c4a74c,ResourceVersion:19828,Generation:0,CreationTimestamp:2019-07-16 14:02:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc002c2c0f0 0xc002c2c0f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2c160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2c180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.40,StartTime:2019-07-16 14:02:54 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.040: INFO: Pod "nginx-deployment-55fb7cb77f-m7qk6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-m7qk6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-m7qk6,UID:486de6fd-1efa-49f3-823f-71fc5f30d907,ResourceVersion:19872,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc002c2c260 0xc002c2c261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2c2e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2c300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.041: INFO: Pod "nginx-deployment-55fb7cb77f-pd9mw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pd9mw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-pd9mw,UID:6c0643bd-fac8-4c44-9a41-b12a02fde903,ResourceVersion:19853,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc002c2c380 0xc002c2c381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2c3f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2c410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.041: INFO: Pod "nginx-deployment-55fb7cb77f-q5vdq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-q5vdq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-q5vdq,UID:f88910df-6dab-42a9-bf28-c84cad9705d2,ResourceVersion:19820,Generation:0,CreationTimestamp:2019-07-16 14:02:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc002c2c490 0xc002c2c491}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2c500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2c520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:53 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:172.25.1.122,StartTime:2019-07-16 14:02:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.042: INFO: Pod "nginx-deployment-55fb7cb77f-t4t7d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-t4t7d,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-t4t7d,UID:216db6af-79d5-44da-ab85-fbf8dd0aeb22,ResourceVersion:19878,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc002c2c600 0xc002c2c601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2c670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2c690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2019-07-16 14:02:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.042: INFO: Pod "nginx-deployment-55fb7cb77f-txdfn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-txdfn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-txdfn,UID:16fa47d9-6cf5-49e2-a7ec-c4acf44df7d7,ResourceVersion:19886,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc002c2c760 0xc002c2c761}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2c7d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2c7f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:56 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.042: INFO: Pod "nginx-deployment-55fb7cb77f-w688q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-w688q,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-w688q,UID:c22119f3-eca2-4a4d-af39-6a694cd0b6c2,ResourceVersion:19876,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc002c2c870 0xc002c2c871}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2c8e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2c900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.043: INFO: Pod "nginx-deployment-55fb7cb77f-zgncn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zgncn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-55fb7cb77f-zgncn,UID:e85957f8-f0c0-4c99-94c5-01c6a65a2718,ResourceVersion:19871,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b271b48e-b150-44d5-989f-498933a0c20c 0xc002c2c980 0xc002c2c981}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2c9f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2ca10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.043: INFO: Pod "nginx-deployment-7b8c6f4498-62pxp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-62pxp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-62pxp,UID:3b807aa3-3e14-45a1-8464-f5d400007112,ResourceVersion:19882,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2ca90 0xc002c2ca91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2caf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2cb10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.043: INFO: Pod "nginx-deployment-7b8c6f4498-876rx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-876rx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-876rx,UID:83b09c3b-bc7a-4732-bf17-dacc7357c97e,ResourceVersion:19719,Generation:0,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2cb90 0xc002c2cb91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2cbf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2cc10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.36,StartTime:2019-07-16 14:02:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 14:02:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e2fc0a95743dc7f9a4723843f7e2806affba69938f0393f19014f52fe1f2f74d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.043: INFO: Pod "nginx-deployment-7b8c6f4498-8lj44" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8lj44,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-8lj44,UID:352173cd-0584-4044-9040-a0a5faec5798,ResourceVersion:19863,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2cce0 0xc002c2cce1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2cd40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2cd60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.044: INFO: Pod "nginx-deployment-7b8c6f4498-8qdh2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8qdh2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-8qdh2,UID:cc4e971f-076a-4a31-a2ef-046663e081d8,ResourceVersion:19881,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2cde0 0xc002c2cde1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2ce40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2ce60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.044: INFO: Pod "nginx-deployment-7b8c6f4498-8qmcg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8qmcg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-8qmcg,UID:45c5adea-f27e-46e3-992f-88b3d865d1b6,ResourceVersion:19851,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2cee0 0xc002c2cee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2cf40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2cf60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.044: INFO: Pod "nginx-deployment-7b8c6f4498-8sqlb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8sqlb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-8sqlb,UID:6d1847fa-b8ee-4dc8-a203-19d02f14b940,ResourceVersion:19737,Generation:0,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2cfe0 0xc002c2cfe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2d040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2d060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.9,PodIP:172.25.0.21,StartTime:2019-07-16 14:02:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 14:02:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://02e891e4dc9014b8c17a6a23044d774e43355415a630e2ffeac051edd602ce85}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.045: INFO: Pod "nginx-deployment-7b8c6f4498-bfzcq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bfzcq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-bfzcq,UID:9c4c8fe4-9007-43b0-acdd-766c56fb18a4,ResourceVersion:19866,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2d130 0xc002c2d131}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2d190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2d1b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.045: INFO: Pod "nginx-deployment-7b8c6f4498-c6ggd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-c6ggd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-c6ggd,UID:27926eed-5825-45a7-8ffe-e51232e3dcae,ResourceVersion:19860,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2d230 0xc002c2d231}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2d290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2d2b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:,StartTime:2019-07-16 14:02:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.045: INFO: Pod "nginx-deployment-7b8c6f4498-d4tvj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d4tvj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-d4tvj,UID:249c3766-5089-4b23-bb98-3b2dd4a210ef,ResourceVersion:19852,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2d370 0xc002c2d371}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2d3d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2d3f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.046: INFO: Pod "nginx-deployment-7b8c6f4498-d7ghz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d7ghz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-d7ghz,UID:6ad7a129-3665-42a5-bf5a-ac88326b2e38,ResourceVersion:19740,Generation:0,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2d470 0xc002c2d471}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2d4d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2d4f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.9,PodIP:172.25.0.20,StartTime:2019-07-16 14:02:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 14:02:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b747d7fbcc1baa93e0c09f71d17e768e80ce1493e0c5f4403dbc62322f7e6674}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.046: INFO: Pod "nginx-deployment-7b8c6f4498-dq79b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dq79b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-dq79b,UID:f29f25a0-4f21-44b7-80eb-007a0321029b,ResourceVersion:19735,Generation:0,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2d5c0 0xc002c2d5c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2d620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2d640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:172.25.1.118,StartTime:2019-07-16 14:02:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 14:02:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c42305361aa8ce0fcf198aea470d712e6deba4f8fd435854cff46bd3116883b8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.046: INFO: Pod "nginx-deployment-7b8c6f4498-f9kdh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-f9kdh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-f9kdh,UID:a0d397e6-513f-4c2c-a167-f08651d1d95a,ResourceVersion:19724,Generation:0,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2d710 0xc002c2d711}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2d770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2d790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.38,StartTime:2019-07-16 14:02:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 14:02:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c3ecef688eba0ae9529c8033f94d273cd8b4a8b34d389952b1959ca7eed946cb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.046: INFO: Pod "nginx-deployment-7b8c6f4498-fggm6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fggm6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-fggm6,UID:79b56934-38d5-473e-b1b9-2cd7daf0797c,ResourceVersion:19879,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2d860 0xc002c2d861}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-nqplr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2d8e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2d900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.047: INFO: Pod "nginx-deployment-7b8c6f4498-jzrgz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jzrgz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-jzrgz,UID:8bc8dd4b-b7f3-4835-a029-bdf4cda0c455,ResourceVersion:19738,Generation:0,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2d980 0xc002c2d981}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2d9e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2da00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:172.25.1.120,StartTime:2019-07-16 14:02:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 14:02:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f281c18c308c74023cec9fc971933e7346a91594cd88069722eb7657d76b6fcd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.047: INFO: Pod "nginx-deployment-7b8c6f4498-m2jsj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m2jsj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-m2jsj,UID:eaffafb6-5f51-4d4e-b080-f98293bb1c4e,ResourceVersion:19880,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2dad0 0xc002c2dad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2db30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2db50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.048: INFO: Pod "nginx-deployment-7b8c6f4498-nnz7h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nnz7h,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-nnz7h,UID:f1b1a4a4-1f89-487e-943e-9d015ada5baf,ResourceVersion:19862,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2dbd0 0xc002c2dbd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2dc30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2dc60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.048: INFO: Pod "nginx-deployment-7b8c6f4498-qgckj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qgckj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-qgckj,UID:85b660bf-49bf-4bf2-b973-974e6fd63229,ResourceVersion:19752,Generation:0,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2dce0 0xc002c2dce1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2dd40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2dd60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:172.25.1.117,StartTime:2019-07-16 14:02:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 14:02:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3f76339d5adc81db61b64820dc6a8b396baf418b774459a05ad88f160bc354c8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.048: INFO: Pod "nginx-deployment-7b8c6f4498-vgpf4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vgpf4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-vgpf4,UID:a6cb5169-0c7d-4a7b-b913-7250603846c8,ResourceVersion:19716,Generation:0,CreationTimestamp:2019-07-16 14:02:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2de30 0xc002c2de31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-m9svg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2de90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c2deb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.37,StartTime:2019-07-16 14:02:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 14:02:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://75c710248e21a838069d7fa600ddc5bbb84e2311e4a2cae2a10f07b5e0f004c9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.049: INFO: Pod "nginx-deployment-7b8c6f4498-xrfm7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xrfm7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-xrfm7,UID:921481a4-ff58-4b38-8db5-4f28e7fb6821,ResourceVersion:19883,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002c2df80 0xc002c2df81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c2dfe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002928000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:02:56.049: INFO: Pod "nginx-deployment-7b8c6f4498-zd7fx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zd7fx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9941,SelfLink:/api/v1/namespaces/deployment-9941/pods/nginx-deployment-7b8c6f4498-zd7fx,UID:e72415ee-39ee-47f9-b4af-d2a33df8ad0f,ResourceVersion:19861,Generation:0,CreationTimestamp:2019-07-16 14:02:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 d7bf89a2-bb82-4cae-b050-a4f3074375b3 0xc002928080 0xc002928081}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwh8s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwh8s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zwh8s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029280e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002928100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:02:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:02:56.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9941" for this suite.
Jul 16 14:03:10.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:03:10.734: INFO: namespace deployment-9941 deletion completed in 14.632593804s

• [SLOW TEST:23.285 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:03:10.737: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-713f0baf-2ddd-43c4-bc79-41478b0a4a08
STEP: Creating configMap with name cm-test-opt-upd-8ee2dc75-0ca5-4afd-a2cd-62f3529450a6
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-713f0baf-2ddd-43c4-bc79-41478b0a4a08
STEP: Updating configmap cm-test-opt-upd-8ee2dc75-0ca5-4afd-a2cd-62f3529450a6
STEP: Creating configMap with name cm-test-opt-create-7aefe772-1a27-44c4-b252-616a07dbb903
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:03:19.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6297" for this suite.
Jul 16 14:03:43.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:03:44.190: INFO: namespace projected-6297 deletion completed in 24.365816561s

• [SLOW TEST:33.454 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:03:44.194: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 16 14:03:49.058: INFO: Successfully updated pod "annotationupdate2013f4d4-0aa5-423d-8849-731e6cb97b1c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:03:51.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4419" for this suite.
Jul 16 14:04:15.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:04:15.468: INFO: namespace projected-4419 deletion completed in 24.342713794s

• [SLOW TEST:31.276 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:04:15.474: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:04:19.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1649" for this suite.
Jul 16 14:04:25.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:04:26.230: INFO: namespace emptydir-wrapper-1649 deletion completed in 6.333782121s

• [SLOW TEST:10.756 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:04:26.235: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 16 14:04:26.349: INFO: Waiting up to 5m0s for pod "pod-b21360a8-de5c-4423-9825-7540c11a7d7a" in namespace "emptydir-6600" to be "success or failure"
Jul 16 14:04:26.362: INFO: Pod "pod-b21360a8-de5c-4423-9825-7540c11a7d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.907274ms
Jul 16 14:04:28.384: INFO: Pod "pod-b21360a8-de5c-4423-9825-7540c11a7d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035019119s
Jul 16 14:04:30.394: INFO: Pod "pod-b21360a8-de5c-4423-9825-7540c11a7d7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045025762s
STEP: Saw pod success
Jul 16 14:04:30.394: INFO: Pod "pod-b21360a8-de5c-4423-9825-7540c11a7d7a" satisfied condition "success or failure"
Jul 16 14:04:30.405: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-b21360a8-de5c-4423-9825-7540c11a7d7a container test-container: <nil>
STEP: delete the pod
Jul 16 14:04:30.493: INFO: Waiting for pod pod-b21360a8-de5c-4423-9825-7540c11a7d7a to disappear
Jul 16 14:04:30.499: INFO: Pod pod-b21360a8-de5c-4423-9825-7540c11a7d7a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:04:30.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6600" for this suite.
Jul 16 14:04:38.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:04:39.385: INFO: namespace emptydir-6600 deletion completed in 8.87716596s

• [SLOW TEST:13.150 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:04:39.386: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:04:39.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3478dfd-74c7-466b-8e12-fd10afee63f1" in namespace "downward-api-2924" to be "success or failure"
Jul 16 14:04:40.058: INFO: Pod "downwardapi-volume-e3478dfd-74c7-466b-8e12-fd10afee63f1": Phase="Pending", Reason="", readiness=false. Elapsed: 250.456273ms
Jul 16 14:04:42.064: INFO: Pod "downwardapi-volume-e3478dfd-74c7-466b-8e12-fd10afee63f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.256871122s
Jul 16 14:04:44.074: INFO: Pod "downwardapi-volume-e3478dfd-74c7-466b-8e12-fd10afee63f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.266553031s
STEP: Saw pod success
Jul 16 14:04:44.074: INFO: Pod "downwardapi-volume-e3478dfd-74c7-466b-8e12-fd10afee63f1" satisfied condition "success or failure"
Jul 16 14:04:44.082: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-e3478dfd-74c7-466b-8e12-fd10afee63f1 container client-container: <nil>
STEP: delete the pod
Jul 16 14:04:44.187: INFO: Waiting for pod downwardapi-volume-e3478dfd-74c7-466b-8e12-fd10afee63f1 to disappear
Jul 16 14:04:44.196: INFO: Pod downwardapi-volume-e3478dfd-74c7-466b-8e12-fd10afee63f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:04:44.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2924" for this suite.
Jul 16 14:04:50.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:04:50.586: INFO: namespace downward-api-2924 deletion completed in 6.379028063s

• [SLOW TEST:11.200 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:04:50.586: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jul 16 14:05:00.894: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:05:00.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0716 14:05:00.894773      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9069" for this suite.
Jul 16 14:05:06.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:05:07.373: INFO: namespace gc-9069 deletion completed in 6.46957954s

• [SLOW TEST:16.787 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:05:07.375: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:05:07.519: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b209a405-04eb-4c50-9c98-22d5bb4b6c94" in namespace "downward-api-6700" to be "success or failure"
Jul 16 14:05:07.539: INFO: Pod "downwardapi-volume-b209a405-04eb-4c50-9c98-22d5bb4b6c94": Phase="Pending", Reason="", readiness=false. Elapsed: 19.512581ms
Jul 16 14:05:09.548: INFO: Pod "downwardapi-volume-b209a405-04eb-4c50-9c98-22d5bb4b6c94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027997392s
Jul 16 14:05:11.555: INFO: Pod "downwardapi-volume-b209a405-04eb-4c50-9c98-22d5bb4b6c94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035494986s
STEP: Saw pod success
Jul 16 14:05:11.555: INFO: Pod "downwardapi-volume-b209a405-04eb-4c50-9c98-22d5bb4b6c94" satisfied condition "success or failure"
Jul 16 14:05:11.562: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-b209a405-04eb-4c50-9c98-22d5bb4b6c94 container client-container: <nil>
STEP: delete the pod
Jul 16 14:05:11.680: INFO: Waiting for pod downwardapi-volume-b209a405-04eb-4c50-9c98-22d5bb4b6c94 to disappear
Jul 16 14:05:11.687: INFO: Pod downwardapi-volume-b209a405-04eb-4c50-9c98-22d5bb4b6c94 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:05:11.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6700" for this suite.
Jul 16 14:05:17.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:05:18.180: INFO: namespace downward-api-6700 deletion completed in 6.483742198s

• [SLOW TEST:10.806 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:05:18.181: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 16 14:05:18.620: INFO: PodSpec: initContainers in spec.initContainers
Jul 16 14:06:21.699: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7ef2d24f-bb29-4bd4-9ccd-f2c0c56e614f", GenerateName:"", Namespace:"init-container-7356", SelfLink:"/api/v1/namespaces/init-container-7356/pods/pod-init-7ef2d24f-bb29-4bd4-9ccd-f2c0c56e614f", UID:"500d735e-fc22-4741-8d21-b3e112f34d3b", ResourceVersion:"21031", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63698882718, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"620128175"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-652lk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0012d4300), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-652lk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-652lk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-652lk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000a368a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"conformance-worker-54b54f4f98-f9trz", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0028fee40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000a36930)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000a36960)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000a36968), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000a3696c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882718, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882718, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882718, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698882718, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.2", PodIP:"172.25.1.139", StartTime:(*v1.Time)(0xc002cb88e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0029d25b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0029d2620)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://36250b61c0127c16af077f64c7d0f27b419935b9a8f167cdd4e9bec7b2b22e58"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002cb8920), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002cb8900), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:06:21.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7356" for this suite.
Jul 16 14:06:45.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:06:46.071: INFO: namespace init-container-7356 deletion completed in 24.356829623s

• [SLOW TEST:87.891 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:06:46.073: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 16 14:06:50.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec pod-sharedvolume-b5c9a605-9bf1-478f-af9f-8c1c3814dff3 -c busybox-main-container --namespace=emptydir-1774 -- cat /usr/share/volumeshare/shareddata.txt'
Jul 16 14:06:51.941: INFO: stderr: ""
Jul 16 14:06:51.941: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:06:51.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1774" for this suite.
Jul 16 14:06:57.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:06:58.501: INFO: namespace emptydir-1774 deletion completed in 6.549043177s

• [SLOW TEST:12.428 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:06:58.506: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 16 14:06:58.663: INFO: Waiting up to 5m0s for pod "pod-9546ca97-866f-4689-baa1-8e3813c3d789" in namespace "emptydir-8481" to be "success or failure"
Jul 16 14:06:58.681: INFO: Pod "pod-9546ca97-866f-4689-baa1-8e3813c3d789": Phase="Pending", Reason="", readiness=false. Elapsed: 17.498567ms
Jul 16 14:07:00.689: INFO: Pod "pod-9546ca97-866f-4689-baa1-8e3813c3d789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026283016s
Jul 16 14:07:02.704: INFO: Pod "pod-9546ca97-866f-4689-baa1-8e3813c3d789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041243694s
STEP: Saw pod success
Jul 16 14:07:02.705: INFO: Pod "pod-9546ca97-866f-4689-baa1-8e3813c3d789" satisfied condition "success or failure"
Jul 16 14:07:02.713: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-9546ca97-866f-4689-baa1-8e3813c3d789 container test-container: <nil>
STEP: delete the pod
Jul 16 14:07:02.876: INFO: Waiting for pod pod-9546ca97-866f-4689-baa1-8e3813c3d789 to disappear
Jul 16 14:07:03.018: INFO: Pod pod-9546ca97-866f-4689-baa1-8e3813c3d789 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:07:03.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8481" for this suite.
Jul 16 14:07:11.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:07:11.540: INFO: namespace emptydir-8481 deletion completed in 8.512049841s

• [SLOW TEST:13.035 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:07:11.545: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jul 16 14:07:11.747: INFO: Waiting up to 5m0s for pod "client-containers-d513efdb-7d53-4160-9712-6c05c90446a7" in namespace "containers-5878" to be "success or failure"
Jul 16 14:07:11.756: INFO: Pod "client-containers-d513efdb-7d53-4160-9712-6c05c90446a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.571472ms
Jul 16 14:07:13.765: INFO: Pod "client-containers-d513efdb-7d53-4160-9712-6c05c90446a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017191236s
Jul 16 14:07:15.774: INFO: Pod "client-containers-d513efdb-7d53-4160-9712-6c05c90446a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026378992s
STEP: Saw pod success
Jul 16 14:07:15.774: INFO: Pod "client-containers-d513efdb-7d53-4160-9712-6c05c90446a7" satisfied condition "success or failure"
Jul 16 14:07:15.782: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod client-containers-d513efdb-7d53-4160-9712-6c05c90446a7 container test-container: <nil>
STEP: delete the pod
Jul 16 14:07:15.893: INFO: Waiting for pod client-containers-d513efdb-7d53-4160-9712-6c05c90446a7 to disappear
Jul 16 14:07:15.898: INFO: Pod client-containers-d513efdb-7d53-4160-9712-6c05c90446a7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:07:15.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5878" for this suite.
Jul 16 14:07:21.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:07:22.292: INFO: namespace containers-5878 deletion completed in 6.386433274s

• [SLOW TEST:10.747 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:07:22.296: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 16 14:07:22.442: INFO: Waiting up to 5m0s for pod "pod-23d4f632-1435-475a-9c22-01ace1b5e054" in namespace "emptydir-9278" to be "success or failure"
Jul 16 14:07:22.447: INFO: Pod "pod-23d4f632-1435-475a-9c22-01ace1b5e054": Phase="Pending", Reason="", readiness=false. Elapsed: 5.226701ms
Jul 16 14:07:24.454: INFO: Pod "pod-23d4f632-1435-475a-9c22-01ace1b5e054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01208711s
Jul 16 14:07:26.466: INFO: Pod "pod-23d4f632-1435-475a-9c22-01ace1b5e054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023328904s
STEP: Saw pod success
Jul 16 14:07:26.466: INFO: Pod "pod-23d4f632-1435-475a-9c22-01ace1b5e054" satisfied condition "success or failure"
Jul 16 14:07:26.474: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-23d4f632-1435-475a-9c22-01ace1b5e054 container test-container: <nil>
STEP: delete the pod
Jul 16 14:07:26.533: INFO: Waiting for pod pod-23d4f632-1435-475a-9c22-01ace1b5e054 to disappear
Jul 16 14:07:26.541: INFO: Pod pod-23d4f632-1435-475a-9c22-01ace1b5e054 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:07:26.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9278" for this suite.
Jul 16 14:07:32.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:07:33.025: INFO: namespace emptydir-9278 deletion completed in 6.458854415s

• [SLOW TEST:10.729 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:07:33.026: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 16 14:07:33.182: INFO: Waiting up to 5m0s for pod "pod-9f43ef64-aaf6-4674-a7cb-8ba7e4eff39b" in namespace "emptydir-5607" to be "success or failure"
Jul 16 14:07:33.190: INFO: Pod "pod-9f43ef64-aaf6-4674-a7cb-8ba7e4eff39b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.154916ms
Jul 16 14:07:35.199: INFO: Pod "pod-9f43ef64-aaf6-4674-a7cb-8ba7e4eff39b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017101829s
Jul 16 14:07:37.212: INFO: Pod "pod-9f43ef64-aaf6-4674-a7cb-8ba7e4eff39b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029865491s
STEP: Saw pod success
Jul 16 14:07:37.212: INFO: Pod "pod-9f43ef64-aaf6-4674-a7cb-8ba7e4eff39b" satisfied condition "success or failure"
Jul 16 14:07:37.219: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-9f43ef64-aaf6-4674-a7cb-8ba7e4eff39b container test-container: <nil>
STEP: delete the pod
Jul 16 14:07:37.402: INFO: Waiting for pod pod-9f43ef64-aaf6-4674-a7cb-8ba7e4eff39b to disappear
Jul 16 14:07:37.430: INFO: Pod pod-9f43ef64-aaf6-4674-a7cb-8ba7e4eff39b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:07:37.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5607" for this suite.
Jul 16 14:07:43.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:07:43.915: INFO: namespace emptydir-5607 deletion completed in 6.4716767s

• [SLOW TEST:10.889 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:07:43.918: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-72ab4e8c-4854-4dd4-b110-9c85539d7b23
STEP: Creating a pod to test consume secrets
Jul 16 14:07:44.058: INFO: Waiting up to 5m0s for pod "pod-secrets-2b51d91a-c13f-44a6-a808-37b4525a6fb7" in namespace "secrets-6333" to be "success or failure"
Jul 16 14:07:44.065: INFO: Pod "pod-secrets-2b51d91a-c13f-44a6-a808-37b4525a6fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.576291ms
Jul 16 14:07:46.074: INFO: Pod "pod-secrets-2b51d91a-c13f-44a6-a808-37b4525a6fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015522643s
Jul 16 14:07:48.088: INFO: Pod "pod-secrets-2b51d91a-c13f-44a6-a808-37b4525a6fb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029564898s
STEP: Saw pod success
Jul 16 14:07:48.088: INFO: Pod "pod-secrets-2b51d91a-c13f-44a6-a808-37b4525a6fb7" satisfied condition "success or failure"
Jul 16 14:07:48.093: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-secrets-2b51d91a-c13f-44a6-a808-37b4525a6fb7 container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:07:48.176: INFO: Waiting for pod pod-secrets-2b51d91a-c13f-44a6-a808-37b4525a6fb7 to disappear
Jul 16 14:07:48.184: INFO: Pod pod-secrets-2b51d91a-c13f-44a6-a808-37b4525a6fb7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:07:48.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6333" for this suite.
Jul 16 14:07:54.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:07:54.741: INFO: namespace secrets-6333 deletion completed in 6.54557634s

• [SLOW TEST:10.824 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:07:54.756: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4901.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4901.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4901.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4901.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 16 14:08:29.153: INFO: DNS probes using dns-test-0437b83a-40a0-476b-9c56-09218d03570a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4901.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4901.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4901.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4901.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 16 14:08:33.484: INFO: DNS probes using dns-test-b24a4e68-be27-4f75-b089-4de81fc8b892 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4901.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4901.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4901.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4901.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 16 14:08:40.289: INFO: DNS probes using dns-test-31b9c351-1b8c-45dd-a0d5-4182c9611d94 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:08:40.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4901" for this suite.
Jul 16 14:08:48.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:08:48.830: INFO: namespace dns-4901 deletion completed in 8.401545673s

• [SLOW TEST:54.075 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:08:48.831: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 16 14:08:51.981: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:08:52.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1155" for this suite.
Jul 16 14:08:58.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:08:58.545: INFO: namespace container-runtime-1155 deletion completed in 6.510755155s

• [SLOW TEST:9.714 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:08:58.554: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jul 16 14:08:58.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-1103'
Jul 16 14:08:59.090: INFO: stderr: ""
Jul 16 14:08:59.090: INFO: stdout: "pod/pause created\n"
Jul 16 14:08:59.090: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 16 14:08:59.090: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1103" to be "running and ready"
Jul 16 14:08:59.095: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.15477ms
Jul 16 14:09:01.303: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.212900741s
Jul 16 14:09:03.309: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.21954251s
Jul 16 14:09:03.309: INFO: Pod "pause" satisfied condition "running and ready"
Jul 16 14:09:03.309: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 16 14:09:03.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 label pods pause testing-label=testing-label-value --namespace=kubectl-1103'
Jul 16 14:09:03.480: INFO: stderr: ""
Jul 16 14:09:03.480: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 16 14:09:03.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pod pause -L testing-label --namespace=kubectl-1103'
Jul 16 14:09:03.596: INFO: stderr: ""
Jul 16 14:09:03.596: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 16 14:09:03.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 label pods pause testing-label- --namespace=kubectl-1103'
Jul 16 14:09:03.769: INFO: stderr: ""
Jul 16 14:09:03.769: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 16 14:09:03.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pod pause -L testing-label --namespace=kubectl-1103'
Jul 16 14:09:03.929: INFO: stderr: ""
Jul 16 14:09:03.929: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jul 16 14:09:03.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-1103'
Jul 16 14:09:04.122: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 14:09:04.122: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 16 14:09:04.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get rc,svc -l name=pause --no-headers --namespace=kubectl-1103'
Jul 16 14:09:04.489: INFO: stderr: "No resources found.\n"
Jul 16 14:09:04.489: INFO: stdout: ""
Jul 16 14:09:04.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -l name=pause --namespace=kubectl-1103 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 14:09:04.620: INFO: stderr: ""
Jul 16 14:09:04.620: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:09:04.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1103" for this suite.
Jul 16 14:09:12.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:09:13.115: INFO: namespace kubectl-1103 deletion completed in 8.480565965s

• [SLOW TEST:14.561 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:09:13.120: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 16 14:09:18.566: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:09:19.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2307" for this suite.
Jul 16 14:09:43.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:09:44.012: INFO: namespace replicaset-2307 deletion completed in 24.372694786s

• [SLOW TEST:30.893 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:09:44.015: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-ded41d21-887d-4e5d-b305-4c137fa64274
STEP: Creating a pod to test consume secrets
Jul 16 14:09:44.194: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-80df8600-d10d-4974-ba83-698ae7c57004" in namespace "projected-5262" to be "success or failure"
Jul 16 14:09:44.199: INFO: Pod "pod-projected-secrets-80df8600-d10d-4974-ba83-698ae7c57004": Phase="Pending", Reason="", readiness=false. Elapsed: 5.846854ms
Jul 16 14:09:46.208: INFO: Pod "pod-projected-secrets-80df8600-d10d-4974-ba83-698ae7c57004": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01469889s
Jul 16 14:09:48.216: INFO: Pod "pod-projected-secrets-80df8600-d10d-4974-ba83-698ae7c57004": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022634122s
STEP: Saw pod success
Jul 16 14:09:48.216: INFO: Pod "pod-projected-secrets-80df8600-d10d-4974-ba83-698ae7c57004" satisfied condition "success or failure"
Jul 16 14:09:48.223: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-secrets-80df8600-d10d-4974-ba83-698ae7c57004 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:09:48.353: INFO: Waiting for pod pod-projected-secrets-80df8600-d10d-4974-ba83-698ae7c57004 to disappear
Jul 16 14:09:48.370: INFO: Pod pod-projected-secrets-80df8600-d10d-4974-ba83-698ae7c57004 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:09:48.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5262" for this suite.
Jul 16 14:09:54.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:09:54.942: INFO: namespace projected-5262 deletion completed in 6.56360441s

• [SLOW TEST:10.928 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:09:54.943: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-38a1e68c-ff9d-4137-a54b-d5eecca36b71
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-38a1e68c-ff9d-4137-a54b-d5eecca36b71
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:11:20.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7727" for this suite.
Jul 16 14:11:44.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:11:45.417: INFO: namespace projected-7727 deletion completed in 24.470337116s

• [SLOW TEST:110.474 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:11:45.420: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-3292/configmap-test-91551e60-45a6-4169-8406-5901aee9cf9c
STEP: Creating a pod to test consume configMaps
Jul 16 14:11:45.631: INFO: Waiting up to 5m0s for pod "pod-configmaps-9dc5f89d-7719-43ec-a96b-56262176cb65" in namespace "configmap-3292" to be "success or failure"
Jul 16 14:11:45.640: INFO: Pod "pod-configmaps-9dc5f89d-7719-43ec-a96b-56262176cb65": Phase="Pending", Reason="", readiness=false. Elapsed: 8.335931ms
Jul 16 14:11:47.646: INFO: Pod "pod-configmaps-9dc5f89d-7719-43ec-a96b-56262176cb65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014875999s
Jul 16 14:11:49.656: INFO: Pod "pod-configmaps-9dc5f89d-7719-43ec-a96b-56262176cb65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024088432s
STEP: Saw pod success
Jul 16 14:11:49.656: INFO: Pod "pod-configmaps-9dc5f89d-7719-43ec-a96b-56262176cb65" satisfied condition "success or failure"
Jul 16 14:11:49.661: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-9dc5f89d-7719-43ec-a96b-56262176cb65 container env-test: <nil>
STEP: delete the pod
Jul 16 14:11:49.847: INFO: Waiting for pod pod-configmaps-9dc5f89d-7719-43ec-a96b-56262176cb65 to disappear
Jul 16 14:11:49.853: INFO: Pod pod-configmaps-9dc5f89d-7719-43ec-a96b-56262176cb65 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:11:49.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3292" for this suite.
Jul 16 14:11:55.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:11:56.230: INFO: namespace configmap-3292 deletion completed in 6.367660635s

• [SLOW TEST:10.810 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:11:56.235: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-649f82eb-3628-4043-b6d6-2152ce85b2b4
STEP: Creating a pod to test consume secrets
Jul 16 14:11:56.388: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-21af0d41-58c9-40fa-a785-0d70f0622c59" in namespace "projected-7857" to be "success or failure"
Jul 16 14:11:56.394: INFO: Pod "pod-projected-secrets-21af0d41-58c9-40fa-a785-0d70f0622c59": Phase="Pending", Reason="", readiness=false. Elapsed: 5.737056ms
Jul 16 14:11:58.554: INFO: Pod "pod-projected-secrets-21af0d41-58c9-40fa-a785-0d70f0622c59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.166496836s
Jul 16 14:12:00.562: INFO: Pod "pod-projected-secrets-21af0d41-58c9-40fa-a785-0d70f0622c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.174497288s
STEP: Saw pod success
Jul 16 14:12:00.563: INFO: Pod "pod-projected-secrets-21af0d41-58c9-40fa-a785-0d70f0622c59" satisfied condition "success or failure"
Jul 16 14:12:00.573: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-secrets-21af0d41-58c9-40fa-a785-0d70f0622c59 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:12:00.669: INFO: Waiting for pod pod-projected-secrets-21af0d41-58c9-40fa-a785-0d70f0622c59 to disappear
Jul 16 14:12:00.674: INFO: Pod pod-projected-secrets-21af0d41-58c9-40fa-a785-0d70f0622c59 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:12:00.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7857" for this suite.
Jul 16 14:12:06.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:12:07.115: INFO: namespace projected-7857 deletion completed in 6.429041487s

• [SLOW TEST:10.881 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:12:07.116: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-wxnz
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 14:12:07.271: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-wxnz" in namespace "subpath-9503" to be "success or failure"
Jul 16 14:12:07.277: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.478893ms
Jul 16 14:12:09.286: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014010678s
Jul 16 14:12:11.295: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Running", Reason="", readiness=true. Elapsed: 4.023024942s
Jul 16 14:12:13.307: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Running", Reason="", readiness=true. Elapsed: 6.035704162s
Jul 16 14:12:15.315: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Running", Reason="", readiness=true. Elapsed: 8.043591973s
Jul 16 14:12:17.324: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Running", Reason="", readiness=true. Elapsed: 10.052639275s
Jul 16 14:12:19.335: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Running", Reason="", readiness=true. Elapsed: 12.063595217s
Jul 16 14:12:21.342: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Running", Reason="", readiness=true. Elapsed: 14.070505795s
Jul 16 14:12:24.350: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Running", Reason="", readiness=true. Elapsed: 17.078632119s
Jul 16 14:12:26.358: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Running", Reason="", readiness=true. Elapsed: 19.086438231s
Jul 16 14:12:28.368: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Running", Reason="", readiness=true. Elapsed: 21.096147566s
Jul 16 14:12:30.375: INFO: Pod "pod-subpath-test-downwardapi-wxnz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.103821723s
STEP: Saw pod success
Jul 16 14:12:30.376: INFO: Pod "pod-subpath-test-downwardapi-wxnz" satisfied condition "success or failure"
Jul 16 14:12:30.381: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-subpath-test-downwardapi-wxnz container test-container-subpath-downwardapi-wxnz: <nil>
STEP: delete the pod
Jul 16 14:12:30.515: INFO: Waiting for pod pod-subpath-test-downwardapi-wxnz to disappear
Jul 16 14:12:30.521: INFO: Pod pod-subpath-test-downwardapi-wxnz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-wxnz
Jul 16 14:12:30.521: INFO: Deleting pod "pod-subpath-test-downwardapi-wxnz" in namespace "subpath-9503"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:12:30.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9503" for this suite.
Jul 16 14:12:36.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:12:37.039: INFO: namespace subpath-9503 deletion completed in 6.501875496s

• [SLOW TEST:29.924 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:12:37.042: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 16 14:12:42.573: INFO: Successfully updated pod "annotationupdate35d83322-143c-4f67-a913-2aaa8ac67400"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:12:44.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8008" for this suite.
Jul 16 14:13:08.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:13:09.030: INFO: namespace downward-api-8008 deletion completed in 24.372305194s

• [SLOW TEST:31.989 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:13:09.031: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-2de24318-80c2-47ea-9de3-4fb914e45b11 in namespace container-probe-7494
Jul 16 14:13:13.219: INFO: Started pod busybox-2de24318-80c2-47ea-9de3-4fb914e45b11 in namespace container-probe-7494
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 14:13:13.226: INFO: Initial restart count of pod busybox-2de24318-80c2-47ea-9de3-4fb914e45b11 is 0
Jul 16 14:13:59.497: INFO: Restart count of pod container-probe-7494/busybox-2de24318-80c2-47ea-9de3-4fb914e45b11 is now 1 (46.270718868s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:13:59.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7494" for this suite.
Jul 16 14:14:05.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:14:06.025: INFO: namespace container-probe-7494 deletion completed in 6.467449164s

• [SLOW TEST:56.993 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:14:06.026: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 14:14:06.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5841'
Jul 16 14:14:06.453: INFO: stderr: ""
Jul 16 14:14:06.453: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 16 14:14:11.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pod e2e-test-nginx-pod --namespace=kubectl-5841 -o json'
Jul 16 14:14:11.646: INFO: stderr: ""
Jul 16 14:14:11.646: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-07-16T14:14:06Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5841\",\n        \"resourceVersion\": \"23131\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5841/pods/e2e-test-nginx-pod\",\n        \"uid\": \"d0cdb66b-8040-40d3-9a33-cb793cd258e7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-4grjs\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"conformance-worker-54b54f4f98-f9trz\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-4grjs\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-4grjs\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-16T14:14:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-16T14:14:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-16T14:14:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-16T14:14:06Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://dfb0f9ff5e117ce1d208074080e71fa63f95f142644a600faae8fd36650a9226\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-16T14:14:08Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.1.157\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-16T14:14:06Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 16 14:14:11.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 replace -f - --namespace=kubectl-5841'
Jul 16 14:14:12.500: INFO: stderr: ""
Jul 16 14:14:12.500: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jul 16 14:14:12.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete pods e2e-test-nginx-pod --namespace=kubectl-5841'
Jul 16 14:14:15.151: INFO: stderr: ""
Jul 16 14:14:15.151: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:14:15.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5841" for this suite.
Jul 16 14:14:21.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:14:21.608: INFO: namespace kubectl-5841 deletion completed in 6.443222162s

• [SLOW TEST:15.582 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:14:21.609: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9634
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 16 14:14:21.774: INFO: Found 0 stateful pods, waiting for 3
Jul 16 14:14:31.858: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:14:31.858: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:14:31.858: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 16 14:14:41.784: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:14:41.784: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:14:41.784: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:14:41.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-9634 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:14:42.654: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 14:14:42.654: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:14:42.654: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 16 14:14:52.738: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 16 14:15:02.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-9634 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:15:03.653: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 16 14:15:03.653: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:15:03.653: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:15:13.701: INFO: Waiting for StatefulSet statefulset-9634/ss2 to complete update
Jul 16 14:15:13.701: INFO: Waiting for Pod statefulset-9634/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 16 14:15:13.701: INFO: Waiting for Pod statefulset-9634/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 16 14:15:13.701: INFO: Waiting for Pod statefulset-9634/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 16 14:15:23.721: INFO: Waiting for StatefulSet statefulset-9634/ss2 to complete update
Jul 16 14:15:23.721: INFO: Waiting for Pod statefulset-9634/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 16 14:15:23.722: INFO: Waiting for Pod statefulset-9634/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 16 14:15:33.756: INFO: Waiting for StatefulSet statefulset-9634/ss2 to complete update
Jul 16 14:15:33.756: INFO: Waiting for Pod statefulset-9634/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 16 14:15:43.716: INFO: Waiting for StatefulSet statefulset-9634/ss2 to complete update
STEP: Rolling back to a previous revision
Jul 16 14:15:53.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-9634 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:15:54.731: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 14:15:54.731: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:15:54.731: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:16:04.797: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 16 14:16:14.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-9634 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:16:15.913: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 16 14:16:15.913: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:16:15.913: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:16:25.975: INFO: Waiting for StatefulSet statefulset-9634/ss2 to complete update
Jul 16 14:16:25.975: INFO: Waiting for Pod statefulset-9634/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 16 14:16:25.975: INFO: Waiting for Pod statefulset-9634/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 16 14:16:25.975: INFO: Waiting for Pod statefulset-9634/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 16 14:16:35.991: INFO: Waiting for StatefulSet statefulset-9634/ss2 to complete update
Jul 16 14:16:35.993: INFO: Waiting for Pod statefulset-9634/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 16 14:16:35.993: INFO: Waiting for Pod statefulset-9634/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 16 14:16:45.992: INFO: Waiting for StatefulSet statefulset-9634/ss2 to complete update
Jul 16 14:16:45.992: INFO: Waiting for Pod statefulset-9634/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 16 14:16:55.993: INFO: Deleting all statefulset in ns statefulset-9634
Jul 16 14:16:56.010: INFO: Scaling statefulset ss2 to 0
Jul 16 14:17:06.051: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:17:06.059: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:17:06.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9634" for this suite.
Jul 16 14:17:14.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:17:14.645: INFO: namespace statefulset-9634 deletion completed in 8.539944522s

• [SLOW TEST:173.036 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:17:14.647: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3287
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 16 14:17:14.931: INFO: Found 0 stateful pods, waiting for 3
Jul 16 14:17:24.954: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:17:24.955: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:17:24.955: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 16 14:17:25.038: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 16 14:17:35.120: INFO: Updating stateful set ss2
Jul 16 14:17:35.134: INFO: Waiting for Pod statefulset-3287/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 16 14:17:45.152: INFO: Waiting for Pod statefulset-3287/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jul 16 14:17:56.053: INFO: Found 2 stateful pods, waiting for 3
Jul 16 14:18:06.063: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:18:06.063: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:18:06.063: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 16 14:18:06.127: INFO: Updating stateful set ss2
Jul 16 14:18:06.145: INFO: Waiting for Pod statefulset-3287/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 16 14:18:16.197: INFO: Updating stateful set ss2
Jul 16 14:18:16.239: INFO: Waiting for StatefulSet statefulset-3287/ss2 to complete update
Jul 16 14:18:16.239: INFO: Waiting for Pod statefulset-3287/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 16 14:18:26.287: INFO: Deleting all statefulset in ns statefulset-3287
Jul 16 14:18:26.294: INFO: Scaling statefulset ss2 to 0
Jul 16 14:18:56.481: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:18:56.491: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:18:56.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3287" for this suite.
Jul 16 14:19:04.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:19:05.128: INFO: namespace statefulset-3287 deletion completed in 8.557670495s

• [SLOW TEST:110.481 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:19:05.131: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-d305c4ad-74e0-4413-9b60-8b928fff868f
STEP: Creating a pod to test consume secrets
Jul 16 14:19:05.341: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-78da1301-de7c-4b74-ae58-4a4b376ef76c" in namespace "projected-1235" to be "success or failure"
Jul 16 14:19:05.356: INFO: Pod "pod-projected-secrets-78da1301-de7c-4b74-ae58-4a4b376ef76c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.283647ms
Jul 16 14:19:07.364: INFO: Pod "pod-projected-secrets-78da1301-de7c-4b74-ae58-4a4b376ef76c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022378255s
Jul 16 14:19:09.372: INFO: Pod "pod-projected-secrets-78da1301-de7c-4b74-ae58-4a4b376ef76c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030737638s
STEP: Saw pod success
Jul 16 14:19:09.372: INFO: Pod "pod-projected-secrets-78da1301-de7c-4b74-ae58-4a4b376ef76c" satisfied condition "success or failure"
Jul 16 14:19:09.379: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-secrets-78da1301-de7c-4b74-ae58-4a4b376ef76c container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:19:09.444: INFO: Waiting for pod pod-projected-secrets-78da1301-de7c-4b74-ae58-4a4b376ef76c to disappear
Jul 16 14:19:09.456: INFO: Pod pod-projected-secrets-78da1301-de7c-4b74-ae58-4a4b376ef76c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:19:09.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1235" for this suite.
Jul 16 14:19:15.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:19:15.922: INFO: namespace projected-1235 deletion completed in 6.456501915s

• [SLOW TEST:10.791 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:19:15.929: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-a215c4b3-07a0-490f-ae91-c5d8faa11585 in namespace container-probe-3554
Jul 16 14:19:20.080: INFO: Started pod test-webserver-a215c4b3-07a0-490f-ae91-c5d8faa11585 in namespace container-probe-3554
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 14:19:20.096: INFO: Initial restart count of pod test-webserver-a215c4b3-07a0-490f-ae91-c5d8faa11585 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:23:21.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3554" for this suite.
Jul 16 14:23:27.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:23:27.765: INFO: namespace container-probe-3554 deletion completed in 6.333079084s

• [SLOW TEST:251.837 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:23:27.766: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-b7385517-3570-4874-9c01-98d1ff94a978
STEP: Creating a pod to test consume configMaps
Jul 16 14:23:27.962: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ddb380b6-1325-46fa-b5bc-31e1cc28fb75" in namespace "projected-9667" to be "success or failure"
Jul 16 14:23:27.967: INFO: Pod "pod-projected-configmaps-ddb380b6-1325-46fa-b5bc-31e1cc28fb75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.844892ms
Jul 16 14:23:29.977: INFO: Pod "pod-projected-configmaps-ddb380b6-1325-46fa-b5bc-31e1cc28fb75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014651261s
Jul 16 14:23:31.983: INFO: Pod "pod-projected-configmaps-ddb380b6-1325-46fa-b5bc-31e1cc28fb75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020576811s
STEP: Saw pod success
Jul 16 14:23:31.983: INFO: Pod "pod-projected-configmaps-ddb380b6-1325-46fa-b5bc-31e1cc28fb75" satisfied condition "success or failure"
Jul 16 14:23:31.988: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-configmaps-ddb380b6-1325-46fa-b5bc-31e1cc28fb75 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:23:32.033: INFO: Waiting for pod pod-projected-configmaps-ddb380b6-1325-46fa-b5bc-31e1cc28fb75 to disappear
Jul 16 14:23:32.038: INFO: Pod pod-projected-configmaps-ddb380b6-1325-46fa-b5bc-31e1cc28fb75 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:23:32.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9667" for this suite.
Jul 16 14:23:38.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:23:38.432: INFO: namespace projected-9667 deletion completed in 6.38377737s

• [SLOW TEST:10.667 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:23:38.435: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-jb62
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 14:23:38.595: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jb62" in namespace "subpath-7102" to be "success or failure"
Jul 16 14:23:38.603: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081105ms
Jul 16 14:23:40.618: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022853369s
Jul 16 14:23:42.625: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 4.029798202s
Jul 16 14:23:44.631: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 6.035918125s
Jul 16 14:23:46.638: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 8.042843976s
Jul 16 14:23:48.648: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 10.053142304s
Jul 16 14:23:50.657: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 12.061895674s
Jul 16 14:23:52.665: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 14.069648636s
Jul 16 14:23:54.671: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 16.076033108s
Jul 16 14:23:56.685: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 18.08937585s
Jul 16 14:23:58.694: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 20.099097754s
Jul 16 14:24:00.707: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Running", Reason="", readiness=true. Elapsed: 22.111335554s
Jul 16 14:24:02.714: INFO: Pod "pod-subpath-test-secret-jb62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.118719272s
STEP: Saw pod success
Jul 16 14:24:02.714: INFO: Pod "pod-subpath-test-secret-jb62" satisfied condition "success or failure"
Jul 16 14:24:02.719: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-subpath-test-secret-jb62 container test-container-subpath-secret-jb62: <nil>
STEP: delete the pod
Jul 16 14:24:02.864: INFO: Waiting for pod pod-subpath-test-secret-jb62 to disappear
Jul 16 14:24:02.869: INFO: Pod pod-subpath-test-secret-jb62 no longer exists
STEP: Deleting pod pod-subpath-test-secret-jb62
Jul 16 14:24:02.869: INFO: Deleting pod "pod-subpath-test-secret-jb62" in namespace "subpath-7102"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:24:02.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7102" for this suite.
Jul 16 14:24:08.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:24:09.274: INFO: namespace subpath-7102 deletion completed in 6.389916416s

• [SLOW TEST:30.840 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:24:09.280: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:24:09.396: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9854da80-c6a2-4871-ae51-1db07039dd77" in namespace "projected-4958" to be "success or failure"
Jul 16 14:24:09.409: INFO: Pod "downwardapi-volume-9854da80-c6a2-4871-ae51-1db07039dd77": Phase="Pending", Reason="", readiness=false. Elapsed: 12.698242ms
Jul 16 14:24:11.417: INFO: Pod "downwardapi-volume-9854da80-c6a2-4871-ae51-1db07039dd77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020818097s
Jul 16 14:24:13.424: INFO: Pod "downwardapi-volume-9854da80-c6a2-4871-ae51-1db07039dd77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027636787s
STEP: Saw pod success
Jul 16 14:24:13.424: INFO: Pod "downwardapi-volume-9854da80-c6a2-4871-ae51-1db07039dd77" satisfied condition "success or failure"
Jul 16 14:24:13.429: INFO: Trying to get logs from node conformance-worker-54b54f4f98-m9svg pod downwardapi-volume-9854da80-c6a2-4871-ae51-1db07039dd77 container client-container: <nil>
STEP: delete the pod
Jul 16 14:24:13.498: INFO: Waiting for pod downwardapi-volume-9854da80-c6a2-4871-ae51-1db07039dd77 to disappear
Jul 16 14:24:13.510: INFO: Pod downwardapi-volume-9854da80-c6a2-4871-ae51-1db07039dd77 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:24:13.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4958" for this suite.
Jul 16 14:24:19.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:24:19.877: INFO: namespace projected-4958 deletion completed in 6.355802128s

• [SLOW TEST:10.597 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:24:19.879: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:24:20.640: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a515a767-e819-45bc-a9ff-461e26aca70b" in namespace "downward-api-9514" to be "success or failure"
Jul 16 14:24:20.645: INFO: Pod "downwardapi-volume-a515a767-e819-45bc-a9ff-461e26aca70b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.634769ms
Jul 16 14:24:22.655: INFO: Pod "downwardapi-volume-a515a767-e819-45bc-a9ff-461e26aca70b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014401163s
Jul 16 14:24:24.663: INFO: Pod "downwardapi-volume-a515a767-e819-45bc-a9ff-461e26aca70b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022264148s
STEP: Saw pod success
Jul 16 14:24:24.663: INFO: Pod "downwardapi-volume-a515a767-e819-45bc-a9ff-461e26aca70b" satisfied condition "success or failure"
Jul 16 14:24:24.669: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-a515a767-e819-45bc-a9ff-461e26aca70b container client-container: <nil>
STEP: delete the pod
Jul 16 14:24:24.717: INFO: Waiting for pod downwardapi-volume-a515a767-e819-45bc-a9ff-461e26aca70b to disappear
Jul 16 14:24:24.724: INFO: Pod downwardapi-volume-a515a767-e819-45bc-a9ff-461e26aca70b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:24:24.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9514" for this suite.
Jul 16 14:24:30.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:24:31.077: INFO: namespace downward-api-9514 deletion completed in 6.336885274s

• [SLOW TEST:11.198 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:24:31.077: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 16 14:24:35.960: INFO: Successfully updated pod "labelsupdate25fc99f3-babe-4831-9c05-505eaedfee6a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:24:38.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9478" for this suite.
Jul 16 14:25:02.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:25:02.541: INFO: namespace downward-api-9478 deletion completed in 24.481515739s

• [SLOW TEST:31.465 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:25:02.546: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 14:25:24.704: INFO: Container started at 2019-07-16 14:25:04 +0000 UTC, pod became ready at 2019-07-16 14:25:23 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:25:24.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-615" for this suite.
Jul 16 14:25:48.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:25:49.046: INFO: namespace container-probe-615 deletion completed in 24.329213134s

• [SLOW TEST:46.501 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:25:49.053: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 16 14:25:52.239: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:25:52.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4721" for this suite.
Jul 16 14:25:58.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:25:58.733: INFO: namespace container-runtime-4721 deletion completed in 6.453677997s

• [SLOW TEST:9.680 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:25:58.734: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:25:58.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c262ad2-1a9b-499f-9e59-4268ed4e3fac" in namespace "projected-318" to be "success or failure"
Jul 16 14:25:58.836: INFO: Pod "downwardapi-volume-8c262ad2-1a9b-499f-9e59-4268ed4e3fac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.530353ms
Jul 16 14:26:00.843: INFO: Pod "downwardapi-volume-8c262ad2-1a9b-499f-9e59-4268ed4e3fac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011106575s
Jul 16 14:26:02.859: INFO: Pod "downwardapi-volume-8c262ad2-1a9b-499f-9e59-4268ed4e3fac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026746122s
STEP: Saw pod success
Jul 16 14:26:02.859: INFO: Pod "downwardapi-volume-8c262ad2-1a9b-499f-9e59-4268ed4e3fac" satisfied condition "success or failure"
Jul 16 14:26:02.864: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-8c262ad2-1a9b-499f-9e59-4268ed4e3fac container client-container: <nil>
STEP: delete the pod
Jul 16 14:26:02.982: INFO: Waiting for pod downwardapi-volume-8c262ad2-1a9b-499f-9e59-4268ed4e3fac to disappear
Jul 16 14:26:02.987: INFO: Pod downwardapi-volume-8c262ad2-1a9b-499f-9e59-4268ed4e3fac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:26:02.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-318" for this suite.
Jul 16 14:26:11.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:26:11.306: INFO: namespace projected-318 deletion completed in 8.310375768s

• [SLOW TEST:12.573 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:26:11.309: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:26:15.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-691" for this suite.
Jul 16 14:26:57.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:26:58.372: INFO: namespace kubelet-test-691 deletion completed in 42.421880096s

• [SLOW TEST:47.064 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:26:58.374: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:26:58.486: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0feb8c43-6b36-44c0-a0c5-7926b3f1e8c6" in namespace "downward-api-7455" to be "success or failure"
Jul 16 14:26:58.491: INFO: Pod "downwardapi-volume-0feb8c43-6b36-44c0-a0c5-7926b3f1e8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.170309ms
Jul 16 14:27:00.505: INFO: Pod "downwardapi-volume-0feb8c43-6b36-44c0-a0c5-7926b3f1e8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019166562s
Jul 16 14:27:02.514: INFO: Pod "downwardapi-volume-0feb8c43-6b36-44c0-a0c5-7926b3f1e8c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028063326s
STEP: Saw pod success
Jul 16 14:27:02.514: INFO: Pod "downwardapi-volume-0feb8c43-6b36-44c0-a0c5-7926b3f1e8c6" satisfied condition "success or failure"
Jul 16 14:27:02.521: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-0feb8c43-6b36-44c0-a0c5-7926b3f1e8c6 container client-container: <nil>
STEP: delete the pod
Jul 16 14:27:02.621: INFO: Waiting for pod downwardapi-volume-0feb8c43-6b36-44c0-a0c5-7926b3f1e8c6 to disappear
Jul 16 14:27:02.646: INFO: Pod downwardapi-volume-0feb8c43-6b36-44c0-a0c5-7926b3f1e8c6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:27:02.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7455" for this suite.
Jul 16 14:27:08.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:27:09.105: INFO: namespace downward-api-7455 deletion completed in 6.444029793s

• [SLOW TEST:10.732 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:27:09.113: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jul 16 14:27:09.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-7912'
Jul 16 14:27:10.574: INFO: stderr: ""
Jul 16 14:27:10.574: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 14:27:10.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7912'
Jul 16 14:27:10.735: INFO: stderr: ""
Jul 16 14:27:10.735: INFO: stdout: "update-demo-nautilus-cbfrf update-demo-nautilus-kbgpw "
Jul 16 14:27:10.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cbfrf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7912'
Jul 16 14:27:10.888: INFO: stderr: ""
Jul 16 14:27:10.888: INFO: stdout: ""
Jul 16 14:27:10.888: INFO: update-demo-nautilus-cbfrf is created but not running
Jul 16 14:27:15.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7912'
Jul 16 14:27:16.076: INFO: stderr: ""
Jul 16 14:27:16.076: INFO: stdout: "update-demo-nautilus-cbfrf update-demo-nautilus-kbgpw "
Jul 16 14:27:16.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cbfrf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7912'
Jul 16 14:27:16.293: INFO: stderr: ""
Jul 16 14:27:16.293: INFO: stdout: "true"
Jul 16 14:27:16.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-cbfrf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7912'
Jul 16 14:27:16.410: INFO: stderr: ""
Jul 16 14:27:16.411: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 14:27:16.411: INFO: validating pod update-demo-nautilus-cbfrf
Jul 16 14:27:16.533: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 14:27:16.534: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 14:27:16.534: INFO: update-demo-nautilus-cbfrf is verified up and running
Jul 16 14:27:16.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-kbgpw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7912'
Jul 16 14:27:16.666: INFO: stderr: ""
Jul 16 14:27:16.666: INFO: stdout: "true"
Jul 16 14:27:16.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-nautilus-kbgpw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7912'
Jul 16 14:27:16.798: INFO: stderr: ""
Jul 16 14:27:16.798: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 16 14:27:16.798: INFO: validating pod update-demo-nautilus-kbgpw
Jul 16 14:27:32.191: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 16 14:27:32.193: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 16 14:27:32.193: INFO: update-demo-nautilus-kbgpw is verified up and running
STEP: rolling-update to new replication controller
Jul 16 14:27:32.199: INFO: scanned /root for discovery docs: <nil>
Jul 16 14:27:32.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7912'
Jul 16 14:27:57.422: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 16 14:27:57.422: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 16 14:27:57.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7912'
Jul 16 14:27:57.601: INFO: stderr: ""
Jul 16 14:27:57.601: INFO: stdout: "update-demo-kitten-ct2qd update-demo-kitten-z5j9d "
Jul 16 14:27:57.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-kitten-ct2qd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7912'
Jul 16 14:27:57.777: INFO: stderr: ""
Jul 16 14:27:57.777: INFO: stdout: "true"
Jul 16 14:27:57.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-kitten-ct2qd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7912'
Jul 16 14:27:57.881: INFO: stderr: ""
Jul 16 14:27:57.881: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 16 14:27:57.881: INFO: validating pod update-demo-kitten-ct2qd
Jul 16 14:27:57.995: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 16 14:27:57.995: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 16 14:27:57.995: INFO: update-demo-kitten-ct2qd is verified up and running
Jul 16 14:27:57.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-kitten-z5j9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7912'
Jul 16 14:27:58.119: INFO: stderr: ""
Jul 16 14:27:58.119: INFO: stdout: "true"
Jul 16 14:27:58.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods update-demo-kitten-z5j9d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7912'
Jul 16 14:27:58.246: INFO: stderr: ""
Jul 16 14:27:58.246: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 16 14:27:58.246: INFO: validating pod update-demo-kitten-z5j9d
Jul 16 14:27:58.389: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 16 14:27:58.389: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 16 14:27:58.389: INFO: update-demo-kitten-z5j9d is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:27:58.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7912" for this suite.
Jul 16 14:28:22.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:28:22.775: INFO: namespace kubectl-7912 deletion completed in 24.373493212s

• [SLOW TEST:73.663 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:28:22.782: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jul 16 14:28:22.947: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-981533691 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:28:23.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3624" for this suite.
Jul 16 14:28:29.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:28:29.910: INFO: namespace kubectl-3624 deletion completed in 6.80410287s

• [SLOW TEST:7.128 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:28:29.914: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:28:57.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9102" for this suite.
Jul 16 14:29:03.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:29:03.881: INFO: namespace namespaces-9102 deletion completed in 6.356553832s
STEP: Destroying namespace "nsdeletetest-7348" for this suite.
Jul 16 14:29:03.897: INFO: Namespace nsdeletetest-7348 was already deleted
STEP: Destroying namespace "nsdeletetest-444" for this suite.
Jul 16 14:29:09.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:29:10.322: INFO: namespace nsdeletetest-444 deletion completed in 6.425353952s

• [SLOW TEST:40.409 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:29:10.323: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:29:35.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5918" for this suite.
Jul 16 14:29:42.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:29:42.474: INFO: namespace container-runtime-5918 deletion completed in 6.472927596s

• [SLOW TEST:32.151 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:29:42.477: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 16 14:29:42.672: INFO: Number of nodes with available pods: 0
Jul 16 14:29:42.672: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 14:29:43.690: INFO: Number of nodes with available pods: 0
Jul 16 14:29:43.690: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 14:29:44.718: INFO: Number of nodes with available pods: 0
Jul 16 14:29:44.719: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 14:29:45.690: INFO: Number of nodes with available pods: 3
Jul 16 14:29:45.690: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 16 14:29:45.760: INFO: Number of nodes with available pods: 3
Jul 16 14:29:45.760: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5818, will wait for the garbage collector to delete the pods
Jul 16 14:29:45.897: INFO: Deleting DaemonSet.extensions daemon-set took: 33.752184ms
Jul 16 14:29:46.398: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.980143ms
Jul 16 14:29:58.712: INFO: Number of nodes with available pods: 0
Jul 16 14:29:58.713: INFO: Number of running nodes: 0, number of available pods: 0
Jul 16 14:29:58.720: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5818/daemonsets","resourceVersion":"27374"},"items":null}

Jul 16 14:29:58.727: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5818/pods","resourceVersion":"27374"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:29:58.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5818" for this suite.
Jul 16 14:30:04.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:30:05.175: INFO: namespace daemonsets-5818 deletion completed in 6.39379235s

• [SLOW TEST:22.698 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:30:05.183: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 16 14:30:09.343: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-cc66d149-495f-4261-8813-ca99ad8b6972,GenerateName:,Namespace:events-3476,SelfLink:/api/v1/namespaces/events-3476/pods/send-events-cc66d149-495f-4261-8813-ca99ad8b6972,UID:d50fc699-a085-47c5-b3a8-17192a41fba6,ResourceVersion:27443,Generation:0,CreationTimestamp:2019-07-16 14:30:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 294860218,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-79k2l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-79k2l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-79k2l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003d2e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0003d2e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:30:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:30:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:30:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:30:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:172.25.1.182,StartTime:2019-07-16 14:30:05 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-16 14:30:07 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://2fb32a2cc9e7fafa54763767be526302e461b230d7097bc0c46283230f71c96e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul 16 14:30:11.353: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 16 14:30:13.362: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:30:13.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3476" for this suite.
Jul 16 14:30:53.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:30:53.859: INFO: namespace events-3476 deletion completed in 40.373850511s

• [SLOW TEST:48.677 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:30:53.860: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-54cd9e7c-f90b-470a-b5fc-186e3db09ef5
STEP: Creating a pod to test consume configMaps
Jul 16 14:30:54.068: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e3ddd89-13cc-4a10-a41c-f4254318dae6" in namespace "projected-3369" to be "success or failure"
Jul 16 14:30:54.072: INFO: Pod "pod-projected-configmaps-4e3ddd89-13cc-4a10-a41c-f4254318dae6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.551809ms
Jul 16 14:30:56.084: INFO: Pod "pod-projected-configmaps-4e3ddd89-13cc-4a10-a41c-f4254318dae6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016452736s
Jul 16 14:30:59.132: INFO: Pod "pod-projected-configmaps-4e3ddd89-13cc-4a10-a41c-f4254318dae6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.064405554s
STEP: Saw pod success
Jul 16 14:30:59.132: INFO: Pod "pod-projected-configmaps-4e3ddd89-13cc-4a10-a41c-f4254318dae6" satisfied condition "success or failure"
Jul 16 14:30:59.139: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-configmaps-4e3ddd89-13cc-4a10-a41c-f4254318dae6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:30:59.264: INFO: Waiting for pod pod-projected-configmaps-4e3ddd89-13cc-4a10-a41c-f4254318dae6 to disappear
Jul 16 14:30:59.268: INFO: Pod pod-projected-configmaps-4e3ddd89-13cc-4a10-a41c-f4254318dae6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:30:59.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3369" for this suite.
Jul 16 14:31:05.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:31:05.632: INFO: namespace projected-3369 deletion completed in 6.354525112s

• [SLOW TEST:11.772 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:31:05.636: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:31:05.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4726355-367d-48ec-a0bc-35e890f55658" in namespace "projected-1345" to be "success or failure"
Jul 16 14:31:05.755: INFO: Pod "downwardapi-volume-d4726355-367d-48ec-a0bc-35e890f55658": Phase="Pending", Reason="", readiness=false. Elapsed: 4.496159ms
Jul 16 14:31:07.763: INFO: Pod "downwardapi-volume-d4726355-367d-48ec-a0bc-35e890f55658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013203142s
Jul 16 14:31:09.775: INFO: Pod "downwardapi-volume-d4726355-367d-48ec-a0bc-35e890f55658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024794248s
STEP: Saw pod success
Jul 16 14:31:09.775: INFO: Pod "downwardapi-volume-d4726355-367d-48ec-a0bc-35e890f55658" satisfied condition "success or failure"
Jul 16 14:31:09.781: INFO: Trying to get logs from node conformance-worker-54b54f4f98-m9svg pod downwardapi-volume-d4726355-367d-48ec-a0bc-35e890f55658 container client-container: <nil>
STEP: delete the pod
Jul 16 14:31:09.837: INFO: Waiting for pod downwardapi-volume-d4726355-367d-48ec-a0bc-35e890f55658 to disappear
Jul 16 14:31:09.843: INFO: Pod downwardapi-volume-d4726355-367d-48ec-a0bc-35e890f55658 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:31:09.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1345" for this suite.
Jul 16 14:31:15.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:31:16.233: INFO: namespace projected-1345 deletion completed in 6.379320713s

• [SLOW TEST:10.597 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:31:16.236: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 16 14:31:20.917: INFO: Successfully updated pod "pod-update-18ef8fa2-f4bc-4e2c-990d-b3585b893961"
STEP: verifying the updated pod is in kubernetes
Jul 16 14:31:20.932: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:31:20.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1906" for this suite.
Jul 16 14:31:44.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:31:45.270: INFO: namespace pods-1906 deletion completed in 24.32914212s

• [SLOW TEST:29.035 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:31:45.273: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:31:51.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2256" for this suite.
Jul 16 14:31:57.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:31:58.055: INFO: namespace namespaces-2256 deletion completed in 6.334414186s
STEP: Destroying namespace "nsdeletetest-2976" for this suite.
Jul 16 14:31:58.060: INFO: Namespace nsdeletetest-2976 was already deleted
STEP: Destroying namespace "nsdeletetest-6833" for this suite.
Jul 16 14:32:04.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:32:04.366: INFO: namespace nsdeletetest-6833 deletion completed in 6.30553004s

• [SLOW TEST:19.093 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:32:04.368: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-495ca3f1-354e-4ab9-ba39-83dfeeb2ff4e
STEP: Creating a pod to test consume configMaps
Jul 16 14:32:04.518: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1916bda2-cab5-483a-85ab-90a715f86fd5" in namespace "projected-1871" to be "success or failure"
Jul 16 14:32:04.524: INFO: Pod "pod-projected-configmaps-1916bda2-cab5-483a-85ab-90a715f86fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.202444ms
Jul 16 14:32:06.545: INFO: Pod "pod-projected-configmaps-1916bda2-cab5-483a-85ab-90a715f86fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026533319s
Jul 16 14:32:08.557: INFO: Pod "pod-projected-configmaps-1916bda2-cab5-483a-85ab-90a715f86fd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038991049s
STEP: Saw pod success
Jul 16 14:32:08.557: INFO: Pod "pod-projected-configmaps-1916bda2-cab5-483a-85ab-90a715f86fd5" satisfied condition "success or failure"
Jul 16 14:32:08.562: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-configmaps-1916bda2-cab5-483a-85ab-90a715f86fd5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:32:08.621: INFO: Waiting for pod pod-projected-configmaps-1916bda2-cab5-483a-85ab-90a715f86fd5 to disappear
Jul 16 14:32:08.627: INFO: Pod pod-projected-configmaps-1916bda2-cab5-483a-85ab-90a715f86fd5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:32:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1871" for this suite.
Jul 16 14:32:14.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:32:14.990: INFO: namespace projected-1871 deletion completed in 6.34675157s

• [SLOW TEST:10.623 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:32:14.992: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:32:19.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5592" for this suite.
Jul 16 14:33:00.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:33:00.623: INFO: namespace kubelet-test-5592 deletion completed in 41.456938784s

• [SLOW TEST:45.631 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:33:00.624: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 14:33:00.801: INFO: Create a RollingUpdate DaemonSet
Jul 16 14:33:00.818: INFO: Check that daemon pods launch on every node of the cluster
Jul 16 14:33:00.853: INFO: Number of nodes with available pods: 0
Jul 16 14:33:00.854: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 14:33:01.870: INFO: Number of nodes with available pods: 0
Jul 16 14:33:01.870: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 14:33:02.869: INFO: Number of nodes with available pods: 0
Jul 16 14:33:02.869: INFO: Node conformance-worker-54b54f4f98-f9trz is running more than one daemon pod
Jul 16 14:33:03.869: INFO: Number of nodes with available pods: 3
Jul 16 14:33:03.869: INFO: Number of running nodes: 3, number of available pods: 3
Jul 16 14:33:03.869: INFO: Update the DaemonSet to trigger a rollout
Jul 16 14:33:03.885: INFO: Updating DaemonSet daemon-set
Jul 16 14:33:15.591: INFO: Roll back the DaemonSet before rollout is complete
Jul 16 14:33:15.736: INFO: Updating DaemonSet daemon-set
Jul 16 14:33:15.736: INFO: Make sure DaemonSet rollback is complete
Jul 16 14:33:15.742: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:15.742: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:16.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:16.757: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:17.789: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:17.789: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:18.761: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:18.761: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:19.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:19.757: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:20.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:20.757: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:21.756: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:21.756: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:22.760: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:22.760: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:23.760: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:23.761: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:24.760: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:24.760: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:25.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:25.757: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:26.759: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:26.759: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:27.842: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:27.843: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:28.765: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:28.765: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:29.760: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:29.761: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:30.760: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:30.760: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:31.762: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:31.762: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:32.760: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:32.760: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:33.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:33.758: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:34.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:34.757: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:35.760: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:35.760: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:36.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:36.757: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:37.795: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:37.795: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:38.758: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:38.758: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:39.758: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:39.758: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:40.756: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:40.756: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:41.758: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:41.758: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:42.759: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:42.759: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:43.763: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:43.763: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:44.759: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:44.759: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:45.758: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:45.758: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:46.765: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:46.765: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:47.771: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:47.771: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:48.762: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:48.762: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:49.764: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:49.764: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:50.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:50.757: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:51.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:51.758: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:52.775: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:52.775: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:53.757: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:53.757: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:54.756: INFO: Wrong image for pod: daemon-set-kxrx6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 16 14:33:54.757: INFO: Pod daemon-set-kxrx6 is not available
Jul 16 14:33:55.760: INFO: Pod daemon-set-kthcb is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2225, will wait for the garbage collector to delete the pods
Jul 16 14:33:55.876: INFO: Deleting DaemonSet.extensions daemon-set took: 24.878554ms
Jul 16 14:33:56.478: INFO: Terminating DaemonSet.extensions daemon-set pods took: 601.372888ms
Jul 16 14:34:30.287: INFO: Number of nodes with available pods: 0
Jul 16 14:34:30.287: INFO: Number of running nodes: 0, number of available pods: 0
Jul 16 14:34:30.296: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2225/daemonsets","resourceVersion":"28503"},"items":null}

Jul 16 14:34:30.301: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2225/pods","resourceVersion":"28503"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:34:30.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2225" for this suite.
Jul 16 14:34:42.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:34:42.694: INFO: namespace daemonsets-2225 deletion completed in 12.354652908s

• [SLOW TEST:102.070 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:34:42.700: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 14:34:42.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-5485'
Jul 16 14:34:42.999: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 14:34:42.999: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jul 16 14:34:47.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5485'
Jul 16 14:34:47.188: INFO: stderr: ""
Jul 16 14:34:47.188: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:34:47.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5485" for this suite.
Jul 16 14:35:11.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:35:12.043: INFO: namespace kubectl-5485 deletion completed in 24.845544637s

• [SLOW TEST:29.344 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:35:12.047: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 14:35:12.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-5213'
Jul 16 14:35:13.551: INFO: stderr: ""
Jul 16 14:35:13.551: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 16 14:35:13.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-5213'
Jul 16 14:35:13.951: INFO: stderr: ""
Jul 16 14:35:13.952: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 16 14:35:14.964: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:35:14.964: INFO: Found 0 / 1
Jul 16 14:35:15.965: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:35:15.965: INFO: Found 0 / 1
Jul 16 14:35:16.959: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:35:16.959: INFO: Found 1 / 1
Jul 16 14:35:16.959: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 16 14:35:16.965: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:35:16.965: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 16 14:35:16.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 describe pod redis-master-fqq9j --namespace=kubectl-5213'
Jul 16 14:35:17.185: INFO: stderr: ""
Jul 16 14:35:17.185: INFO: stdout: "Name:           redis-master-fqq9j\nNamespace:      kubectl-5213\nPriority:       0\nNode:           conformance-worker-54b54f4f98-f9trz/192.168.1.2\nStart Time:     Tue, 16 Jul 2019 14:35:13 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             172.25.1.189\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://651da9df8fed11077c910e1a22f7576b104d114f88f4d58f058ff14a59cbc262\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 16 Jul 2019 14:35:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-cxjlw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-cxjlw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-cxjlw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                          Message\n  ----    ------     ----  ----                                          -------\n  Normal  Scheduled  4s    default-scheduler                             Successfully assigned kubectl-5213/redis-master-fqq9j to conformance-worker-54b54f4f98-f9trz\n  Normal  Pulled     2s    kubelet, conformance-worker-54b54f4f98-f9trz  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, conformance-worker-54b54f4f98-f9trz  Created container redis-master\n  Normal  Started    1s    kubelet, conformance-worker-54b54f4f98-f9trz  Started container redis-master\n"
Jul 16 14:35:17.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 describe rc redis-master --namespace=kubectl-5213'
Jul 16 14:35:17.366: INFO: stderr: ""
Jul 16 14:35:17.366: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5213\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-fqq9j\n"
Jul 16 14:35:17.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 describe service redis-master --namespace=kubectl-5213'
Jul 16 14:35:17.536: INFO: stderr: ""
Jul 16 14:35:17.536: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5213\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.10.10.154\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.25.1.189:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 16 14:35:17.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 describe node conformance-worker-54b54f4f98-f9trz'
Jul 16 14:35:17.777: INFO: stderr: ""
Jul 16 14:35:17.777: INFO: stdout: "Name:               conformance-worker-54b54f4f98-f9trz\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=5cefe5e0-6d3c-4d37-b0e4-838f620190a1\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=cbk\n                    failure-domain.beta.kubernetes.io/zone=cbk1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=conformance-worker-54b54f4f98-f9trz\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=dd393eae-f809-41ba-9bbf-96c20fedd9dd\nAnnotations:        cluster.k8s.io/machine: kube-system/conformance-worker-54b54f4f98-f9trz\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"ae:ca:23:fb:a1:ca\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.2\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 16 Jul 2019 12:58:26 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 16 Jul 2019 14:34:48 +0000   Tue, 16 Jul 2019 12:58:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 16 Jul 2019 14:34:48 +0000   Tue, 16 Jul 2019 12:58:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 16 Jul 2019 14:34:48 +0000   Tue, 16 Jul 2019 12:58:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 16 Jul 2019 14:34:48 +0000   Tue, 16 Jul 2019 12:59:16 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.1.2\n  ExternalIP:  185.56.129.204\n  Hostname:    conformance-worker-54b54f4f98-f9trz\nCapacity:\n attachable-volumes-cinder:  256\n cpu:                        2\n ephemeral-storage:          50633164Ki\n hugepages-2Mi:              0\n memory:                     8168208Ki\n pods:                       110\nAllocatable:\n attachable-volumes-cinder:  256\n cpu:                        1800m\n ephemeral-storage:          44516040218\n hugepages-2Mi:              0\n memory:                     7861008Ki\n pods:                       110\nSystem Info:\n Machine ID:                 4330b8cd3b3d4d90bb56d7c634e996dd\n System UUID:                4330B8CD-3B3D-4D90-BB56-D7C634E996DD\n Boot ID:                    98b46ec4-bc08-4c3e-adeb-d3fdbfa61d4a\n Kernel Version:             4.15.0-54-generic\n OS Image:                   Ubuntu 18.04.2 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.2\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     172.25.1.0/24\nProviderID:                  openstack:///4330b8cd-3b3d-4d90-bb56-d7c634e996dd\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-mgtcd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m\n  kube-system                canal-4r5mb                                                350m (19%)    100m (5%)   50Mi (0%)        50Mi (0%)      96m\n  kube-system                kube-proxy-9l5xc                                           75m (4%)      250m (13%)  50Mi (0%)        250Mi (3%)     96m\n  kube-system                node-exporter-dk6wg                                        3m (0%)       200m (11%)  16Mi (0%)        50Mi (0%)      96m\n  kubectl-5213               redis-master-fqq9j                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  velero                     restic-dqv4x                                               5m (0%)       100m (5%)   32Mi (0%)        300Mi (3%)     96m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        433m (24%)  650m (36%)\n  memory                     148Mi (1%)  650Mi (8%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-cinder  0           0\nEvents:\n  Type    Reason                   Age                From                                             Message\n  ----    ------                   ----               ----                                             -------\n  Normal  NodeHasSufficientMemory  96m (x7 over 97m)  kubelet, conformance-worker-54b54f4f98-f9trz     Node conformance-worker-54b54f4f98-f9trz status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    96m (x7 over 97m)  kubelet, conformance-worker-54b54f4f98-f9trz     Node conformance-worker-54b54f4f98-f9trz status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     96m (x7 over 97m)  kubelet, conformance-worker-54b54f4f98-f9trz     Node conformance-worker-54b54f4f98-f9trz status is now: NodeHasSufficientPID\n  Normal  Starting                 96m                kube-proxy, conformance-worker-54b54f4f98-f9trz  Starting kube-proxy.\n  Normal  NodeReady                96m                kubelet, conformance-worker-54b54f4f98-f9trz     Node conformance-worker-54b54f4f98-f9trz status is now: NodeReady\n"
Jul 16 14:35:17.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 describe namespace kubectl-5213'
Jul 16 14:35:17.945: INFO: stderr: ""
Jul 16 14:35:17.946: INFO: stdout: "Name:         kubectl-5213\nLabels:       e2e-framework=kubectl\n              e2e-run=022f6f13-70a3-4a75-b2a8-0ac8d2825466\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:35:17.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5213" for this suite.
Jul 16 14:35:41.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:35:42.316: INFO: namespace kubectl-5213 deletion completed in 24.361386407s

• [SLOW TEST:30.270 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:35:42.319: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5580
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5580
STEP: Creating statefulset with conflicting port in namespace statefulset-5580
STEP: Waiting until pod test-pod will start running in namespace statefulset-5580
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5580
Jul 16 14:35:46.508: INFO: Observed stateful pod in namespace: statefulset-5580, name: ss-0, uid: e78788d6-a7dd-417e-a31f-8f663e819aa3, status phase: Pending. Waiting for statefulset controller to delete.
Jul 16 14:35:52.812: INFO: Observed stateful pod in namespace: statefulset-5580, name: ss-0, uid: e78788d6-a7dd-417e-a31f-8f663e819aa3, status phase: Failed. Waiting for statefulset controller to delete.
Jul 16 14:35:52.861: INFO: Observed stateful pod in namespace: statefulset-5580, name: ss-0, uid: e78788d6-a7dd-417e-a31f-8f663e819aa3, status phase: Failed. Waiting for statefulset controller to delete.
Jul 16 14:35:52.895: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5580
STEP: Removing pod with conflicting port in namespace statefulset-5580
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5580 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 16 14:35:56.998: INFO: Deleting all statefulset in ns statefulset-5580
Jul 16 14:35:57.008: INFO: Scaling statefulset ss to 0
Jul 16 14:36:07.055: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:36:07.062: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:36:07.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5580" for this suite.
Jul 16 14:36:13.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:36:13.503: INFO: namespace statefulset-5580 deletion completed in 6.396870082s

• [SLOW TEST:31.183 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:36:13.504: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-c1c4054f-79f1-49d0-8198-0a193dc8f1d7
STEP: Creating a pod to test consume secrets
Jul 16 14:36:13.640: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2062cce-d180-42eb-81a5-1e6d51cbd09b" in namespace "projected-4552" to be "success or failure"
Jul 16 14:36:13.648: INFO: Pod "pod-projected-secrets-e2062cce-d180-42eb-81a5-1e6d51cbd09b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.145347ms
Jul 16 14:36:15.666: INFO: Pod "pod-projected-secrets-e2062cce-d180-42eb-81a5-1e6d51cbd09b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02577641s
Jul 16 14:36:17.680: INFO: Pod "pod-projected-secrets-e2062cce-d180-42eb-81a5-1e6d51cbd09b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039361399s
STEP: Saw pod success
Jul 16 14:36:17.680: INFO: Pod "pod-projected-secrets-e2062cce-d180-42eb-81a5-1e6d51cbd09b" satisfied condition "success or failure"
Jul 16 14:36:17.686: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-secrets-e2062cce-d180-42eb-81a5-1e6d51cbd09b container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:36:17.810: INFO: Waiting for pod pod-projected-secrets-e2062cce-d180-42eb-81a5-1e6d51cbd09b to disappear
Jul 16 14:36:17.838: INFO: Pod pod-projected-secrets-e2062cce-d180-42eb-81a5-1e6d51cbd09b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:36:17.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4552" for this suite.
Jul 16 14:36:25.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:36:26.541: INFO: namespace projected-4552 deletion completed in 8.663974181s

• [SLOW TEST:13.037 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:36:26.546: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-c33ca7c2-374d-46b2-9f64-eae0804688be
STEP: Creating a pod to test consume secrets
Jul 16 14:36:27.022: INFO: Waiting up to 5m0s for pod "pod-secrets-3fd12ecc-7675-47ca-8717-862668db1d76" in namespace "secrets-1753" to be "success or failure"
Jul 16 14:36:27.027: INFO: Pod "pod-secrets-3fd12ecc-7675-47ca-8717-862668db1d76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860993ms
Jul 16 14:36:29.034: INFO: Pod "pod-secrets-3fd12ecc-7675-47ca-8717-862668db1d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011683335s
Jul 16 14:36:31.053: INFO: Pod "pod-secrets-3fd12ecc-7675-47ca-8717-862668db1d76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030795568s
STEP: Saw pod success
Jul 16 14:36:31.053: INFO: Pod "pod-secrets-3fd12ecc-7675-47ca-8717-862668db1d76" satisfied condition "success or failure"
Jul 16 14:36:31.058: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-secrets-3fd12ecc-7675-47ca-8717-862668db1d76 container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:36:31.190: INFO: Waiting for pod pod-secrets-3fd12ecc-7675-47ca-8717-862668db1d76 to disappear
Jul 16 14:36:31.197: INFO: Pod pod-secrets-3fd12ecc-7675-47ca-8717-862668db1d76 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:36:31.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1753" for this suite.
Jul 16 14:36:37.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:36:37.554: INFO: namespace secrets-1753 deletion completed in 6.346782114s

• [SLOW TEST:11.009 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:36:37.558: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jul 16 14:36:37.655: INFO: Waiting up to 5m0s for pod "var-expansion-81b6755c-88cf-4033-9f69-df45c5384173" in namespace "var-expansion-6542" to be "success or failure"
Jul 16 14:36:37.661: INFO: Pod "var-expansion-81b6755c-88cf-4033-9f69-df45c5384173": Phase="Pending", Reason="", readiness=false. Elapsed: 5.121684ms
Jul 16 14:36:39.669: INFO: Pod "var-expansion-81b6755c-88cf-4033-9f69-df45c5384173": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013934251s
Jul 16 14:36:41.677: INFO: Pod "var-expansion-81b6755c-88cf-4033-9f69-df45c5384173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021045194s
STEP: Saw pod success
Jul 16 14:36:41.677: INFO: Pod "var-expansion-81b6755c-88cf-4033-9f69-df45c5384173" satisfied condition "success or failure"
Jul 16 14:36:41.683: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod var-expansion-81b6755c-88cf-4033-9f69-df45c5384173 container dapi-container: <nil>
STEP: delete the pod
Jul 16 14:36:41.775: INFO: Waiting for pod var-expansion-81b6755c-88cf-4033-9f69-df45c5384173 to disappear
Jul 16 14:36:41.785: INFO: Pod var-expansion-81b6755c-88cf-4033-9f69-df45c5384173 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:36:41.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6542" for this suite.
Jul 16 14:36:47.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:36:48.180: INFO: namespace var-expansion-6542 deletion completed in 6.386409925s

• [SLOW TEST:10.623 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:36:48.183: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-02958c5e-b4dd-4c09-bd87-9ba2d201e548
STEP: Creating a pod to test consume secrets
Jul 16 14:36:48.334: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5cdb89d5-5ef3-43c6-b0c3-f7cc62568c81" in namespace "projected-8063" to be "success or failure"
Jul 16 14:36:48.351: INFO: Pod "pod-projected-secrets-5cdb89d5-5ef3-43c6-b0c3-f7cc62568c81": Phase="Pending", Reason="", readiness=false. Elapsed: 16.848147ms
Jul 16 14:36:50.361: INFO: Pod "pod-projected-secrets-5cdb89d5-5ef3-43c6-b0c3-f7cc62568c81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026869441s
Jul 16 14:36:52.394: INFO: Pod "pod-projected-secrets-5cdb89d5-5ef3-43c6-b0c3-f7cc62568c81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059994168s
STEP: Saw pod success
Jul 16 14:36:52.395: INFO: Pod "pod-projected-secrets-5cdb89d5-5ef3-43c6-b0c3-f7cc62568c81" satisfied condition "success or failure"
Jul 16 14:36:52.407: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-projected-secrets-5cdb89d5-5ef3-43c6-b0c3-f7cc62568c81 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:36:52.464: INFO: Waiting for pod pod-projected-secrets-5cdb89d5-5ef3-43c6-b0c3-f7cc62568c81 to disappear
Jul 16 14:36:52.470: INFO: Pod pod-projected-secrets-5cdb89d5-5ef3-43c6-b0c3-f7cc62568c81 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:36:52.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8063" for this suite.
Jul 16 14:36:58.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:36:58.844: INFO: namespace projected-8063 deletion completed in 6.36460536s

• [SLOW TEST:10.662 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:36:58.866: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-9ef1df4e-430c-42c9-af07-c98c73289621 in namespace container-probe-7287
Jul 16 14:37:03.089: INFO: Started pod liveness-9ef1df4e-430c-42c9-af07-c98c73289621 in namespace container-probe-7287
STEP: checking the pod's current state and verifying that restartCount is present
Jul 16 14:37:03.095: INFO: Initial restart count of pod liveness-9ef1df4e-430c-42c9-af07-c98c73289621 is 0
Jul 16 14:37:23.201: INFO: Restart count of pod container-probe-7287/liveness-9ef1df4e-430c-42c9-af07-c98c73289621 is now 1 (20.106381693s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:37:23.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7287" for this suite.
Jul 16 14:37:29.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:37:29.735: INFO: namespace container-probe-7287 deletion completed in 6.480432736s

• [SLOW TEST:30.869 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:37:29.736: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-9409
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9409 to expose endpoints map[]
Jul 16 14:37:29.902: INFO: Get endpoints failed (12.142049ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jul 16 14:37:37.923: INFO: successfully validated that service endpoint-test2 in namespace services-9409 exposes endpoints map[] (8.033127391s elapsed)
STEP: Creating pod pod1 in namespace services-9409
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9409 to expose endpoints map[pod1:[80]]
Jul 16 14:37:41.045: INFO: successfully validated that service endpoint-test2 in namespace services-9409 exposes endpoints map[pod1:[80]] (3.081856145s elapsed)
STEP: Creating pod pod2 in namespace services-9409
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9409 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 16 14:37:44.928: INFO: successfully validated that service endpoint-test2 in namespace services-9409 exposes endpoints map[pod1:[80] pod2:[80]] (3.843412564s elapsed)
STEP: Deleting pod pod1 in namespace services-9409
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9409 to expose endpoints map[pod2:[80]]
Jul 16 14:37:45.177: INFO: successfully validated that service endpoint-test2 in namespace services-9409 exposes endpoints map[pod2:[80]] (198.91554ms elapsed)
STEP: Deleting pod pod2 in namespace services-9409
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9409 to expose endpoints map[]
Jul 16 14:37:45.428: INFO: successfully validated that service endpoint-test2 in namespace services-9409 exposes endpoints map[] (206.321168ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:37:45.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9409" for this suite.
Jul 16 14:38:09.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:38:10.123: INFO: namespace services-9409 deletion completed in 24.416898603s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:40.387 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:38:10.124: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-7965/secret-test-51907ba0-73f2-4f6b-93f3-0940656afc6a
STEP: Creating a pod to test consume secrets
Jul 16 14:38:10.280: INFO: Waiting up to 5m0s for pod "pod-configmaps-067be4a1-88db-49b1-8d2e-7f177c1be737" in namespace "secrets-7965" to be "success or failure"
Jul 16 14:38:10.286: INFO: Pod "pod-configmaps-067be4a1-88db-49b1-8d2e-7f177c1be737": Phase="Pending", Reason="", readiness=false. Elapsed: 5.462799ms
Jul 16 14:38:12.300: INFO: Pod "pod-configmaps-067be4a1-88db-49b1-8d2e-7f177c1be737": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020112028s
Jul 16 14:38:14.311: INFO: Pod "pod-configmaps-067be4a1-88db-49b1-8d2e-7f177c1be737": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031384651s
STEP: Saw pod success
Jul 16 14:38:14.312: INFO: Pod "pod-configmaps-067be4a1-88db-49b1-8d2e-7f177c1be737" satisfied condition "success or failure"
Jul 16 14:38:14.328: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-067be4a1-88db-49b1-8d2e-7f177c1be737 container env-test: <nil>
STEP: delete the pod
Jul 16 14:38:14.401: INFO: Waiting for pod pod-configmaps-067be4a1-88db-49b1-8d2e-7f177c1be737 to disappear
Jul 16 14:38:14.406: INFO: Pod pod-configmaps-067be4a1-88db-49b1-8d2e-7f177c1be737 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:38:14.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7965" for this suite.
Jul 16 14:38:22.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:38:22.860: INFO: namespace secrets-7965 deletion completed in 8.445899849s

• [SLOW TEST:12.736 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:38:22.861: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 16 14:38:33.277: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0716 14:38:33.277023      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:38:33.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1330" for this suite.
Jul 16 14:38:41.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:38:41.699: INFO: namespace gc-1330 deletion completed in 8.414675207s

• [SLOW TEST:18.838 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:38:41.700: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 16 14:38:41.841: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8629a47-32a8-47d6-9b7f-309cb98915bc" in namespace "projected-6128" to be "success or failure"
Jul 16 14:38:41.856: INFO: Pod "downwardapi-volume-b8629a47-32a8-47d6-9b7f-309cb98915bc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.200332ms
Jul 16 14:38:43.864: INFO: Pod "downwardapi-volume-b8629a47-32a8-47d6-9b7f-309cb98915bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022510514s
Jul 16 14:38:45.873: INFO: Pod "downwardapi-volume-b8629a47-32a8-47d6-9b7f-309cb98915bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031714463s
STEP: Saw pod success
Jul 16 14:38:45.873: INFO: Pod "downwardapi-volume-b8629a47-32a8-47d6-9b7f-309cb98915bc" satisfied condition "success or failure"
Jul 16 14:38:45.881: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod downwardapi-volume-b8629a47-32a8-47d6-9b7f-309cb98915bc container client-container: <nil>
STEP: delete the pod
Jul 16 14:38:45.994: INFO: Waiting for pod downwardapi-volume-b8629a47-32a8-47d6-9b7f-309cb98915bc to disappear
Jul 16 14:38:46.008: INFO: Pod downwardapi-volume-b8629a47-32a8-47d6-9b7f-309cb98915bc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:38:46.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6128" for this suite.
Jul 16 14:38:52.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:38:52.476: INFO: namespace projected-6128 deletion completed in 6.447734365s

• [SLOW TEST:10.776 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:38:52.477: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-0551ee96-84fe-4977-84ff-aaeaaa7cf619
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:38:52.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6352" for this suite.
Jul 16 14:38:58.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:38:59.110: INFO: namespace secrets-6352 deletion completed in 6.48972297s

• [SLOW TEST:6.633 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:38:59.111: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 16 14:38:59.217: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:39:04.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4629" for this suite.
Jul 16 14:39:10.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:39:11.088: INFO: namespace init-container-4629 deletion completed in 6.416297728s

• [SLOW TEST:11.977 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:39:11.089: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-7zhh
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 14:39:11.223: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7zhh" in namespace "subpath-3770" to be "success or failure"
Jul 16 14:39:11.248: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Pending", Reason="", readiness=false. Elapsed: 24.954181ms
Jul 16 14:39:13.266: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042631548s
Jul 16 14:39:15.274: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 4.051203889s
Jul 16 14:39:17.285: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 6.062162332s
Jul 16 14:39:19.305: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 8.081846479s
Jul 16 14:39:21.317: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 10.093540242s
Jul 16 14:39:23.330: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 12.107035346s
Jul 16 14:39:25.338: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 14.115180277s
Jul 16 14:39:27.345: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 16.122239112s
Jul 16 14:39:29.365: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 18.141705186s
Jul 16 14:39:31.372: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 20.148790944s
Jul 16 14:39:33.379: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Running", Reason="", readiness=true. Elapsed: 22.155709374s
Jul 16 14:39:35.387: INFO: Pod "pod-subpath-test-configmap-7zhh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.163365959s
STEP: Saw pod success
Jul 16 14:39:35.387: INFO: Pod "pod-subpath-test-configmap-7zhh" satisfied condition "success or failure"
Jul 16 14:39:35.395: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-subpath-test-configmap-7zhh container test-container-subpath-configmap-7zhh: <nil>
STEP: delete the pod
Jul 16 14:39:35.575: INFO: Waiting for pod pod-subpath-test-configmap-7zhh to disappear
Jul 16 14:39:35.581: INFO: Pod pod-subpath-test-configmap-7zhh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7zhh
Jul 16 14:39:35.581: INFO: Deleting pod "pod-subpath-test-configmap-7zhh" in namespace "subpath-3770"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:39:35.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3770" for this suite.
Jul 16 14:39:41.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:39:41.982: INFO: namespace subpath-3770 deletion completed in 6.348086711s

• [SLOW TEST:30.893 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:39:41.983: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul 16 14:39:46.188: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-981533691 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 16 14:39:56.463: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:39:56.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3357" for this suite.
Jul 16 14:40:02.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:40:03.150: INFO: namespace pods-3357 deletion completed in 6.662194654s

• [SLOW TEST:21.167 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:40:03.154: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4518
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 16 14:40:03.345: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 16 14:40:29.615: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.207:8080/dial?request=hostName&protocol=udp&host=172.25.0.43&port=8081&tries=1'] Namespace:pod-network-test-4518 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:40:29.615: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:41:01.886: INFO: Waiting for endpoints: map[]
Jul 16 14:41:01.897: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.207:8080/dial?request=hostName&protocol=udp&host=172.25.2.67&port=8081&tries=1'] Namespace:pod-network-test-4518 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:41:01.897: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:41:02.526: INFO: Waiting for endpoints: map[]
Jul 16 14:41:02.534: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.207:8080/dial?request=hostName&protocol=udp&host=172.25.1.206&port=8081&tries=1'] Namespace:pod-network-test-4518 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 16 14:41:02.534: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
Jul 16 14:41:03.231: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:41:03.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4518" for this suite.
Jul 16 14:41:27.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:41:27.708: INFO: namespace pod-network-test-4518 deletion completed in 24.466211802s

• [SLOW TEST:84.555 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:41:27.718: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jul 16 14:41:27.882: INFO: Waiting up to 5m0s for pod "client-containers-03c5b835-94f6-4268-9a5b-b32a3420cd46" in namespace "containers-3993" to be "success or failure"
Jul 16 14:41:27.890: INFO: Pod "client-containers-03c5b835-94f6-4268-9a5b-b32a3420cd46": Phase="Pending", Reason="", readiness=false. Elapsed: 7.954301ms
Jul 16 14:41:30.052: INFO: Pod "client-containers-03c5b835-94f6-4268-9a5b-b32a3420cd46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169836903s
Jul 16 14:41:32.063: INFO: Pod "client-containers-03c5b835-94f6-4268-9a5b-b32a3420cd46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180888666s
STEP: Saw pod success
Jul 16 14:41:32.063: INFO: Pod "client-containers-03c5b835-94f6-4268-9a5b-b32a3420cd46" satisfied condition "success or failure"
Jul 16 14:41:32.070: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod client-containers-03c5b835-94f6-4268-9a5b-b32a3420cd46 container test-container: <nil>
STEP: delete the pod
Jul 16 14:41:32.257: INFO: Waiting for pod client-containers-03c5b835-94f6-4268-9a5b-b32a3420cd46 to disappear
Jul 16 14:41:32.263: INFO: Pod client-containers-03c5b835-94f6-4268-9a5b-b32a3420cd46 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:41:32.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3993" for this suite.
Jul 16 14:41:38.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:41:38.734: INFO: namespace containers-3993 deletion completed in 6.462401231s

• [SLOW TEST:11.016 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:41:38.735: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 16 14:41:41.933: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:41:41.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5631" for this suite.
Jul 16 14:41:48.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:41:48.459: INFO: namespace container-runtime-5631 deletion completed in 6.426907981s

• [SLOW TEST:9.725 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:41:48.469: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-9767b239-1a03-4cfc-a130-b8e4aab38ee8
STEP: Creating a pod to test consume configMaps
Jul 16 14:41:48.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-37745904-f2e6-4f5a-8ddc-5347b3486555" in namespace "configmap-9510" to be "success or failure"
Jul 16 14:41:48.697: INFO: Pod "pod-configmaps-37745904-f2e6-4f5a-8ddc-5347b3486555": Phase="Pending", Reason="", readiness=false. Elapsed: 8.561015ms
Jul 16 14:41:50.706: INFO: Pod "pod-configmaps-37745904-f2e6-4f5a-8ddc-5347b3486555": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017618278s
Jul 16 14:41:52.796: INFO: Pod "pod-configmaps-37745904-f2e6-4f5a-8ddc-5347b3486555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.107272379s
STEP: Saw pod success
Jul 16 14:41:52.797: INFO: Pod "pod-configmaps-37745904-f2e6-4f5a-8ddc-5347b3486555" satisfied condition "success or failure"
Jul 16 14:41:52.814: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-37745904-f2e6-4f5a-8ddc-5347b3486555 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:41:52.922: INFO: Waiting for pod pod-configmaps-37745904-f2e6-4f5a-8ddc-5347b3486555 to disappear
Jul 16 14:41:52.928: INFO: Pod pod-configmaps-37745904-f2e6-4f5a-8ddc-5347b3486555 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:41:52.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9510" for this suite.
Jul 16 14:42:01.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:42:01.419: INFO: namespace configmap-9510 deletion completed in 8.483555827s

• [SLOW TEST:12.951 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:42:01.420: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-df2d4019-cba7-4c95-856a-ebe6240d5064
STEP: Creating a pod to test consume secrets
Jul 16 14:42:02.157: INFO: Waiting up to 5m0s for pod "pod-secrets-f14c60ce-fd10-403c-85f3-f34d74114bf0" in namespace "secrets-84" to be "success or failure"
Jul 16 14:42:02.164: INFO: Pod "pod-secrets-f14c60ce-fd10-403c-85f3-f34d74114bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124524ms
Jul 16 14:42:04.173: INFO: Pod "pod-secrets-f14c60ce-fd10-403c-85f3-f34d74114bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015458176s
Jul 16 14:42:06.182: INFO: Pod "pod-secrets-f14c60ce-fd10-403c-85f3-f34d74114bf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024918268s
STEP: Saw pod success
Jul 16 14:42:06.183: INFO: Pod "pod-secrets-f14c60ce-fd10-403c-85f3-f34d74114bf0" satisfied condition "success or failure"
Jul 16 14:42:06.188: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-secrets-f14c60ce-fd10-403c-85f3-f34d74114bf0 container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:42:06.288: INFO: Waiting for pod pod-secrets-f14c60ce-fd10-403c-85f3-f34d74114bf0 to disappear
Jul 16 14:42:06.295: INFO: Pod pod-secrets-f14c60ce-fd10-403c-85f3-f34d74114bf0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:42:06.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-84" for this suite.
Jul 16 14:42:14.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:42:14.781: INFO: namespace secrets-84 deletion completed in 8.476256558s
STEP: Destroying namespace "secret-namespace-744" for this suite.
Jul 16 14:42:22.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:42:23.186: INFO: namespace secret-namespace-744 deletion completed in 8.40483929s

• [SLOW TEST:21.766 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:42:23.191: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 16 14:42:23.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-4074'
Jul 16 14:42:24.373: INFO: stderr: ""
Jul 16 14:42:24.373: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 16 14:42:25.383: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:42:25.383: INFO: Found 0 / 1
Jul 16 14:42:26.386: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:42:26.386: INFO: Found 0 / 1
Jul 16 14:42:27.382: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:42:27.383: INFO: Found 1 / 1
Jul 16 14:42:27.383: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 16 14:42:27.390: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:42:27.390: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 16 14:42:27.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 patch pod redis-master-4svm9 --namespace=kubectl-4074 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 16 14:42:27.558: INFO: stderr: ""
Jul 16 14:42:27.558: INFO: stdout: "pod/redis-master-4svm9 patched\n"
STEP: checking annotations
Jul 16 14:42:27.564: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 14:42:27.564: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:42:27.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4074" for this suite.
Jul 16 14:42:51.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:42:52.332: INFO: namespace kubectl-4074 deletion completed in 24.757890519s

• [SLOW TEST:29.141 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:42:52.334: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 16 14:42:55.614: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:42:55.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7065" for this suite.
Jul 16 14:43:03.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:43:04.221: INFO: namespace container-runtime-7065 deletion completed in 8.525544579s

• [SLOW TEST:11.887 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:43:04.222: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 14:43:04.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7071'
Jul 16 14:43:04.524: INFO: stderr: ""
Jul 16 14:43:04.524: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jul 16 14:43:04.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete pods e2e-test-nginx-pod --namespace=kubectl-7071'
Jul 16 14:43:22.919: INFO: stderr: ""
Jul 16 14:43:22.920: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:43:22.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7071" for this suite.
Jul 16 14:43:31.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:43:31.319: INFO: namespace kubectl-7071 deletion completed in 8.352056688s

• [SLOW TEST:27.097 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:43:31.323: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 16 14:43:31.480: INFO: Waiting up to 5m0s for pod "pod-0b592471-dc23-49d5-9b9a-16d27122d2e6" in namespace "emptydir-601" to be "success or failure"
Jul 16 14:43:31.492: INFO: Pod "pod-0b592471-dc23-49d5-9b9a-16d27122d2e6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.617808ms
Jul 16 14:43:33.504: INFO: Pod "pod-0b592471-dc23-49d5-9b9a-16d27122d2e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023940635s
Jul 16 14:43:35.512: INFO: Pod "pod-0b592471-dc23-49d5-9b9a-16d27122d2e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031913935s
STEP: Saw pod success
Jul 16 14:43:35.512: INFO: Pod "pod-0b592471-dc23-49d5-9b9a-16d27122d2e6" satisfied condition "success or failure"
Jul 16 14:43:35.519: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-0b592471-dc23-49d5-9b9a-16d27122d2e6 container test-container: <nil>
STEP: delete the pod
Jul 16 14:43:35.567: INFO: Waiting for pod pod-0b592471-dc23-49d5-9b9a-16d27122d2e6 to disappear
Jul 16 14:43:35.572: INFO: Pod pod-0b592471-dc23-49d5-9b9a-16d27122d2e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:43:35.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-601" for this suite.
Jul 16 14:43:41.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:43:42.213: INFO: namespace emptydir-601 deletion completed in 6.632890433s

• [SLOW TEST:10.891 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:43:42.214: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:43:47.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1410" for this suite.
Jul 16 14:44:17.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:44:17.822: INFO: namespace replication-controller-1410 deletion completed in 30.417943072s

• [SLOW TEST:35.609 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:44:17.824: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:44:17.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4432" for this suite.
Jul 16 14:44:41.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:44:42.338: INFO: namespace pods-4432 deletion completed in 24.366086399s

• [SLOW TEST:24.514 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:44:42.339: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 16 14:44:42.469: INFO: Waiting up to 5m0s for pod "pod-af5bdbe3-eb84-4397-8cf1-8d9a3ef7a5d3" in namespace "emptydir-4262" to be "success or failure"
Jul 16 14:44:42.475: INFO: Pod "pod-af5bdbe3-eb84-4397-8cf1-8d9a3ef7a5d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093891ms
Jul 16 14:44:44.488: INFO: Pod "pod-af5bdbe3-eb84-4397-8cf1-8d9a3ef7a5d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019764791s
Jul 16 14:44:46.496: INFO: Pod "pod-af5bdbe3-eb84-4397-8cf1-8d9a3ef7a5d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027143371s
STEP: Saw pod success
Jul 16 14:44:46.496: INFO: Pod "pod-af5bdbe3-eb84-4397-8cf1-8d9a3ef7a5d3" satisfied condition "success or failure"
Jul 16 14:44:46.503: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-af5bdbe3-eb84-4397-8cf1-8d9a3ef7a5d3 container test-container: <nil>
STEP: delete the pod
Jul 16 14:44:46.615: INFO: Waiting for pod pod-af5bdbe3-eb84-4397-8cf1-8d9a3ef7a5d3 to disappear
Jul 16 14:44:46.620: INFO: Pod pod-af5bdbe3-eb84-4397-8cf1-8d9a3ef7a5d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:44:46.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4262" for this suite.
Jul 16 14:44:52.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:44:53.010: INFO: namespace emptydir-4262 deletion completed in 6.38129576s

• [SLOW TEST:10.671 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:44:53.016: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 14:44:53.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9966'
Jul 16 14:44:53.285: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 14:44:53.285: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jul 16 14:44:53.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete jobs e2e-test-nginx-job --namespace=kubectl-9966'
Jul 16 14:44:53.454: INFO: stderr: ""
Jul 16 14:44:53.454: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:44:53.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9966" for this suite.
Jul 16 14:45:17.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:45:17.872: INFO: namespace kubectl-9966 deletion completed in 24.408138124s

• [SLOW TEST:24.857 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:45:17.873: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 16 14:45:26.155: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:26.169: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 14:45:28.169: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:28.177: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 14:45:30.169: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:30.178: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 14:45:32.169: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:32.176: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 14:45:34.169: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:34.188: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 14:45:36.169: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:36.177: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 14:45:38.169: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:38.177: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 14:45:40.169: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:40.194: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 14:45:42.169: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:42.175: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 16 14:45:44.169: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 16 14:45:44.185: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:45:44.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3467" for this suite.
Jul 16 14:46:08.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:46:08.857: INFO: namespace container-lifecycle-hook-3467 deletion completed in 24.651783653s

• [SLOW TEST:50.984 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:46:08.858: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 16 14:46:09.789: INFO: Pod name wrapped-volume-race-23568709-6c6b-43c9-86d5-1a63594aa7e7: Found 1 pods out of 5
Jul 16 14:46:14.801: INFO: Pod name wrapped-volume-race-23568709-6c6b-43c9-86d5-1a63594aa7e7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-23568709-6c6b-43c9-86d5-1a63594aa7e7 in namespace emptydir-wrapper-1050, will wait for the garbage collector to delete the pods
Jul 16 14:46:28.932: INFO: Deleting ReplicationController wrapped-volume-race-23568709-6c6b-43c9-86d5-1a63594aa7e7 took: 27.440823ms
Jul 16 14:46:29.432: INFO: Terminating ReplicationController wrapped-volume-race-23568709-6c6b-43c9-86d5-1a63594aa7e7 pods took: 500.397634ms
STEP: Creating RC which spawns configmap-volume pods
Jul 16 14:47:34.577: INFO: Pod name wrapped-volume-race-be6361b1-1c72-408b-bd77-6bc9ed4c8bba: Found 0 pods out of 5
Jul 16 14:47:39.593: INFO: Pod name wrapped-volume-race-be6361b1-1c72-408b-bd77-6bc9ed4c8bba: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-be6361b1-1c72-408b-bd77-6bc9ed4c8bba in namespace emptydir-wrapper-1050, will wait for the garbage collector to delete the pods
Jul 16 14:47:54.054: INFO: Deleting ReplicationController wrapped-volume-race-be6361b1-1c72-408b-bd77-6bc9ed4c8bba took: 209.278551ms
Jul 16 14:47:54.755: INFO: Terminating ReplicationController wrapped-volume-race-be6361b1-1c72-408b-bd77-6bc9ed4c8bba pods took: 700.654589ms
STEP: Creating RC which spawns configmap-volume pods
Jul 16 14:48:33.029: INFO: Pod name wrapped-volume-race-849992e1-a29e-4eb0-a31b-4685b2173d52: Found 0 pods out of 5
Jul 16 14:48:38.044: INFO: Pod name wrapped-volume-race-849992e1-a29e-4eb0-a31b-4685b2173d52: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-849992e1-a29e-4eb0-a31b-4685b2173d52 in namespace emptydir-wrapper-1050, will wait for the garbage collector to delete the pods
Jul 16 14:48:52.207: INFO: Deleting ReplicationController wrapped-volume-race-849992e1-a29e-4eb0-a31b-4685b2173d52 took: 46.432237ms
Jul 16 14:48:52.807: INFO: Terminating ReplicationController wrapped-volume-race-849992e1-a29e-4eb0-a31b-4685b2173d52 pods took: 600.365839ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:49:34.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1050" for this suite.
Jul 16 14:49:44.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:49:44.669: INFO: namespace emptydir-wrapper-1050 deletion completed in 10.36477992s

• [SLOW TEST:215.811 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:49:44.671: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7460
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7460
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7460
Jul 16 14:49:44.813: INFO: Found 0 stateful pods, waiting for 1
Jul 16 14:49:54.826: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 16 14:49:54.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:49:55.734: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 14:49:55.735: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:49:55.735: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:49:55.741: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 16 14:50:05.751: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:50:05.751: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:50:06.313: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999656s
Jul 16 14:50:07.321: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994147275s
Jul 16 14:50:08.332: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985796659s
Jul 16 14:50:09.341: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.975225841s
Jul 16 14:50:10.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.965474656s
Jul 16 14:50:11.362: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.956255528s
Jul 16 14:50:12.369: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.945575025s
Jul 16 14:50:13.385: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.937678778s
Jul 16 14:50:14.419: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.922778027s
Jul 16 14:50:15.427: INFO: Verifying statefulset ss doesn't scale past 1 for another 888.2177ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7460
Jul 16 14:50:16.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:50:17.286: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 16 14:50:17.286: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:50:17.287: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:50:17.294: INFO: Found 1 stateful pods, waiting for 3
Jul 16 14:50:27.313: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:50:27.314: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 16 14:50:27.314: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 16 14:50:27.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:50:28.164: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 14:50:28.164: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:50:28.164: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:50:28.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:50:29.030: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 14:50:29.030: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:50:29.030: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:50:29.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 16 14:50:29.931: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 16 14:50:29.931: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 16 14:50:29.931: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 16 14:50:29.931: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:50:29.950: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 16 14:51:12.276: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:51:12.277: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:51:12.277: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 16 14:51:12.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999488s
Jul 16 14:51:13.317: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990289939s
Jul 16 14:51:14.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975001063s
Jul 16 14:51:15.337: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.966657309s
Jul 16 14:51:16.354: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.95476517s
Jul 16 14:51:17.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937795071s
Jul 16 14:51:18.371: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.929456554s
Jul 16 14:51:19.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.921431519s
Jul 16 14:51:20.386: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.914603663s
Jul 16 14:51:21.397: INFO: Verifying statefulset ss doesn't scale past 3 for another 906.121713ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7460
Jul 16 14:51:22.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:51:23.458: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 16 14:51:23.458: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:51:23.458: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:51:23.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:51:24.464: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 16 14:51:24.464: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 16 14:51:24.464: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 16 14:51:24.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:51:25.506: INFO: rc: 126
Jul 16 14:51:25.507: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "read init-p: connection reset by peer": unknown
 command terminated with exit code 126
 [] <nil> 0xc002ff4d20 exit status 126 <nil> <nil> true [0xc0007741f0 0xc000774410 0xc0007744f0] [0xc0007741f0 0xc000774410 0xc0007744f0] [0xc0007743a8 0xc000774498] [0x9d17b0 0x9d17b0] 0xc0029dfaa0 <nil>}:
Command stdout:
OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "read init-p: connection reset by peer": unknown

stderr:
command terminated with exit code 126

error:
exit status 126
Jul 16 14:51:35.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:51:35.846: INFO: rc: 1
Jul 16 14:51:35.846: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002ff50e0 exit status 1 <nil> <nil> true [0xc000774518 0xc000774880 0xc000774aa8] [0xc000774518 0xc000774880 0xc000774aa8] [0xc000774728 0xc000774a68] [0x9d17b0 0x9d17b0] 0xc0019b2960 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jul 16 14:51:45.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:51:46.013: INFO: rc: 1
Jul 16 14:51:46.014: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ff5470 exit status 1 <nil> <nil> true [0xc000774b68 0xc000774c50 0xc000774e10] [0xc000774b68 0xc000774c50 0xc000774e10] [0xc000774bd8 0xc000774d38] [0x9d17b0 0x9d17b0] 0xc0019b35c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:51:56.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:51:56.162: INFO: rc: 1
Jul 16 14:51:56.163: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001955920 exit status 1 <nil> <nil> true [0xc001bde8e8 0xc001bde970 0xc001bde9c8] [0xc001bde8e8 0xc001bde970 0xc001bde9c8] [0xc001bde940 0xc001bde9a8] [0x9d17b0 0x9d17b0] 0xc001eee0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:52:06.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:52:06.332: INFO: rc: 1
Jul 16 14:52:06.332: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028aa180 exit status 1 <nil> <nil> true [0xc000774ed8 0xc000775070 0xc0007751a0] [0xc000774ed8 0xc000775070 0xc0007751a0] [0xc000774ff0 0xc000775108] [0x9d17b0 0x9d17b0] 0xc0024e4cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:52:16.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:52:16.473: INFO: rc: 1
Jul 16 14:52:16.473: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001955c80 exit status 1 <nil> <nil> true [0xc001bde9f8 0xc001bdea48 0xc001bdeab8] [0xc001bde9f8 0xc001bdea48 0xc001bdeab8] [0xc001bdea38 0xc001bdeaa0] [0x9d17b0 0x9d17b0] 0xc001eeec00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:52:26.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:52:27.324: INFO: rc: 1
Jul 16 14:52:27.325: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025c8030 exit status 1 <nil> <nil> true [0xc001bdeac8 0xc001bdeb68 0xc001bdebc0] [0xc001bdeac8 0xc001bdeb68 0xc001bdebc0] [0xc001bdeb48 0xc001bdebb0] [0x9d17b0 0x9d17b0] 0xc001eef7a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:52:37.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:52:38.041: INFO: rc: 1
Jul 16 14:52:38.041: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028aa510 exit status 1 <nil> <nil> true [0xc000775230 0xc000775300 0xc000775340] [0xc000775230 0xc000775300 0xc000775340] [0xc0007752d0 0xc000775338] [0x9d17b0 0x9d17b0] 0xc00210cd20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:52:48.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:52:48.216: INFO: rc: 1
Jul 16 14:52:48.217: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025c83c0 exit status 1 <nil> <nil> true [0xc001bdebe0 0xc001bdec98 0xc001bdece0] [0xc001bdebe0 0xc001bdec98 0xc001bdece0] [0xc001bdec60 0xc001bdecc0] [0x9d17b0 0x9d17b0] 0xc00209acc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:52:58.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:52:58.337: INFO: rc: 1
Jul 16 14:52:58.338: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025c8780 exit status 1 <nil> <nil> true [0xc001bdecf8 0xc001bded50 0xc001bdedd8] [0xc001bdecf8 0xc001bded50 0xc001bdedd8] [0xc001bded40 0xc001bded88] [0x9d17b0 0x9d17b0] 0xc001ea6780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:53:08.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:53:08.523: INFO: rc: 1
Jul 16 14:53:08.524: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001954330 exit status 1 <nil> <nil> true [0xc0008e40b8 0xc0008e4830 0xc0008e4a28] [0xc0008e40b8 0xc0008e4830 0xc0008e4a28] [0xc0008e42c0 0xc0008e4970] [0x9d17b0 0x9d17b0] 0xc00209acc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:53:18.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:53:18.659: INFO: rc: 1
Jul 16 14:53:18.660: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019546c0 exit status 1 <nil> <nil> true [0xc0008e4a78 0xc0008e4bf8 0xc0008e4e50] [0xc0008e4a78 0xc0008e4bf8 0xc0008e4e50] [0xc0008e4ae8 0xc0008e4e28] [0x9d17b0 0x9d17b0] 0xc0024e5740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:53:28.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:53:28.810: INFO: rc: 1
Jul 16 14:53:28.810: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001954f60 exit status 1 <nil> <nil> true [0xc0008e5020 0xc0008e5288 0xc0008e56f8] [0xc0008e5020 0xc0008e5288 0xc0008e56f8] [0xc0008e5218 0xc0008e5600] [0x9d17b0 0x9d17b0] 0xc001eee8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:53:38.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:53:38.953: INFO: rc: 1
Jul 16 14:53:38.954: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001955320 exit status 1 <nil> <nil> true [0xc0008e5750 0xc0008e5a38 0xc0008e5d08] [0xc0008e5750 0xc0008e5a38 0xc0008e5d08] [0xc0008e5998 0xc0008e5c48] [0x9d17b0 0x9d17b0] 0xc001eef380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:53:48.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:53:49.098: INFO: rc: 1
Jul 16 14:53:49.098: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001955710 exit status 1 <nil> <nil> true [0xc0008e5e00 0xc0008e5ed8 0xc0008e5f90] [0xc0008e5e00 0xc0008e5ed8 0xc0008e5f90] [0xc0008e5eb8 0xc0008e5f78] [0x9d17b0 0x9d17b0] 0xc001eeff20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:53:59.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:53:59.243: INFO: rc: 1
Jul 16 14:53:59.244: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001955ad0 exit status 1 <nil> <nil> true [0xc0008e5fa0 0xc000011dd8 0xc001bde000] [0xc0008e5fa0 0xc000011dd8 0xc001bde000] [0xc000011d40 0xc00016a3d8] [0x9d17b0 0x9d17b0] 0xc0019b2de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:54:09.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:54:09.361: INFO: rc: 1
Jul 16 14:54:09.361: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001955e60 exit status 1 <nil> <nil> true [0xc001bde078 0xc001bde1b0 0xc001bde288] [0xc001bde078 0xc001bde1b0 0xc001bde288] [0xc001bde150 0xc001bde230] [0x9d17b0 0x9d17b0] 0xc0019b3aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:54:19.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:54:19.495: INFO: rc: 1
Jul 16 14:54:19.495: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003584390 exit status 1 <nil> <nil> true [0xc000774000 0xc000774110 0xc000774368] [0xc000774000 0xc000774110 0xc000774368] [0xc000774100 0xc0007741f0] [0x9d17b0 0x9d17b0] 0xc001ffea80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:54:29.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:54:29.651: INFO: rc: 1
Jul 16 14:54:29.651: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035848a0 exit status 1 <nil> <nil> true [0xc0007743a8 0xc000774498 0xc000774568] [0xc0007743a8 0xc000774498 0xc000774568] [0xc000774440 0xc000774518] [0x9d17b0 0x9d17b0] 0xc001fff620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:54:39.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:54:39.811: INFO: rc: 1
Jul 16 14:54:39.811: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003584c30 exit status 1 <nil> <nil> true [0xc000774728 0xc000774a68 0xc000774bc0] [0xc000774728 0xc000774a68 0xc000774bc0] [0xc000774968 0xc000774b68] [0x9d17b0 0x9d17b0] 0xc00255e480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:54:49.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:54:49.975: INFO: rc: 1
Jul 16 14:54:49.975: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ff4240 exit status 1 <nil> <nil> true [0xc001bde300 0xc001bde388 0xc001bde460] [0xc001bde300 0xc001bde388 0xc001bde460] [0xc001bde368 0xc001bde440] [0x9d17b0 0x9d17b0] 0xc0021fc840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:54:59.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:55:00.123: INFO: rc: 1
Jul 16 14:55:00.124: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ff45d0 exit status 1 <nil> <nil> true [0xc001bde4b8 0xc001bde518 0xc001bde5e8] [0xc001bde4b8 0xc001bde518 0xc001bde5e8] [0xc001bde508 0xc001bde5a0] [0x9d17b0 0x9d17b0] 0xc0021fd680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:55:10.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:55:10.235: INFO: rc: 1
Jul 16 14:55:10.235: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001954360 exit status 1 <nil> <nil> true [0xc000011d40 0xc0008e4010 0xc0008e42c0] [0xc000011d40 0xc0008e4010 0xc0008e42c0] [0xc000011ff8 0xc0008e41d0] [0x9d17b0 0x9d17b0] 0xc001ffea80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:55:20.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:55:20.376: INFO: rc: 1
Jul 16 14:55:20.376: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001954750 exit status 1 <nil> <nil> true [0xc0008e4830 0xc0008e4a28 0xc0008e4ae8] [0xc0008e4830 0xc0008e4a28 0xc0008e4ae8] [0xc0008e4970 0xc0008e4ad8] [0x9d17b0 0x9d17b0] 0xc001fff620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:55:30.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:55:30.522: INFO: rc: 1
Jul 16 14:55:30.522: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ff4360 exit status 1 <nil> <nil> true [0xc001bde000 0xc001bde150 0xc001bde230] [0xc001bde000 0xc001bde150 0xc001bde230] [0xc001bde0d8 0xc001bde218] [0x9d17b0 0x9d17b0] 0xc0019b2c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:55:40.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:55:40.691: INFO: rc: 1
Jul 16 14:55:40.692: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ff4720 exit status 1 <nil> <nil> true [0xc001bde288 0xc001bde368 0xc001bde440] [0xc001bde288 0xc001bde368 0xc001bde440] [0xc001bde348 0xc001bde3b0] [0x9d17b0 0x9d17b0] 0xc0019b3920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:55:50.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:55:50.791: INFO: rc: 1
Jul 16 14:55:50.791: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ff4ab0 exit status 1 <nil> <nil> true [0xc001bde460 0xc001bde508 0xc001bde5a0] [0xc001bde460 0xc001bde508 0xc001bde5a0] [0xc001bde4e0 0xc001bde558] [0x9d17b0 0x9d17b0] 0xc001eee5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:56:00.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:56:00.949: INFO: rc: 1
Jul 16 14:56:00.950: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ff4e40 exit status 1 <nil> <nil> true [0xc001bde5e8 0xc001bde648 0xc001bde6f8] [0xc001bde5e8 0xc001bde648 0xc001bde6f8] [0xc001bde600 0xc001bde6c8] [0x9d17b0 0x9d17b0] 0xc001eef140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:56:10.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:56:11.052: INFO: rc: 1
Jul 16 14:56:11.052: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ff5200 exit status 1 <nil> <nil> true [0xc001bde718 0xc001bde770 0xc001bde7d8] [0xc001bde718 0xc001bde770 0xc001bde7d8] [0xc001bde748 0xc001bde7d0] [0x9d17b0 0x9d17b0] 0xc001eefc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:56:21.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:56:21.186: INFO: rc: 1
Jul 16 14:56:21.186: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001955050 exit status 1 <nil> <nil> true [0xc0008e4bf8 0xc0008e4e50 0xc0008e5218] [0xc0008e4bf8 0xc0008e4e50 0xc0008e5218] [0xc0008e4e28 0xc0008e5138] [0x9d17b0 0x9d17b0] 0xc0024e5200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 16 14:56:31.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 exec --namespace=statefulset-7460 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 16 14:56:31.323: INFO: rc: 1
Jul 16 14:56:31.324: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Jul 16 14:56:31.324: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 16 14:56:31.359: INFO: Deleting all statefulset in ns statefulset-7460
Jul 16 14:56:31.380: INFO: Scaling statefulset ss to 0
Jul 16 14:56:31.398: INFO: Waiting for statefulset status.replicas updated to 0
Jul 16 14:56:31.403: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:56:31.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7460" for this suite.
Jul 16 14:56:37.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:56:37.880: INFO: namespace statefulset-7460 deletion completed in 6.429899733s

• [SLOW TEST:413.210 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:56:37.881: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:56:37.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8405" for this suite.
Jul 16 14:56:44.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:56:44.349: INFO: namespace services-8405 deletion completed in 6.361788362s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.469 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:56:44.353: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-207 to expose endpoints map[]
Jul 16 14:56:44.569: INFO: Get endpoints failed (4.893029ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul 16 14:56:45.580: INFO: successfully validated that service multi-endpoint-test in namespace services-207 exposes endpoints map[] (1.015542979s elapsed)
STEP: Creating pod pod1 in namespace services-207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-207 to expose endpoints map[pod1:[100]]
Jul 16 14:56:48.655: INFO: successfully validated that service multi-endpoint-test in namespace services-207 exposes endpoints map[pod1:[100]] (3.055755068s elapsed)
STEP: Creating pod pod2 in namespace services-207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-207 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 16 14:56:51.792: INFO: successfully validated that service multi-endpoint-test in namespace services-207 exposes endpoints map[pod1:[100] pod2:[101]] (3.122425826s elapsed)
STEP: Deleting pod pod1 in namespace services-207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-207 to expose endpoints map[pod2:[101]]
Jul 16 14:56:52.932: INFO: successfully validated that service multi-endpoint-test in namespace services-207 exposes endpoints map[pod2:[101]] (1.079296401s elapsed)
STEP: Deleting pod pod2 in namespace services-207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-207 to expose endpoints map[]
Jul 16 14:56:53.012: INFO: successfully validated that service multi-endpoint-test in namespace services-207 exposes endpoints map[] (28.188651ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:56:53.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-207" for this suite.
Jul 16 14:57:17.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:57:17.652: INFO: namespace services-207 deletion completed in 24.439238836s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:33.300 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:57:17.662: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-62754845-3484-4fca-98ec-a7633c867dc2
STEP: Creating a pod to test consume secrets
Jul 16 14:57:17.812: INFO: Waiting up to 5m0s for pod "pod-secrets-94687aee-1dc2-406f-8c42-72e948003796" in namespace "secrets-893" to be "success or failure"
Jul 16 14:57:17.825: INFO: Pod "pod-secrets-94687aee-1dc2-406f-8c42-72e948003796": Phase="Pending", Reason="", readiness=false. Elapsed: 12.953344ms
Jul 16 14:57:19.846: INFO: Pod "pod-secrets-94687aee-1dc2-406f-8c42-72e948003796": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033663546s
Jul 16 14:57:21.853: INFO: Pod "pod-secrets-94687aee-1dc2-406f-8c42-72e948003796": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041538407s
STEP: Saw pod success
Jul 16 14:57:21.854: INFO: Pod "pod-secrets-94687aee-1dc2-406f-8c42-72e948003796" satisfied condition "success or failure"
Jul 16 14:57:21.859: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-secrets-94687aee-1dc2-406f-8c42-72e948003796 container secret-volume-test: <nil>
STEP: delete the pod
Jul 16 14:57:22.002: INFO: Waiting for pod pod-secrets-94687aee-1dc2-406f-8c42-72e948003796 to disappear
Jul 16 14:57:22.015: INFO: Pod pod-secrets-94687aee-1dc2-406f-8c42-72e948003796 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:57:22.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-893" for this suite.
Jul 16 14:57:32.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:57:32.436: INFO: namespace secrets-893 deletion completed in 10.411696676s

• [SLOW TEST:14.775 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:57:32.438: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-62253856-64bb-4ae4-b5ed-ba33fbd94c5f
STEP: Creating a pod to test consume configMaps
Jul 16 14:57:32.606: INFO: Waiting up to 5m0s for pod "pod-configmaps-44f71e54-a54b-428f-9d82-df76333a4953" in namespace "configmap-2207" to be "success or failure"
Jul 16 14:57:32.612: INFO: Pod "pod-configmaps-44f71e54-a54b-428f-9d82-df76333a4953": Phase="Pending", Reason="", readiness=false. Elapsed: 6.213244ms
Jul 16 14:57:34.618: INFO: Pod "pod-configmaps-44f71e54-a54b-428f-9d82-df76333a4953": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012631514s
Jul 16 14:57:36.628: INFO: Pod "pod-configmaps-44f71e54-a54b-428f-9d82-df76333a4953": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02183677s
STEP: Saw pod success
Jul 16 14:57:36.628: INFO: Pod "pod-configmaps-44f71e54-a54b-428f-9d82-df76333a4953" satisfied condition "success or failure"
Jul 16 14:57:36.633: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-configmaps-44f71e54-a54b-428f-9d82-df76333a4953 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 16 14:57:36.707: INFO: Waiting for pod pod-configmaps-44f71e54-a54b-428f-9d82-df76333a4953 to disappear
Jul 16 14:57:36.712: INFO: Pod pod-configmaps-44f71e54-a54b-428f-9d82-df76333a4953 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:57:36.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2207" for this suite.
Jul 16 14:57:42.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:57:43.067: INFO: namespace configmap-2207 deletion completed in 6.347801246s

• [SLOW TEST:10.630 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:57:43.068: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 14:57:43.245: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 16 14:57:48.263: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 16 14:57:48.264: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 16 14:57:48.327: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-6348,SelfLink:/apis/apps/v1/namespaces/deployment-6348/deployments/test-cleanup-deployment,UID:9cd633b4-37fb-4274-9eaa-57bf1c9261b8,ResourceVersion:35230,Generation:1,CreationTimestamp:2019-07-16 14:57:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jul 16 14:57:48.336: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-6348,SelfLink:/apis/apps/v1/namespaces/deployment-6348/replicasets/test-cleanup-deployment-55bbcbc84c,UID:cf5ab6a5-7267-4ac2-b8b3-2a3f1a060eaa,ResourceVersion:35232,Generation:1,CreationTimestamp:2019-07-16 14:57:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 9cd633b4-37fb-4274-9eaa-57bf1c9261b8 0xc00346db07 0xc00346db08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 14:57:48.336: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul 16 14:57:48.336: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-6348,SelfLink:/apis/apps/v1/namespaces/deployment-6348/replicasets/test-cleanup-controller,UID:8fc8ebc0-6829-4cb9-a945-55d8c5ca5007,ResourceVersion:35231,Generation:1,CreationTimestamp:2019-07-16 14:57:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 9cd633b4-37fb-4274-9eaa-57bf1c9261b8 0xc00346da37 0xc00346da38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 16 14:57:48.362: INFO: Pod "test-cleanup-controller-55v8j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-55v8j,GenerateName:test-cleanup-controller-,Namespace:deployment-6348,SelfLink:/api/v1/namespaces/deployment-6348/pods/test-cleanup-controller-55v8j,UID:44f8ecbe-9f47-440d-b80e-f95c0a2852d5,ResourceVersion:35223,Generation:0,CreationTimestamp:2019-07-16 14:57:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 8fc8ebc0-6829-4cb9-a945-55d8c5ca5007 0xc002ba2697 0xc002ba2698}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-99xs8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-99xs8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-99xs8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ba2700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ba2720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:57:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:57:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:57:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:57:43 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:172.25.1.241,StartTime:2019-07-16 14:57:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-16 14:57:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://216b6ceac0fc05975bdb98d6e9f1c028842942cd9085a6588189c6bf4ed61dfc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 16 14:57:48.362: INFO: Pod "test-cleanup-deployment-55bbcbc84c-7qzjd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-7qzjd,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-6348,SelfLink:/api/v1/namespaces/deployment-6348/pods/test-cleanup-deployment-55bbcbc84c-7qzjd,UID:fa45fdd7-9abf-4d7d-b4e5-225b68036db6,ResourceVersion:35234,Generation:0,CreationTimestamp:2019-07-16 14:57:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c cf5ab6a5-7267-4ac2-b8b3-2a3f1a060eaa 0xc002ba2807 0xc002ba2808}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-99xs8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-99xs8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-99xs8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ba2870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ba2890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:57:48.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6348" for this suite.
Jul 16 14:57:54.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:57:54.846: INFO: namespace deployment-6348 deletion completed in 6.435354529s

• [SLOW TEST:11.778 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:57:54.849: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 16 14:57:54.974: INFO: Waiting up to 5m0s for pod "pod-bd4a29e2-4160-4bae-825e-8a9d6b4d7c80" in namespace "emptydir-4171" to be "success or failure"
Jul 16 14:57:55.004: INFO: Pod "pod-bd4a29e2-4160-4bae-825e-8a9d6b4d7c80": Phase="Pending", Reason="", readiness=false. Elapsed: 30.035922ms
Jul 16 14:57:57.010: INFO: Pod "pod-bd4a29e2-4160-4bae-825e-8a9d6b4d7c80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036423518s
Jul 16 14:57:59.024: INFO: Pod "pod-bd4a29e2-4160-4bae-825e-8a9d6b4d7c80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049924988s
STEP: Saw pod success
Jul 16 14:57:59.024: INFO: Pod "pod-bd4a29e2-4160-4bae-825e-8a9d6b4d7c80" satisfied condition "success or failure"
Jul 16 14:57:59.029: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-bd4a29e2-4160-4bae-825e-8a9d6b4d7c80 container test-container: <nil>
STEP: delete the pod
Jul 16 14:57:59.156: INFO: Waiting for pod pod-bd4a29e2-4160-4bae-825e-8a9d6b4d7c80 to disappear
Jul 16 14:57:59.174: INFO: Pod pod-bd4a29e2-4160-4bae-825e-8a9d6b4d7c80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:57:59.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4171" for this suite.
Jul 16 14:58:05.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:58:05.493: INFO: namespace emptydir-4171 deletion completed in 6.310203769s

• [SLOW TEST:10.644 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:58:05.495: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 14:58:05.620: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:58:06.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-260" for this suite.
Jul 16 14:58:12.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:58:13.381: INFO: namespace custom-resource-definition-260 deletion completed in 6.429168831s

• [SLOW TEST:7.887 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:58:13.383: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-5q5z
STEP: Creating a pod to test atomic-volume-subpath
Jul 16 14:58:13.577: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5q5z" in namespace "subpath-3261" to be "success or failure"
Jul 16 14:58:13.583: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.766554ms
Jul 16 14:58:15.592: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015150641s
Jul 16 14:58:17.602: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.025178141s
Jul 16 14:58:19.613: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 6.03568546s
Jul 16 14:58:21.621: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 8.043295196s
Jul 16 14:58:23.628: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 10.051103043s
Jul 16 14:58:25.637: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 12.059817315s
Jul 16 14:58:27.645: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 14.067932815s
Jul 16 14:58:29.653: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 16.075597413s
Jul 16 14:58:31.661: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 18.083919789s
Jul 16 14:58:33.669: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 20.091650289s
Jul 16 14:58:35.677: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Running", Reason="", readiness=true. Elapsed: 22.099752671s
Jul 16 14:58:37.692: INFO: Pod "pod-subpath-test-projected-5q5z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.115126922s
STEP: Saw pod success
Jul 16 14:58:37.693: INFO: Pod "pod-subpath-test-projected-5q5z" satisfied condition "success or failure"
Jul 16 14:58:37.700: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-subpath-test-projected-5q5z container test-container-subpath-projected-5q5z: <nil>
STEP: delete the pod
Jul 16 14:58:37.957: INFO: Waiting for pod pod-subpath-test-projected-5q5z to disappear
Jul 16 14:58:37.963: INFO: Pod pod-subpath-test-projected-5q5z no longer exists
STEP: Deleting pod pod-subpath-test-projected-5q5z
Jul 16 14:58:37.963: INFO: Deleting pod "pod-subpath-test-projected-5q5z" in namespace "subpath-3261"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:58:37.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3261" for this suite.
Jul 16 14:58:44.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:58:44.358: INFO: namespace subpath-3261 deletion completed in 6.37745558s

• [SLOW TEST:30.975 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:58:44.361: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 16 14:58:52.929: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:58:52.936: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:58:54.936: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:58:55.016: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:58:56.936: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:58:56.944: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:58:58.939: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:58:58.951: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:59:00.936: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:59:00.947: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:59:02.936: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:59:02.944: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:59:04.936: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:59:04.944: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:59:06.936: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:59:06.950: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:59:08.936: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:59:08.943: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:59:10.936: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:59:10.943: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 16 14:59:12.936: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 16 14:59:13.008: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 14:59:13.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1351" for this suite.
Jul 16 14:59:37.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 14:59:37.464: INFO: namespace container-lifecycle-hook-1351 deletion completed in 24.388894949s

• [SLOW TEST:53.104 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 14:59:37.466: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 14:59:37.635: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 16 14:59:42.645: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 16 14:59:42.645: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 16 14:59:44.653: INFO: Creating deployment "test-rollover-deployment"
Jul 16 14:59:44.672: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 16 14:59:46.683: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 16 14:59:46.695: INFO: Ensure that both replica sets have 1 created replica
Jul 16 14:59:46.706: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 16 14:59:46.725: INFO: Updating deployment test-rollover-deployment
Jul 16 14:59:46.725: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 16 14:59:48.736: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 16 14:59:48.758: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 16 14:59:48.772: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:59:48.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885987, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:59:50.792: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:59:50.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885989, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:59:52.817: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:59:52.818: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885989, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:59:54.792: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:59:54.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885989, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:59:56.785: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:59:56.786: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885989, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 14:59:58.787: INFO: all replica sets need to contain the pod-template-hash label
Jul 16 14:59:58.787: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885989, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698885984, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 16 15:00:00.793: INFO: 
Jul 16 15:00:00.793: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 16 15:00:00.811: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-2973,SelfLink:/apis/apps/v1/namespaces/deployment-2973/deployments/test-rollover-deployment,UID:34685ca7-2551-442f-b0fd-45ddb4688b92,ResourceVersion:35877,Generation:2,CreationTimestamp:2019-07-16 14:59:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-16 14:59:44 +0000 UTC 2019-07-16 14:59:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-16 14:59:59 +0000 UTC 2019-07-16 14:59:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 16 15:00:00.817: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-2973,SelfLink:/apis/apps/v1/namespaces/deployment-2973/replicasets/test-rollover-deployment-854595fc44,UID:38526e02-97ac-495d-97bd-31e03ef88a31,ResourceVersion:35865,Generation:2,CreationTimestamp:2019-07-16 14:59:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 34685ca7-2551-442f-b0fd-45ddb4688b92 0xc002af88e7 0xc002af88e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 16 15:00:00.817: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 16 15:00:00.817: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-2973,SelfLink:/apis/apps/v1/namespaces/deployment-2973/replicasets/test-rollover-controller,UID:56a4a916-39cd-4424-b06a-14f47c8e76db,ResourceVersion:35876,Generation:2,CreationTimestamp:2019-07-16 14:59:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 34685ca7-2551-442f-b0fd-45ddb4688b92 0xc002af87f7 0xc002af87f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 15:00:00.817: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-2973,SelfLink:/apis/apps/v1/namespaces/deployment-2973/replicasets/test-rollover-deployment-9b8b997cf,UID:d93dc68d-0e9c-4909-86ad-6cd67c9a919a,ResourceVersion:35815,Generation:2,CreationTimestamp:2019-07-16 14:59:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 34685ca7-2551-442f-b0fd-45ddb4688b92 0xc002af89b0 0xc002af89b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 16 15:00:00.823: INFO: Pod "test-rollover-deployment-854595fc44-2nslg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-2nslg,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-2973,SelfLink:/api/v1/namespaces/deployment-2973/pods/test-rollover-deployment-854595fc44-2nslg,UID:55bbc968-01b5-49ec-bd20-ad2c82b4bf8a,ResourceVersion:35830,Generation:0,CreationTimestamp:2019-07-16 14:59:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 38526e02-97ac-495d-97bd-31e03ef88a31 0xc002af9587 0xc002af9588}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7q9ws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7q9ws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7q9ws true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker-54b54f4f98-f9trz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af95f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af9610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:59:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:59:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:59:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-16 14:59:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.2,PodIP:172.25.1.247,StartTime:2019-07-16 14:59:47 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-16 14:59:48 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2e34fa3531095ab5090a62793eb753369d64b7abb6c98a5d9609c01af5dedb5e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:00:00.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2973" for this suite.
Jul 16 15:00:08.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:00:09.164: INFO: namespace deployment-2973 deletion completed in 8.333030615s

• [SLOW TEST:31.699 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 15:00:09.168: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 16 15:00:09.374: INFO: Waiting up to 5m0s for pod "pod-3f6887c7-cef9-40db-b06d-886485daeaa4" in namespace "emptydir-4196" to be "success or failure"
Jul 16 15:00:09.380: INFO: Pod "pod-3f6887c7-cef9-40db-b06d-886485daeaa4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665938ms
Jul 16 15:00:11.388: INFO: Pod "pod-3f6887c7-cef9-40db-b06d-886485daeaa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014303593s
Jul 16 15:00:13.410: INFO: Pod "pod-3f6887c7-cef9-40db-b06d-886485daeaa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036088094s
STEP: Saw pod success
Jul 16 15:00:13.410: INFO: Pod "pod-3f6887c7-cef9-40db-b06d-886485daeaa4" satisfied condition "success or failure"
Jul 16 15:00:13.416: INFO: Trying to get logs from node conformance-worker-54b54f4f98-f9trz pod pod-3f6887c7-cef9-40db-b06d-886485daeaa4 container test-container: <nil>
STEP: delete the pod
Jul 16 15:00:13.517: INFO: Waiting for pod pod-3f6887c7-cef9-40db-b06d-886485daeaa4 to disappear
Jul 16 15:00:13.523: INFO: Pod pod-3f6887c7-cef9-40db-b06d-886485daeaa4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:00:13.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4196" for this suite.
Jul 16 15:00:21.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:00:21.902: INFO: namespace emptydir-4196 deletion completed in 8.37005249s

• [SLOW TEST:12.735 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 15:00:21.906: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 16 15:00:22.340: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 16 15:00:22.357: INFO: Waiting for terminating namespaces to be deleted...
Jul 16 15:00:22.368: INFO: 
Logging pods the kubelet thinks is on node conformance-worker-54b54f4f98-f9trz before test
Jul 16 15:00:22.468: INFO: restic-dqv4x from velero started at 2019-07-16 12:58:27 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.468: INFO: 	Container velero ready: true, restart count 0
Jul 16 15:00:22.468: INFO: kube-proxy-9l5xc from kube-system started at 2019-07-16 12:58:27 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.468: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 15:00:22.469: INFO: node-exporter-dk6wg from kube-system started at 2019-07-16 12:58:27 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.469: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 15:00:22.469: INFO: canal-4r5mb from kube-system started at 2019-07-16 12:58:27 +0000 UTC (3 container statuses recorded)
Jul 16 15:00:22.469: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 15:00:22.469: INFO: 	Container install-cni ready: true, restart count 0
Jul 16 15:00:22.469: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 16 15:00:22.469: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-16 13:04:36 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.469: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 16 15:00:22.469: INFO: sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-mgtcd from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 15:00:22.469: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 16 15:00:22.469: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 16 15:00:22.469: INFO: 
Logging pods the kubelet thinks is on node conformance-worker-54b54f4f98-m9svg before test
Jul 16 15:00:22.543: INFO: canal-mtsjt from kube-system started at 2019-07-16 12:58:37 +0000 UTC (3 container statuses recorded)
Jul 16 15:00:22.543: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 15:00:22.543: INFO: 	Container install-cni ready: true, restart count 0
Jul 16 15:00:22.543: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 16 15:00:22.543: INFO: sonobuoy-e2e-job-352202216adf4593 from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 15:00:22.543: INFO: 	Container e2e ready: true, restart count 0
Jul 16 15:00:22.543: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 16 15:00:22.543: INFO: sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-t84ms from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 15:00:22.543: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 16 15:00:22.543: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 16 15:00:22.543: INFO: restic-vssb6 from velero started at 2019-07-16 12:58:37 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.543: INFO: 	Container velero ready: true, restart count 0
Jul 16 15:00:22.543: INFO: kube-proxy-nwtx4 from kube-system started at 2019-07-16 12:58:37 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.543: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 15:00:22.543: INFO: node-exporter-qrl7j from kube-system started at 2019-07-16 12:58:37 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.543: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 15:00:22.543: INFO: 
Logging pods the kubelet thinks is on node conformance-worker-54b54f4f98-nqplr before test
Jul 16 15:00:22.628: INFO: coredns-6f56645db5-86hnn from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.628: INFO: 	Container coredns ready: true, restart count 0
Jul 16 15:00:22.628: INFO: velero-79764c8c47-l746r from velero started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.628: INFO: 	Container velero ready: true, restart count 0
Jul 16 15:00:22.628: INFO: restic-pbp8t from velero started at 2019-07-16 12:58:09 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.629: INFO: 	Container velero ready: true, restart count 0
Jul 16 15:00:22.629: INFO: kube-proxy-vwpcz from kube-system started at 2019-07-16 12:58:10 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.629: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 16 15:00:22.629: INFO: node-exporter-p6bc4 from kube-system started at 2019-07-16 12:58:10 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.629: INFO: 	Container node-exporter ready: true, restart count 0
Jul 16 15:00:22.629: INFO: canal-qhfws from kube-system started at 2019-07-16 12:58:10 +0000 UTC (3 container statuses recorded)
Jul 16 15:00:22.629: INFO: 	Container calico-node ready: true, restart count 0
Jul 16 15:00:22.629: INFO: 	Container install-cni ready: true, restart count 0
Jul 16 15:00:22.629: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 16 15:00:22.629: INFO: coredns-6f56645db5-hxmmb from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.629: INFO: 	Container coredns ready: true, restart count 0
Jul 16 15:00:22.629: INFO: cluster-autoscaler-7b894d5874-6bxbm from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.629: INFO: 	Container cluster-autoscaler ready: true, restart count 1
Jul 16 15:00:22.630: INFO: webterminal-64d57d745b-z8m6c from webterminal started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.630: INFO: 	Container webterminal ready: true, restart count 0
Jul 16 15:00:22.630: INFO: openvpn-client-95bcd78-6vrxz from kube-system started at 2019-07-16 12:59:02 +0000 UTC (2 container statuses recorded)
Jul 16 15:00:22.630: INFO: 	Container dnat-controller ready: true, restart count 0
Jul 16 15:00:22.630: INFO: 	Container openvpn-client ready: true, restart count 0
Jul 16 15:00:22.630: INFO: tiller-deploy-7c7d69dd75-kxhk8 from kube-system started at 2019-07-16 12:59:02 +0000 UTC (1 container statuses recorded)
Jul 16 15:00:22.630: INFO: 	Container tiller ready: true, restart count 0
Jul 16 15:00:22.630: INFO: sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-v2cx8 from heptio-sonobuoy started at 2019-07-16 13:04:46 +0000 UTC (2 container statuses recorded)
Jul 16 15:00:22.630: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 16 15:00:22.630: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node conformance-worker-54b54f4f98-f9trz
STEP: verifying the node has the label node conformance-worker-54b54f4f98-m9svg
STEP: verifying the node has the label node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.880: INFO: Pod sonobuoy requesting resource cpu=0m on Node conformance-worker-54b54f4f98-f9trz
Jul 16 15:00:22.880: INFO: Pod sonobuoy-e2e-job-352202216adf4593 requesting resource cpu=0m on Node conformance-worker-54b54f4f98-m9svg
Jul 16 15:00:22.880: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-mgtcd requesting resource cpu=0m on Node conformance-worker-54b54f4f98-f9trz
Jul 16 15:00:22.880: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-t84ms requesting resource cpu=0m on Node conformance-worker-54b54f4f98-m9svg
Jul 16 15:00:22.880: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ada67182f7c4045-v2cx8 requesting resource cpu=0m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.880: INFO: Pod canal-4r5mb requesting resource cpu=350m on Node conformance-worker-54b54f4f98-f9trz
Jul 16 15:00:22.880: INFO: Pod canal-mtsjt requesting resource cpu=350m on Node conformance-worker-54b54f4f98-m9svg
Jul 16 15:00:22.880: INFO: Pod canal-qhfws requesting resource cpu=350m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.880: INFO: Pod cluster-autoscaler-7b894d5874-6bxbm requesting resource cpu=10m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.881: INFO: Pod coredns-6f56645db5-86hnn requesting resource cpu=100m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.881: INFO: Pod coredns-6f56645db5-hxmmb requesting resource cpu=100m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.881: INFO: Pod kube-proxy-9l5xc requesting resource cpu=75m on Node conformance-worker-54b54f4f98-f9trz
Jul 16 15:00:22.881: INFO: Pod kube-proxy-nwtx4 requesting resource cpu=75m on Node conformance-worker-54b54f4f98-m9svg
Jul 16 15:00:22.881: INFO: Pod kube-proxy-vwpcz requesting resource cpu=75m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.881: INFO: Pod node-exporter-dk6wg requesting resource cpu=3m on Node conformance-worker-54b54f4f98-f9trz
Jul 16 15:00:22.881: INFO: Pod node-exporter-p6bc4 requesting resource cpu=3m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.881: INFO: Pod node-exporter-qrl7j requesting resource cpu=3m on Node conformance-worker-54b54f4f98-m9svg
Jul 16 15:00:22.881: INFO: Pod openvpn-client-95bcd78-6vrxz requesting resource cpu=30m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.881: INFO: Pod tiller-deploy-7c7d69dd75-kxhk8 requesting resource cpu=0m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.881: INFO: Pod restic-dqv4x requesting resource cpu=5m on Node conformance-worker-54b54f4f98-f9trz
Jul 16 15:00:22.881: INFO: Pod restic-pbp8t requesting resource cpu=5m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.881: INFO: Pod restic-vssb6 requesting resource cpu=5m on Node conformance-worker-54b54f4f98-m9svg
Jul 16 15:00:22.882: INFO: Pod velero-79764c8c47-l746r requesting resource cpu=10m on Node conformance-worker-54b54f4f98-nqplr
Jul 16 15:00:22.882: INFO: Pod webterminal-64d57d745b-z8m6c requesting resource cpu=0m on Node conformance-worker-54b54f4f98-nqplr
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f9e7a0c-c457-4f7c-b16b-79e4fd13b3d7.15b1eb43ffd95ddc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-960/filler-pod-2f9e7a0c-c457-4f7c-b16b-79e4fd13b3d7 to conformance-worker-54b54f4f98-f9trz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f9e7a0c-c457-4f7c-b16b-79e4fd13b3d7.15b1eb4460ba3d3c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f9e7a0c-c457-4f7c-b16b-79e4fd13b3d7.15b1eb446b70864c], Reason = [Created], Message = [Created container filler-pod-2f9e7a0c-c457-4f7c-b16b-79e4fd13b3d7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f9e7a0c-c457-4f7c-b16b-79e4fd13b3d7.15b1eb448399898a], Reason = [Started], Message = [Started container filler-pod-2f9e7a0c-c457-4f7c-b16b-79e4fd13b3d7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d22d4c1-ec1f-4689-ab84-0589a88fa7ca.15b1eb440284adde], Reason = [Scheduled], Message = [Successfully assigned sched-pred-960/filler-pod-5d22d4c1-ec1f-4689-ab84-0589a88fa7ca to conformance-worker-54b54f4f98-m9svg]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d22d4c1-ec1f-4689-ab84-0589a88fa7ca.15b1eb447ceb71b7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d22d4c1-ec1f-4689-ab84-0589a88fa7ca.15b1eb44864e0849], Reason = [Created], Message = [Created container filler-pod-5d22d4c1-ec1f-4689-ab84-0589a88fa7ca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d22d4c1-ec1f-4689-ab84-0589a88fa7ca.15b1eb44a230ba5c], Reason = [Started], Message = [Started container filler-pod-5d22d4c1-ec1f-4689-ab84-0589a88fa7ca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9e5d4e2-80f7-40d3-a7db-dafc6888e437.15b1eb43fefdc156], Reason = [Scheduled], Message = [Successfully assigned sched-pred-960/filler-pod-b9e5d4e2-80f7-40d3-a7db-dafc6888e437 to conformance-worker-54b54f4f98-nqplr]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9e5d4e2-80f7-40d3-a7db-dafc6888e437.15b1eb44645c701d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9e5d4e2-80f7-40d3-a7db-dafc6888e437.15b1eb4472c8884b], Reason = [Created], Message = [Created container filler-pod-b9e5d4e2-80f7-40d3-a7db-dafc6888e437]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9e5d4e2-80f7-40d3-a7db-dafc6888e437.15b1eb448c0168cc], Reason = [Started], Message = [Started container filler-pod-b9e5d4e2-80f7-40d3-a7db-dafc6888e437]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b1eb44f7dbacb6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node conformance-worker-54b54f4f98-f9trz
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node conformance-worker-54b54f4f98-m9svg
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node conformance-worker-54b54f4f98-nqplr
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:00:28.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-960" for this suite.
Jul 16 15:00:36.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:00:37.026: INFO: namespace sched-pred-960 deletion completed in 8.559727877s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:15.121 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 15:00:37.030: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul 16 15:01:19.208: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0716 15:01:19.208711      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:01:19.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7186" for this suite.
Jul 16 15:01:29.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:01:29.561: INFO: namespace gc-7186 deletion completed in 10.343083836s

• [SLOW TEST:52.531 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 15:01:29.565: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3889.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3889.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 16 15:01:34.883: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c: the server could not find the requested resource (get pods dns-test-5e261986-63c8-45a9-b557-1ced8d11706c)
Jul 16 15:01:34.930: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c: the server could not find the requested resource (get pods dns-test-5e261986-63c8-45a9-b557-1ced8d11706c)
Jul 16 15:01:34.942: INFO: Unable to read wheezy_udp@PodARecord from pod dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c: the server could not find the requested resource (get pods dns-test-5e261986-63c8-45a9-b557-1ced8d11706c)
Jul 16 15:01:35.031: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c: the server could not find the requested resource (get pods dns-test-5e261986-63c8-45a9-b557-1ced8d11706c)
Jul 16 15:01:35.042: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c: the server could not find the requested resource (get pods dns-test-5e261986-63c8-45a9-b557-1ced8d11706c)
Jul 16 15:01:35.065: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c: the server could not find the requested resource (get pods dns-test-5e261986-63c8-45a9-b557-1ced8d11706c)
Jul 16 15:01:35.075: INFO: Unable to read jessie_udp@PodARecord from pod dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c: the server could not find the requested resource (get pods dns-test-5e261986-63c8-45a9-b557-1ced8d11706c)
Jul 16 15:01:35.085: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c: the server could not find the requested resource (get pods dns-test-5e261986-63c8-45a9-b557-1ced8d11706c)
Jul 16 15:01:35.085: INFO: Lookups using dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul 16 15:01:40.700: INFO: DNS probes using dns-3889/dns-test-5e261986-63c8-45a9-b557-1ced8d11706c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:01:40.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3889" for this suite.
Jul 16 15:01:46.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:01:47.100: INFO: namespace dns-3889 deletion completed in 6.32276607s

• [SLOW TEST:17.537 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 15:01:47.104: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jul 16 15:01:47.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 create -f - --namespace=kubectl-8173'
Jul 16 15:01:47.813: INFO: stderr: ""
Jul 16 15:01:47.813: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jul 16 15:01:48.858: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 15:01:48.858: INFO: Found 0 / 1
Jul 16 15:01:49.822: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 15:01:49.822: INFO: Found 0 / 1
Jul 16 15:01:50.820: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 15:01:50.820: INFO: Found 1 / 1
Jul 16 15:01:50.820: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 16 15:01:50.828: INFO: Selector matched 1 pods for map[app:redis]
Jul 16 15:01:50.828: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 16 15:01:50.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 logs redis-master-fmmcf redis-master --namespace=kubectl-8173'
Jul 16 15:01:51.063: INFO: stderr: ""
Jul 16 15:01:51.063: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 Jul 15:01:49.885 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jul 15:01:49.885 # Server started, Redis version 3.2.12\n1:M 16 Jul 15:01:49.885 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jul 15:01:49.885 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 16 15:01:51.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 log redis-master-fmmcf redis-master --namespace=kubectl-8173 --tail=1'
Jul 16 15:01:51.211: INFO: stderr: ""
Jul 16 15:01:51.211: INFO: stdout: "1:M 16 Jul 15:01:49.885 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 16 15:01:51.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 log redis-master-fmmcf redis-master --namespace=kubectl-8173 --limit-bytes=1'
Jul 16 15:01:51.388: INFO: stderr: ""
Jul 16 15:01:51.388: INFO: stdout: " "
STEP: exposing timestamps
Jul 16 15:01:51.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 log redis-master-fmmcf redis-master --namespace=kubectl-8173 --tail=1 --timestamps'
Jul 16 15:01:51.606: INFO: stderr: ""
Jul 16 15:01:51.606: INFO: stdout: "2019-07-16T15:01:49.886405057Z 1:M 16 Jul 15:01:49.885 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 16 15:01:54.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 log redis-master-fmmcf redis-master --namespace=kubectl-8173 --since=1s'
Jul 16 15:01:54.361: INFO: stderr: ""
Jul 16 15:01:54.361: INFO: stdout: ""
Jul 16 15:01:54.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 log redis-master-fmmcf redis-master --namespace=kubectl-8173 --since=24h'
Jul 16 15:01:54.659: INFO: stderr: ""
Jul 16 15:01:54.659: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 Jul 15:01:49.885 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jul 15:01:49.885 # Server started, Redis version 3.2.12\n1:M 16 Jul 15:01:49.885 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jul 15:01:49.885 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jul 16 15:01:54.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete --grace-period=0 --force -f - --namespace=kubectl-8173'
Jul 16 15:01:54.823: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 16 15:01:54.823: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul 16 15:01:54.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get rc,svc -l name=nginx --no-headers --namespace=kubectl-8173'
Jul 16 15:01:55.057: INFO: stderr: "No resources found.\n"
Jul 16 15:01:55.057: INFO: stdout: ""
Jul 16 15:01:55.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 get pods -l name=nginx --namespace=kubectl-8173 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 16 15:01:55.236: INFO: stderr: ""
Jul 16 15:01:55.236: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:01:55.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8173" for this suite.
Jul 16 15:02:19.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:02:19.591: INFO: namespace kubectl-8173 deletion completed in 24.345305342s

• [SLOW TEST:32.488 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 15:02:19.593: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 16 15:02:19.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3586'
Jul 16 15:02:19.918: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 16 15:02:19.918: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jul 16 15:02:21.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-981533691 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3586'
Jul 16 15:02:22.088: INFO: stderr: ""
Jul 16 15:02:22.088: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:02:22.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3586" for this suite.
Jul 16 15:03:52.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:03:52.524: INFO: namespace kubectl-3586 deletion completed in 1m30.426316851s

• [SLOW TEST:92.932 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 15:03:52.532: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 16 15:03:52.962: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5165,SelfLink:/api/v1/namespaces/watch-5165/configmaps/e2e-watch-test-resource-version,UID:15772b85-5f66-4e4d-b09c-4bea35693754,ResourceVersion:37019,Generation:0,CreationTimestamp:2019-07-16 15:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 16 15:03:52.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5165,SelfLink:/api/v1/namespaces/watch-5165/configmaps/e2e-watch-test-resource-version,UID:15772b85-5f66-4e4d-b09c-4bea35693754,ResourceVersion:37020,Generation:0,CreationTimestamp:2019-07-16 15:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:03:52.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5165" for this suite.
Jul 16 15:04:01.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:04:01.416: INFO: namespace watch-5165 deletion completed in 8.441601058s

• [SLOW TEST:8.884 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 15:04:01.420: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-30615dff-fecc-4f4e-8602-5cd2f74a2641
STEP: Creating secret with name s-test-opt-upd-912e743d-8821-455b-92bc-8a32bb1ae7f7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-30615dff-fecc-4f4e-8602-5cd2f74a2641
STEP: Updating secret s-test-opt-upd-912e743d-8821-455b-92bc-8a32bb1ae7f7
STEP: Creating secret with name s-test-opt-create-e4645140-972a-48fe-b127-3766de665875
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:04:10.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2505" for this suite.
Jul 16 15:04:34.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:04:34.834: INFO: namespace secrets-2505 deletion completed in 24.366848903s

• [SLOW TEST:33.414 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 16 15:04:34.843: INFO: >>> kubeConfig: /tmp/kubeconfig-981533691
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 16 15:04:34.971: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 16 15:04:36.091: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 16 15:04:36.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7463" for this suite.
Jul 16 15:04:42.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 16 15:04:42.509: INFO: namespace replication-controller-7463 deletion completed in 6.39786861s

• [SLOW TEST:7.667 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSJul 16 15:04:42.510: INFO: Running AfterSuite actions on all nodes
Jul 16 15:04:42.511: INFO: Running AfterSuite actions on node 1
Jul 16 15:04:42.511: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 7103.058 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h58m27.54730777s
Test Suite Passed
