I0729 01:38:35.736825      14 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-518829307
I0729 01:38:35.738255      14 e2e.go:241] Starting e2e run "80461da4-1194-457e-b778-7e12a55048d7" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1564364308 - Will randomize all specs
Will run 215 of 4413 specs

Jul 29 01:38:36.121: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 01:38:36.149: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 29 01:38:36.522: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 29 01:38:36.564: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 29 01:38:36.565: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jul 29 01:38:36.565: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 29 01:38:36.580: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Jul 29 01:38:36.580: INFO: e2e test version: v1.15.1
Jul 29 01:38:36.582: INFO: kube-apiserver version: v1.15.1
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:38:36.583: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
Jul 29 01:38:36.651: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 01:38:36.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-378ec927-df56-45b9-8728-6f1d8bcc1b88" in namespace "projected-3791" to be "success or failure"
Jul 29 01:38:36.726: INFO: Pod "downwardapi-volume-378ec927-df56-45b9-8728-6f1d8bcc1b88": Phase="Pending", Reason="", readiness=false. Elapsed: 33.124978ms
Jul 29 01:38:38.733: INFO: Pod "downwardapi-volume-378ec927-df56-45b9-8728-6f1d8bcc1b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039842122s
Jul 29 01:38:40.738: INFO: Pod "downwardapi-volume-378ec927-df56-45b9-8728-6f1d8bcc1b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045277704s
STEP: Saw pod success
Jul 29 01:38:40.739: INFO: Pod "downwardapi-volume-378ec927-df56-45b9-8728-6f1d8bcc1b88" satisfied condition "success or failure"
Jul 29 01:38:40.742: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-378ec927-df56-45b9-8728-6f1d8bcc1b88 container client-container: <nil>
STEP: delete the pod
Jul 29 01:38:40.794: INFO: Waiting for pod downwardapi-volume-378ec927-df56-45b9-8728-6f1d8bcc1b88 to disappear
Jul 29 01:38:40.800: INFO: Pod downwardapi-volume-378ec927-df56-45b9-8728-6f1d8bcc1b88 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:38:40.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3791" for this suite.
Jul 29 01:38:46.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:38:47.068: INFO: namespace projected-3791 deletion completed in 6.253827222s

• [SLOW TEST:10.485 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:38:47.072: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-qjf2
STEP: Creating a pod to test atomic-volume-subpath
Jul 29 01:38:47.192: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qjf2" in namespace "subpath-7051" to be "success or failure"
Jul 29 01:38:47.210: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.397274ms
Jul 29 01:38:49.217: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025158381s
Jul 29 01:38:51.239: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 4.046793449s
Jul 29 01:38:53.244: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 6.052258873s
Jul 29 01:38:55.265: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 8.0735437s
Jul 29 01:38:57.276: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 10.083929302s
Jul 29 01:38:59.282: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 12.090253737s
Jul 29 01:39:01.287: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 14.09502107s
Jul 29 01:39:03.301: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 16.109301256s
Jul 29 01:39:05.325: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 18.132993268s
Jul 29 01:39:07.330: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 20.137867734s
Jul 29 01:39:09.372: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Running", Reason="", readiness=true. Elapsed: 22.180381646s
Jul 29 01:39:11.382: INFO: Pod "pod-subpath-test-configmap-qjf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.190108859s
STEP: Saw pod success
Jul 29 01:39:11.382: INFO: Pod "pod-subpath-test-configmap-qjf2" satisfied condition "success or failure"
Jul 29 01:39:11.388: INFO: Trying to get logs from node conformance1 pod pod-subpath-test-configmap-qjf2 container test-container-subpath-configmap-qjf2: <nil>
STEP: delete the pod
Jul 29 01:39:11.490: INFO: Waiting for pod pod-subpath-test-configmap-qjf2 to disappear
Jul 29 01:39:11.507: INFO: Pod pod-subpath-test-configmap-qjf2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qjf2
Jul 29 01:39:11.507: INFO: Deleting pod "pod-subpath-test-configmap-qjf2" in namespace "subpath-7051"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:39:11.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7051" for this suite.
Jul 29 01:39:19.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:39:19.899: INFO: namespace subpath-7051 deletion completed in 8.360570563s

• [SLOW TEST:32.828 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:39:19.907: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-1fa08df2-b8c1-4b3b-880c-973d332ae129
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:39:20.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-888" for this suite.
Jul 29 01:39:26.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:39:26.683: INFO: namespace secrets-888 deletion completed in 6.371955257s

• [SLOW TEST:6.776 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:39:26.691: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 29 01:39:26.833: INFO: Waiting up to 5m0s for pod "downward-api-56699f1e-ae58-4812-bcbd-57eef555029e" in namespace "downward-api-2614" to be "success or failure"
Jul 29 01:39:26.859: INFO: Pod "downward-api-56699f1e-ae58-4812-bcbd-57eef555029e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.447557ms
Jul 29 01:39:28.864: INFO: Pod "downward-api-56699f1e-ae58-4812-bcbd-57eef555029e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03159376s
Jul 29 01:39:30.869: INFO: Pod "downward-api-56699f1e-ae58-4812-bcbd-57eef555029e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035958088s
Jul 29 01:39:32.875: INFO: Pod "downward-api-56699f1e-ae58-4812-bcbd-57eef555029e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042142204s
STEP: Saw pod success
Jul 29 01:39:32.875: INFO: Pod "downward-api-56699f1e-ae58-4812-bcbd-57eef555029e" satisfied condition "success or failure"
Jul 29 01:39:32.879: INFO: Trying to get logs from node conformance1 pod downward-api-56699f1e-ae58-4812-bcbd-57eef555029e container dapi-container: <nil>
STEP: delete the pod
Jul 29 01:39:32.916: INFO: Waiting for pod downward-api-56699f1e-ae58-4812-bcbd-57eef555029e to disappear
Jul 29 01:39:32.941: INFO: Pod downward-api-56699f1e-ae58-4812-bcbd-57eef555029e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:39:32.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2614" for this suite.
Jul 29 01:39:38.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:39:39.206: INFO: namespace downward-api-2614 deletion completed in 6.259338229s

• [SLOW TEST:12.515 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:39:39.206: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-573bfff1-0ae7-49c7-989c-4d553b7c2aaa
STEP: Creating a pod to test consume secrets
Jul 29 01:39:39.384: INFO: Waiting up to 5m0s for pod "pod-secrets-24754052-f187-4ffd-af05-536fa0534da6" in namespace "secrets-4485" to be "success or failure"
Jul 29 01:39:39.396: INFO: Pod "pod-secrets-24754052-f187-4ffd-af05-536fa0534da6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.4455ms
Jul 29 01:39:41.406: INFO: Pod "pod-secrets-24754052-f187-4ffd-af05-536fa0534da6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021620613s
Jul 29 01:39:43.416: INFO: Pod "pod-secrets-24754052-f187-4ffd-af05-536fa0534da6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032101191s
Jul 29 01:39:45.421: INFO: Pod "pod-secrets-24754052-f187-4ffd-af05-536fa0534da6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037260474s
STEP: Saw pod success
Jul 29 01:39:45.421: INFO: Pod "pod-secrets-24754052-f187-4ffd-af05-536fa0534da6" satisfied condition "success or failure"
Jul 29 01:39:45.437: INFO: Trying to get logs from node conformance1 pod pod-secrets-24754052-f187-4ffd-af05-536fa0534da6 container secret-env-test: <nil>
STEP: delete the pod
Jul 29 01:39:45.501: INFO: Waiting for pod pod-secrets-24754052-f187-4ffd-af05-536fa0534da6 to disappear
Jul 29 01:39:45.505: INFO: Pod pod-secrets-24754052-f187-4ffd-af05-536fa0534da6 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:39:45.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4485" for this suite.
Jul 29 01:39:53.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:39:54.045: INFO: namespace secrets-4485 deletion completed in 8.509377973s

• [SLOW TEST:14.840 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:39:54.046: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-638ef70e-0a51-47bb-8468-f223ceba5e36
STEP: Creating a pod to test consume configMaps
Jul 29 01:39:54.310: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ca657da-7605-468c-a65d-191cc3722c98" in namespace "configmap-6829" to be "success or failure"
Jul 29 01:39:54.318: INFO: Pod "pod-configmaps-8ca657da-7605-468c-a65d-191cc3722c98": Phase="Pending", Reason="", readiness=false. Elapsed: 8.231661ms
Jul 29 01:39:56.323: INFO: Pod "pod-configmaps-8ca657da-7605-468c-a65d-191cc3722c98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013057363s
Jul 29 01:39:58.328: INFO: Pod "pod-configmaps-8ca657da-7605-468c-a65d-191cc3722c98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018052409s
STEP: Saw pod success
Jul 29 01:39:58.329: INFO: Pod "pod-configmaps-8ca657da-7605-468c-a65d-191cc3722c98" satisfied condition "success or failure"
Jul 29 01:39:58.333: INFO: Trying to get logs from node conformance0 pod pod-configmaps-8ca657da-7605-468c-a65d-191cc3722c98 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 01:39:58.402: INFO: Waiting for pod pod-configmaps-8ca657da-7605-468c-a65d-191cc3722c98 to disappear
Jul 29 01:39:58.418: INFO: Pod pod-configmaps-8ca657da-7605-468c-a65d-191cc3722c98 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:39:58.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6829" for this suite.
Jul 29 01:40:04.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:40:04.855: INFO: namespace configmap-6829 deletion completed in 6.427797895s

• [SLOW TEST:10.809 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:40:04.858: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jul 29 01:40:04.976: INFO: Waiting up to 5m0s for pod "var-expansion-28c5886e-7375-4118-b0d9-600466bcc613" in namespace "var-expansion-8044" to be "success or failure"
Jul 29 01:40:05.008: INFO: Pod "var-expansion-28c5886e-7375-4118-b0d9-600466bcc613": Phase="Pending", Reason="", readiness=false. Elapsed: 32.061737ms
Jul 29 01:40:07.013: INFO: Pod "var-expansion-28c5886e-7375-4118-b0d9-600466bcc613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037055889s
Jul 29 01:40:09.019: INFO: Pod "var-expansion-28c5886e-7375-4118-b0d9-600466bcc613": Phase="Running", Reason="", readiness=true. Elapsed: 4.043102904s
Jul 29 01:40:11.025: INFO: Pod "var-expansion-28c5886e-7375-4118-b0d9-600466bcc613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048503006s
STEP: Saw pod success
Jul 29 01:40:11.025: INFO: Pod "var-expansion-28c5886e-7375-4118-b0d9-600466bcc613" satisfied condition "success or failure"
Jul 29 01:40:11.029: INFO: Trying to get logs from node conformance1 pod var-expansion-28c5886e-7375-4118-b0d9-600466bcc613 container dapi-container: <nil>
STEP: delete the pod
Jul 29 01:40:11.076: INFO: Waiting for pod var-expansion-28c5886e-7375-4118-b0d9-600466bcc613 to disappear
Jul 29 01:40:11.092: INFO: Pod var-expansion-28c5886e-7375-4118-b0d9-600466bcc613 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:40:11.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8044" for this suite.
Jul 29 01:40:17.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:40:17.491: INFO: namespace var-expansion-8044 deletion completed in 6.391623917s

• [SLOW TEST:12.633 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:40:17.499: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 29 01:40:17.634: INFO: Waiting up to 5m0s for pod "pod-389a5644-47c0-4f33-86aa-9cae92a0aea7" in namespace "emptydir-7679" to be "success or failure"
Jul 29 01:40:17.666: INFO: Pod "pod-389a5644-47c0-4f33-86aa-9cae92a0aea7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.381006ms
Jul 29 01:40:19.671: INFO: Pod "pod-389a5644-47c0-4f33-86aa-9cae92a0aea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036933338s
Jul 29 01:40:21.675: INFO: Pod "pod-389a5644-47c0-4f33-86aa-9cae92a0aea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041515906s
STEP: Saw pod success
Jul 29 01:40:21.676: INFO: Pod "pod-389a5644-47c0-4f33-86aa-9cae92a0aea7" satisfied condition "success or failure"
Jul 29 01:40:21.679: INFO: Trying to get logs from node conformance0 pod pod-389a5644-47c0-4f33-86aa-9cae92a0aea7 container test-container: <nil>
STEP: delete the pod
Jul 29 01:40:21.708: INFO: Waiting for pod pod-389a5644-47c0-4f33-86aa-9cae92a0aea7 to disappear
Jul 29 01:40:21.712: INFO: Pod pod-389a5644-47c0-4f33-86aa-9cae92a0aea7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:40:21.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7679" for this suite.
Jul 29 01:40:27.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:40:27.920: INFO: namespace emptydir-7679 deletion completed in 6.200747399s

• [SLOW TEST:10.422 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:40:27.921: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 29 01:40:32.758: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-630 pod-service-account-567aec1b-81d6-4b1f-bac9-ba84eaa916b2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 29 01:40:35.263: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-630 pod-service-account-567aec1b-81d6-4b1f-bac9-ba84eaa916b2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 29 01:40:35.745: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-630 pod-service-account-567aec1b-81d6-4b1f-bac9-ba84eaa916b2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:40:36.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-630" for this suite.
Jul 29 01:40:42.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:40:42.466: INFO: namespace svcaccounts-630 deletion completed in 6.173915928s

• [SLOW TEST:14.546 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:40:42.470: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 29 01:40:42.559: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-a,UID:cfb78001-dfe0-43f7-b721-1747c14bdf11,ResourceVersion:1035,Generation:0,CreationTimestamp:2019-07-29 01:40:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 29 01:40:42.559: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-a,UID:cfb78001-dfe0-43f7-b721-1747c14bdf11,ResourceVersion:1035,Generation:0,CreationTimestamp:2019-07-29 01:40:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 29 01:40:52.570: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-a,UID:cfb78001-dfe0-43f7-b721-1747c14bdf11,ResourceVersion:1041,Generation:0,CreationTimestamp:2019-07-29 01:40:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 29 01:40:52.570: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-a,UID:cfb78001-dfe0-43f7-b721-1747c14bdf11,ResourceVersion:1041,Generation:0,CreationTimestamp:2019-07-29 01:40:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 29 01:41:02.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-a,UID:cfb78001-dfe0-43f7-b721-1747c14bdf11,ResourceVersion:1046,Generation:0,CreationTimestamp:2019-07-29 01:40:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 29 01:41:02.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-a,UID:cfb78001-dfe0-43f7-b721-1747c14bdf11,ResourceVersion:1046,Generation:0,CreationTimestamp:2019-07-29 01:40:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 29 01:41:12.590: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-a,UID:cfb78001-dfe0-43f7-b721-1747c14bdf11,ResourceVersion:1052,Generation:0,CreationTimestamp:2019-07-29 01:40:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 29 01:41:12.590: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-a,UID:cfb78001-dfe0-43f7-b721-1747c14bdf11,ResourceVersion:1052,Generation:0,CreationTimestamp:2019-07-29 01:40:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 29 01:41:22.601: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-b,UID:dd030db6-e988-4939-812f-c0e08c12011a,ResourceVersion:1058,Generation:0,CreationTimestamp:2019-07-29 01:41:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 29 01:41:22.601: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-b,UID:dd030db6-e988-4939-812f-c0e08c12011a,ResourceVersion:1058,Generation:0,CreationTimestamp:2019-07-29 01:41:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 29 01:41:32.609: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-b,UID:dd030db6-e988-4939-812f-c0e08c12011a,ResourceVersion:1063,Generation:0,CreationTimestamp:2019-07-29 01:41:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 29 01:41:32.609: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7917,SelfLink:/api/v1/namespaces/watch-7917/configmaps/e2e-watch-test-configmap-b,UID:dd030db6-e988-4939-812f-c0e08c12011a,ResourceVersion:1063,Generation:0,CreationTimestamp:2019-07-29 01:41:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:41:42.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7917" for this suite.
Jul 29 01:41:48.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:41:48.816: INFO: namespace watch-7917 deletion completed in 6.199698912s

• [SLOW TEST:66.347 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:41:48.823: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jul 29 01:41:48.900: INFO: Waiting up to 5m0s for pod "client-containers-80e768d9-68d3-4f1f-97a1-c2d60315ce26" in namespace "containers-2257" to be "success or failure"
Jul 29 01:41:48.907: INFO: Pod "client-containers-80e768d9-68d3-4f1f-97a1-c2d60315ce26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.374757ms
Jul 29 01:41:50.916: INFO: Pod "client-containers-80e768d9-68d3-4f1f-97a1-c2d60315ce26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015793029s
Jul 29 01:41:52.922: INFO: Pod "client-containers-80e768d9-68d3-4f1f-97a1-c2d60315ce26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021540605s
STEP: Saw pod success
Jul 29 01:41:52.922: INFO: Pod "client-containers-80e768d9-68d3-4f1f-97a1-c2d60315ce26" satisfied condition "success or failure"
Jul 29 01:41:52.928: INFO: Trying to get logs from node conformance0 pod client-containers-80e768d9-68d3-4f1f-97a1-c2d60315ce26 container test-container: <nil>
STEP: delete the pod
Jul 29 01:41:53.272: INFO: Waiting for pod client-containers-80e768d9-68d3-4f1f-97a1-c2d60315ce26 to disappear
Jul 29 01:41:53.280: INFO: Pod client-containers-80e768d9-68d3-4f1f-97a1-c2d60315ce26 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:41:53.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2257" for this suite.
Jul 29 01:41:59.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:41:59.499: INFO: namespace containers-2257 deletion completed in 6.210074367s

• [SLOW TEST:10.676 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:41:59.500: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 29 01:41:59.570: INFO: Waiting up to 5m0s for pod "downward-api-e5ec22d7-808b-4d45-9b50-8a56f639f800" in namespace "downward-api-9497" to be "success or failure"
Jul 29 01:41:59.605: INFO: Pod "downward-api-e5ec22d7-808b-4d45-9b50-8a56f639f800": Phase="Pending", Reason="", readiness=false. Elapsed: 35.051211ms
Jul 29 01:42:01.611: INFO: Pod "downward-api-e5ec22d7-808b-4d45-9b50-8a56f639f800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041330669s
Jul 29 01:42:03.640: INFO: Pod "downward-api-e5ec22d7-808b-4d45-9b50-8a56f639f800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069786195s
STEP: Saw pod success
Jul 29 01:42:03.640: INFO: Pod "downward-api-e5ec22d7-808b-4d45-9b50-8a56f639f800" satisfied condition "success or failure"
Jul 29 01:42:03.647: INFO: Trying to get logs from node conformance1 pod downward-api-e5ec22d7-808b-4d45-9b50-8a56f639f800 container dapi-container: <nil>
STEP: delete the pod
Jul 29 01:42:03.749: INFO: Waiting for pod downward-api-e5ec22d7-808b-4d45-9b50-8a56f639f800 to disappear
Jul 29 01:42:03.755: INFO: Pod downward-api-e5ec22d7-808b-4d45-9b50-8a56f639f800 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:42:03.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9497" for this suite.
Jul 29 01:42:09.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:42:09.977: INFO: namespace downward-api-9497 deletion completed in 6.210609858s

• [SLOW TEST:10.478 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:42:09.982: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jul 29 01:42:10.060: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-518829307 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:42:10.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1370" for this suite.
Jul 29 01:42:16.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:42:16.414: INFO: namespace kubectl-1370 deletion completed in 6.19698875s

• [SLOW TEST:6.432 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:42:16.422: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 29 01:42:16.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6605'
Jul 29 01:42:16.706: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 29 01:42:16.706: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 29 01:42:16.716: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-27nkv]
Jul 29 01:42:16.716: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-27nkv" in namespace "kubectl-6605" to be "running and ready"
Jul 29 01:42:16.721: INFO: Pod "e2e-test-nginx-rc-27nkv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.793138ms
Jul 29 01:42:18.760: INFO: Pod "e2e-test-nginx-rc-27nkv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043711103s
Jul 29 01:42:20.765: INFO: Pod "e2e-test-nginx-rc-27nkv": Phase="Running", Reason="", readiness=true. Elapsed: 4.048651004s
Jul 29 01:42:20.765: INFO: Pod "e2e-test-nginx-rc-27nkv" satisfied condition "running and ready"
Jul 29 01:42:20.765: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-27nkv]
Jul 29 01:42:20.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 logs rc/e2e-test-nginx-rc --namespace=kubectl-6605'
Jul 29 01:42:21.003: INFO: stderr: ""
Jul 29 01:42:21.004: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jul 29 01:42:21.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete rc e2e-test-nginx-rc --namespace=kubectl-6605'
Jul 29 01:42:21.179: INFO: stderr: ""
Jul 29 01:42:21.179: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:42:21.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6605" for this suite.
Jul 29 01:42:43.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:42:43.411: INFO: namespace kubectl-6605 deletion completed in 22.223757205s

• [SLOW TEST:26.989 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:42:43.414: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:42:43.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3545" for this suite.
Jul 29 01:43:05.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:43:05.798: INFO: namespace kubelet-test-3545 deletion completed in 22.213018206s

• [SLOW TEST:22.384 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:43:05.802: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 01:43:05.873: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 29 01:43:10.879: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 29 01:43:10.879: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 29 01:43:12.885: INFO: Creating deployment "test-rollover-deployment"
Jul 29 01:43:12.902: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 29 01:43:14.926: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 29 01:43:14.944: INFO: Ensure that both replica sets have 1 created replica
Jul 29 01:43:14.956: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 29 01:43:14.976: INFO: Updating deployment test-rollover-deployment
Jul 29 01:43:14.976: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 29 01:43:17.036: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 29 01:43:17.046: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 29 01:43:17.066: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 01:43:17.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961395, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 01:43:19.076: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 01:43:19.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961397, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 01:43:21.076: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 01:43:21.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961397, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 01:43:23.079: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 01:43:23.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961397, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 01:43:25.079: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 01:43:25.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961397, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 01:43:27.079: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 01:43:27.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961397, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961392, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 01:43:29.079: INFO: 
Jul 29 01:43:29.079: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 29 01:43:29.126: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-769,SelfLink:/apis/apps/v1/namespaces/deployment-769/deployments/test-rollover-deployment,UID:58f647d9-cab7-4207-b3ed-4acb0626328e,ResourceVersion:1301,Generation:2,CreationTimestamp:2019-07-29 01:43:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-29 01:43:12 +0000 UTC 2019-07-29 01:43:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-29 01:43:28 +0000 UTC 2019-07-29 01:43:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 29 01:43:29.135: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-769,SelfLink:/apis/apps/v1/namespaces/deployment-769/replicasets/test-rollover-deployment-854595fc44,UID:f8158741-dc28-4cc1-a1e5-61a42abc797e,ResourceVersion:1290,Generation:2,CreationTimestamp:2019-07-29 01:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 58f647d9-cab7-4207-b3ed-4acb0626328e 0xc002fa5737 0xc002fa5738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 29 01:43:29.135: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 29 01:43:29.135: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-769,SelfLink:/apis/apps/v1/namespaces/deployment-769/replicasets/test-rollover-controller,UID:071b925b-dca7-4983-b50d-a9aebefc4970,ResourceVersion:1300,Generation:2,CreationTimestamp:2019-07-29 01:43:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 58f647d9-cab7-4207-b3ed-4acb0626328e 0xc002fa565f 0xc002fa5670}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 29 01:43:29.135: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-769,SelfLink:/apis/apps/v1/namespaces/deployment-769/replicasets/test-rollover-deployment-9b8b997cf,UID:50dfe5d7-9b01-4ee9-9b23-6d11032b0113,ResourceVersion:1270,Generation:2,CreationTimestamp:2019-07-29 01:43:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 58f647d9-cab7-4207-b3ed-4acb0626328e 0xc002fa57f0 0xc002fa57f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 29 01:43:29.143: INFO: Pod "test-rollover-deployment-854595fc44-vcmv7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-vcmv7,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-769,SelfLink:/api/v1/namespaces/deployment-769/pods/test-rollover-deployment-854595fc44-vcmv7,UID:f8c97484-401e-4645-8a93-4b08bf9dc5ca,ResourceVersion:1283,Generation:0,CreationTimestamp:2019-07-29 01:43:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 f8158741-dc28-4cc1-a1e5-61a42abc797e 0xc001e425a7 0xc001e425a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-92xsm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-92xsm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-92xsm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e42620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e42640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 01:43:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 01:43:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 01:43:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 01:43:15 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:10.244.1.10,StartTime:2019-07-29 01:43:15 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-29 01:43:16 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2e306a716ce5c6c27125e9e9def50fbb41be359adb39f3c46ae7fec62722754a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:43:29.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-769" for this suite.
Jul 29 01:43:35.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:43:35.404: INFO: namespace deployment-769 deletion completed in 6.254427335s

• [SLOW TEST:29.603 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:43:35.407: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1554
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 29 01:43:35.577: INFO: Found 0 stateful pods, waiting for 3
Jul 29 01:43:45.589: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 01:43:45.590: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 01:43:45.590: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 29 01:43:45.632: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 29 01:43:55.685: INFO: Updating stateful set ss2
Jul 29 01:43:55.744: INFO: Waiting for Pod statefulset-1554/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jul 29 01:44:06.070: INFO: Found 2 stateful pods, waiting for 3
Jul 29 01:44:16.080: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 01:44:16.080: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 01:44:16.080: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 29 01:44:16.115: INFO: Updating stateful set ss2
Jul 29 01:44:16.149: INFO: Waiting for Pod statefulset-1554/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 29 01:44:26.169: INFO: Waiting for Pod statefulset-1554/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 29 01:44:36.189: INFO: Updating stateful set ss2
Jul 29 01:44:36.259: INFO: Waiting for StatefulSet statefulset-1554/ss2 to complete update
Jul 29 01:44:36.259: INFO: Waiting for Pod statefulset-1554/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 29 01:44:46.268: INFO: Deleting all statefulset in ns statefulset-1554
Jul 29 01:44:46.273: INFO: Scaling statefulset ss2 to 0
Jul 29 01:45:16.356: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 01:45:16.361: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:45:16.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1554" for this suite.
Jul 29 01:45:22.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:45:22.586: INFO: namespace statefulset-1554 deletion completed in 6.188586797s

• [SLOW TEST:107.180 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:45:22.595: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-919c967b-43be-4d24-ac8a-421544dc4d98
STEP: Creating secret with name s-test-opt-upd-60e41ff9-b79e-4c13-a0ff-6474f3ab48c6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-919c967b-43be-4d24-ac8a-421544dc4d98
STEP: Updating secret s-test-opt-upd-60e41ff9-b79e-4c13-a0ff-6474f3ab48c6
STEP: Creating secret with name s-test-opt-create-b02e03b7-3c2e-4fc4-bc90-91a0e0286e65
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:45:28.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8585" for this suite.
Jul 29 01:45:50.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:45:51.135: INFO: namespace projected-8585 deletion completed in 22.269065606s

• [SLOW TEST:28.540 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:45:51.135: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 01:45:51.216: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:45:52.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5778" for this suite.
Jul 29 01:45:58.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:45:58.821: INFO: namespace custom-resource-definition-5778 deletion completed in 6.196211379s

• [SLOW TEST:7.686 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:45:58.825: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 29 01:45:59.157: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 29 01:46:04.163: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:46:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3712" for this suite.
Jul 29 01:46:11.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:46:11.474: INFO: namespace replication-controller-3712 deletion completed in 6.277273036s

• [SLOW TEST:12.650 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:46:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-6d47
STEP: Creating a pod to test atomic-volume-subpath
Jul 29 01:46:11.676: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6d47" in namespace "subpath-4201" to be "success or failure"
Jul 29 01:46:11.697: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Pending", Reason="", readiness=false. Elapsed: 20.984593ms
Jul 29 01:46:13.718: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042115857s
Jul 29 01:46:15.724: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 4.04790208s
Jul 29 01:46:17.729: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 6.052877816s
Jul 29 01:46:19.739: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 8.062867973s
Jul 29 01:46:21.745: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 10.068775557s
Jul 29 01:46:23.752: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 12.075885818s
Jul 29 01:46:25.757: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 14.080737812s
Jul 29 01:46:27.762: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 16.085891857s
Jul 29 01:46:29.767: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 18.091334122s
Jul 29 01:46:31.773: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 20.097525755s
Jul 29 01:46:33.789: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Running", Reason="", readiness=true. Elapsed: 22.11337012s
Jul 29 01:46:35.798: INFO: Pod "pod-subpath-test-configmap-6d47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.121850966s
STEP: Saw pod success
Jul 29 01:46:35.798: INFO: Pod "pod-subpath-test-configmap-6d47" satisfied condition "success or failure"
Jul 29 01:46:35.803: INFO: Trying to get logs from node conformance0 pod pod-subpath-test-configmap-6d47 container test-container-subpath-configmap-6d47: <nil>
STEP: delete the pod
Jul 29 01:46:35.854: INFO: Waiting for pod pod-subpath-test-configmap-6d47 to disappear
Jul 29 01:46:35.859: INFO: Pod pod-subpath-test-configmap-6d47 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6d47
Jul 29 01:46:35.859: INFO: Deleting pod "pod-subpath-test-configmap-6d47" in namespace "subpath-4201"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:46:35.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4201" for this suite.
Jul 29 01:46:41.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:46:42.105: INFO: namespace subpath-4201 deletion completed in 6.236471351s

• [SLOW TEST:30.630 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:46:42.110: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 29 01:46:42.185: INFO: Waiting up to 5m0s for pod "downward-api-a43a00cb-b758-4897-8b28-02131aa3a4a2" in namespace "downward-api-2830" to be "success or failure"
Jul 29 01:46:42.191: INFO: Pod "downward-api-a43a00cb-b758-4897-8b28-02131aa3a4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.160334ms
Jul 29 01:46:44.197: INFO: Pod "downward-api-a43a00cb-b758-4897-8b28-02131aa3a4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012197714s
Jul 29 01:46:46.206: INFO: Pod "downward-api-a43a00cb-b758-4897-8b28-02131aa3a4a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020577141s
STEP: Saw pod success
Jul 29 01:46:46.206: INFO: Pod "downward-api-a43a00cb-b758-4897-8b28-02131aa3a4a2" satisfied condition "success or failure"
Jul 29 01:46:46.212: INFO: Trying to get logs from node conformance1 pod downward-api-a43a00cb-b758-4897-8b28-02131aa3a4a2 container dapi-container: <nil>
STEP: delete the pod
Jul 29 01:46:46.325: INFO: Waiting for pod downward-api-a43a00cb-b758-4897-8b28-02131aa3a4a2 to disappear
Jul 29 01:46:46.333: INFO: Pod downward-api-a43a00cb-b758-4897-8b28-02131aa3a4a2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:46:46.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2830" for this suite.
Jul 29 01:46:52.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:46:52.558: INFO: namespace downward-api-2830 deletion completed in 6.215228537s

• [SLOW TEST:10.448 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:46:52.575: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 29 01:46:52.651: INFO: Waiting up to 5m0s for pod "pod-ac6fe1fd-806b-4553-830d-0c6476f8c47f" in namespace "emptydir-3849" to be "success or failure"
Jul 29 01:46:52.660: INFO: Pod "pod-ac6fe1fd-806b-4553-830d-0c6476f8c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.055175ms
Jul 29 01:46:54.666: INFO: Pod "pod-ac6fe1fd-806b-4553-830d-0c6476f8c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014752529s
Jul 29 01:46:56.673: INFO: Pod "pod-ac6fe1fd-806b-4553-830d-0c6476f8c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021813426s
Jul 29 01:46:58.683: INFO: Pod "pod-ac6fe1fd-806b-4553-830d-0c6476f8c47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031898124s
STEP: Saw pod success
Jul 29 01:46:58.683: INFO: Pod "pod-ac6fe1fd-806b-4553-830d-0c6476f8c47f" satisfied condition "success or failure"
Jul 29 01:46:58.689: INFO: Trying to get logs from node conformance0 pod pod-ac6fe1fd-806b-4553-830d-0c6476f8c47f container test-container: <nil>
STEP: delete the pod
Jul 29 01:46:58.721: INFO: Waiting for pod pod-ac6fe1fd-806b-4553-830d-0c6476f8c47f to disappear
Jul 29 01:46:58.725: INFO: Pod pod-ac6fe1fd-806b-4553-830d-0c6476f8c47f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:46:58.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3849" for this suite.
Jul 29 01:47:04.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:47:04.925: INFO: namespace emptydir-3849 deletion completed in 6.190726271s

• [SLOW TEST:12.351 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:47:04.926: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jul 29 01:47:05.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 --namespace=kubectl-2996 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 29 01:47:10.621: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 29 01:47:10.621: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:47:12.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2996" for this suite.
Jul 29 01:47:18.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:47:18.858: INFO: namespace kubectl-2996 deletion completed in 6.221548393s

• [SLOW TEST:13.932 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:47:18.870: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-6a3112ed-cadd-40ba-9909-bb95625b0fe1
STEP: Creating a pod to test consume configMaps
Jul 29 01:47:19.276: INFO: Waiting up to 5m0s for pod "pod-configmaps-1da191d7-db9b-4087-b1f6-18fe2a1087a6" in namespace "configmap-8514" to be "success or failure"
Jul 29 01:47:19.290: INFO: Pod "pod-configmaps-1da191d7-db9b-4087-b1f6-18fe2a1087a6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.92511ms
Jul 29 01:47:21.295: INFO: Pod "pod-configmaps-1da191d7-db9b-4087-b1f6-18fe2a1087a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019024704s
Jul 29 01:47:23.302: INFO: Pod "pod-configmaps-1da191d7-db9b-4087-b1f6-18fe2a1087a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026115194s
STEP: Saw pod success
Jul 29 01:47:23.302: INFO: Pod "pod-configmaps-1da191d7-db9b-4087-b1f6-18fe2a1087a6" satisfied condition "success or failure"
Jul 29 01:47:23.312: INFO: Trying to get logs from node conformance0 pod pod-configmaps-1da191d7-db9b-4087-b1f6-18fe2a1087a6 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 01:47:23.357: INFO: Waiting for pod pod-configmaps-1da191d7-db9b-4087-b1f6-18fe2a1087a6 to disappear
Jul 29 01:47:23.366: INFO: Pod pod-configmaps-1da191d7-db9b-4087-b1f6-18fe2a1087a6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:47:23.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8514" for this suite.
Jul 29 01:47:29.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:47:29.605: INFO: namespace configmap-8514 deletion completed in 6.233969518s

• [SLOW TEST:10.736 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:47:29.605: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-42cffeb2-abb2-49c2-a8af-36c4c98f9e4b
STEP: Creating secret with name secret-projected-all-test-volume-6f8d06cc-3477-496f-bb1a-3fe48438d1e0
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 29 01:47:29.802: INFO: Waiting up to 5m0s for pod "projected-volume-08f2ea60-ac43-4383-9b18-f9f149e6dd22" in namespace "projected-1004" to be "success or failure"
Jul 29 01:47:29.867: INFO: Pod "projected-volume-08f2ea60-ac43-4383-9b18-f9f149e6dd22": Phase="Pending", Reason="", readiness=false. Elapsed: 64.961956ms
Jul 29 01:47:31.873: INFO: Pod "projected-volume-08f2ea60-ac43-4383-9b18-f9f149e6dd22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071049778s
Jul 29 01:47:33.879: INFO: Pod "projected-volume-08f2ea60-ac43-4383-9b18-f9f149e6dd22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076626356s
STEP: Saw pod success
Jul 29 01:47:33.879: INFO: Pod "projected-volume-08f2ea60-ac43-4383-9b18-f9f149e6dd22" satisfied condition "success or failure"
Jul 29 01:47:33.884: INFO: Trying to get logs from node conformance1 pod projected-volume-08f2ea60-ac43-4383-9b18-f9f149e6dd22 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 29 01:47:33.924: INFO: Waiting for pod projected-volume-08f2ea60-ac43-4383-9b18-f9f149e6dd22 to disappear
Jul 29 01:47:33.934: INFO: Pod projected-volume-08f2ea60-ac43-4383-9b18-f9f149e6dd22 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:47:33.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1004" for this suite.
Jul 29 01:47:39.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:47:40.219: INFO: namespace projected-1004 deletion completed in 6.268062055s

• [SLOW TEST:10.614 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:47:40.220: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:47:44.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7147" for this suite.
Jul 29 01:48:30.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:48:30.569: INFO: namespace kubelet-test-7147 deletion completed in 46.223833506s

• [SLOW TEST:50.349 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:48:30.574: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 29 01:48:30.677: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8531,SelfLink:/api/v1/namespaces/watch-8531/configmaps/e2e-watch-test-resource-version,UID:52500490-13ad-45ce-9c29-fefa82cea597,ResourceVersion:2024,Generation:0,CreationTimestamp:2019-07-29 01:48:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 29 01:48:30.678: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8531,SelfLink:/api/v1/namespaces/watch-8531/configmaps/e2e-watch-test-resource-version,UID:52500490-13ad-45ce-9c29-fefa82cea597,ResourceVersion:2025,Generation:0,CreationTimestamp:2019-07-29 01:48:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:48:30.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8531" for this suite.
Jul 29 01:48:36.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:48:36.945: INFO: namespace watch-8531 deletion completed in 6.25991324s

• [SLOW TEST:6.372 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:48:36.947: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jul 29 01:48:37.035: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5255" to be "success or failure"
Jul 29 01:48:37.049: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 13.679657ms
Jul 29 01:48:39.058: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022814195s
Jul 29 01:48:41.067: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031839269s
STEP: Saw pod success
Jul 29 01:48:41.067: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 29 01:48:41.075: INFO: Trying to get logs from node conformance0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 29 01:48:41.114: INFO: Waiting for pod pod-host-path-test to disappear
Jul 29 01:48:41.117: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:48:41.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5255" for this suite.
Jul 29 01:48:47.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:48:47.347: INFO: namespace hostpath-5255 deletion completed in 6.224548675s

• [SLOW TEST:10.401 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:48:47.349: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 29 01:48:47.801: INFO: Pod name wrapped-volume-race-b25b8a65-f21a-4623-b948-261c77efa9b4: Found 0 pods out of 5
Jul 29 01:48:52.814: INFO: Pod name wrapped-volume-race-b25b8a65-f21a-4623-b948-261c77efa9b4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b25b8a65-f21a-4623-b948-261c77efa9b4 in namespace emptydir-wrapper-6383, will wait for the garbage collector to delete the pods
Jul 29 01:49:04.921: INFO: Deleting ReplicationController wrapped-volume-race-b25b8a65-f21a-4623-b948-261c77efa9b4 took: 13.576775ms
Jul 29 01:49:05.322: INFO: Terminating ReplicationController wrapped-volume-race-b25b8a65-f21a-4623-b948-261c77efa9b4 pods took: 400.502855ms
STEP: Creating RC which spawns configmap-volume pods
Jul 29 01:49:47.546: INFO: Pod name wrapped-volume-race-e96f512d-7c77-46dd-8088-f6779f6b4e42: Found 0 pods out of 5
Jul 29 01:49:52.575: INFO: Pod name wrapped-volume-race-e96f512d-7c77-46dd-8088-f6779f6b4e42: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e96f512d-7c77-46dd-8088-f6779f6b4e42 in namespace emptydir-wrapper-6383, will wait for the garbage collector to delete the pods
Jul 29 01:50:06.688: INFO: Deleting ReplicationController wrapped-volume-race-e96f512d-7c77-46dd-8088-f6779f6b4e42 took: 18.766768ms
Jul 29 01:50:07.088: INFO: Terminating ReplicationController wrapped-volume-race-e96f512d-7c77-46dd-8088-f6779f6b4e42 pods took: 400.27259ms
STEP: Creating RC which spawns configmap-volume pods
Jul 29 01:50:47.621: INFO: Pod name wrapped-volume-race-bb7695f0-d9d2-43cd-b377-dded267a3bff: Found 0 pods out of 5
Jul 29 01:50:52.631: INFO: Pod name wrapped-volume-race-bb7695f0-d9d2-43cd-b377-dded267a3bff: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bb7695f0-d9d2-43cd-b377-dded267a3bff in namespace emptydir-wrapper-6383, will wait for the garbage collector to delete the pods
Jul 29 01:51:06.738: INFO: Deleting ReplicationController wrapped-volume-race-bb7695f0-d9d2-43cd-b377-dded267a3bff took: 13.158798ms
Jul 29 01:51:07.338: INFO: Terminating ReplicationController wrapped-volume-race-bb7695f0-d9d2-43cd-b377-dded267a3bff pods took: 600.470531ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:51:49.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6383" for this suite.
Jul 29 01:51:57.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:51:57.381: INFO: namespace emptydir-wrapper-6383 deletion completed in 8.282030996s

• [SLOW TEST:190.033 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:51:57.382: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 29 01:51:57.493: INFO: Waiting up to 5m0s for pod "downward-api-d4b1de0c-7baa-4231-b048-c55614afb7fc" in namespace "downward-api-9212" to be "success or failure"
Jul 29 01:51:57.528: INFO: Pod "downward-api-d4b1de0c-7baa-4231-b048-c55614afb7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 34.711976ms
Jul 29 01:51:59.534: INFO: Pod "downward-api-d4b1de0c-7baa-4231-b048-c55614afb7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040760635s
Jul 29 01:52:01.541: INFO: Pod "downward-api-d4b1de0c-7baa-4231-b048-c55614afb7fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048229894s
STEP: Saw pod success
Jul 29 01:52:01.541: INFO: Pod "downward-api-d4b1de0c-7baa-4231-b048-c55614afb7fc" satisfied condition "success or failure"
Jul 29 01:52:01.548: INFO: Trying to get logs from node conformance1 pod downward-api-d4b1de0c-7baa-4231-b048-c55614afb7fc container dapi-container: <nil>
STEP: delete the pod
Jul 29 01:52:01.598: INFO: Waiting for pod downward-api-d4b1de0c-7baa-4231-b048-c55614afb7fc to disappear
Jul 29 01:52:01.639: INFO: Pod downward-api-d4b1de0c-7baa-4231-b048-c55614afb7fc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:52:01.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9212" for this suite.
Jul 29 01:52:07.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:52:07.851: INFO: namespace downward-api-9212 deletion completed in 6.202396124s

• [SLOW TEST:10.470 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:52:07.852: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 01:52:07.982: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5184ff6c-f963-4e98-ac5b-5cafb13cea7f" in namespace "downward-api-1774" to be "success or failure"
Jul 29 01:52:07.987: INFO: Pod "downwardapi-volume-5184ff6c-f963-4e98-ac5b-5cafb13cea7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.965349ms
Jul 29 01:52:09.992: INFO: Pod "downwardapi-volume-5184ff6c-f963-4e98-ac5b-5cafb13cea7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010309755s
Jul 29 01:52:11.999: INFO: Pod "downwardapi-volume-5184ff6c-f963-4e98-ac5b-5cafb13cea7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016690332s
STEP: Saw pod success
Jul 29 01:52:11.999: INFO: Pod "downwardapi-volume-5184ff6c-f963-4e98-ac5b-5cafb13cea7f" satisfied condition "success or failure"
Jul 29 01:52:12.005: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-5184ff6c-f963-4e98-ac5b-5cafb13cea7f container client-container: <nil>
STEP: delete the pod
Jul 29 01:52:12.336: INFO: Waiting for pod downwardapi-volume-5184ff6c-f963-4e98-ac5b-5cafb13cea7f to disappear
Jul 29 01:52:12.344: INFO: Pod downwardapi-volume-5184ff6c-f963-4e98-ac5b-5cafb13cea7f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:52:12.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1774" for this suite.
Jul 29 01:52:18.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:52:18.543: INFO: namespace downward-api-1774 deletion completed in 6.191305748s

• [SLOW TEST:10.691 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:52:18.548: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 01:52:18.603: INFO: Creating deployment "test-recreate-deployment"
Jul 29 01:52:18.618: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 29 01:52:18.637: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 29 01:52:20.653: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 29 01:52:20.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961938, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961938, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961938, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699961938, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 01:52:22.665: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 29 01:52:22.683: INFO: Updating deployment test-recreate-deployment
Jul 29 01:52:22.683: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 29 01:52:22.938: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-9044,SelfLink:/apis/apps/v1/namespaces/deployment-9044/deployments/test-recreate-deployment,UID:6c46bdad-27f5-478c-8fbc-07125339dadf,ResourceVersion:3100,Generation:2,CreationTimestamp:2019-07-29 01:52:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-29 01:52:22 +0000 UTC 2019-07-29 01:52:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-29 01:52:22 +0000 UTC 2019-07-29 01:52:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul 29 01:52:22.947: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-9044,SelfLink:/apis/apps/v1/namespaces/deployment-9044/replicasets/test-recreate-deployment-5c8c9cc69d,UID:2b13e981-b8a5-4fcd-a19f-504751f0ee34,ResourceVersion:3096,Generation:1,CreationTimestamp:2019-07-29 01:52:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 6c46bdad-27f5-478c-8fbc-07125339dadf 0xc00287f697 0xc00287f698}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 29 01:52:22.947: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 29 01:52:22.947: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-9044,SelfLink:/apis/apps/v1/namespaces/deployment-9044/replicasets/test-recreate-deployment-6df85df6b9,UID:b3a4bb3d-c159-40b8-ac15-3e6fddba7c9e,ResourceVersion:3088,Generation:2,CreationTimestamp:2019-07-29 01:52:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 6c46bdad-27f5-478c-8fbc-07125339dadf 0xc00287f757 0xc00287f758}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 29 01:52:22.954: INFO: Pod "test-recreate-deployment-5c8c9cc69d-hftz8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-hftz8,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-9044,SelfLink:/api/v1/namespaces/deployment-9044/pods/test-recreate-deployment-5c8c9cc69d-hftz8,UID:ddc9933b-8ed4-4834-8aa2-7bbb6e9b028a,ResourceVersion:3098,Generation:0,CreationTimestamp:2019-07-29 01:52:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 2b13e981-b8a5-4fcd-a19f-504751f0ee34 0xc001dfe047 0xc001dfe048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nthlt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nthlt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nthlt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001dfe0c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001dfe0e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 01:52:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 01:52:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 01:52:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 01:52:22 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:,StartTime:2019-07-29 01:52:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:52:22.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9044" for this suite.
Jul 29 01:52:31.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:52:31.565: INFO: namespace deployment-9044 deletion completed in 8.579868511s

• [SLOW TEST:13.017 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:52:31.566: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 29 01:52:31.670: INFO: namespace kubectl-2629
Jul 29 01:52:31.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-2629'
Jul 29 01:52:32.390: INFO: stderr: ""
Jul 29 01:52:32.390: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 29 01:52:33.395: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 01:52:33.395: INFO: Found 0 / 1
Jul 29 01:52:34.396: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 01:52:34.396: INFO: Found 0 / 1
Jul 29 01:52:35.403: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 01:52:35.403: INFO: Found 0 / 1
Jul 29 01:52:36.398: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 01:52:36.398: INFO: Found 0 / 1
Jul 29 01:52:37.397: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 01:52:37.397: INFO: Found 1 / 1
Jul 29 01:52:37.397: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 29 01:52:37.402: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 01:52:37.402: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 29 01:52:37.402: INFO: wait on redis-master startup in kubectl-2629 
Jul 29 01:52:37.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 logs redis-master-q5smc redis-master --namespace=kubectl-2629'
Jul 29 01:52:37.564: INFO: stderr: ""
Jul 29 01:52:37.564: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Jul 01:52:35.961 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Jul 01:52:35.987 # Server started, Redis version 3.2.12\n1:M 29 Jul 01:52:35.987 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Jul 01:52:35.987 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 29 01:52:37.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2629'
Jul 29 01:52:37.954: INFO: stderr: ""
Jul 29 01:52:37.954: INFO: stdout: "service/rm2 exposed\n"
Jul 29 01:52:37.969: INFO: Service rm2 in namespace kubectl-2629 found.
STEP: exposing service
Jul 29 01:52:39.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2629'
Jul 29 01:52:40.156: INFO: stderr: ""
Jul 29 01:52:40.156: INFO: stdout: "service/rm3 exposed\n"
Jul 29 01:52:40.167: INFO: Service rm3 in namespace kubectl-2629 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:52:42.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2629" for this suite.
Jul 29 01:53:06.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:53:06.416: INFO: namespace kubectl-2629 deletion completed in 24.207881761s

• [SLOW TEST:34.851 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:53:06.419: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 29 01:53:14.569: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 01:53:14.752: INFO: Pod pod-with-prestop-http-hook still exists
Jul 29 01:53:16.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 01:53:16.758: INFO: Pod pod-with-prestop-http-hook still exists
Jul 29 01:53:18.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 01:53:18.758: INFO: Pod pod-with-prestop-http-hook still exists
Jul 29 01:53:20.753: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 01:53:20.759: INFO: Pod pod-with-prestop-http-hook still exists
Jul 29 01:53:22.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 01:53:22.757: INFO: Pod pod-with-prestop-http-hook still exists
Jul 29 01:53:24.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 01:53:24.760: INFO: Pod pod-with-prestop-http-hook still exists
Jul 29 01:53:26.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 01:53:26.758: INFO: Pod pod-with-prestop-http-hook still exists
Jul 29 01:53:28.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 01:53:28.758: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:53:28.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8418" for this suite.
Jul 29 01:53:40.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:53:40.986: INFO: namespace container-lifecycle-hook-8418 deletion completed in 12.208296024s

• [SLOW TEST:34.568 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:53:40.996: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 29 01:53:41.057: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 29 01:53:41.070: INFO: Waiting for terminating namespaces to be deleted...
Jul 29 01:53:41.075: INFO: 
Logging pods the kubelet thinks is on node conformance0 before test
Jul 29 01:53:41.086: INFO: sonobuoy-e2e-job-feb2cf4d6c56458c from heptio-sonobuoy started at 2019-07-29 01:38:22 +0000 UTC (2 container statuses recorded)
Jul 29 01:53:41.086: INFO: 	Container e2e ready: true, restart count 0
Jul 29 01:53:41.086: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 01:53:41.086: INFO: kube-flannel-ds-q9cx6 from kube-system started at 2019-07-29 01:33:59 +0000 UTC (1 container statuses recorded)
Jul 29 01:53:41.087: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 29 01:53:41.087: INFO: ingress-default-backend-6bd67fff4c-vzlm8 from ingress-haproxy started at 2019-07-29 01:34:15 +0000 UTC (1 container statuses recorded)
Jul 29 01:53:41.087: INFO: 	Container ingress-default-backend ready: true, restart count 0
Jul 29 01:53:41.087: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-29 01:38:17 +0000 UTC (1 container statuses recorded)
Jul 29 01:53:41.088: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 29 01:53:41.088: INFO: nirmata-cni-installer-gstxd from nirmata started at 2019-07-29 01:34:09 +0000 UTC (1 container statuses recorded)
Jul 29 01:53:41.088: INFO: 	Container install-cni ready: true, restart count 0
Jul 29 01:53:41.088: INFO: haproxy-ingress-98858d4d6-fzcg2 from ingress-haproxy started at 2019-07-29 01:34:16 +0000 UTC (1 container statuses recorded)
Jul 29 01:53:41.088: INFO: 	Container haproxy-ingress ready: true, restart count 0
Jul 29 01:53:41.089: INFO: sonobuoy-systemd-logs-daemon-set-5e30946e92f64e9d-v422w from heptio-sonobuoy started at 2019-07-29 01:38:22 +0000 UTC (2 container statuses recorded)
Jul 29 01:53:41.089: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 01:53:41.089: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 01:53:41.089: INFO: 
Logging pods the kubelet thinks is on node conformance1 before test
Jul 29 01:53:41.099: INFO: sonobuoy-systemd-logs-daemon-set-5e30946e92f64e9d-ljfqt from heptio-sonobuoy started at 2019-07-29 01:38:22 +0000 UTC (2 container statuses recorded)
Jul 29 01:53:41.100: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 01:53:41.100: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 01:53:41.100: INFO: kube-flannel-ds-gmpdt from kube-system started at 2019-07-29 01:33:20 +0000 UTC (1 container statuses recorded)
Jul 29 01:53:41.101: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 29 01:53:41.101: INFO: nirmata-cni-installer-swqlb from nirmata started at 2019-07-29 01:33:34 +0000 UTC (1 container statuses recorded)
Jul 29 01:53:41.101: INFO: 	Container install-cni ready: true, restart count 0
Jul 29 01:53:41.101: INFO: kube-dns-6475b5b555-kv5d8 from kube-system started at 2019-07-29 01:33:36 +0000 UTC (3 container statuses recorded)
Jul 29 01:53:41.102: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 29 01:53:41.102: INFO: 	Container kubedns ready: true, restart count 0
Jul 29 01:53:41.102: INFO: 	Container sidecar ready: true, restart count 0
Jul 29 01:53:41.102: INFO: nirmata-kube-controller-7c556b44bc-b5rnr from nirmata started at 2019-07-29 01:33:36 +0000 UTC (1 container statuses recorded)
Jul 29 01:53:41.103: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Jul 29 01:53:41.103: INFO: metrics-server-8454c7c9c6-htc58 from kube-system started at 2019-07-29 01:33:36 +0000 UTC (1 container statuses recorded)
Jul 29 01:53:41.103: INFO: 	Container metrics-server ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f4b8c26f-551f-473b-9ecb-fbb7307ace63 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-f4b8c26f-551f-473b-9ecb-fbb7307ace63 off the node conformance1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f4b8c26f-551f-473b-9ecb-fbb7307ace63
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:53:49.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8731" for this suite.
Jul 29 01:54:11.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:54:11.585: INFO: namespace sched-pred-8731 deletion completed in 22.247335927s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:30.590 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:54:11.587: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jul 29 01:54:11.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 cluster-info'
Jul 29 01:54:11.888: INFO: stderr: ""
Jul 29 01:54:11.888: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:54:11.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8135" for this suite.
Jul 29 01:54:17.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:54:18.124: INFO: namespace kubectl-8135 deletion completed in 6.228731486s

• [SLOW TEST:6.537 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:54:18.127: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-edddbe00-42e0-4840-92bc-69f143aa85cb
STEP: Creating a pod to test consume secrets
Jul 29 01:54:18.209: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1c07b2a1-2d42-4b22-ae46-dbc2721a5ac8" in namespace "projected-5054" to be "success or failure"
Jul 29 01:54:18.216: INFO: Pod "pod-projected-secrets-1c07b2a1-2d42-4b22-ae46-dbc2721a5ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.778046ms
Jul 29 01:54:20.221: INFO: Pod "pod-projected-secrets-1c07b2a1-2d42-4b22-ae46-dbc2721a5ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011981738s
Jul 29 01:54:22.227: INFO: Pod "pod-projected-secrets-1c07b2a1-2d42-4b22-ae46-dbc2721a5ac8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017879434s
STEP: Saw pod success
Jul 29 01:54:22.227: INFO: Pod "pod-projected-secrets-1c07b2a1-2d42-4b22-ae46-dbc2721a5ac8" satisfied condition "success or failure"
Jul 29 01:54:22.233: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-1c07b2a1-2d42-4b22-ae46-dbc2721a5ac8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 29 01:54:22.268: INFO: Waiting for pod pod-projected-secrets-1c07b2a1-2d42-4b22-ae46-dbc2721a5ac8 to disappear
Jul 29 01:54:22.280: INFO: Pod pod-projected-secrets-1c07b2a1-2d42-4b22-ae46-dbc2721a5ac8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:54:22.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5054" for this suite.
Jul 29 01:54:28.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:54:28.492: INFO: namespace projected-5054 deletion completed in 6.204190074s

• [SLOW TEST:10.366 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:54:28.499: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 29 01:54:28.577: INFO: Waiting up to 5m0s for pod "pod-6773fe1b-9908-4c8e-923c-12b05fc75d09" in namespace "emptydir-6782" to be "success or failure"
Jul 29 01:54:28.584: INFO: Pod "pod-6773fe1b-9908-4c8e-923c-12b05fc75d09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.599123ms
Jul 29 01:54:30.593: INFO: Pod "pod-6773fe1b-9908-4c8e-923c-12b05fc75d09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015865979s
Jul 29 01:54:32.599: INFO: Pod "pod-6773fe1b-9908-4c8e-923c-12b05fc75d09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021233848s
STEP: Saw pod success
Jul 29 01:54:32.599: INFO: Pod "pod-6773fe1b-9908-4c8e-923c-12b05fc75d09" satisfied condition "success or failure"
Jul 29 01:54:32.603: INFO: Trying to get logs from node conformance0 pod pod-6773fe1b-9908-4c8e-923c-12b05fc75d09 container test-container: <nil>
STEP: delete the pod
Jul 29 01:54:32.641: INFO: Waiting for pod pod-6773fe1b-9908-4c8e-923c-12b05fc75d09 to disappear
Jul 29 01:54:32.648: INFO: Pod pod-6773fe1b-9908-4c8e-923c-12b05fc75d09 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:54:32.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6782" for this suite.
Jul 29 01:54:38.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:54:38.851: INFO: namespace emptydir-6782 deletion completed in 6.197250463s

• [SLOW TEST:10.353 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:54:38.854: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-83a4a836-2452-4274-a4ae-f5bdf7230d48
STEP: Creating a pod to test consume secrets
Jul 29 01:54:38.932: INFO: Waiting up to 5m0s for pod "pod-secrets-f8ecf2bd-334f-488f-9ddd-822538566831" in namespace "secrets-406" to be "success or failure"
Jul 29 01:54:38.939: INFO: Pod "pod-secrets-f8ecf2bd-334f-488f-9ddd-822538566831": Phase="Pending", Reason="", readiness=false. Elapsed: 6.679245ms
Jul 29 01:54:40.945: INFO: Pod "pod-secrets-f8ecf2bd-334f-488f-9ddd-822538566831": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011836476s
Jul 29 01:54:42.956: INFO: Pod "pod-secrets-f8ecf2bd-334f-488f-9ddd-822538566831": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023410057s
STEP: Saw pod success
Jul 29 01:54:42.957: INFO: Pod "pod-secrets-f8ecf2bd-334f-488f-9ddd-822538566831" satisfied condition "success or failure"
Jul 29 01:54:42.964: INFO: Trying to get logs from node conformance0 pod pod-secrets-f8ecf2bd-334f-488f-9ddd-822538566831 container secret-volume-test: <nil>
STEP: delete the pod
Jul 29 01:54:42.997: INFO: Waiting for pod pod-secrets-f8ecf2bd-334f-488f-9ddd-822538566831 to disappear
Jul 29 01:54:43.005: INFO: Pod pod-secrets-f8ecf2bd-334f-488f-9ddd-822538566831 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:54:43.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-406" for this suite.
Jul 29 01:54:49.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:54:49.269: INFO: namespace secrets-406 deletion completed in 6.259092382s

• [SLOW TEST:10.416 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:54:49.271: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-8138683c-ea37-4731-b106-9726fd6e682b
STEP: Creating a pod to test consume configMaps
Jul 29 01:54:49.393: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dfd54cab-bfc0-47f8-aaab-53d23d74cf4a" in namespace "projected-5154" to be "success or failure"
Jul 29 01:54:49.397: INFO: Pod "pod-projected-configmaps-dfd54cab-bfc0-47f8-aaab-53d23d74cf4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.255838ms
Jul 29 01:54:51.403: INFO: Pod "pod-projected-configmaps-dfd54cab-bfc0-47f8-aaab-53d23d74cf4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00957646s
Jul 29 01:54:53.408: INFO: Pod "pod-projected-configmaps-dfd54cab-bfc0-47f8-aaab-53d23d74cf4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014795827s
STEP: Saw pod success
Jul 29 01:54:53.408: INFO: Pod "pod-projected-configmaps-dfd54cab-bfc0-47f8-aaab-53d23d74cf4a" satisfied condition "success or failure"
Jul 29 01:54:53.414: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-dfd54cab-bfc0-47f8-aaab-53d23d74cf4a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 01:54:53.463: INFO: Waiting for pod pod-projected-configmaps-dfd54cab-bfc0-47f8-aaab-53d23d74cf4a to disappear
Jul 29 01:54:53.468: INFO: Pod pod-projected-configmaps-dfd54cab-bfc0-47f8-aaab-53d23d74cf4a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:54:53.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5154" for this suite.
Jul 29 01:54:59.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:54:59.725: INFO: namespace projected-5154 deletion completed in 6.250804015s

• [SLOW TEST:10.454 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:54:59.726: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5025
I0729 01:54:59.829494      14 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5025, replica count: 1
I0729 01:55:00.880433      14 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 01:55:01.880882      14 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 01:55:02.881287      14 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 01:55:03.885260      14 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 01:55:04.012: INFO: Created: latency-svc-dhbbk
Jul 29 01:55:04.030: INFO: Got endpoints: latency-svc-dhbbk [44.422776ms]
Jul 29 01:55:04.073: INFO: Created: latency-svc-6l5zc
Jul 29 01:55:04.087: INFO: Created: latency-svc-8sxwk
Jul 29 01:55:04.099: INFO: Got endpoints: latency-svc-6l5zc [66.604745ms]
Jul 29 01:55:04.113: INFO: Got endpoints: latency-svc-8sxwk [80.069818ms]
Jul 29 01:55:04.116: INFO: Created: latency-svc-wrsmb
Jul 29 01:55:04.132: INFO: Got endpoints: latency-svc-wrsmb [98.239547ms]
Jul 29 01:55:04.148: INFO: Created: latency-svc-xdlgk
Jul 29 01:55:04.155: INFO: Created: latency-svc-srnnv
Jul 29 01:55:04.186: INFO: Created: latency-svc-srjjg
Jul 29 01:55:04.192: INFO: Got endpoints: latency-svc-xdlgk [162.087071ms]
Jul 29 01:55:04.192: INFO: Got endpoints: latency-svc-srnnv [158.455832ms]
Jul 29 01:55:04.225: INFO: Got endpoints: latency-svc-srjjg [91.974183ms]
Jul 29 01:55:04.236: INFO: Created: latency-svc-t2m6t
Jul 29 01:55:04.237: INFO: Got endpoints: latency-svc-t2m6t [202.826675ms]
Jul 29 01:55:04.237: INFO: Created: latency-svc-j9l5m
Jul 29 01:55:04.244: INFO: Created: latency-svc-wrtrt
Jul 29 01:55:04.249: INFO: Got endpoints: latency-svc-j9l5m [215.259946ms]
Jul 29 01:55:04.261: INFO: Got endpoints: latency-svc-wrtrt [230.308521ms]
Jul 29 01:55:04.284: INFO: Created: latency-svc-wjvpg
Jul 29 01:55:04.284: INFO: Got endpoints: latency-svc-wjvpg [252.086366ms]
Jul 29 01:55:04.290: INFO: Created: latency-svc-g4c27
Jul 29 01:55:04.306: INFO: Got endpoints: latency-svc-g4c27 [275.093564ms]
Jul 29 01:55:04.307: INFO: Created: latency-svc-x6rmm
Jul 29 01:55:04.339: INFO: Got endpoints: latency-svc-x6rmm [307.798237ms]
Jul 29 01:55:04.342: INFO: Created: latency-svc-57xcw
Jul 29 01:55:04.357: INFO: Created: latency-svc-wthc2
Jul 29 01:55:04.370: INFO: Got endpoints: latency-svc-57xcw [338.310513ms]
Jul 29 01:55:04.380: INFO: Created: latency-svc-hdw9w
Jul 29 01:55:04.394: INFO: Got endpoints: latency-svc-hdw9w [362.002342ms]
Jul 29 01:55:04.398: INFO: Created: latency-svc-zpq6r
Jul 29 01:55:04.408: INFO: Got endpoints: latency-svc-wthc2 [376.114414ms]
Jul 29 01:55:04.422: INFO: Got endpoints: latency-svc-zpq6r [390.335894ms]
Jul 29 01:55:04.433: INFO: Created: latency-svc-msx88
Jul 29 01:55:04.452: INFO: Created: latency-svc-7fzhf
Jul 29 01:55:04.454: INFO: Got endpoints: latency-svc-msx88 [354.702495ms]
Jul 29 01:55:04.465: INFO: Got endpoints: latency-svc-7fzhf [352.246378ms]
Jul 29 01:55:04.483: INFO: Created: latency-svc-8h428
Jul 29 01:55:04.495: INFO: Got endpoints: latency-svc-8h428 [302.737811ms]
Jul 29 01:55:04.498: INFO: Created: latency-svc-j2gzw
Jul 29 01:55:04.526: INFO: Created: latency-svc-txmkt
Jul 29 01:55:04.536: INFO: Got endpoints: latency-svc-j2gzw [343.463971ms]
Jul 29 01:55:04.558: INFO: Created: latency-svc-s624g
Jul 29 01:55:04.565: INFO: Got endpoints: latency-svc-txmkt [339.783306ms]
Jul 29 01:55:04.607: INFO: Got endpoints: latency-svc-s624g [370.441876ms]
Jul 29 01:55:04.870: INFO: Created: latency-svc-vbp4b
Jul 29 01:55:04.888: INFO: Created: latency-svc-pmlcg
Jul 29 01:55:04.891: INFO: Got endpoints: latency-svc-vbp4b [641.679853ms]
Jul 29 01:55:04.921: INFO: Created: latency-svc-8gm6w
Jul 29 01:55:04.931: INFO: Got endpoints: latency-svc-pmlcg [670.143162ms]
Jul 29 01:55:04.949: INFO: Created: latency-svc-d5bkv
Jul 29 01:55:04.960: INFO: Got endpoints: latency-svc-8gm6w [675.135964ms]
Jul 29 01:55:05.015: INFO: Created: latency-svc-b9s2h
Jul 29 01:55:05.019: INFO: Got endpoints: latency-svc-d5bkv [713.345721ms]
Jul 29 01:55:05.046: INFO: Created: latency-svc-vs4qw
Jul 29 01:55:05.051: INFO: Created: latency-svc-zrlnk
Jul 29 01:55:05.067: INFO: Got endpoints: latency-svc-b9s2h [728.234853ms]
Jul 29 01:55:05.088: INFO: Created: latency-svc-lhjx8
Jul 29 01:55:05.100: INFO: Got endpoints: latency-svc-vs4qw [706.26291ms]
Jul 29 01:55:05.101: INFO: Got endpoints: latency-svc-zrlnk [731.344894ms]
Jul 29 01:55:05.133: INFO: Got endpoints: latency-svc-lhjx8 [725.307033ms]
Jul 29 01:55:05.133: INFO: Created: latency-svc-8hhm6
Jul 29 01:55:05.133: INFO: Got endpoints: latency-svc-8hhm6 [710.827558ms]
Jul 29 01:55:05.152: INFO: Created: latency-svc-zmfd5
Jul 29 01:55:05.156: INFO: Got endpoints: latency-svc-zmfd5 [702.493024ms]
Jul 29 01:55:05.221: INFO: Created: latency-svc-p7lqk
Jul 29 01:55:05.221: INFO: Got endpoints: latency-svc-p7lqk [755.772527ms]
Jul 29 01:55:05.232: INFO: Created: latency-svc-7clc5
Jul 29 01:55:05.232: INFO: Got endpoints: latency-svc-7clc5 [737.464049ms]
Jul 29 01:55:05.233: INFO: Created: latency-svc-hv5gp
Jul 29 01:55:05.239: INFO: Created: latency-svc-x5hcq
Jul 29 01:55:05.239: INFO: Got endpoints: latency-svc-x5hcq [702.978208ms]
Jul 29 01:55:05.241: INFO: Created: latency-svc-5pbf5
Jul 29 01:55:05.241: INFO: Got endpoints: latency-svc-5pbf5 [676.114014ms]
Jul 29 01:55:05.243: INFO: Created: latency-svc-ghxwk
Jul 29 01:55:05.271: INFO: Created: latency-svc-ksg8l
Jul 29 01:55:05.277: INFO: Got endpoints: latency-svc-ghxwk [669.771886ms]
Jul 29 01:55:05.277: INFO: Got endpoints: latency-svc-hv5gp [386.139723ms]
Jul 29 01:55:05.290: INFO: Created: latency-svc-2tjlj
Jul 29 01:55:05.312: INFO: Got endpoints: latency-svc-ksg8l [381.149214ms]
Jul 29 01:55:05.319: INFO: Created: latency-svc-mb96k
Jul 29 01:55:05.354: INFO: Created: latency-svc-pmjp9
Jul 29 01:55:05.362: INFO: Got endpoints: latency-svc-mb96k [342.483057ms]
Jul 29 01:55:05.362: INFO: Got endpoints: latency-svc-2tjlj [402.290282ms]
Jul 29 01:55:05.378: INFO: Got endpoints: latency-svc-pmjp9 [311.183111ms]
Jul 29 01:55:05.394: INFO: Created: latency-svc-vtvk5
Jul 29 01:55:05.415: INFO: Got endpoints: latency-svc-vtvk5 [314.486861ms]
Jul 29 01:55:05.432: INFO: Created: latency-svc-2pg64
Jul 29 01:55:05.446: INFO: Created: latency-svc-fp6qf
Jul 29 01:55:05.458: INFO: Created: latency-svc-f4wtb
Jul 29 01:55:05.464: INFO: Got endpoints: latency-svc-fp6qf [331.077454ms]
Jul 29 01:55:05.472: INFO: Got endpoints: latency-svc-2pg64 [370.373233ms]
Jul 29 01:55:05.478: INFO: Created: latency-svc-qprkb
Jul 29 01:55:05.500: INFO: Got endpoints: latency-svc-f4wtb [366.820195ms]
Jul 29 01:55:05.515: INFO: Got endpoints: latency-svc-qprkb [358.686582ms]
Jul 29 01:55:05.521: INFO: Created: latency-svc-bsmzm
Jul 29 01:55:05.533: INFO: Got endpoints: latency-svc-bsmzm [312.211831ms]
Jul 29 01:55:05.573: INFO: Created: latency-svc-4skt2
Jul 29 01:55:05.573: INFO: Created: latency-svc-lfhgh
Jul 29 01:55:05.584: INFO: Got endpoints: latency-svc-lfhgh [342.888176ms]
Jul 29 01:55:05.588: INFO: Created: latency-svc-xnjfz
Jul 29 01:55:05.606: INFO: Got endpoints: latency-svc-4skt2 [373.121467ms]
Jul 29 01:55:05.612: INFO: Got endpoints: latency-svc-xnjfz [373.624575ms]
Jul 29 01:55:05.621: INFO: Created: latency-svc-hbxjr
Jul 29 01:55:05.640: INFO: Created: latency-svc-799ps
Jul 29 01:55:05.646: INFO: Got endpoints: latency-svc-hbxjr [368.609357ms]
Jul 29 01:55:05.663: INFO: Created: latency-svc-mvbdz
Jul 29 01:55:05.678: INFO: Got endpoints: latency-svc-mvbdz [365.806411ms]
Jul 29 01:55:05.678: INFO: Got endpoints: latency-svc-799ps [400.9619ms]
Jul 29 01:55:05.710: INFO: Created: latency-svc-x9mm5
Jul 29 01:55:05.719: INFO: Created: latency-svc-kt4t8
Jul 29 01:55:05.739: INFO: Got endpoints: latency-svc-kt4t8 [376.468871ms]
Jul 29 01:55:05.753: INFO: Created: latency-svc-f5kpg
Jul 29 01:55:05.755: INFO: Got endpoints: latency-svc-f5kpg [376.26436ms]
Jul 29 01:55:05.755: INFO: Got endpoints: latency-svc-x9mm5 [392.589461ms]
Jul 29 01:55:05.771: INFO: Created: latency-svc-nnnsz
Jul 29 01:55:05.780: INFO: Got endpoints: latency-svc-nnnsz [364.50018ms]
Jul 29 01:55:05.781: INFO: Created: latency-svc-n2fh9
Jul 29 01:55:05.810: INFO: Created: latency-svc-sx5kv
Jul 29 01:55:05.818: INFO: Created: latency-svc-hvtln
Jul 29 01:55:05.829: INFO: Got endpoints: latency-svc-hvtln [329.210759ms]
Jul 29 01:55:05.830: INFO: Got endpoints: latency-svc-n2fh9 [365.76007ms]
Jul 29 01:55:05.831: INFO: Got endpoints: latency-svc-sx5kv [359.017882ms]
Jul 29 01:55:05.841: INFO: Created: latency-svc-zttv9
Jul 29 01:55:05.848: INFO: Created: latency-svc-xtq2v
Jul 29 01:55:05.859: INFO: Created: latency-svc-q49q8
Jul 29 01:55:05.870: INFO: Got endpoints: latency-svc-xtq2v [336.329436ms]
Jul 29 01:55:05.870: INFO: Got endpoints: latency-svc-zttv9 [354.420313ms]
Jul 29 01:55:05.881: INFO: Got endpoints: latency-svc-q49q8 [296.891073ms]
Jul 29 01:55:05.896: INFO: Created: latency-svc-q8fcq
Jul 29 01:55:05.904: INFO: Created: latency-svc-kj26m
Jul 29 01:55:05.907: INFO: Got endpoints: latency-svc-q8fcq [301.801298ms]
Jul 29 01:55:05.920: INFO: Created: latency-svc-56j9w
Jul 29 01:55:05.922: INFO: Got endpoints: latency-svc-kj26m [309.358172ms]
Jul 29 01:55:05.947: INFO: Created: latency-svc-mxwdv
Jul 29 01:55:05.953: INFO: Got endpoints: latency-svc-56j9w [306.989661ms]
Jul 29 01:55:05.975: INFO: Created: latency-svc-6g4vh
Jul 29 01:55:05.976: INFO: Got endpoints: latency-svc-mxwdv [297.731717ms]
Jul 29 01:55:06.039: INFO: Created: latency-svc-t9cl2
Jul 29 01:55:06.054: INFO: Got endpoints: latency-svc-6g4vh [375.567023ms]
Jul 29 01:55:06.077: INFO: Created: latency-svc-cg5mg
Jul 29 01:55:06.086: INFO: Got endpoints: latency-svc-t9cl2 [347.637308ms]
Jul 29 01:55:06.103: INFO: Created: latency-svc-k5hrx
Jul 29 01:55:06.104: INFO: Created: latency-svc-czz74
Jul 29 01:55:06.126: INFO: Created: latency-svc-2x4lv
Jul 29 01:55:06.136: INFO: Created: latency-svc-wl5kk
Jul 29 01:55:06.146: INFO: Got endpoints: latency-svc-cg5mg [390.605324ms]
Jul 29 01:55:06.166: INFO: Created: latency-svc-dqp7k
Jul 29 01:55:06.192: INFO: Got endpoints: latency-svc-czz74 [436.959907ms]
Jul 29 01:55:06.240: INFO: Created: latency-svc-6p9xw
Jul 29 01:55:06.242: INFO: Created: latency-svc-8w5jh
Jul 29 01:55:06.251: INFO: Created: latency-svc-zptg5
Jul 29 01:55:06.257: INFO: Got endpoints: latency-svc-k5hrx [476.895041ms]
Jul 29 01:55:06.273: INFO: Created: latency-svc-g48qh
Jul 29 01:55:06.288: INFO: Got endpoints: latency-svc-2x4lv [458.455794ms]
Jul 29 01:55:06.301: INFO: Created: latency-svc-hhvxn
Jul 29 01:55:06.310: INFO: Created: latency-svc-5thml
Jul 29 01:55:06.331: INFO: Got endpoints: latency-svc-wl5kk [500.761389ms]
Jul 29 01:55:06.363: INFO: Created: latency-svc-tw7sn
Jul 29 01:55:06.363: INFO: Created: latency-svc-kn7v6
Jul 29 01:55:06.363: INFO: Created: latency-svc-rp2tl
Jul 29 01:55:06.370: INFO: Created: latency-svc-w4ck6
Jul 29 01:55:06.380: INFO: Got endpoints: latency-svc-dqp7k [549.178098ms]
Jul 29 01:55:06.403: INFO: Created: latency-svc-gf5kw
Jul 29 01:55:06.408: INFO: Created: latency-svc-24llt
Jul 29 01:55:06.425: INFO: Created: latency-svc-pqpkw
Jul 29 01:55:06.451: INFO: Got endpoints: latency-svc-6p9xw [580.681909ms]
Jul 29 01:55:06.451: INFO: Created: latency-svc-k8kdw
Jul 29 01:55:06.459: INFO: Created: latency-svc-6dwvg
Jul 29 01:55:06.529: INFO: Got endpoints: latency-svc-8w5jh [658.823525ms]
Jul 29 01:55:06.536: INFO: Got endpoints: latency-svc-zptg5 [654.942114ms]
Jul 29 01:55:06.550: INFO: Created: latency-svc-rlkgr
Jul 29 01:55:06.586: INFO: Got endpoints: latency-svc-g48qh [677.940218ms]
Jul 29 01:55:06.596: INFO: Created: latency-svc-j9ckp
Jul 29 01:55:06.596: INFO: Created: latency-svc-h4v9c
Jul 29 01:55:06.613: INFO: Created: latency-svc-fwzgq
Jul 29 01:55:06.624: INFO: Got endpoints: latency-svc-hhvxn [702.159242ms]
Jul 29 01:55:06.644: INFO: Created: latency-svc-5hgcv
Jul 29 01:55:06.682: INFO: Got endpoints: latency-svc-5thml [728.585327ms]
Jul 29 01:55:06.699: INFO: Created: latency-svc-z98hv
Jul 29 01:55:06.725: INFO: Got endpoints: latency-svc-rp2tl [749.020843ms]
Jul 29 01:55:06.748: INFO: Created: latency-svc-mt4pn
Jul 29 01:55:06.774: INFO: Got endpoints: latency-svc-kn7v6 [720.081808ms]
Jul 29 01:55:06.793: INFO: Created: latency-svc-sp8js
Jul 29 01:55:06.825: INFO: Got endpoints: latency-svc-tw7sn [738.437802ms]
Jul 29 01:55:06.901: INFO: Created: latency-svc-mfknw
Jul 29 01:55:06.908: INFO: Got endpoints: latency-svc-w4ck6 [761.96114ms]
Jul 29 01:55:06.955: INFO: Got endpoints: latency-svc-24llt [763.217844ms]
Jul 29 01:55:07.248: INFO: Got endpoints: latency-svc-k8kdw [916.944749ms]
Jul 29 01:55:07.249: INFO: Got endpoints: latency-svc-gf5kw [991.802779ms]
Jul 29 01:55:07.249: INFO: Got endpoints: latency-svc-pqpkw [960.95473ms]
Jul 29 01:55:07.251: INFO: Got endpoints: latency-svc-6dwvg [870.491225ms]
Jul 29 01:55:07.295: INFO: Got endpoints: latency-svc-rlkgr [844.035519ms]
Jul 29 01:55:07.325: INFO: Created: latency-svc-mr6sf
Jul 29 01:55:07.326: INFO: Got endpoints: latency-svc-j9ckp [797.015149ms]
Jul 29 01:55:07.327: INFO: Got endpoints: latency-svc-h4v9c [790.984224ms]
Jul 29 01:55:07.352: INFO: Got endpoints: latency-svc-fwzgq [766.786922ms]
Jul 29 01:55:07.432: INFO: Created: latency-svc-gzx5w
Jul 29 01:55:07.432: INFO: Got endpoints: latency-svc-5hgcv [807.394627ms]
Jul 29 01:55:07.432: INFO: Created: latency-svc-9s8tz
Jul 29 01:55:07.756: INFO: Got endpoints: latency-svc-sp8js [981.863123ms]
Jul 29 01:55:07.756: INFO: Got endpoints: latency-svc-mt4pn [1.031024231s]
Jul 29 01:55:07.757: INFO: Got endpoints: latency-svc-z98hv [1.074965779s]
Jul 29 01:55:07.759: INFO: Created: latency-svc-25kpx
Jul 29 01:55:07.786: INFO: Got endpoints: latency-svc-mr6sf [877.995166ms]
Jul 29 01:55:07.786: INFO: Got endpoints: latency-svc-mfknw [960.91573ms]
Jul 29 01:55:07.812: INFO: Created: latency-svc-rkt6v
Jul 29 01:55:07.832: INFO: Got endpoints: latency-svc-9s8tz [583.760373ms]
Jul 29 01:55:07.833: INFO: Got endpoints: latency-svc-gzx5w [877.842699ms]
Jul 29 01:55:07.863: INFO: Created: latency-svc-4qf2g
Jul 29 01:55:07.869: INFO: Got endpoints: latency-svc-rkt6v [621.251394ms]
Jul 29 01:55:07.870: INFO: Got endpoints: latency-svc-25kpx [619.213491ms]
Jul 29 01:55:07.886: INFO: Created: latency-svc-mqgcl
Jul 29 01:55:07.896: INFO: Got endpoints: latency-svc-4qf2g [109.671ms]
Jul 29 01:55:07.898: INFO: Created: latency-svc-xzbvn
Jul 29 01:55:07.925: INFO: Created: latency-svc-bl7df
Jul 29 01:55:07.943: INFO: Created: latency-svc-xgx72
Jul 29 01:55:07.962: INFO: Created: latency-svc-7tf2x
Jul 29 01:55:07.970: INFO: Got endpoints: latency-svc-mqgcl [675.409736ms]
Jul 29 01:55:07.996: INFO: Created: latency-svc-25xmj
Jul 29 01:55:08.000: INFO: Got endpoints: latency-svc-xzbvn [750.986088ms]
Jul 29 01:55:08.000: INFO: Created: latency-svc-bblpj
Jul 29 01:55:08.024: INFO: Created: latency-svc-8xg95
Jul 29 01:55:08.038: INFO: Got endpoints: latency-svc-bl7df [711.880795ms]
Jul 29 01:55:08.062: INFO: Created: latency-svc-m8mth
Jul 29 01:55:08.093: INFO: Got endpoints: latency-svc-xgx72 [766.161145ms]
Jul 29 01:55:08.121: INFO: Created: latency-svc-66hkk
Jul 29 01:55:08.134: INFO: Got endpoints: latency-svc-7tf2x [781.430599ms]
Jul 29 01:55:08.141: INFO: Created: latency-svc-lxmjd
Jul 29 01:55:08.156: INFO: Created: latency-svc-54d64
Jul 29 01:55:08.177: INFO: Got endpoints: latency-svc-bblpj [744.32579ms]
Jul 29 01:55:08.199: INFO: Created: latency-svc-2zqvh
Jul 29 01:55:08.206: INFO: Created: latency-svc-jt8dz
Jul 29 01:55:08.242: INFO: Created: latency-svc-pgxz8
Jul 29 01:55:08.246: INFO: Got endpoints: latency-svc-25xmj [489.898198ms]
Jul 29 01:55:08.269: INFO: Created: latency-svc-nscs5
Jul 29 01:55:08.276: INFO: Got endpoints: latency-svc-8xg95 [519.141694ms]
Jul 29 01:55:08.285: INFO: Created: latency-svc-92kp7
Jul 29 01:55:08.300: INFO: Created: latency-svc-q5gxt
Jul 29 01:55:08.313: INFO: Created: latency-svc-82sxc
Jul 29 01:55:08.325: INFO: Created: latency-svc-wrzlp
Jul 29 01:55:08.331: INFO: Got endpoints: latency-svc-m8mth [574.805752ms]
Jul 29 01:55:08.358: INFO: Created: latency-svc-cpxdk
Jul 29 01:55:08.358: INFO: Created: latency-svc-742zs
Jul 29 01:55:08.381: INFO: Created: latency-svc-6xzsx
Jul 29 01:55:08.389: INFO: Got endpoints: latency-svc-66hkk [602.625052ms]
Jul 29 01:55:08.398: INFO: Created: latency-svc-mqqhr
Jul 29 01:55:08.412: INFO: Created: latency-svc-lq5dz
Jul 29 01:55:08.424: INFO: Got endpoints: latency-svc-lxmjd [591.429177ms]
Jul 29 01:55:08.450: INFO: Created: latency-svc-wlff6
Jul 29 01:55:08.472: INFO: Got endpoints: latency-svc-54d64 [638.262475ms]
Jul 29 01:55:08.487: INFO: Created: latency-svc-hpkrq
Jul 29 01:55:08.521: INFO: Got endpoints: latency-svc-2zqvh [651.42872ms]
Jul 29 01:55:08.539: INFO: Created: latency-svc-9xnwb
Jul 29 01:55:08.573: INFO: Got endpoints: latency-svc-jt8dz [703.695412ms]
Jul 29 01:55:08.590: INFO: Created: latency-svc-zzbpm
Jul 29 01:55:08.622: INFO: Got endpoints: latency-svc-pgxz8 [726.044007ms]
Jul 29 01:55:08.639: INFO: Created: latency-svc-wr222
Jul 29 01:55:08.673: INFO: Got endpoints: latency-svc-nscs5 [702.062957ms]
Jul 29 01:55:08.691: INFO: Created: latency-svc-5brn2
Jul 29 01:55:08.722: INFO: Got endpoints: latency-svc-92kp7 [721.694596ms]
Jul 29 01:55:08.737: INFO: Created: latency-svc-29d4l
Jul 29 01:55:08.783: INFO: Got endpoints: latency-svc-q5gxt [744.687958ms]
Jul 29 01:55:08.800: INFO: Created: latency-svc-p2qrg
Jul 29 01:55:08.828: INFO: Got endpoints: latency-svc-82sxc [734.079341ms]
Jul 29 01:55:08.841: INFO: Created: latency-svc-8brz7
Jul 29 01:55:08.886: INFO: Got endpoints: latency-svc-wrzlp [752.231299ms]
Jul 29 01:55:08.911: INFO: Created: latency-svc-rgmkl
Jul 29 01:55:08.924: INFO: Got endpoints: latency-svc-cpxdk [747.009273ms]
Jul 29 01:55:08.946: INFO: Created: latency-svc-zjpft
Jul 29 01:55:08.974: INFO: Got endpoints: latency-svc-742zs [727.862643ms]
Jul 29 01:55:09.010: INFO: Created: latency-svc-cc67x
Jul 29 01:55:09.023: INFO: Got endpoints: latency-svc-6xzsx [747.118825ms]
Jul 29 01:55:09.047: INFO: Created: latency-svc-bh78t
Jul 29 01:55:09.071: INFO: Got endpoints: latency-svc-mqqhr [740.023866ms]
Jul 29 01:55:09.091: INFO: Created: latency-svc-52djp
Jul 29 01:55:09.121: INFO: Got endpoints: latency-svc-lq5dz [732.402905ms]
Jul 29 01:55:09.147: INFO: Created: latency-svc-4pmgs
Jul 29 01:55:09.174: INFO: Got endpoints: latency-svc-wlff6 [750.099196ms]
Jul 29 01:55:09.193: INFO: Created: latency-svc-65vll
Jul 29 01:55:09.226: INFO: Got endpoints: latency-svc-hpkrq [754.460324ms]
Jul 29 01:55:09.245: INFO: Created: latency-svc-sc87s
Jul 29 01:55:09.282: INFO: Got endpoints: latency-svc-9xnwb [760.482183ms]
Jul 29 01:55:09.303: INFO: Created: latency-svc-8rfmb
Jul 29 01:55:09.322: INFO: Got endpoints: latency-svc-zzbpm [749.042523ms]
Jul 29 01:55:09.344: INFO: Created: latency-svc-r579c
Jul 29 01:55:09.374: INFO: Got endpoints: latency-svc-wr222 [752.204838ms]
Jul 29 01:55:09.392: INFO: Created: latency-svc-lrllt
Jul 29 01:55:09.421: INFO: Got endpoints: latency-svc-5brn2 [748.606853ms]
Jul 29 01:55:09.444: INFO: Created: latency-svc-p4792
Jul 29 01:55:09.475: INFO: Got endpoints: latency-svc-29d4l [752.596996ms]
Jul 29 01:55:09.495: INFO: Created: latency-svc-5g6xj
Jul 29 01:55:09.524: INFO: Got endpoints: latency-svc-p2qrg [740.253044ms]
Jul 29 01:55:09.538: INFO: Created: latency-svc-mpxsh
Jul 29 01:55:09.573: INFO: Got endpoints: latency-svc-8brz7 [744.908665ms]
Jul 29 01:55:09.592: INFO: Created: latency-svc-xgbmf
Jul 29 01:55:09.623: INFO: Got endpoints: latency-svc-rgmkl [736.624118ms]
Jul 29 01:55:09.640: INFO: Created: latency-svc-rkgbf
Jul 29 01:55:09.677: INFO: Got endpoints: latency-svc-zjpft [751.804082ms]
Jul 29 01:55:09.704: INFO: Created: latency-svc-tn9xb
Jul 29 01:55:09.735: INFO: Got endpoints: latency-svc-cc67x [760.33153ms]
Jul 29 01:55:09.763: INFO: Created: latency-svc-lxfsw
Jul 29 01:55:09.781: INFO: Got endpoints: latency-svc-bh78t [757.448918ms]
Jul 29 01:55:09.834: INFO: Created: latency-svc-qgdg8
Jul 29 01:55:09.843: INFO: Got endpoints: latency-svc-52djp [771.576546ms]
Jul 29 01:55:09.872: INFO: Created: latency-svc-f5lrp
Jul 29 01:55:09.884: INFO: Got endpoints: latency-svc-4pmgs [762.786575ms]
Jul 29 01:55:09.932: INFO: Created: latency-svc-nffgb
Jul 29 01:55:09.943: INFO: Got endpoints: latency-svc-65vll [768.746923ms]
Jul 29 01:55:09.961: INFO: Created: latency-svc-pqhfx
Jul 29 01:55:09.976: INFO: Got endpoints: latency-svc-sc87s [749.798009ms]
Jul 29 01:55:10.014: INFO: Created: latency-svc-vvp54
Jul 29 01:55:10.024: INFO: Got endpoints: latency-svc-8rfmb [741.934464ms]
Jul 29 01:55:10.057: INFO: Created: latency-svc-wht4c
Jul 29 01:55:10.084: INFO: Got endpoints: latency-svc-r579c [761.344529ms]
Jul 29 01:55:10.114: INFO: Created: latency-svc-6sfs6
Jul 29 01:55:10.123: INFO: Got endpoints: latency-svc-lrllt [748.123767ms]
Jul 29 01:55:10.145: INFO: Created: latency-svc-mqlns
Jul 29 01:55:10.174: INFO: Got endpoints: latency-svc-p4792 [752.736881ms]
Jul 29 01:55:10.202: INFO: Created: latency-svc-wzbxw
Jul 29 01:55:10.226: INFO: Got endpoints: latency-svc-5g6xj [750.858905ms]
Jul 29 01:55:10.244: INFO: Created: latency-svc-bkwgh
Jul 29 01:55:10.272: INFO: Got endpoints: latency-svc-mpxsh [748.119751ms]
Jul 29 01:55:10.297: INFO: Created: latency-svc-kzzp6
Jul 29 01:55:10.324: INFO: Got endpoints: latency-svc-xgbmf [750.892259ms]
Jul 29 01:55:10.342: INFO: Created: latency-svc-wz5wk
Jul 29 01:55:10.374: INFO: Got endpoints: latency-svc-rkgbf [750.00366ms]
Jul 29 01:55:10.392: INFO: Created: latency-svc-tt5z6
Jul 29 01:55:10.423: INFO: Got endpoints: latency-svc-tn9xb [745.97046ms]
Jul 29 01:55:10.444: INFO: Created: latency-svc-6gvbs
Jul 29 01:55:10.471: INFO: Got endpoints: latency-svc-lxfsw [736.078122ms]
Jul 29 01:55:10.496: INFO: Created: latency-svc-tzpsh
Jul 29 01:55:10.523: INFO: Got endpoints: latency-svc-qgdg8 [742.245381ms]
Jul 29 01:55:10.536: INFO: Created: latency-svc-ww57c
Jul 29 01:55:10.583: INFO: Got endpoints: latency-svc-f5lrp [739.945172ms]
Jul 29 01:55:10.600: INFO: Created: latency-svc-q4z5h
Jul 29 01:55:10.625: INFO: Got endpoints: latency-svc-nffgb [740.713692ms]
Jul 29 01:55:10.646: INFO: Created: latency-svc-zq7rd
Jul 29 01:55:10.676: INFO: Got endpoints: latency-svc-pqhfx [732.560439ms]
Jul 29 01:55:10.697: INFO: Created: latency-svc-6l5p8
Jul 29 01:55:10.726: INFO: Got endpoints: latency-svc-vvp54 [749.575287ms]
Jul 29 01:55:10.744: INFO: Created: latency-svc-d5j4h
Jul 29 01:55:10.773: INFO: Got endpoints: latency-svc-wht4c [748.689605ms]
Jul 29 01:55:10.803: INFO: Created: latency-svc-trk5s
Jul 29 01:55:10.843: INFO: Got endpoints: latency-svc-6sfs6 [758.509586ms]
Jul 29 01:55:10.860: INFO: Created: latency-svc-4nmsm
Jul 29 01:55:10.876: INFO: Got endpoints: latency-svc-mqlns [753.104215ms]
Jul 29 01:55:11.018: INFO: Created: latency-svc-q7p42
Jul 29 01:55:11.025: INFO: Got endpoints: latency-svc-bkwgh [798.884139ms]
Jul 29 01:55:11.026: INFO: Got endpoints: latency-svc-wzbxw [851.278698ms]
Jul 29 01:55:11.033: INFO: Got endpoints: latency-svc-kzzp6 [760.379054ms]
Jul 29 01:55:11.048: INFO: Created: latency-svc-km42q
Jul 29 01:55:11.053: INFO: Created: latency-svc-fm8hj
Jul 29 01:55:11.066: INFO: Created: latency-svc-xmwq7
Jul 29 01:55:11.078: INFO: Got endpoints: latency-svc-wz5wk [754.675729ms]
Jul 29 01:55:11.095: INFO: Created: latency-svc-qpq9f
Jul 29 01:55:11.122: INFO: Got endpoints: latency-svc-tt5z6 [747.930058ms]
Jul 29 01:55:11.137: INFO: Created: latency-svc-swsvn
Jul 29 01:55:11.174: INFO: Got endpoints: latency-svc-6gvbs [750.9136ms]
Jul 29 01:55:11.193: INFO: Created: latency-svc-jz22b
Jul 29 01:55:11.226: INFO: Got endpoints: latency-svc-tzpsh [754.869375ms]
Jul 29 01:55:11.241: INFO: Created: latency-svc-7c2j2
Jul 29 01:55:11.275: INFO: Got endpoints: latency-svc-ww57c [751.290903ms]
Jul 29 01:55:11.292: INFO: Created: latency-svc-qmsrb
Jul 29 01:55:11.329: INFO: Got endpoints: latency-svc-q4z5h [745.660597ms]
Jul 29 01:55:11.349: INFO: Created: latency-svc-nklx7
Jul 29 01:55:11.375: INFO: Got endpoints: latency-svc-zq7rd [749.689823ms]
Jul 29 01:55:11.392: INFO: Created: latency-svc-kf4fm
Jul 29 01:55:11.424: INFO: Got endpoints: latency-svc-6l5p8 [747.729367ms]
Jul 29 01:55:11.443: INFO: Created: latency-svc-sc4mz
Jul 29 01:55:11.474: INFO: Got endpoints: latency-svc-d5j4h [747.908834ms]
Jul 29 01:55:11.494: INFO: Created: latency-svc-2ssvw
Jul 29 01:55:11.527: INFO: Got endpoints: latency-svc-trk5s [754.113336ms]
Jul 29 01:55:11.551: INFO: Created: latency-svc-n4dml
Jul 29 01:55:11.573: INFO: Got endpoints: latency-svc-4nmsm [729.821177ms]
Jul 29 01:55:11.590: INFO: Created: latency-svc-2hs69
Jul 29 01:55:11.628: INFO: Got endpoints: latency-svc-q7p42 [752.156061ms]
Jul 29 01:55:11.648: INFO: Created: latency-svc-g68pc
Jul 29 01:55:11.677: INFO: Got endpoints: latency-svc-km42q [651.856608ms]
Jul 29 01:55:11.692: INFO: Created: latency-svc-sqfwl
Jul 29 01:55:11.722: INFO: Got endpoints: latency-svc-fm8hj [696.469411ms]
Jul 29 01:55:11.746: INFO: Created: latency-svc-vtkvt
Jul 29 01:55:11.774: INFO: Got endpoints: latency-svc-xmwq7 [741.415567ms]
Jul 29 01:55:11.786: INFO: Created: latency-svc-w8v7d
Jul 29 01:55:11.831: INFO: Got endpoints: latency-svc-qpq9f [752.04809ms]
Jul 29 01:55:11.866: INFO: Created: latency-svc-6tnvt
Jul 29 01:55:11.882: INFO: Got endpoints: latency-svc-swsvn [759.744051ms]
Jul 29 01:55:11.927: INFO: Got endpoints: latency-svc-jz22b [752.195915ms]
Jul 29 01:55:11.977: INFO: Got endpoints: latency-svc-7c2j2 [751.08238ms]
Jul 29 01:55:12.033: INFO: Got endpoints: latency-svc-qmsrb [758.520169ms]
Jul 29 01:55:12.071: INFO: Got endpoints: latency-svc-nklx7 [742.204272ms]
Jul 29 01:55:12.125: INFO: Got endpoints: latency-svc-kf4fm [749.545861ms]
Jul 29 01:55:12.171: INFO: Got endpoints: latency-svc-sc4mz [746.884627ms]
Jul 29 01:55:12.223: INFO: Got endpoints: latency-svc-2ssvw [749.004213ms]
Jul 29 01:55:12.276: INFO: Got endpoints: latency-svc-n4dml [749.033534ms]
Jul 29 01:55:12.325: INFO: Got endpoints: latency-svc-2hs69 [751.250167ms]
Jul 29 01:55:12.374: INFO: Got endpoints: latency-svc-g68pc [745.691004ms]
Jul 29 01:55:12.426: INFO: Got endpoints: latency-svc-sqfwl [749.098932ms]
Jul 29 01:55:12.476: INFO: Got endpoints: latency-svc-vtkvt [753.58194ms]
Jul 29 01:55:12.525: INFO: Got endpoints: latency-svc-w8v7d [750.715295ms]
Jul 29 01:55:12.574: INFO: Got endpoints: latency-svc-6tnvt [743.467807ms]
Jul 29 01:55:12.574: INFO: Latencies: [66.604745ms 80.069818ms 91.974183ms 98.239547ms 109.671ms 158.455832ms 162.087071ms 202.826675ms 215.259946ms 230.308521ms 252.086366ms 275.093564ms 296.891073ms 297.731717ms 301.801298ms 302.737811ms 306.989661ms 307.798237ms 309.358172ms 311.183111ms 312.211831ms 314.486861ms 329.210759ms 331.077454ms 336.329436ms 338.310513ms 339.783306ms 342.483057ms 342.888176ms 343.463971ms 347.637308ms 352.246378ms 354.420313ms 354.702495ms 358.686582ms 359.017882ms 362.002342ms 364.50018ms 365.76007ms 365.806411ms 366.820195ms 368.609357ms 370.373233ms 370.441876ms 373.121467ms 373.624575ms 375.567023ms 376.114414ms 376.26436ms 376.468871ms 381.149214ms 386.139723ms 390.335894ms 390.605324ms 392.589461ms 400.9619ms 402.290282ms 436.959907ms 458.455794ms 476.895041ms 489.898198ms 500.761389ms 519.141694ms 549.178098ms 574.805752ms 580.681909ms 583.760373ms 591.429177ms 602.625052ms 619.213491ms 621.251394ms 638.262475ms 641.679853ms 651.42872ms 651.856608ms 654.942114ms 658.823525ms 669.771886ms 670.143162ms 675.135964ms 675.409736ms 676.114014ms 677.940218ms 696.469411ms 702.062957ms 702.159242ms 702.493024ms 702.978208ms 703.695412ms 706.26291ms 710.827558ms 711.880795ms 713.345721ms 720.081808ms 721.694596ms 725.307033ms 726.044007ms 727.862643ms 728.234853ms 728.585327ms 729.821177ms 731.344894ms 732.402905ms 732.560439ms 734.079341ms 736.078122ms 736.624118ms 737.464049ms 738.437802ms 739.945172ms 740.023866ms 740.253044ms 740.713692ms 741.415567ms 741.934464ms 742.204272ms 742.245381ms 743.467807ms 744.32579ms 744.687958ms 744.908665ms 745.660597ms 745.691004ms 745.97046ms 746.884627ms 747.009273ms 747.118825ms 747.729367ms 747.908834ms 747.930058ms 748.119751ms 748.123767ms 748.606853ms 748.689605ms 749.004213ms 749.020843ms 749.033534ms 749.042523ms 749.098932ms 749.545861ms 749.575287ms 749.689823ms 749.798009ms 750.00366ms 750.099196ms 750.715295ms 750.858905ms 750.892259ms 750.9136ms 750.986088ms 751.08238ms 751.250167ms 751.290903ms 751.804082ms 752.04809ms 752.156061ms 752.195915ms 752.204838ms 752.231299ms 752.596996ms 752.736881ms 753.104215ms 753.58194ms 754.113336ms 754.460324ms 754.675729ms 754.869375ms 755.772527ms 757.448918ms 758.509586ms 758.520169ms 759.744051ms 760.33153ms 760.379054ms 760.482183ms 761.344529ms 761.96114ms 762.786575ms 763.217844ms 766.161145ms 766.786922ms 768.746923ms 771.576546ms 781.430599ms 790.984224ms 797.015149ms 798.884139ms 807.394627ms 844.035519ms 851.278698ms 870.491225ms 877.842699ms 877.995166ms 916.944749ms 960.91573ms 960.95473ms 981.863123ms 991.802779ms 1.031024231s 1.074965779s]
Jul 29 01:55:12.575: INFO: 50 %ile: 729.821177ms
Jul 29 01:55:12.575: INFO: 90 %ile: 766.786922ms
Jul 29 01:55:12.575: INFO: 99 %ile: 1.031024231s
Jul 29 01:55:12.575: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:55:12.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5025" for this suite.
Jul 29 01:55:30.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:55:30.820: INFO: namespace svc-latency-5025 deletion completed in 18.236286322s

• [SLOW TEST:31.095 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:55:30.823: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 29 01:55:35.456: INFO: Successfully updated pod "labelsupdate6f98fe37-d6ac-4481-ba1d-a32c7c0afa35"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:55:37.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4003" for this suite.
Jul 29 01:55:59.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:55:59.724: INFO: namespace downward-api-4003 deletion completed in 22.226348954s

• [SLOW TEST:28.901 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:55:59.729: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-01948ed0-7023-445e-8079-4acf5371cbeb
STEP: Creating a pod to test consume secrets
Jul 29 01:55:59.805: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-482901c8-fd49-42d5-8fb3-d68ed3d1104c" in namespace "projected-7345" to be "success or failure"
Jul 29 01:55:59.816: INFO: Pod "pod-projected-secrets-482901c8-fd49-42d5-8fb3-d68ed3d1104c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.800908ms
Jul 29 01:56:01.823: INFO: Pod "pod-projected-secrets-482901c8-fd49-42d5-8fb3-d68ed3d1104c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018349572s
Jul 29 01:56:03.829: INFO: Pod "pod-projected-secrets-482901c8-fd49-42d5-8fb3-d68ed3d1104c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024386399s
STEP: Saw pod success
Jul 29 01:56:03.830: INFO: Pod "pod-projected-secrets-482901c8-fd49-42d5-8fb3-d68ed3d1104c" satisfied condition "success or failure"
Jul 29 01:56:03.835: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-482901c8-fd49-42d5-8fb3-d68ed3d1104c container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 29 01:56:03.876: INFO: Waiting for pod pod-projected-secrets-482901c8-fd49-42d5-8fb3-d68ed3d1104c to disappear
Jul 29 01:56:03.880: INFO: Pod pod-projected-secrets-482901c8-fd49-42d5-8fb3-d68ed3d1104c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:56:03.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7345" for this suite.
Jul 29 01:56:09.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:56:10.127: INFO: namespace projected-7345 deletion completed in 6.239577439s

• [SLOW TEST:10.398 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:56:10.130: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 29 01:56:10.193: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 29 01:56:10.208: INFO: Waiting for terminating namespaces to be deleted...
Jul 29 01:56:10.214: INFO: 
Logging pods the kubelet thinks is on node conformance0 before test
Jul 29 01:56:10.225: INFO: kube-flannel-ds-q9cx6 from kube-system started at 2019-07-29 01:33:59 +0000 UTC (1 container statuses recorded)
Jul 29 01:56:10.225: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 29 01:56:10.225: INFO: ingress-default-backend-6bd67fff4c-vzlm8 from ingress-haproxy started at 2019-07-29 01:34:15 +0000 UTC (1 container statuses recorded)
Jul 29 01:56:10.225: INFO: 	Container ingress-default-backend ready: true, restart count 0
Jul 29 01:56:10.225: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-29 01:38:17 +0000 UTC (1 container statuses recorded)
Jul 29 01:56:10.225: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 29 01:56:10.225: INFO: sonobuoy-e2e-job-feb2cf4d6c56458c from heptio-sonobuoy started at 2019-07-29 01:38:22 +0000 UTC (2 container statuses recorded)
Jul 29 01:56:10.225: INFO: 	Container e2e ready: true, restart count 0
Jul 29 01:56:10.225: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 01:56:10.225: INFO: nirmata-cni-installer-gstxd from nirmata started at 2019-07-29 01:34:09 +0000 UTC (1 container statuses recorded)
Jul 29 01:56:10.225: INFO: 	Container install-cni ready: true, restart count 0
Jul 29 01:56:10.225: INFO: haproxy-ingress-98858d4d6-fzcg2 from ingress-haproxy started at 2019-07-29 01:34:16 +0000 UTC (1 container statuses recorded)
Jul 29 01:56:10.225: INFO: 	Container haproxy-ingress ready: true, restart count 0
Jul 29 01:56:10.225: INFO: sonobuoy-systemd-logs-daemon-set-5e30946e92f64e9d-v422w from heptio-sonobuoy started at 2019-07-29 01:38:22 +0000 UTC (2 container statuses recorded)
Jul 29 01:56:10.225: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 01:56:10.226: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 01:56:10.226: INFO: 
Logging pods the kubelet thinks is on node conformance1 before test
Jul 29 01:56:10.236: INFO: kube-flannel-ds-gmpdt from kube-system started at 2019-07-29 01:33:20 +0000 UTC (1 container statuses recorded)
Jul 29 01:56:10.236: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 29 01:56:10.236: INFO: nirmata-cni-installer-swqlb from nirmata started at 2019-07-29 01:33:34 +0000 UTC (1 container statuses recorded)
Jul 29 01:56:10.236: INFO: 	Container install-cni ready: true, restart count 0
Jul 29 01:56:10.236: INFO: kube-dns-6475b5b555-kv5d8 from kube-system started at 2019-07-29 01:33:36 +0000 UTC (3 container statuses recorded)
Jul 29 01:56:10.236: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 29 01:56:10.236: INFO: 	Container kubedns ready: true, restart count 0
Jul 29 01:56:10.236: INFO: 	Container sidecar ready: true, restart count 0
Jul 29 01:56:10.236: INFO: nirmata-kube-controller-7c556b44bc-b5rnr from nirmata started at 2019-07-29 01:33:36 +0000 UTC (1 container statuses recorded)
Jul 29 01:56:10.236: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Jul 29 01:56:10.236: INFO: metrics-server-8454c7c9c6-htc58 from kube-system started at 2019-07-29 01:33:36 +0000 UTC (1 container statuses recorded)
Jul 29 01:56:10.236: INFO: 	Container metrics-server ready: true, restart count 0
Jul 29 01:56:10.236: INFO: sonobuoy-systemd-logs-daemon-set-5e30946e92f64e9d-ljfqt from heptio-sonobuoy started at 2019-07-29 01:38:22 +0000 UTC (2 container statuses recorded)
Jul 29 01:56:10.236: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 01:56:10.236: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b5be0411cc35a5], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:56:11.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5932" for this suite.
Jul 29 01:56:17.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:56:17.491: INFO: namespace sched-pred-5932 deletion completed in 6.204346942s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.362 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:56:17.498: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-fe6edb4b-6578-4050-89d6-1d7acef197e5
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:56:21.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8639" for this suite.
Jul 29 01:56:43.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:56:43.915: INFO: namespace configmap-8639 deletion completed in 22.219676389s

• [SLOW TEST:26.418 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:56:43.920: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4627, will wait for the garbage collector to delete the pods
Jul 29 01:56:48.069: INFO: Deleting Job.batch foo took: 9.78608ms
Jul 29 01:56:48.369: INFO: Terminating Job.batch foo pods took: 300.529728ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:57:24.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4627" for this suite.
Jul 29 01:57:30.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:57:30.380: INFO: namespace job-4627 deletion completed in 6.281430068s

• [SLOW TEST:46.460 seconds]
[sig-apps] Job
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:57:30.381: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 29 01:57:30.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6420'
Jul 29 01:57:30.698: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 29 01:57:30.698: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jul 29 01:57:32.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6420'
Jul 29 01:57:32.971: INFO: stderr: ""
Jul 29 01:57:32.971: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:57:32.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6420" for this suite.
Jul 29 01:57:39.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:57:39.195: INFO: namespace kubectl-6420 deletion completed in 6.218219294s

• [SLOW TEST:8.814 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:57:39.195: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 01:57:39.268: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3ed49df-4c1a-4f73-a7bd-bae6f0f3a830" in namespace "projected-3616" to be "success or failure"
Jul 29 01:57:39.276: INFO: Pod "downwardapi-volume-c3ed49df-4c1a-4f73-a7bd-bae6f0f3a830": Phase="Pending", Reason="", readiness=false. Elapsed: 7.532966ms
Jul 29 01:57:41.282: INFO: Pod "downwardapi-volume-c3ed49df-4c1a-4f73-a7bd-bae6f0f3a830": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013272526s
Jul 29 01:57:43.288: INFO: Pod "downwardapi-volume-c3ed49df-4c1a-4f73-a7bd-bae6f0f3a830": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018970944s
STEP: Saw pod success
Jul 29 01:57:43.288: INFO: Pod "downwardapi-volume-c3ed49df-4c1a-4f73-a7bd-bae6f0f3a830" satisfied condition "success or failure"
Jul 29 01:57:43.293: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-c3ed49df-4c1a-4f73-a7bd-bae6f0f3a830 container client-container: <nil>
STEP: delete the pod
Jul 29 01:57:43.333: INFO: Waiting for pod downwardapi-volume-c3ed49df-4c1a-4f73-a7bd-bae6f0f3a830 to disappear
Jul 29 01:57:43.338: INFO: Pod downwardapi-volume-c3ed49df-4c1a-4f73-a7bd-bae6f0f3a830 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:57:43.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3616" for this suite.
Jul 29 01:57:49.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:57:49.679: INFO: namespace projected-3616 deletion completed in 6.331663753s

• [SLOW TEST:10.484 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:57:49.686: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jul 29 01:57:50.304: INFO: created pod pod-service-account-defaultsa
Jul 29 01:57:50.304: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 29 01:57:50.315: INFO: created pod pod-service-account-mountsa
Jul 29 01:57:50.315: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 29 01:57:50.380: INFO: created pod pod-service-account-nomountsa
Jul 29 01:57:50.380: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 29 01:57:50.409: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 29 01:57:50.409: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 29 01:57:50.461: INFO: created pod pod-service-account-mountsa-mountspec
Jul 29 01:57:50.461: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 29 01:57:50.522: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 29 01:57:50.522: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 29 01:57:50.556: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 29 01:57:50.556: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 29 01:57:50.596: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 29 01:57:50.596: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 29 01:57:50.618: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 29 01:57:50.618: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:57:50.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1391" for this suite.
Jul 29 01:58:14.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:58:14.902: INFO: namespace svcaccounts-1391 deletion completed in 24.252440937s

• [SLOW TEST:25.217 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:58:14.910: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-4996
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4996
STEP: Deleting pre-stop pod
Jul 29 01:58:28.104: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:58:28.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4996" for this suite.
Jul 29 01:59:14.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:59:14.535: INFO: namespace prestop-4996 deletion completed in 46.377781471s

• [SLOW TEST:59.626 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:59:14.535: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jul 29 01:59:14.604: INFO: Waiting up to 5m0s for pod "client-containers-ec606db3-d92e-4379-a1f7-d3311171e31c" in namespace "containers-1399" to be "success or failure"
Jul 29 01:59:14.630: INFO: Pod "client-containers-ec606db3-d92e-4379-a1f7-d3311171e31c": Phase="Pending", Reason="", readiness=false. Elapsed: 25.758757ms
Jul 29 01:59:16.637: INFO: Pod "client-containers-ec606db3-d92e-4379-a1f7-d3311171e31c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032319076s
Jul 29 01:59:18.644: INFO: Pod "client-containers-ec606db3-d92e-4379-a1f7-d3311171e31c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039299185s
STEP: Saw pod success
Jul 29 01:59:18.644: INFO: Pod "client-containers-ec606db3-d92e-4379-a1f7-d3311171e31c" satisfied condition "success or failure"
Jul 29 01:59:18.650: INFO: Trying to get logs from node conformance1 pod client-containers-ec606db3-d92e-4379-a1f7-d3311171e31c container test-container: <nil>
STEP: delete the pod
Jul 29 01:59:18.710: INFO: Waiting for pod client-containers-ec606db3-d92e-4379-a1f7-d3311171e31c to disappear
Jul 29 01:59:18.717: INFO: Pod client-containers-ec606db3-d92e-4379-a1f7-d3311171e31c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:59:18.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1399" for this suite.
Jul 29 01:59:24.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:59:24.995: INFO: namespace containers-1399 deletion completed in 6.270182776s

• [SLOW TEST:10.460 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:59:24.997: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-de17e65a-c5a9-45dc-a34d-b2a66f8f2455
STEP: Creating a pod to test consume secrets
Jul 29 01:59:25.083: INFO: Waiting up to 5m0s for pod "pod-secrets-f228c35d-f7b0-417c-984c-b78b717acbad" in namespace "secrets-4033" to be "success or failure"
Jul 29 01:59:25.096: INFO: Pod "pod-secrets-f228c35d-f7b0-417c-984c-b78b717acbad": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019248ms
Jul 29 01:59:27.102: INFO: Pod "pod-secrets-f228c35d-f7b0-417c-984c-b78b717acbad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018828058s
Jul 29 01:59:29.109: INFO: Pod "pod-secrets-f228c35d-f7b0-417c-984c-b78b717acbad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025317108s
STEP: Saw pod success
Jul 29 01:59:29.109: INFO: Pod "pod-secrets-f228c35d-f7b0-417c-984c-b78b717acbad" satisfied condition "success or failure"
Jul 29 01:59:29.128: INFO: Trying to get logs from node conformance0 pod pod-secrets-f228c35d-f7b0-417c-984c-b78b717acbad container secret-volume-test: <nil>
STEP: delete the pod
Jul 29 01:59:29.305: INFO: Waiting for pod pod-secrets-f228c35d-f7b0-417c-984c-b78b717acbad to disappear
Jul 29 01:59:29.313: INFO: Pod pod-secrets-f228c35d-f7b0-417c-984c-b78b717acbad no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 01:59:29.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4033" for this suite.
Jul 29 01:59:35.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 01:59:35.547: INFO: namespace secrets-4033 deletion completed in 6.223481733s

• [SLOW TEST:10.550 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 01:59:35.548: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:00:01.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1241" for this suite.
Jul 29 02:00:07.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:00:07.334: INFO: namespace container-runtime-1241 deletion completed in 6.223640422s

• [SLOW TEST:31.787 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:00:07.340: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-2497e736-4641-4bcf-a7f4-e790172a5774
STEP: Creating a pod to test consume secrets
Jul 29 02:00:07.433: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8570268c-1841-4dee-a9ac-e28c9d5e3d0d" in namespace "projected-1676" to be "success or failure"
Jul 29 02:00:07.446: INFO: Pod "pod-projected-secrets-8570268c-1841-4dee-a9ac-e28c9d5e3d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.677419ms
Jul 29 02:00:09.452: INFO: Pod "pod-projected-secrets-8570268c-1841-4dee-a9ac-e28c9d5e3d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01895222s
Jul 29 02:00:11.458: INFO: Pod "pod-projected-secrets-8570268c-1841-4dee-a9ac-e28c9d5e3d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025288953s
STEP: Saw pod success
Jul 29 02:00:11.458: INFO: Pod "pod-projected-secrets-8570268c-1841-4dee-a9ac-e28c9d5e3d0d" satisfied condition "success or failure"
Jul 29 02:00:11.462: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-8570268c-1841-4dee-a9ac-e28c9d5e3d0d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 29 02:00:11.495: INFO: Waiting for pod pod-projected-secrets-8570268c-1841-4dee-a9ac-e28c9d5e3d0d to disappear
Jul 29 02:00:11.504: INFO: Pod pod-projected-secrets-8570268c-1841-4dee-a9ac-e28c9d5e3d0d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:00:11.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1676" for this suite.
Jul 29 02:00:17.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:00:17.755: INFO: namespace projected-1676 deletion completed in 6.243542937s

• [SLOW TEST:10.415 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:00:17.756: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3673.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3673.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3673.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3673.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3673.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3673.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3673.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3673.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3673.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3673.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3673.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3673.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3673.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 220.190.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.190.220_udp@PTR;check="$$(dig +tcp +noall +answer +search 220.190.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.190.220_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3673.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3673.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3673.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3673.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3673.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3673.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3673.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3673.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3673.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3673.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3673.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3673.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3673.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 220.190.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.190.220_udp@PTR;check="$$(dig +tcp +noall +answer +search 220.190.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.190.220_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 29 02:00:24.290: INFO: Unable to read jessie_udp@dns-test-service.dns-3673.svc.cluster.local from pod dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda: the server could not find the requested resource (get pods dns-test-38e9050d-7660-4669-8ee5-a03207b1edda)
Jul 29 02:00:24.298: INFO: Unable to read jessie_tcp@dns-test-service.dns-3673.svc.cluster.local from pod dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda: the server could not find the requested resource (get pods dns-test-38e9050d-7660-4669-8ee5-a03207b1edda)
Jul 29 02:00:24.306: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3673.svc.cluster.local from pod dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda: the server could not find the requested resource (get pods dns-test-38e9050d-7660-4669-8ee5-a03207b1edda)
Jul 29 02:00:24.314: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3673.svc.cluster.local from pod dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda: the server could not find the requested resource (get pods dns-test-38e9050d-7660-4669-8ee5-a03207b1edda)
Jul 29 02:00:24.321: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-3673.svc.cluster.local from pod dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda: the server could not find the requested resource (get pods dns-test-38e9050d-7660-4669-8ee5-a03207b1edda)
Jul 29 02:00:24.328: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-3673.svc.cluster.local from pod dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda: the server could not find the requested resource (get pods dns-test-38e9050d-7660-4669-8ee5-a03207b1edda)
Jul 29 02:00:24.337: INFO: Unable to read jessie_udp@PodARecord from pod dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda: the server could not find the requested resource (get pods dns-test-38e9050d-7660-4669-8ee5-a03207b1edda)
Jul 29 02:00:24.344: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda: the server could not find the requested resource (get pods dns-test-38e9050d-7660-4669-8ee5-a03207b1edda)
Jul 29 02:00:24.357: INFO: Lookups using dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda failed for: [jessie_udp@dns-test-service.dns-3673.svc.cluster.local jessie_tcp@dns-test-service.dns-3673.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3673.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3673.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-3673.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-3673.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul 29 02:00:29.500: INFO: DNS probes using dns-3673/dns-test-38e9050d-7660-4669-8ee5-a03207b1edda succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:00:29.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3673" for this suite.
Jul 29 02:00:35.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:00:36.004: INFO: namespace dns-3673 deletion completed in 6.250803185s

• [SLOW TEST:18.249 seconds]
[sig-network] DNS
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:00:36.006: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:00:36.193: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1c957c86-5a11-42df-91ad-fad76abf1b76", Controller:(*bool)(0xc00306aef6), BlockOwnerDeletion:(*bool)(0xc00306aef7)}}
Jul 29 02:00:36.215: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"54170bc5-29b9-4583-8847-b786d9f2bdb2", Controller:(*bool)(0xc00306b0a6), BlockOwnerDeletion:(*bool)(0xc00306b0a7)}}
Jul 29 02:00:36.239: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"23de50fa-e91e-4d69-a8f5-e363c53ec642", Controller:(*bool)(0xc001dff1de), BlockOwnerDeletion:(*bool)(0xc001dff1df)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:00:41.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3302" for this suite.
Jul 29 02:00:47.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:00:47.479: INFO: namespace gc-3302 deletion completed in 6.210836195s

• [SLOW TEST:11.473 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:00:47.480: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:00:47.599: INFO: Create a RollingUpdate DaemonSet
Jul 29 02:00:47.607: INFO: Check that daemon pods launch on every node of the cluster
Jul 29 02:00:47.629: INFO: Number of nodes with available pods: 0
Jul 29 02:00:47.630: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:00:48.644: INFO: Number of nodes with available pods: 0
Jul 29 02:00:48.644: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:00:49.643: INFO: Number of nodes with available pods: 0
Jul 29 02:00:49.643: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:00:50.643: INFO: Number of nodes with available pods: 1
Jul 29 02:00:50.643: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:00:51.643: INFO: Number of nodes with available pods: 1
Jul 29 02:00:51.643: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:00:52.642: INFO: Number of nodes with available pods: 2
Jul 29 02:00:52.642: INFO: Number of running nodes: 2, number of available pods: 2
Jul 29 02:00:52.642: INFO: Update the DaemonSet to trigger a rollout
Jul 29 02:00:52.655: INFO: Updating DaemonSet daemon-set
Jul 29 02:00:56.695: INFO: Roll back the DaemonSet before rollout is complete
Jul 29 02:00:56.713: INFO: Updating DaemonSet daemon-set
Jul 29 02:00:56.713: INFO: Make sure DaemonSet rollback is complete
Jul 29 02:00:56.730: INFO: Wrong image for pod: daemon-set-6v9qh. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 29 02:00:56.730: INFO: Pod daemon-set-6v9qh is not available
Jul 29 02:00:57.759: INFO: Pod daemon-set-tdsmf is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2060, will wait for the garbage collector to delete the pods
Jul 29 02:00:57.856: INFO: Deleting DaemonSet.extensions daemon-set took: 13.689429ms
Jul 29 02:00:58.256: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.323233ms
Jul 29 02:01:07.464: INFO: Number of nodes with available pods: 0
Jul 29 02:01:07.464: INFO: Number of running nodes: 0, number of available pods: 0
Jul 29 02:01:07.472: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2060/daemonsets","resourceVersion":"5516"},"items":null}

Jul 29 02:01:07.478: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2060/pods","resourceVersion":"5516"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:01:07.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2060" for this suite.
Jul 29 02:01:13.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:01:13.748: INFO: namespace daemonsets-2060 deletion completed in 6.246248007s

• [SLOW TEST:26.268 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:01:13.749: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 29 02:01:18.409: INFO: Successfully updated pod "annotationupdateef08db8e-ead7-4901-9154-7b3ae818be0d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:01:20.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5654" for this suite.
Jul 29 02:01:42.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:01:42.790: INFO: namespace downward-api-5654 deletion completed in 22.221555712s

• [SLOW TEST:29.042 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:01:42.794: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 29 02:01:47.949: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:01:48.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7068" for this suite.
Jul 29 02:02:13.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:02:13.239: INFO: namespace replicaset-7068 deletion completed in 24.253573461s

• [SLOW TEST:30.445 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:02:13.240: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-cff3cebe-e951-4e9e-8968-7d0a32943811
STEP: Creating a pod to test consume configMaps
Jul 29 02:02:13.328: INFO: Waiting up to 5m0s for pod "pod-configmaps-281436c7-2f9f-4caf-b907-68307d4c6f53" in namespace "configmap-677" to be "success or failure"
Jul 29 02:02:13.354: INFO: Pod "pod-configmaps-281436c7-2f9f-4caf-b907-68307d4c6f53": Phase="Pending", Reason="", readiness=false. Elapsed: 25.815808ms
Jul 29 02:02:15.359: INFO: Pod "pod-configmaps-281436c7-2f9f-4caf-b907-68307d4c6f53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031204307s
Jul 29 02:02:17.364: INFO: Pod "pod-configmaps-281436c7-2f9f-4caf-b907-68307d4c6f53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035998114s
STEP: Saw pod success
Jul 29 02:02:17.364: INFO: Pod "pod-configmaps-281436c7-2f9f-4caf-b907-68307d4c6f53" satisfied condition "success or failure"
Jul 29 02:02:17.369: INFO: Trying to get logs from node conformance0 pod pod-configmaps-281436c7-2f9f-4caf-b907-68307d4c6f53 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 02:02:17.401: INFO: Waiting for pod pod-configmaps-281436c7-2f9f-4caf-b907-68307d4c6f53 to disappear
Jul 29 02:02:17.405: INFO: Pod pod-configmaps-281436c7-2f9f-4caf-b907-68307d4c6f53 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:02:17.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-677" for this suite.
Jul 29 02:02:23.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:02:23.688: INFO: namespace configmap-677 deletion completed in 6.275234552s

• [SLOW TEST:10.448 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:02:23.688: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 29 02:02:23.874: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:02:30.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-10" for this suite.
Jul 29 02:02:36.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:02:36.320: INFO: namespace init-container-10 deletion completed in 6.219855524s

• [SLOW TEST:12.632 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:02:36.327: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-6747/configmap-test-97f4ee60-0ba4-4dd1-b613-e04437ecf40b
STEP: Creating a pod to test consume configMaps
Jul 29 02:02:36.421: INFO: Waiting up to 5m0s for pod "pod-configmaps-dbf0a522-c61a-4758-abca-d5cf25fd9846" in namespace "configmap-6747" to be "success or failure"
Jul 29 02:02:36.434: INFO: Pod "pod-configmaps-dbf0a522-c61a-4758-abca-d5cf25fd9846": Phase="Pending", Reason="", readiness=false. Elapsed: 12.840988ms
Jul 29 02:02:38.441: INFO: Pod "pod-configmaps-dbf0a522-c61a-4758-abca-d5cf25fd9846": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020237568s
Jul 29 02:02:40.447: INFO: Pod "pod-configmaps-dbf0a522-c61a-4758-abca-d5cf25fd9846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02566035s
STEP: Saw pod success
Jul 29 02:02:40.447: INFO: Pod "pod-configmaps-dbf0a522-c61a-4758-abca-d5cf25fd9846" satisfied condition "success or failure"
Jul 29 02:02:40.452: INFO: Trying to get logs from node conformance0 pod pod-configmaps-dbf0a522-c61a-4758-abca-d5cf25fd9846 container env-test: <nil>
STEP: delete the pod
Jul 29 02:02:40.496: INFO: Waiting for pod pod-configmaps-dbf0a522-c61a-4758-abca-d5cf25fd9846 to disappear
Jul 29 02:02:40.502: INFO: Pod pod-configmaps-dbf0a522-c61a-4758-abca-d5cf25fd9846 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:02:40.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6747" for this suite.
Jul 29 02:02:46.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:02:46.712: INFO: namespace configmap-6747 deletion completed in 6.203391785s

• [SLOW TEST:10.385 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:02:46.716: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:02:46.786: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:02:50.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3918" for this suite.
Jul 29 02:03:36.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:03:37.063: INFO: namespace pods-3918 deletion completed in 46.201124362s

• [SLOW TEST:50.347 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:03:37.063: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jul 29 02:03:37.175: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-518829307 proxy --unix-socket=/tmp/kubectl-proxy-unix946563102/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:03:37.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4032" for this suite.
Jul 29 02:03:43.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:03:43.551: INFO: namespace kubectl-4032 deletion completed in 6.252624832s

• [SLOW TEST:6.488 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:03:43.552: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0729 02:03:53.669128      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 29 02:03:53.669: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:03:53.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7688" for this suite.
Jul 29 02:03:59.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:03:59.878: INFO: namespace gc-7688 deletion completed in 6.203097315s

• [SLOW TEST:16.326 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:03:59.878: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-6c0152d2-0de3-4e01-a103-86403827a1fe
STEP: Creating a pod to test consume configMaps
Jul 29 02:03:59.970: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d6090df7-50d6-4163-9068-b6039cb42da7" in namespace "projected-5620" to be "success or failure"
Jul 29 02:03:59.989: INFO: Pod "pod-projected-configmaps-d6090df7-50d6-4163-9068-b6039cb42da7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.888138ms
Jul 29 02:04:01.995: INFO: Pod "pod-projected-configmaps-d6090df7-50d6-4163-9068-b6039cb42da7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024874971s
Jul 29 02:04:04.001: INFO: Pod "pod-projected-configmaps-d6090df7-50d6-4163-9068-b6039cb42da7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030308732s
STEP: Saw pod success
Jul 29 02:04:04.001: INFO: Pod "pod-projected-configmaps-d6090df7-50d6-4163-9068-b6039cb42da7" satisfied condition "success or failure"
Jul 29 02:04:04.013: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-d6090df7-50d6-4163-9068-b6039cb42da7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 02:04:04.065: INFO: Waiting for pod pod-projected-configmaps-d6090df7-50d6-4163-9068-b6039cb42da7 to disappear
Jul 29 02:04:04.071: INFO: Pod pod-projected-configmaps-d6090df7-50d6-4163-9068-b6039cb42da7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:04:04.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5620" for this suite.
Jul 29 02:04:10.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:04:10.326: INFO: namespace projected-5620 deletion completed in 6.243872952s

• [SLOW TEST:10.448 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:04:10.335: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:04:10.396: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 29 02:04:10.422: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 29 02:04:15.433: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 29 02:04:15.433: INFO: Creating deployment "test-rolling-update-deployment"
Jul 29 02:04:15.447: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 29 02:04:15.478: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 29 02:04:17.488: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 29 02:04:17.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962655, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962655, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962655, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962655, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 02:04:19.498: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 29 02:04:19.512: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4724,SelfLink:/apis/apps/v1/namespaces/deployment-4724/deployments/test-rolling-update-deployment,UID:b0b2faba-deef-4f0d-9600-98e32ec33d42,ResourceVersion:5939,Generation:1,CreationTimestamp:2019-07-29 02:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-29 02:04:15 +0000 UTC 2019-07-29 02:04:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-29 02:04:18 +0000 UTC 2019-07-29 02:04:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 29 02:04:19.519: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-4724,SelfLink:/apis/apps/v1/namespaces/deployment-4724/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:6bf3ad2c-f8d6-4506-993c-adcd63e0fb4f,ResourceVersion:5928,Generation:1,CreationTimestamp:2019-07-29 02:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b0b2faba-deef-4f0d-9600-98e32ec33d42 0xc001a4d2c7 0xc001a4d2c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 29 02:04:19.519: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 29 02:04:19.520: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4724,SelfLink:/apis/apps/v1/namespaces/deployment-4724/replicasets/test-rolling-update-controller,UID:790b8a9c-8d73-4449-a839-55f799eadf79,ResourceVersion:5938,Generation:2,CreationTimestamp:2019-07-29 02:04:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b0b2faba-deef-4f0d-9600-98e32ec33d42 0xc001a4d1ef 0xc001a4d200}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 29 02:04:19.525: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-lkg5c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-lkg5c,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-4724,SelfLink:/api/v1/namespaces/deployment-4724/pods/test-rolling-update-deployment-79f6b9d75c-lkg5c,UID:a87aa15c-ea4a-4ed3-8d54-b5d7e66f6773,ResourceVersion:5927,Generation:0,CreationTimestamp:2019-07-29 02:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 6bf3ad2c-f8d6-4506-993c-adcd63e0fb4f 0xc001a4dba7 0xc001a4dba8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qwq9j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qwq9j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-qwq9j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a4dc20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a4dc40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:04:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:04:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:04:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:04:15 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:10.244.1.66,StartTime:2019-07-29 02:04:15 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-29 02:04:17 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9dbe0c71cf08ccce0d5b2ed134bb7d8451219f61db318f8f36a000acaafff504}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:04:19.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4724" for this suite.
Jul 29 02:04:25.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:04:25.769: INFO: namespace deployment-4724 deletion completed in 6.236593409s

• [SLOW TEST:15.434 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:04:25.773: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 29 02:04:28.891: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:04:28.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2737" for this suite.
Jul 29 02:04:34.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:04:35.168: INFO: namespace container-runtime-2737 deletion completed in 6.207988398s

• [SLOW TEST:9.395 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:04:35.169: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:04:39.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2817" for this suite.
Jul 29 02:04:45.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:04:45.635: INFO: namespace emptydir-wrapper-2817 deletion completed in 6.239404969s

• [SLOW TEST:10.466 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:04:45.635: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 29 02:04:45.694: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jul 29 02:04:46.394: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 29 02:04:48.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962686, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962686, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962686, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962686, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 02:04:50.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962686, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962686, loc:(*time.Location)(0x80c0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962686, loc:(*time.Location)(0x80c0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699962686, loc:(*time.Location)(0x80c0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 02:04:54.222: INFO: Waited 1.744744886s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:04:54.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3475" for this suite.
Jul 29 02:05:00.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:05:01.017: INFO: namespace aggregator-3475 deletion completed in 6.230523504s

• [SLOW TEST:15.383 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:05:01.022: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jul 29 02:05:05.127: INFO: Pod pod-hostip-72ba4b6a-266f-4913-bc33-65884259df2b has hostIP: 10.10.1.213
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:05:05.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2641" for this suite.
Jul 29 02:05:29.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:05:29.421: INFO: namespace pods-2641 deletion completed in 24.287058413s

• [SLOW TEST:28.399 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:05:29.423: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:05:29.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc2fe9eb-9b5e-46c4-95e2-6eb83aa66682" in namespace "projected-4823" to be "success or failure"
Jul 29 02:05:29.587: INFO: Pod "downwardapi-volume-dc2fe9eb-9b5e-46c4-95e2-6eb83aa66682": Phase="Pending", Reason="", readiness=false. Elapsed: 5.414424ms
Jul 29 02:05:31.592: INFO: Pod "downwardapi-volume-dc2fe9eb-9b5e-46c4-95e2-6eb83aa66682": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010328914s
Jul 29 02:05:33.600: INFO: Pod "downwardapi-volume-dc2fe9eb-9b5e-46c4-95e2-6eb83aa66682": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018723516s
Jul 29 02:05:35.606: INFO: Pod "downwardapi-volume-dc2fe9eb-9b5e-46c4-95e2-6eb83aa66682": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024460819s
STEP: Saw pod success
Jul 29 02:05:35.606: INFO: Pod "downwardapi-volume-dc2fe9eb-9b5e-46c4-95e2-6eb83aa66682" satisfied condition "success or failure"
Jul 29 02:05:35.610: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-dc2fe9eb-9b5e-46c4-95e2-6eb83aa66682 container client-container: <nil>
STEP: delete the pod
Jul 29 02:05:35.664: INFO: Waiting for pod downwardapi-volume-dc2fe9eb-9b5e-46c4-95e2-6eb83aa66682 to disappear
Jul 29 02:05:35.684: INFO: Pod downwardapi-volume-dc2fe9eb-9b5e-46c4-95e2-6eb83aa66682 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:05:35.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4823" for this suite.
Jul 29 02:05:41.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:05:42.044: INFO: namespace projected-4823 deletion completed in 6.347931595s

• [SLOW TEST:12.621 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:05:42.046: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-896aded2-dac8-40ae-9b7e-d08030d3d2e3 in namespace container-probe-1898
Jul 29 02:05:46.191: INFO: Started pod test-webserver-896aded2-dac8-40ae-9b7e-d08030d3d2e3 in namespace container-probe-1898
STEP: checking the pod's current state and verifying that restartCount is present
Jul 29 02:05:46.197: INFO: Initial restart count of pod test-webserver-896aded2-dac8-40ae-9b7e-d08030d3d2e3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:09:47.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1898" for this suite.
Jul 29 02:09:53.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:09:53.558: INFO: namespace container-probe-1898 deletion completed in 6.255776043s

• [SLOW TEST:251.512 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:09:53.559: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-96725d26-55e0-4c9b-98e5-047089f5bf90
STEP: Creating a pod to test consume secrets
Jul 29 02:09:53.717: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1c079cae-7698-4730-932a-d440d918cfa9" in namespace "projected-7389" to be "success or failure"
Jul 29 02:09:53.727: INFO: Pod "pod-projected-secrets-1c079cae-7698-4730-932a-d440d918cfa9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.50741ms
Jul 29 02:09:55.731: INFO: Pod "pod-projected-secrets-1c079cae-7698-4730-932a-d440d918cfa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014075107s
Jul 29 02:09:57.735: INFO: Pod "pod-projected-secrets-1c079cae-7698-4730-932a-d440d918cfa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01823562s
STEP: Saw pod success
Jul 29 02:09:57.735: INFO: Pod "pod-projected-secrets-1c079cae-7698-4730-932a-d440d918cfa9" satisfied condition "success or failure"
Jul 29 02:09:57.738: INFO: Trying to get logs from node conformance1 pod pod-projected-secrets-1c079cae-7698-4730-932a-d440d918cfa9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 29 02:09:57.766: INFO: Waiting for pod pod-projected-secrets-1c079cae-7698-4730-932a-d440d918cfa9 to disappear
Jul 29 02:09:57.772: INFO: Pod pod-projected-secrets-1c079cae-7698-4730-932a-d440d918cfa9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:09:57.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7389" for this suite.
Jul 29 02:10:03.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:10:03.973: INFO: namespace projected-7389 deletion completed in 6.196256325s

• [SLOW TEST:10.415 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:10:03.978: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 29 02:10:08.730: INFO: Successfully updated pod "labelsupdate5b97eb2e-8ef4-449d-adf7-cf2de378c802"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:10:10.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3826" for this suite.
Jul 29 02:10:32.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:10:32.991: INFO: namespace projected-3826 deletion completed in 22.224084587s

• [SLOW TEST:29.014 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:10:32.991: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 29 02:10:33.100: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:10:44.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2601" for this suite.
Jul 29 02:10:50.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:10:50.236: INFO: namespace pods-2601 deletion completed in 6.171514272s

• [SLOW TEST:17.245 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:10:50.240: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:10:55.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2676" for this suite.
Jul 29 02:11:01.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:11:02.119: INFO: namespace watch-2676 deletion completed in 6.265594943s

• [SLOW TEST:11.879 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:11:02.120: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 29 02:11:06.728: INFO: Successfully updated pod "pod-update-a26f5f4d-88e3-4d02-88a5-b3f4335c7cab"
STEP: verifying the updated pod is in kubernetes
Jul 29 02:11:06.738: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:11:06.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3645" for this suite.
Jul 29 02:11:28.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:11:28.992: INFO: namespace pods-3645 deletion completed in 22.248443943s

• [SLOW TEST:26.873 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:11:29.001: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5074
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 29 02:11:29.088: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 29 02:11:55.331: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.52:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5074 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:11:55.331: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:11:55.759: INFO: Found all expected endpoints: [netserver-0]
Jul 29 02:11:55.765: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.73:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5074 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:11:55.766: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:11:56.144: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:11:56.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5074" for this suite.
Jul 29 02:12:20.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:12:20.308: INFO: namespace pod-network-test-5074 deletion completed in 24.158737583s

• [SLOW TEST:51.309 seconds]
[sig-network] Networking
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:12:20.317: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:12:20.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-762'
Jul 29 02:12:21.183: INFO: stderr: ""
Jul 29 02:12:21.183: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 29 02:12:21.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-762'
Jul 29 02:12:21.625: INFO: stderr: ""
Jul 29 02:12:21.625: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 29 02:12:22.857: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 02:12:22.857: INFO: Found 0 / 1
Jul 29 02:12:23.631: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 02:12:23.631: INFO: Found 1 / 1
Jul 29 02:12:23.631: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 29 02:12:23.638: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 02:12:23.638: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 29 02:12:23.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 describe pod redis-master-ncbxm --namespace=kubectl-762'
Jul 29 02:12:23.825: INFO: stderr: ""
Jul 29 02:12:23.825: INFO: stdout: "Name:           redis-master-ncbxm\nNamespace:      kubectl-762\nPriority:       0\nNode:           conformance1/10.10.1.89\nStart Time:     Mon, 29 Jul 2019 02:12:21 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.244.0.53\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://7f59ae68954bbfcc837daafc6e3d13820d66435bf4ab477ec13c2e66eb81465d\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 29 Jul 2019 02:12:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zbgtp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-zbgtp:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-zbgtp\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                   Message\n  ----    ------     ----  ----                   -------\n  Normal  Scheduled  2s    default-scheduler      Successfully assigned kubectl-762/redis-master-ncbxm to conformance1\n  Normal  Pulled     1s    kubelet, conformance1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, conformance1  Created container redis-master\n  Normal  Started    0s    kubelet, conformance1  Started container redis-master\n"
Jul 29 02:12:23.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 describe rc redis-master --namespace=kubectl-762'
Jul 29 02:12:24.178: INFO: stderr: ""
Jul 29 02:12:24.178: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-762\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-ncbxm\n"
Jul 29 02:12:24.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 describe service redis-master --namespace=kubectl-762'
Jul 29 02:12:24.351: INFO: stderr: ""
Jul 29 02:12:24.351: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-762\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.10.69.15\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.0.53:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 29 02:12:24.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 describe node conformance0'
Jul 29 02:12:24.552: INFO: stderr: ""
Jul 29 02:12:24.552: INFO: stdout: "Name:               conformance0\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=conformance0\n                    kubernetes.io/os=linux\n                    nirmata.io/cluster.name=conformance\n                    nirmata.io/cluster.role=worker\n                    node-role.kubernetes.io/worker=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"2e:c2:44:63:72:49\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.10.1.213\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 29 Jul 2019 01:33:57 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 29 Jul 2019 02:12:12 +0000   Mon, 29 Jul 2019 01:33:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 29 Jul 2019 02:12:12 +0000   Mon, 29 Jul 2019 01:33:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 29 Jul 2019 02:12:12 +0000   Mon, 29 Jul 2019 01:33:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 29 Jul 2019 02:12:12 +0000   Mon, 29 Jul 2019 01:34:08 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.1.213\n  Hostname:    conformance0\nCapacity:\n cpu:                2\n ephemeral-storage:  31166436Ki\n hugepages-2Mi:      0\n memory:             8149868Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  28722987371\n hugepages-2Mi:      0\n memory:             8047468Ki\n pods:               110\nSystem Info:\n Machine ID:                 cf9039846e817bf110c3933d5c3e0c56\n System UUID:                EB0EE6DC-ED5B-CF59-01CC-0B8140179331\n Boot ID:                    f12f5c7c-83bb-4358-963d-42a51333dd53\n Kernel Version:             4.4.0-131-generic\n OS Image:                   Debian GNU/Linux 9 (stretch)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.6\n Kubelet Version:            v1.15.1\n Kube-Proxy Version:         v1.15.1\nPodCIDR:                     10.244.1.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  heptio-sonobuoy            sonobuoy-e2e-job-feb2cf4d6c56458c                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-5e30946e92f64e9d-v422w    0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  ingress-haproxy            haproxy-ingress-98858d4d6-fzcg2                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\n  ingress-haproxy            ingress-default-backend-6bd67fff4c-vzlm8                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\n  kube-system                kube-flannel-ds-q9cx6                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\n  nirmata                    nirmata-cni-installer-gstxd                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\nEvents:\n  Type    Reason                   Age   From                   Message\n  ----    ------                   ----  ----                   -------\n  Normal  Starting                 38m   kubelet, conformance0  Starting kubelet.\n  Normal  NodeHasSufficientMemory  38m   kubelet, conformance0  Node conformance0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    38m   kubelet, conformance0  Node conformance0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     38m   kubelet, conformance0  Node conformance0 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  38m   kubelet, conformance0  Updated Node Allocatable limit across pods\n  Normal  NodeReady                38m   kubelet, conformance0  Node conformance0 status is now: NodeReady\n"
Jul 29 02:12:24.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 describe namespace kubectl-762'
Jul 29 02:12:24.727: INFO: stderr: ""
Jul 29 02:12:24.727: INFO: stdout: "Name:         kubectl-762\nLabels:       e2e-framework=kubectl\n              e2e-run=80461da4-1194-457e-b778-7e12a55048d7\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:12:24.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-762" for this suite.
Jul 29 02:12:46.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:12:46.895: INFO: namespace kubectl-762 deletion completed in 22.161718092s

• [SLOW TEST:26.578 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:12:46.895: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:12:46.979: INFO: (0) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.680332ms)
Jul 29 02:12:46.985: INFO: (1) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.368646ms)
Jul 29 02:12:46.991: INFO: (2) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.160202ms)
Jul 29 02:12:46.999: INFO: (3) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.900212ms)
Jul 29 02:12:47.006: INFO: (4) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.057801ms)
Jul 29 02:12:47.013: INFO: (5) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.746738ms)
Jul 29 02:12:47.019: INFO: (6) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.768557ms)
Jul 29 02:12:47.026: INFO: (7) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.317446ms)
Jul 29 02:12:47.033: INFO: (8) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.99997ms)
Jul 29 02:12:47.040: INFO: (9) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.059234ms)
Jul 29 02:12:47.046: INFO: (10) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.021918ms)
Jul 29 02:12:47.054: INFO: (11) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.031763ms)
Jul 29 02:12:47.061: INFO: (12) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.033155ms)
Jul 29 02:12:47.067: INFO: (13) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.862955ms)
Jul 29 02:12:47.073: INFO: (14) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.727979ms)
Jul 29 02:12:47.080: INFO: (15) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.024472ms)
Jul 29 02:12:47.088: INFO: (16) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.247502ms)
Jul 29 02:12:47.095: INFO: (17) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.497703ms)
Jul 29 02:12:47.102: INFO: (18) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.651285ms)
Jul 29 02:12:47.110: INFO: (19) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.440067ms)
[AfterEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:12:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8429" for this suite.
Jul 29 02:12:53.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:12:53.265: INFO: namespace proxy-8429 deletion completed in 6.14953201s

• [SLOW TEST:6.370 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:12:53.268: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:12:53.391: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43d3a58c-e3be-46e7-a706-29a925f36fac" in namespace "projected-6557" to be "success or failure"
Jul 29 02:12:53.399: INFO: Pod "downwardapi-volume-43d3a58c-e3be-46e7-a706-29a925f36fac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.951019ms
Jul 29 02:12:55.404: INFO: Pod "downwardapi-volume-43d3a58c-e3be-46e7-a706-29a925f36fac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013226096s
Jul 29 02:12:57.410: INFO: Pod "downwardapi-volume-43d3a58c-e3be-46e7-a706-29a925f36fac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018351958s
STEP: Saw pod success
Jul 29 02:12:57.410: INFO: Pod "downwardapi-volume-43d3a58c-e3be-46e7-a706-29a925f36fac" satisfied condition "success or failure"
Jul 29 02:12:57.413: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-43d3a58c-e3be-46e7-a706-29a925f36fac container client-container: <nil>
STEP: delete the pod
Jul 29 02:12:57.447: INFO: Waiting for pod downwardapi-volume-43d3a58c-e3be-46e7-a706-29a925f36fac to disappear
Jul 29 02:12:57.468: INFO: Pod downwardapi-volume-43d3a58c-e3be-46e7-a706-29a925f36fac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:12:57.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6557" for this suite.
Jul 29 02:13:03.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:13:03.641: INFO: namespace projected-6557 deletion completed in 6.167595735s

• [SLOW TEST:10.373 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:13:03.648: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:13:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5921" for this suite.
Jul 29 02:13:15.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:13:16.289: INFO: namespace namespaces-5921 deletion completed in 6.450677199s
STEP: Destroying namespace "nsdeletetest-5345" for this suite.
Jul 29 02:13:16.293: INFO: Namespace nsdeletetest-5345 was already deleted
STEP: Destroying namespace "nsdeletetest-9698" for this suite.
Jul 29 02:13:22.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:13:22.457: INFO: namespace nsdeletetest-9698 deletion completed in 6.164342419s

• [SLOW TEST:18.810 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:13:22.458: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 29 02:13:22.527: INFO: Waiting up to 5m0s for pod "pod-3bb76368-a787-41bf-a74e-4d47e26fc696" in namespace "emptydir-2762" to be "success or failure"
Jul 29 02:13:22.548: INFO: Pod "pod-3bb76368-a787-41bf-a74e-4d47e26fc696": Phase="Pending", Reason="", readiness=false. Elapsed: 19.942667ms
Jul 29 02:13:24.553: INFO: Pod "pod-3bb76368-a787-41bf-a74e-4d47e26fc696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024866153s
Jul 29 02:13:26.571: INFO: Pod "pod-3bb76368-a787-41bf-a74e-4d47e26fc696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04362843s
STEP: Saw pod success
Jul 29 02:13:26.571: INFO: Pod "pod-3bb76368-a787-41bf-a74e-4d47e26fc696" satisfied condition "success or failure"
Jul 29 02:13:26.577: INFO: Trying to get logs from node conformance1 pod pod-3bb76368-a787-41bf-a74e-4d47e26fc696 container test-container: <nil>
STEP: delete the pod
Jul 29 02:13:26.640: INFO: Waiting for pod pod-3bb76368-a787-41bf-a74e-4d47e26fc696 to disappear
Jul 29 02:13:26.651: INFO: Pod pod-3bb76368-a787-41bf-a74e-4d47e26fc696 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:13:26.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2762" for this suite.
Jul 29 02:13:32.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:13:32.851: INFO: namespace emptydir-2762 deletion completed in 6.192010554s

• [SLOW TEST:10.393 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:13:32.856: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-cf4q
STEP: Creating a pod to test atomic-volume-subpath
Jul 29 02:13:32.944: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-cf4q" in namespace "subpath-4054" to be "success or failure"
Jul 29 02:13:33.272: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Pending", Reason="", readiness=false. Elapsed: 327.660637ms
Jul 29 02:13:35.277: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.332593163s
Jul 29 02:13:37.282: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 4.33770838s
Jul 29 02:13:39.287: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 6.343083227s
Jul 29 02:13:41.293: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 8.348663944s
Jul 29 02:13:43.298: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 10.353262308s
Jul 29 02:13:45.307: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 12.362857538s
Jul 29 02:13:47.314: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 14.369270895s
Jul 29 02:13:49.319: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 16.374654146s
Jul 29 02:13:51.325: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 18.380493113s
Jul 29 02:13:53.329: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 20.384936041s
Jul 29 02:13:55.335: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Running", Reason="", readiness=true. Elapsed: 22.391031518s
Jul 29 02:13:57.341: INFO: Pod "pod-subpath-test-downwardapi-cf4q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.396367447s
STEP: Saw pod success
Jul 29 02:13:57.341: INFO: Pod "pod-subpath-test-downwardapi-cf4q" satisfied condition "success or failure"
Jul 29 02:13:57.345: INFO: Trying to get logs from node conformance1 pod pod-subpath-test-downwardapi-cf4q container test-container-subpath-downwardapi-cf4q: <nil>
STEP: delete the pod
Jul 29 02:13:57.391: INFO: Waiting for pod pod-subpath-test-downwardapi-cf4q to disappear
Jul 29 02:13:57.398: INFO: Pod pod-subpath-test-downwardapi-cf4q no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-cf4q
Jul 29 02:13:57.398: INFO: Deleting pod "pod-subpath-test-downwardapi-cf4q" in namespace "subpath-4054"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:13:57.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4054" for this suite.
Jul 29 02:14:03.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:14:03.571: INFO: namespace subpath-4054 deletion completed in 6.163106129s

• [SLOW TEST:30.716 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:14:03.578: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6401.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6401.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6401.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6401.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6401.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6401.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 29 02:14:09.711: INFO: DNS probes using dns-6401/dns-test-53664934-f16f-4641-b8b2-51328f23afca succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:14:09.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6401" for this suite.
Jul 29 02:14:15.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:14:15.913: INFO: namespace dns-6401 deletion completed in 6.179327417s

• [SLOW TEST:12.335 seconds]
[sig-network] DNS
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:14:15.916: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:14:15.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 version'
Jul 29 02:14:16.094: INFO: stderr: ""
Jul 29 02:14:16.094: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.1\", GitCommit:\"4485c6f18cee9a5d3c3b4e523bd27972b1b53892\", GitTreeState:\"clean\", BuildDate:\"2019-07-18T09:18:22Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.1\", GitCommit:\"4485c6f18cee9a5d3c3b4e523bd27972b1b53892\", GitTreeState:\"clean\", BuildDate:\"2019-07-18T09:09:21Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:14:16.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6291" for this suite.
Jul 29 02:14:22.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:14:22.246: INFO: namespace kubectl-6291 deletion completed in 6.145903154s

• [SLOW TEST:6.330 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:14:22.248: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:14:22.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b73eb562-7b01-4143-b60e-11c9185b0d6d" in namespace "downward-api-3362" to be "success or failure"
Jul 29 02:14:22.352: INFO: Pod "downwardapi-volume-b73eb562-7b01-4143-b60e-11c9185b0d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.441539ms
Jul 29 02:14:24.357: INFO: Pod "downwardapi-volume-b73eb562-7b01-4143-b60e-11c9185b0d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015759051s
Jul 29 02:14:26.363: INFO: Pod "downwardapi-volume-b73eb562-7b01-4143-b60e-11c9185b0d6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021293044s
STEP: Saw pod success
Jul 29 02:14:26.363: INFO: Pod "downwardapi-volume-b73eb562-7b01-4143-b60e-11c9185b0d6d" satisfied condition "success or failure"
Jul 29 02:14:26.367: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-b73eb562-7b01-4143-b60e-11c9185b0d6d container client-container: <nil>
STEP: delete the pod
Jul 29 02:14:26.407: INFO: Waiting for pod downwardapi-volume-b73eb562-7b01-4143-b60e-11c9185b0d6d to disappear
Jul 29 02:14:26.412: INFO: Pod downwardapi-volume-b73eb562-7b01-4143-b60e-11c9185b0d6d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:14:26.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3362" for this suite.
Jul 29 02:14:32.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:14:32.585: INFO: namespace downward-api-3362 deletion completed in 6.166352526s

• [SLOW TEST:10.337 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:14:32.591: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8944 to expose endpoints map[]
Jul 29 02:14:32.691: INFO: Get endpoints failed (15.21923ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jul 29 02:14:33.696: INFO: successfully validated that service endpoint-test2 in namespace services-8944 exposes endpoints map[] (1.020347973s elapsed)
STEP: Creating pod pod1 in namespace services-8944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8944 to expose endpoints map[pod1:[80]]
Jul 29 02:14:36.769: INFO: successfully validated that service endpoint-test2 in namespace services-8944 exposes endpoints map[pod1:[80]] (3.057712479s elapsed)
STEP: Creating pod pod2 in namespace services-8944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8944 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 29 02:14:39.842: INFO: successfully validated that service endpoint-test2 in namespace services-8944 exposes endpoints map[pod1:[80] pod2:[80]] (3.061392967s elapsed)
STEP: Deleting pod pod1 in namespace services-8944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8944 to expose endpoints map[pod2:[80]]
Jul 29 02:14:39.950: INFO: successfully validated that service endpoint-test2 in namespace services-8944 exposes endpoints map[pod2:[80]] (93.882912ms elapsed)
STEP: Deleting pod pod2 in namespace services-8944
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8944 to expose endpoints map[]
Jul 29 02:14:39.993: INFO: successfully validated that service endpoint-test2 in namespace services-8944 exposes endpoints map[] (20.488667ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:14:40.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8944" for this suite.
Jul 29 02:15:02.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:15:02.271: INFO: namespace services-8944 deletion completed in 22.193997617s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:29.682 seconds]
[sig-network] Services
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:15:02.278: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:15:02.327: INFO: Creating deployment "nginx-deployment"
Jul 29 02:15:02.344: INFO: Waiting for observed generation 1
Jul 29 02:15:04.364: INFO: Waiting for all required pods to come up
Jul 29 02:15:04.371: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 29 02:15:10.418: INFO: Waiting for deployment "nginx-deployment" to complete
Jul 29 02:15:10.424: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul 29 02:15:10.435: INFO: Updating deployment nginx-deployment
Jul 29 02:15:10.435: INFO: Waiting for observed generation 2
Jul 29 02:15:12.468: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 29 02:15:12.472: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 29 02:15:12.478: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 29 02:15:12.491: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 29 02:15:12.491: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 29 02:15:12.494: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 29 02:15:12.504: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul 29 02:15:12.504: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul 29 02:15:12.517: INFO: Updating deployment nginx-deployment
Jul 29 02:15:12.517: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul 29 02:15:12.605: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 29 02:15:14.732: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 29 02:15:15.040: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-2815,SelfLink:/apis/apps/v1/namespaces/deployment-2815/deployments/nginx-deployment,UID:65b662ce-4ac3-4a78-8f3f-a9096918d311,ResourceVersion:7274,Generation:3,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-07-29 02:15:12 +0000 UTC 2019-07-29 02:15:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-29 02:15:13 +0000 UTC 2019-07-29 02:15:02 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul 29 02:15:15.085: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-2815,SelfLink:/apis/apps/v1/namespaces/deployment-2815/replicasets/nginx-deployment-55fb7cb77f,UID:87f65eb6-bbce-48c9-af37-49c69063f6f3,ResourceVersion:7271,Generation:3,CreationTimestamp:2019-07-29 02:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 65b662ce-4ac3-4a78-8f3f-a9096918d311 0xc0031ce9d7 0xc0031ce9d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 29 02:15:15.085: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul 29 02:15:15.085: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-2815,SelfLink:/apis/apps/v1/namespaces/deployment-2815/replicasets/nginx-deployment-7b8c6f4498,UID:ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2,ResourceVersion:7255,Generation:3,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 65b662ce-4ac3-4a78-8f3f-a9096918d311 0xc0031ceaa7 0xc0031ceaa8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul 29 02:15:15.227: INFO: Pod "nginx-deployment-55fb7cb77f-4ffkp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4ffkp,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-4ffkp,UID:e293c31a-8ca4-4106-b3a3-c585ce676fe4,ResourceVersion:7281,Generation:0,CreationTimestamp:2019-07-29 02:15:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc002751240 0xc002751241}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027512c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027512e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.227: INFO: Pod "nginx-deployment-55fb7cb77f-4q8wg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4q8wg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-4q8wg,UID:8de7d53d-29f8-44f7-a2b7-8ecfe95c8de2,ResourceVersion:7192,Generation:0,CreationTimestamp:2019-07-29 02:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc0027513b0 0xc0027513b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:11 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:,StartTime:2019-07-29 02:15:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.227: INFO: Pod "nginx-deployment-55fb7cb77f-4rn2v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4rn2v,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-4rn2v,UID:0fae817b-9ae4-4a74-af86-ca1c0ae64939,ResourceVersion:7169,Generation:0,CreationTimestamp:2019-07-29 02:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc002751530 0xc002751531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027515b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027515d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:,StartTime:2019-07-29 02:15:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.228: INFO: Pod "nginx-deployment-55fb7cb77f-54dsr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-54dsr,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-54dsr,UID:da335115-3030-4b7a-bda4-85ebe2accffc,ResourceVersion:7299,Generation:0,CreationTimestamp:2019-07-29 02:15:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc0027516a0 0xc0027516a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.228: INFO: Pod "nginx-deployment-55fb7cb77f-59pdq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-59pdq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-59pdq,UID:4d8d6406-2fc3-4165-8f9b-b35f056fcf21,ResourceVersion:7248,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc002751830 0xc002751831}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027518b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027518d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.228: INFO: Pod "nginx-deployment-55fb7cb77f-8chqb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8chqb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-8chqb,UID:7ca552ce-2fe6-4e1e-a799-63bfb6b21e13,ResourceVersion:7276,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc002751950 0xc002751951}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027519d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027519f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.229: INFO: Pod "nginx-deployment-55fb7cb77f-cmfm8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cmfm8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-cmfm8,UID:49856812-d398-46e9-abcf-525f756e9089,ResourceVersion:7263,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc002751ac0 0xc002751ac1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.229: INFO: Pod "nginx-deployment-55fb7cb77f-gj94x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gj94x,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-gj94x,UID:c1f603ba-474e-4822-90fc-1ba69ca0fdc8,ResourceVersion:7246,Generation:0,CreationTimestamp:2019-07-29 02:15:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc002751c30 0xc002751c31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.230: INFO: Pod "nginx-deployment-55fb7cb77f-hnlxx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hnlxx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-hnlxx,UID:58eea140-a3b0-44ff-a179-6524ee9383d9,ResourceVersion:7165,Generation:0,CreationTimestamp:2019-07-29 02:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc002751d60 0xc002751d61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.230: INFO: Pod "nginx-deployment-55fb7cb77f-jrgqr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jrgqr,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-jrgqr,UID:c7b9daa4-0e67-4483-9e49-0cd10690ae5f,ResourceVersion:7250,Generation:0,CreationTimestamp:2019-07-29 02:15:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc002751ed0 0xc002751ed1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751f50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751f70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.230: INFO: Pod "nginx-deployment-55fb7cb77f-mjbsb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mjbsb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-mjbsb,UID:786a1560-2385-4006-96c9-e4783b30be7e,ResourceVersion:7152,Generation:0,CreationTimestamp:2019-07-29 02:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc002751ff0 0xc002751ff1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003184110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003184250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.231: INFO: Pod "nginx-deployment-55fb7cb77f-nwqbf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-nwqbf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-nwqbf,UID:03bae96f-149c-4fff-a853-5072d034b2a6,ResourceVersion:7267,Generation:0,CreationTimestamp:2019-07-29 02:15:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc0031844b0 0xc0031844b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003184680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031846a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.231: INFO: Pod "nginx-deployment-55fb7cb77f-rncjs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rncjs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-55fb7cb77f-rncjs,UID:72fbcfad-49dd-4151-bcfa-dfe6becfea7c,ResourceVersion:7186,Generation:0,CreationTimestamp:2019-07-29 02:15:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 87f65eb6-bbce-48c9-af37-49c69063f6f3 0xc0031848b0 0xc0031848b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003184930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003184950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:,StartTime:2019-07-29 02:15:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.232: INFO: Pod "nginx-deployment-7b8c6f4498-4rw7f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4rw7f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-4rw7f,UID:5b7051c4-0fad-49c2-8b0a-7b6dfa36dc23,ResourceVersion:7273,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003184a20 0xc003184a21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003184a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003184ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.232: INFO: Pod "nginx-deployment-7b8c6f4498-6546v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6546v,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-6546v,UID:fdd7b0b7-2e6f-4ce3-a989-0a2cbbcacc9f,ResourceVersion:7290,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003184ca0 0xc003184ca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003184d10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003184d30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.232: INFO: Pod "nginx-deployment-7b8c6f4498-7w9xk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7w9xk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-7w9xk,UID:da6ba06e-49de-434a-b98f-72e5bf97f8e0,ResourceVersion:7228,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003184e00 0xc003184e01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003184e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003184e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:,StartTime:2019-07-29 02:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.232: INFO: Pod "nginx-deployment-7b8c6f4498-8q794" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8q794,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-8q794,UID:dbd1553a-49cb-42dd-9cdb-c21ead350f5f,ResourceVersion:7297,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003184f50 0xc003184f51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003184fc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003184fe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.233: INFO: Pod "nginx-deployment-7b8c6f4498-clv7b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-clv7b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-clv7b,UID:4a5cd528-45ec-49d5-8ae7-fa3ca9073dd4,ResourceVersion:7260,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc0031850a0 0xc0031850a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.233: INFO: Pod "nginx-deployment-7b8c6f4498-cq8d2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cq8d2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-cq8d2,UID:7afba212-7911-4932-9443-66277cd93cf9,ResourceVersion:7117,Generation:0,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc0031851f0 0xc0031851f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:10.244.1.81,StartTime:2019-07-29 02:15:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-29 02:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://687af8314d11fec6c645d631c14d7f2c6c5ee29891df2b931edb6565f1606598}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.233: INFO: Pod "nginx-deployment-7b8c6f4498-f8dh7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-f8dh7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-f8dh7,UID:80bd1e66-24e3-40a7-8ce0-2865939d8594,ResourceVersion:7114,Generation:0,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003185350 0xc003185351}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031853c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031853e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:10.244.1.79,StartTime:2019-07-29 02:15:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-29 02:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://296c0f9a05ff6d22bddb0b21d062d679d4f67644ebc13b21eaa43e7c9625c97f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.233: INFO: Pod "nginx-deployment-7b8c6f4498-fhfzm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fhfzm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-fhfzm,UID:6fe4ed3a-60d3-4ad3-9258-7eafcee530ac,ResourceVersion:7209,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc0031854b0 0xc0031854b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:,StartTime:2019-07-29 02:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.234: INFO: Pod "nginx-deployment-7b8c6f4498-j7vjg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-j7vjg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-j7vjg,UID:9c940202-bf86-41c7-9e3d-f69ad123607c,ResourceVersion:7251,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003185600 0xc003185601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.234: INFO: Pod "nginx-deployment-7b8c6f4498-j99l8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-j99l8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-j99l8,UID:c15abfdb-2336-42c8-b0e8-a84923eca1ab,ResourceVersion:7138,Generation:0,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003185710 0xc003185711}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031857a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:10.244.0.61,StartTime:2019-07-29 02:15:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-29 02:15:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://190fb51826e4216d3682673c29bcddd4c5a94589162857a0f2dbf61c14e6b556}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.234: INFO: Pod "nginx-deployment-7b8c6f4498-jtvpc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jtvpc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-jtvpc,UID:72512661-29b7-4387-99c4-ee990ad9cf23,ResourceVersion:7129,Generation:0,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003185870 0xc003185871}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031858e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:10.244.0.59,StartTime:2019-07-29 02:15:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-29 02:15:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://8f5c4f976c71f7268881daa198d1d7862572be206854d8f539e1d80a754f20ab}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.234: INFO: Pod "nginx-deployment-7b8c6f4498-ldqrq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ldqrq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-ldqrq,UID:ab3995ae-ef6c-4e7d-9b2e-7013646c0e76,ResourceVersion:7126,Generation:0,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc0031859d0 0xc0031859d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:10.244.0.60,StartTime:2019-07-29 02:15:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-29 02:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://c88bc92163370860a11f9809811e697653c9f7ca23ad010d19d08720ab41d586}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.235: INFO: Pod "nginx-deployment-7b8c6f4498-mksn4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mksn4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-mksn4,UID:047c2bc3-4789-4c00-b910-fadfe1f35d8f,ResourceVersion:7287,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003185b30 0xc003185b31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185bc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.235: INFO: Pod "nginx-deployment-7b8c6f4498-n972b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-n972b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-n972b,UID:8ad1a8e2-5351-460f-98fd-30a407a3a0da,ResourceVersion:7295,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003185c80 0xc003185c81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.249: INFO: Pod "nginx-deployment-7b8c6f4498-ns8bh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ns8bh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-ns8bh,UID:c7ca5e8f-a357-4501-aabe-73cf3a9b945d,ResourceVersion:7119,Generation:0,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003185dd0 0xc003185dd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185e40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185e60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:10.244.1.82,StartTime:2019-07-29 02:15:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-29 02:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://c3b088bf4beee7019dc906be34acc6c7326cd98c39eb8379bcce05bbe49d1ba2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.250: INFO: Pod "nginx-deployment-7b8c6f4498-pwjtz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pwjtz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-pwjtz,UID:71d46154-e451-495f-bbed-021af9e7a1f4,ResourceVersion:7270,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc003185f30 0xc003185f31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003185fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003185fe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:,StartTime:2019-07-29 02:15:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.250: INFO: Pod "nginx-deployment-7b8c6f4498-sfbmf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sfbmf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-sfbmf,UID:6ffac626-ddb2-4248-994a-b4509084a7e2,ResourceVersion:7232,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc002cf60a0 0xc002cf60a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002cf6110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002cf6130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:12 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:,StartTime:2019-07-29 02:15:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.250: INFO: Pod "nginx-deployment-7b8c6f4498-t2p55" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t2p55,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-t2p55,UID:3d38db3b-ae34-4f67-b2ec-34607f171c94,ResourceVersion:7123,Generation:0,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc002cf61f0 0xc002cf61f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002cf6260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002cf6280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:10.244.1.80,StartTime:2019-07-29 02:15:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-29 02:15:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://2c748d86ce821f268b231993ccfde4d5f93d980e3919f6b563376aac32c52c82}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.250: INFO: Pod "nginx-deployment-7b8c6f4498-zbx7t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zbx7t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-zbx7t,UID:5f45f564-9424-4893-b862-d22ed8cb2bd6,ResourceVersion:7106,Generation:0,CreationTimestamp:2019-07-29 02:15:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc002cf6350 0xc002cf6351}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002cf63d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002cf63f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:02 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:10.244.1.78,StartTime:2019-07-29 02:15:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-29 02:15:06 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://7fa26004cd949ed8e099c48a59fb6d35ff614c88364e93e8361add03fb763f1a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 29 02:15:15.251: INFO: Pod "nginx-deployment-7b8c6f4498-znz6m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-znz6m,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2815,SelfLink:/api/v1/namespaces/deployment-2815/pods/nginx-deployment-7b8c6f4498-znz6m,UID:a463ea4f-89f9-4c40-9c7e-3b5fceab6cee,ResourceVersion:7253,Generation:0,CreationTimestamp:2019-07-29 02:15:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 ff5f7203-ccd9-4ec5-918e-8efdf2b32fc2 0xc002cf64c0 0xc002cf64c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9fl8d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9fl8d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9fl8d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002cf6530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002cf6550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:15:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:15:15.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2815" for this suite.
Jul 29 02:15:25.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:15:26.008: INFO: namespace deployment-2815 deletion completed in 10.718259863s

• [SLOW TEST:23.730 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:15:26.010: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0729 02:15:36.890151      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 29 02:15:36.890: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:15:36.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4458" for this suite.
Jul 29 02:15:44.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:15:45.087: INFO: namespace gc-4458 deletion completed in 8.193226338s

• [SLOW TEST:19.078 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:15:45.089: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:15:49.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9704" for this suite.
Jul 29 02:15:55.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:15:55.590: INFO: namespace kubelet-test-9704 deletion completed in 6.168526578s

• [SLOW TEST:10.502 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:15:55.596: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-67e2c224-0dda-4f85-9d66-3e001b6ef13f in namespace container-probe-3982
Jul 29 02:15:59.678: INFO: Started pod liveness-67e2c224-0dda-4f85-9d66-3e001b6ef13f in namespace container-probe-3982
STEP: checking the pod's current state and verifying that restartCount is present
Jul 29 02:15:59.682: INFO: Initial restart count of pod liveness-67e2c224-0dda-4f85-9d66-3e001b6ef13f is 0
Jul 29 02:16:19.741: INFO: Restart count of pod container-probe-3982/liveness-67e2c224-0dda-4f85-9d66-3e001b6ef13f is now 1 (20.05901602s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:16:19.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3982" for this suite.
Jul 29 02:16:25.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:16:25.929: INFO: namespace container-probe-3982 deletion completed in 6.156079934s

• [SLOW TEST:30.334 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:16:25.938: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 29 02:16:26.056: INFO: Number of nodes with available pods: 0
Jul 29 02:16:26.056: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:16:27.070: INFO: Number of nodes with available pods: 0
Jul 29 02:16:27.070: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:16:28.069: INFO: Number of nodes with available pods: 0
Jul 29 02:16:28.069: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:16:29.076: INFO: Number of nodes with available pods: 1
Jul 29 02:16:29.076: INFO: Node conformance1 is running more than one daemon pod
Jul 29 02:16:30.068: INFO: Number of nodes with available pods: 2
Jul 29 02:16:30.069: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 29 02:16:30.105: INFO: Number of nodes with available pods: 1
Jul 29 02:16:30.105: INFO: Node conformance1 is running more than one daemon pod
Jul 29 02:16:31.116: INFO: Number of nodes with available pods: 1
Jul 29 02:16:31.116: INFO: Node conformance1 is running more than one daemon pod
Jul 29 02:16:32.114: INFO: Number of nodes with available pods: 1
Jul 29 02:16:32.115: INFO: Node conformance1 is running more than one daemon pod
Jul 29 02:16:33.115: INFO: Number of nodes with available pods: 1
Jul 29 02:16:33.115: INFO: Node conformance1 is running more than one daemon pod
Jul 29 02:16:34.115: INFO: Number of nodes with available pods: 2
Jul 29 02:16:34.115: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9584, will wait for the garbage collector to delete the pods
Jul 29 02:16:34.186: INFO: Deleting DaemonSet.extensions daemon-set took: 11.213377ms
Jul 29 02:16:34.486: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.390956ms
Jul 29 02:16:47.493: INFO: Number of nodes with available pods: 0
Jul 29 02:16:47.493: INFO: Number of running nodes: 0, number of available pods: 0
Jul 29 02:16:47.496: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9584/daemonsets","resourceVersion":"7829"},"items":null}

Jul 29 02:16:47.500: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9584/pods","resourceVersion":"7829"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:16:47.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9584" for this suite.
Jul 29 02:16:53.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:16:53.685: INFO: namespace daemonsets-9584 deletion completed in 6.166287786s

• [SLOW TEST:27.747 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:16:53.693: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4059
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 29 02:16:53.755: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 29 02:17:13.894: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.101 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4059 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:17:13.894: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:17:15.244: INFO: Found all expected endpoints: [netserver-0]
Jul 29 02:17:15.250: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.82 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4059 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:17:15.250: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:17:16.654: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:17:16.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4059" for this suite.
Jul 29 02:17:40.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:17:40.865: INFO: namespace pod-network-test-4059 deletion completed in 24.205437002s

• [SLOW TEST:47.172 seconds]
[sig-network] Networking
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:17:40.871: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:17:40.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fbb0371-37fe-4a46-b96d-000b86b392a9" in namespace "downward-api-1062" to be "success or failure"
Jul 29 02:17:40.965: INFO: Pod "downwardapi-volume-2fbb0371-37fe-4a46-b96d-000b86b392a9": Phase="Pending", Reason="", readiness=false. Elapsed: 25.537778ms
Jul 29 02:17:42.981: INFO: Pod "downwardapi-volume-2fbb0371-37fe-4a46-b96d-000b86b392a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041814764s
Jul 29 02:17:44.986: INFO: Pod "downwardapi-volume-2fbb0371-37fe-4a46-b96d-000b86b392a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046925612s
STEP: Saw pod success
Jul 29 02:17:44.987: INFO: Pod "downwardapi-volume-2fbb0371-37fe-4a46-b96d-000b86b392a9" satisfied condition "success or failure"
Jul 29 02:17:44.992: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-2fbb0371-37fe-4a46-b96d-000b86b392a9 container client-container: <nil>
STEP: delete the pod
Jul 29 02:17:45.021: INFO: Waiting for pod downwardapi-volume-2fbb0371-37fe-4a46-b96d-000b86b392a9 to disappear
Jul 29 02:17:45.040: INFO: Pod downwardapi-volume-2fbb0371-37fe-4a46-b96d-000b86b392a9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:17:45.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1062" for this suite.
Jul 29 02:17:51.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:17:51.217: INFO: namespace downward-api-1062 deletion completed in 6.17134955s

• [SLOW TEST:10.347 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:17:51.219: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 29 02:17:57.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec pod-sharedvolume-31d71a72-0cfe-4218-bac4-804fd344ba55 -c busybox-main-container --namespace=emptydir-3878 -- cat /usr/share/volumeshare/shareddata.txt'
Jul 29 02:17:57.823: INFO: stderr: ""
Jul 29 02:17:57.823: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:17:57.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3878" for this suite.
Jul 29 02:18:03.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:18:03.988: INFO: namespace emptydir-3878 deletion completed in 6.159634507s

• [SLOW TEST:12.770 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:18:03.989: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:18:26.105: INFO: Container started at 2019-07-29 02:18:06 +0000 UTC, pod became ready at 2019-07-29 02:18:24 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:18:26.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9996" for this suite.
Jul 29 02:18:40.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:18:40.284: INFO: namespace container-probe-9996 deletion completed in 14.171247869s

• [SLOW TEST:36.295 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:18:40.286: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jul 29 02:18:40.360: INFO: Waiting up to 5m0s for pod "client-containers-95355426-1ff5-41fb-8de1-f3d6d3a06ede" in namespace "containers-7100" to be "success or failure"
Jul 29 02:18:40.378: INFO: Pod "client-containers-95355426-1ff5-41fb-8de1-f3d6d3a06ede": Phase="Pending", Reason="", readiness=false. Elapsed: 17.330362ms
Jul 29 02:18:42.395: INFO: Pod "client-containers-95355426-1ff5-41fb-8de1-f3d6d3a06ede": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034972694s
Jul 29 02:18:44.401: INFO: Pod "client-containers-95355426-1ff5-41fb-8de1-f3d6d3a06ede": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040377191s
STEP: Saw pod success
Jul 29 02:18:44.401: INFO: Pod "client-containers-95355426-1ff5-41fb-8de1-f3d6d3a06ede" satisfied condition "success or failure"
Jul 29 02:18:44.406: INFO: Trying to get logs from node conformance1 pod client-containers-95355426-1ff5-41fb-8de1-f3d6d3a06ede container test-container: <nil>
STEP: delete the pod
Jul 29 02:18:44.458: INFO: Waiting for pod client-containers-95355426-1ff5-41fb-8de1-f3d6d3a06ede to disappear
Jul 29 02:18:44.478: INFO: Pod client-containers-95355426-1ff5-41fb-8de1-f3d6d3a06ede no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:18:44.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7100" for this suite.
Jul 29 02:18:50.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:18:50.682: INFO: namespace containers-7100 deletion completed in 6.199405635s

• [SLOW TEST:10.396 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:18:50.682: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:18:50.747: INFO: Waiting up to 5m0s for pod "downwardapi-volume-152a6d0f-3e5d-4cb5-bac3-a7d51a9a3812" in namespace "projected-6729" to be "success or failure"
Jul 29 02:18:50.756: INFO: Pod "downwardapi-volume-152a6d0f-3e5d-4cb5-bac3-a7d51a9a3812": Phase="Pending", Reason="", readiness=false. Elapsed: 9.218705ms
Jul 29 02:18:52.762: INFO: Pod "downwardapi-volume-152a6d0f-3e5d-4cb5-bac3-a7d51a9a3812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014532697s
Jul 29 02:18:54.767: INFO: Pod "downwardapi-volume-152a6d0f-3e5d-4cb5-bac3-a7d51a9a3812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019486013s
STEP: Saw pod success
Jul 29 02:18:54.767: INFO: Pod "downwardapi-volume-152a6d0f-3e5d-4cb5-bac3-a7d51a9a3812" satisfied condition "success or failure"
Jul 29 02:18:54.771: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-152a6d0f-3e5d-4cb5-bac3-a7d51a9a3812 container client-container: <nil>
STEP: delete the pod
Jul 29 02:18:54.804: INFO: Waiting for pod downwardapi-volume-152a6d0f-3e5d-4cb5-bac3-a7d51a9a3812 to disappear
Jul 29 02:18:54.820: INFO: Pod downwardapi-volume-152a6d0f-3e5d-4cb5-bac3-a7d51a9a3812 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:18:54.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6729" for this suite.
Jul 29 02:19:00.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:19:01.001: INFO: namespace projected-6729 deletion completed in 6.174723783s

• [SLOW TEST:10.319 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:19:01.003: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9868
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-9868
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9868
Jul 29 02:19:01.107: INFO: Found 0 stateful pods, waiting for 1
Jul 29 02:19:11.114: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 29 02:19:11.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:19:11.682: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:19:11.682: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:19:11.682: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 29 02:19:11.692: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 29 02:19:21.697: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 02:19:21.697: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 02:19:21.721: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:21.721: INFO: ss-0  conformance1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:01 +0000 UTC  }]
Jul 29 02:19:21.721: INFO: 
Jul 29 02:19:21.721: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 29 02:19:22.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991403898s
Jul 29 02:19:23.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985187462s
Jul 29 02:19:24.739: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979892565s
Jul 29 02:19:25.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974077776s
Jul 29 02:19:26.749: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968722703s
Jul 29 02:19:27.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963457617s
Jul 29 02:19:28.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957903092s
Jul 29 02:19:29.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952665295s
Jul 29 02:19:30.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.335512ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9868
Jul 29 02:19:31.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:19:32.270: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 29 02:19:32.270: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 29 02:19:32.270: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 29 02:19:32.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:19:32.812: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 29 02:19:32.812: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 29 02:19:32.812: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 29 02:19:32.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:19:33.289: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 29 02:19:33.289: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 29 02:19:33.289: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 29 02:19:33.294: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 02:19:33.294: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 02:19:33.294: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 29 02:19:33.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:19:33.799: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:19:33.799: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:19:33.799: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 29 02:19:33.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:19:34.323: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:19:34.323: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:19:34.323: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 29 02:19:34.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:19:34.865: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:19:34.865: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:19:34.865: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 29 02:19:34.865: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 02:19:34.870: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 29 02:19:44.879: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 02:19:44.879: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 02:19:44.880: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 02:19:44.903: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:44.903: INFO: ss-0  conformance1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:01 +0000 UTC  }]
Jul 29 02:19:44.903: INFO: ss-1  conformance0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:44.903: INFO: ss-2  conformance0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:44.903: INFO: 
Jul 29 02:19:44.903: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 29 02:19:45.910: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:45.910: INFO: ss-0  conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:01 +0000 UTC  }]
Jul 29 02:19:45.910: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:45.910: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:45.910: INFO: 
Jul 29 02:19:45.910: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 29 02:19:46.934: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:46.934: INFO: ss-1  conformance0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:46.934: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:46.934: INFO: 
Jul 29 02:19:46.934: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 29 02:19:47.945: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:47.945: INFO: ss-1  conformance0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:47.945: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:47.945: INFO: 
Jul 29 02:19:47.945: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 29 02:19:48.950: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:48.950: INFO: ss-1  conformance0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:48.950: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:48.950: INFO: 
Jul 29 02:19:48.950: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 29 02:19:49.955: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:49.955: INFO: ss-1  conformance0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:49.955: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:49.955: INFO: 
Jul 29 02:19:49.955: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 29 02:19:50.962: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:50.962: INFO: ss-1  conformance0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:50.962: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:50.962: INFO: 
Jul 29 02:19:50.962: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 29 02:19:51.967: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:51.967: INFO: ss-1  conformance0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:51.968: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:51.968: INFO: 
Jul 29 02:19:51.968: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 29 02:19:52.974: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:52.974: INFO: ss-1  conformance0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:52.974: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:52.974: INFO: 
Jul 29 02:19:52.975: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 29 02:19:53.980: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 29 02:19:53.980: INFO: ss-1  conformance0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:53.981: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:19:21 +0000 UTC  }]
Jul 29 02:19:53.981: INFO: 
Jul 29 02:19:53.981: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9868
Jul 29 02:19:54.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:19:55.199: INFO: rc: 1
Jul 29 02:19:55.200: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001881020 exit status 1 <nil> <nil> true [0xc003716e40 0xc003716e58 0xc003716e70] [0xc003716e40 0xc003716e58 0xc003716e70] [0xc003716e50 0xc003716e68] [0x9d17b0 0x9d17b0] 0xc0036b6960 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jul 29 02:20:05.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:20:05.378: INFO: rc: 1
Jul 29 02:20:05.378: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304b860 exit status 1 <nil> <nil> true [0xc000011d00 0xc000011db0 0xc000011ea0] [0xc000011d00 0xc000011db0 0xc000011ea0] [0xc000011d98 0xc000011e40] [0x9d17b0 0x9d17b0] 0xc0029de6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:20:15.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:20:15.544: INFO: rc: 1
Jul 29 02:20:15.545: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304bbf0 exit status 1 <nil> <nil> true [0xc000011ee8 0xc00160e080 0xc00160e198] [0xc000011ee8 0xc00160e080 0xc00160e198] [0xc000011fa0 0xc00160e130] [0x9d17b0 0x9d17b0] 0xc0029dea20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:20:25.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:20:25.682: INFO: rc: 1
Jul 29 02:20:25.682: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0018813e0 exit status 1 <nil> <nil> true [0xc003716e78 0xc003716e90 0xc003716ea8] [0xc003716e78 0xc003716e90 0xc003716ea8] [0xc003716e88 0xc003716ea0] [0x9d17b0 0x9d17b0] 0xc0036b6ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:20:35.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:20:35.876: INFO: rc: 1
Jul 29 02:20:35.876: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002a34030 exit status 1 <nil> <nil> true [0xc00160e1a0 0xc00160e2c0 0xc00160e430] [0xc00160e1a0 0xc00160e2c0 0xc00160e430] [0xc00160e1d0 0xc00160e3d8] [0x9d17b0 0x9d17b0] 0xc0029ded80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:20:45.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:20:46.055: INFO: rc: 1
Jul 29 02:20:46.055: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002a34420 exit status 1 <nil> <nil> true [0xc00160e440 0xc00160e4f0 0xc00160e560] [0xc00160e440 0xc00160e4f0 0xc00160e560] [0xc00160e4c0 0xc00160e538] [0x9d17b0 0x9d17b0] 0xc0029df0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:20:56.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:20:56.198: INFO: rc: 1
Jul 29 02:20:56.198: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304a330 exit status 1 <nil> <nil> true [0xc00362a008 0xc00362a040 0xc00362a070] [0xc00362a008 0xc00362a040 0xc00362a070] [0xc00362a038 0xc00362a068] [0x9d17b0 0x9d17b0] 0xc0035d4a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:21:06.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:21:06.342: INFO: rc: 1
Jul 29 02:21:06.342: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003014360 exit status 1 <nil> <nil> true [0xc000010030 0xc000010540 0xc000010958] [0xc000010030 0xc000010540 0xc000010958] [0xc000010330 0xc0000108b0] [0x9d17b0 0x9d17b0] 0xc002af23c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:21:16.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:21:16.490: INFO: rc: 1
Jul 29 02:21:16.490: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003014720 exit status 1 <nil> <nil> true [0xc000010b10 0xc0000110c8 0xc000011248] [0xc000010b10 0xc0000110c8 0xc000011248] [0xc000010fe0 0xc000011158] [0x9d17b0 0x9d17b0] 0xc002af2960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:21:26.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:21:26.701: INFO: rc: 1
Jul 29 02:21:26.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304a6c0 exit status 1 <nil> <nil> true [0xc00362a078 0xc00362a0b0 0xc00362a0e8] [0xc00362a078 0xc00362a0b0 0xc00362a0e8] [0xc00362a0a8 0xc00362a0c8] [0x9d17b0 0x9d17b0] 0xc0035d5020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:21:36.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:21:36.854: INFO: rc: 1
Jul 29 02:21:36.854: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003014ae0 exit status 1 <nil> <nil> true [0xc0000112c0 0xc000011588 0xc000011640] [0xc0000112c0 0xc000011588 0xc000011640] [0xc000011488 0xc000011630] [0x9d17b0 0x9d17b0] 0xc002af2f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:21:46.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:21:47.001: INFO: rc: 1
Jul 29 02:21:47.002: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003014e70 exit status 1 <nil> <nil> true [0xc0000116c8 0xc000011850 0xc000011900] [0xc0000116c8 0xc000011850 0xc000011900] [0xc0000117f0 0xc0000118c0] [0x9d17b0 0x9d17b0] 0xc002af33e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:21:57.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:21:57.151: INFO: rc: 1
Jul 29 02:21:57.152: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304aa80 exit status 1 <nil> <nil> true [0xc00362a0f0 0xc00362a120 0xc00362a150] [0xc00362a0f0 0xc00362a120 0xc00362a150] [0xc00362a110 0xc00362a148] [0x9d17b0 0x9d17b0] 0xc0035d5380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:22:07.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:22:07.288: INFO: rc: 1
Jul 29 02:22:07.288: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304ae10 exit status 1 <nil> <nil> true [0xc00362a178 0xc00362a1c8 0xc00362a200] [0xc00362a178 0xc00362a1c8 0xc00362a200] [0xc00362a1a8 0xc00362a1e8] [0x9d17b0 0x9d17b0] 0xc0035d56e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:22:17.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:22:17.455: INFO: rc: 1
Jul 29 02:22:17.455: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304b1d0 exit status 1 <nil> <nil> true [0xc00362a208 0xc00362a248 0xc00362a2b0] [0xc00362a208 0xc00362a248 0xc00362a2b0] [0xc00362a230 0xc00362a2a8] [0x9d17b0 0x9d17b0] 0xc0035d5a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:22:27.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:22:27.806: INFO: rc: 1
Jul 29 02:22:27.806: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003015230 exit status 1 <nil> <nil> true [0xc000011950 0xc000011a70 0xc000011b30] [0xc000011950 0xc000011a70 0xc000011b30] [0xc0000119c8 0xc000011b10] [0x9d17b0 0x9d17b0] 0xc002af3920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:22:37.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:22:37.966: INFO: rc: 1
Jul 29 02:22:37.966: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304b560 exit status 1 <nil> <nil> true [0xc00362a2b8 0xc00362a2f0 0xc00362a310] [0xc00362a2b8 0xc00362a2f0 0xc00362a310] [0xc00362a2d8 0xc00362a300] [0x9d17b0 0x9d17b0] 0xc0035d5e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:22:47.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:22:48.106: INFO: rc: 1
Jul 29 02:22:48.106: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304b920 exit status 1 <nil> <nil> true [0xc00362a348 0xc00362a390 0xc00362a3d0] [0xc00362a348 0xc00362a390 0xc00362a3d0] [0xc00362a370 0xc00362a3c8] [0x9d17b0 0x9d17b0] 0xc002086180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:22:58.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:22:58.260: INFO: rc: 1
Jul 29 02:22:58.260: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304a360 exit status 1 <nil> <nil> true [0xc00362a008 0xc00362a040 0xc00362a070] [0xc00362a008 0xc00362a040 0xc00362a070] [0xc00362a038 0xc00362a068] [0x9d17b0 0x9d17b0] 0xc0035d4a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:23:08.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:23:08.401: INFO: rc: 1
Jul 29 02:23:08.401: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003014390 exit status 1 <nil> <nil> true [0xc000010030 0xc000010540 0xc000010958] [0xc000010030 0xc000010540 0xc000010958] [0xc000010330 0xc0000108b0] [0x9d17b0 0x9d17b0] 0xc0020862a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:23:18.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:23:18.540: INFO: rc: 1
Jul 29 02:23:18.540: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030147b0 exit status 1 <nil> <nil> true [0xc000010b10 0xc0000110c8 0xc000011248] [0xc000010b10 0xc0000110c8 0xc000011248] [0xc000010fe0 0xc000011158] [0x9d17b0 0x9d17b0] 0xc002086600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:23:28.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:23:28.683: INFO: rc: 1
Jul 29 02:23:28.683: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003014b70 exit status 1 <nil> <nil> true [0xc0000112c0 0xc000011588 0xc000011640] [0xc0000112c0 0xc000011588 0xc000011640] [0xc000011488 0xc000011630] [0x9d17b0 0x9d17b0] 0xc002086960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:23:38.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:23:39.128: INFO: rc: 1
Jul 29 02:23:39.128: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003014f30 exit status 1 <nil> <nil> true [0xc0000116c8 0xc000011850 0xc000011900] [0xc0000116c8 0xc000011850 0xc000011900] [0xc0000117f0 0xc0000118c0] [0x9d17b0 0x9d17b0] 0xc002086d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:23:49.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:23:49.328: INFO: rc: 1
Jul 29 02:23:49.328: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003015350 exit status 1 <nil> <nil> true [0xc000011950 0xc000011a70 0xc000011b30] [0xc000011950 0xc000011a70 0xc000011b30] [0xc0000119c8 0xc000011b10] [0x9d17b0 0x9d17b0] 0xc002087200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:23:59.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:23:59.483: INFO: rc: 1
Jul 29 02:23:59.483: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003015710 exit status 1 <nil> <nil> true [0xc000011b50 0xc000011c50 0xc000011d00] [0xc000011b50 0xc000011c50 0xc000011d00] [0xc000011c00 0xc000011cb0] [0x9d17b0 0x9d17b0] 0xc0020875c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:24:09.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:24:09.630: INFO: rc: 1
Jul 29 02:24:09.630: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304a720 exit status 1 <nil> <nil> true [0xc00362a078 0xc00362a0b0 0xc00362a0e8] [0xc00362a078 0xc00362a0b0 0xc00362a0e8] [0xc00362a0a8 0xc00362a0c8] [0x9d17b0 0x9d17b0] 0xc0035d5020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:24:19.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:24:19.787: INFO: rc: 1
Jul 29 02:24:19.787: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304aae0 exit status 1 <nil> <nil> true [0xc00362a0f0 0xc00362a120 0xc00362a150] [0xc00362a0f0 0xc00362a120 0xc00362a150] [0xc00362a110 0xc00362a148] [0x9d17b0 0x9d17b0] 0xc0035d5380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:24:29.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:24:29.921: INFO: rc: 1
Jul 29 02:24:29.922: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003015aa0 exit status 1 <nil> <nil> true [0xc000011d58 0xc000011e08 0xc000011ee8] [0xc000011d58 0xc000011e08 0xc000011ee8] [0xc000011db0 0xc000011ea0] [0x9d17b0 0x9d17b0] 0xc0020879e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:24:39.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:24:40.071: INFO: rc: 1
Jul 29 02:24:40.071: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003015e30 exit status 1 <nil> <nil> true [0xc000011f58 0xc003716008 0xc003716020] [0xc000011f58 0xc003716008 0xc003716020] [0xc003716000 0xc003716018] [0x9d17b0 0x9d17b0] 0xc002087f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:24:50.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:24:50.212: INFO: rc: 1
Jul 29 02:24:50.212: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00304aed0 exit status 1 <nil> <nil> true [0xc00362a178 0xc00362a1c8 0xc00362a200] [0xc00362a178 0xc00362a1c8 0xc00362a200] [0xc00362a1a8 0xc00362a1e8] [0x9d17b0 0x9d17b0] 0xc0035d56e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 29 02:25:00.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-9868 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:25:00.350: INFO: rc: 1
Jul 29 02:25:00.350: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Jul 29 02:25:00.350: INFO: Scaling statefulset ss to 0
Jul 29 02:25:00.367: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 29 02:25:00.371: INFO: Deleting all statefulset in ns statefulset-9868
Jul 29 02:25:00.376: INFO: Scaling statefulset ss to 0
Jul 29 02:25:00.388: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 02:25:00.392: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:25:00.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9868" for this suite.
Jul 29 02:25:06.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:25:06.614: INFO: namespace statefulset-9868 deletion completed in 6.187523534s

• [SLOW TEST:365.612 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:25:06.617: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 29 02:25:11.216: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f034ea75-2e92-46e2-a49f-5cb4722da323"
Jul 29 02:25:11.216: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f034ea75-2e92-46e2-a49f-5cb4722da323" in namespace "pods-1114" to be "terminated due to deadline exceeded"
Jul 29 02:25:11.221: INFO: Pod "pod-update-activedeadlineseconds-f034ea75-2e92-46e2-a49f-5cb4722da323": Phase="Running", Reason="", readiness=true. Elapsed: 4.660306ms
Jul 29 02:25:13.227: INFO: Pod "pod-update-activedeadlineseconds-f034ea75-2e92-46e2-a49f-5cb4722da323": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.010500208s
Jul 29 02:25:13.227: INFO: Pod "pod-update-activedeadlineseconds-f034ea75-2e92-46e2-a49f-5cb4722da323" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:25:13.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1114" for this suite.
Jul 29 02:25:19.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:25:19.416: INFO: namespace pods-1114 deletion completed in 6.183410974s

• [SLOW TEST:12.799 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:25:19.417: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:25:19.487: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd96742a-6173-4fc3-b2d7-2820fa50ad40" in namespace "downward-api-5660" to be "success or failure"
Jul 29 02:25:19.492: INFO: Pod "downwardapi-volume-cd96742a-6173-4fc3-b2d7-2820fa50ad40": Phase="Pending", Reason="", readiness=false. Elapsed: 5.053953ms
Jul 29 02:25:21.497: INFO: Pod "downwardapi-volume-cd96742a-6173-4fc3-b2d7-2820fa50ad40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010127033s
Jul 29 02:25:23.503: INFO: Pod "downwardapi-volume-cd96742a-6173-4fc3-b2d7-2820fa50ad40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015648348s
STEP: Saw pod success
Jul 29 02:25:23.503: INFO: Pod "downwardapi-volume-cd96742a-6173-4fc3-b2d7-2820fa50ad40" satisfied condition "success or failure"
Jul 29 02:25:23.506: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-cd96742a-6173-4fc3-b2d7-2820fa50ad40 container client-container: <nil>
STEP: delete the pod
Jul 29 02:25:23.538: INFO: Waiting for pod downwardapi-volume-cd96742a-6173-4fc3-b2d7-2820fa50ad40 to disappear
Jul 29 02:25:23.549: INFO: Pod downwardapi-volume-cd96742a-6173-4fc3-b2d7-2820fa50ad40 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:25:23.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5660" for this suite.
Jul 29 02:25:29.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:25:29.732: INFO: namespace downward-api-5660 deletion completed in 6.177174105s

• [SLOW TEST:10.316 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:25:29.733: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-b08b759d-6958-4f1b-8ae6-81ab9c7ae19b
STEP: Creating configMap with name cm-test-opt-upd-f739d362-149e-4617-aad0-2b74a686551e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b08b759d-6958-4f1b-8ae6-81ab9c7ae19b
STEP: Updating configmap cm-test-opt-upd-f739d362-149e-4617-aad0-2b74a686551e
STEP: Creating configMap with name cm-test-opt-create-20ea0e38-528f-4f5c-b604-b54135f83ad5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:25:35.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5961" for this suite.
Jul 29 02:25:59.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:26:00.122: INFO: namespace configmap-5961 deletion completed in 24.174418038s

• [SLOW TEST:30.389 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:26:00.122: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:26:00.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3929" for this suite.
Jul 29 02:26:22.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:26:22.403: INFO: namespace pods-3929 deletion completed in 22.175298432s

• [SLOW TEST:22.282 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:26:22.409: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bdab05d6-f8e2-4558-ad3f-2bdc10426cb1
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-bdab05d6-f8e2-4558-ad3f-2bdc10426cb1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:27:37.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1237" for this suite.
Jul 29 02:27:59.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:27:59.356: INFO: namespace projected-1237 deletion completed in 22.203194113s

• [SLOW TEST:96.947 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:27:59.358: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 29 02:27:59.441: INFO: Waiting up to 5m0s for pod "pod-994e1521-d7eb-4231-96c7-d1f494238fd6" in namespace "emptydir-5781" to be "success or failure"
Jul 29 02:27:59.453: INFO: Pod "pod-994e1521-d7eb-4231-96c7-d1f494238fd6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.431498ms
Jul 29 02:28:01.459: INFO: Pod "pod-994e1521-d7eb-4231-96c7-d1f494238fd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017397205s
Jul 29 02:28:03.465: INFO: Pod "pod-994e1521-d7eb-4231-96c7-d1f494238fd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023538377s
STEP: Saw pod success
Jul 29 02:28:03.465: INFO: Pod "pod-994e1521-d7eb-4231-96c7-d1f494238fd6" satisfied condition "success or failure"
Jul 29 02:28:03.469: INFO: Trying to get logs from node conformance1 pod pod-994e1521-d7eb-4231-96c7-d1f494238fd6 container test-container: <nil>
STEP: delete the pod
Jul 29 02:28:03.495: INFO: Waiting for pod pod-994e1521-d7eb-4231-96c7-d1f494238fd6 to disappear
Jul 29 02:28:03.501: INFO: Pod pod-994e1521-d7eb-4231-96c7-d1f494238fd6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:28:03.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5781" for this suite.
Jul 29 02:28:09.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:28:09.766: INFO: namespace emptydir-5781 deletion completed in 6.256432763s

• [SLOW TEST:10.408 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:28:09.766: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 29 02:28:09.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-5127'
Jul 29 02:28:10.281: INFO: stderr: ""
Jul 29 02:28:10.281: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 29 02:28:11.287: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 02:28:11.287: INFO: Found 0 / 1
Jul 29 02:28:12.286: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 02:28:12.286: INFO: Found 0 / 1
Jul 29 02:28:13.286: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 02:28:13.286: INFO: Found 0 / 1
Jul 29 02:28:14.286: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 02:28:14.286: INFO: Found 1 / 1
Jul 29 02:28:14.286: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 29 02:28:14.290: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 02:28:14.290: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 29 02:28:14.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 patch pod redis-master-gp9st --namespace=kubectl-5127 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 29 02:28:14.452: INFO: stderr: ""
Jul 29 02:28:14.452: INFO: stdout: "pod/redis-master-gp9st patched\n"
STEP: checking annotations
Jul 29 02:28:14.457: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 02:28:14.457: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:28:14.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5127" for this suite.
Jul 29 02:28:36.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:28:36.627: INFO: namespace kubectl-5127 deletion completed in 22.164121248s

• [SLOW TEST:26.860 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:28:36.628: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 29 02:28:36.735: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:28:41.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8049" for this suite.
Jul 29 02:28:47.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:28:47.911: INFO: namespace init-container-8049 deletion completed in 6.258271313s

• [SLOW TEST:11.283 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:28:47.915: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0729 02:29:18.107666      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 29 02:29:18.107: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:29:18.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5664" for this suite.
Jul 29 02:29:26.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:29:26.292: INFO: namespace gc-5664 deletion completed in 8.17978635s

• [SLOW TEST:38.378 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:29:26.295: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 29 02:29:26.403: INFO: Number of nodes with available pods: 0
Jul 29 02:29:26.403: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:27.414: INFO: Number of nodes with available pods: 0
Jul 29 02:29:27.414: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:28.415: INFO: Number of nodes with available pods: 0
Jul 29 02:29:28.415: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:29.413: INFO: Number of nodes with available pods: 2
Jul 29 02:29:29.413: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 29 02:29:29.445: INFO: Number of nodes with available pods: 1
Jul 29 02:29:29.446: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:30.456: INFO: Number of nodes with available pods: 1
Jul 29 02:29:30.456: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:31.478: INFO: Number of nodes with available pods: 1
Jul 29 02:29:31.479: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:32.457: INFO: Number of nodes with available pods: 1
Jul 29 02:29:32.457: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:33.457: INFO: Number of nodes with available pods: 1
Jul 29 02:29:33.457: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:34.469: INFO: Number of nodes with available pods: 1
Jul 29 02:29:34.469: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:35.456: INFO: Number of nodes with available pods: 1
Jul 29 02:29:35.456: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:36.456: INFO: Number of nodes with available pods: 1
Jul 29 02:29:36.456: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:37.473: INFO: Number of nodes with available pods: 1
Jul 29 02:29:37.473: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:38.455: INFO: Number of nodes with available pods: 1
Jul 29 02:29:38.455: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:39.457: INFO: Number of nodes with available pods: 1
Jul 29 02:29:39.457: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:29:40.456: INFO: Number of nodes with available pods: 2
Jul 29 02:29:40.456: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8998, will wait for the garbage collector to delete the pods
Jul 29 02:29:40.524: INFO: Deleting DaemonSet.extensions daemon-set took: 10.438512ms
Jul 29 02:29:40.825: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.424554ms
Jul 29 02:29:54.129: INFO: Number of nodes with available pods: 0
Jul 29 02:29:54.129: INFO: Number of running nodes: 0, number of available pods: 0
Jul 29 02:29:54.133: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8998/daemonsets","resourceVersion":"8932"},"items":null}

Jul 29 02:29:54.136: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8998/pods","resourceVersion":"8932"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:29:54.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8998" for this suite.
Jul 29 02:30:00.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:30:00.316: INFO: namespace daemonsets-8998 deletion completed in 6.163662638s

• [SLOW TEST:34.022 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:30:00.318: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:30:00.371: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:30:04.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6499" for this suite.
Jul 29 02:30:42.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:30:42.933: INFO: namespace pods-6499 deletion completed in 38.184749664s

• [SLOW TEST:42.615 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:30:42.934: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 29 02:30:42.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-8870'
Jul 29 02:30:43.156: INFO: stderr: ""
Jul 29 02:30:43.156: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 29 02:30:48.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pod e2e-test-nginx-pod --namespace=kubectl-8870 -o json'
Jul 29 02:30:48.344: INFO: stderr: ""
Jul 29 02:30:48.344: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-07-29T02:30:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-8870\",\n        \"resourceVersion\": \"9016\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8870/pods/e2e-test-nginx-pod\",\n        \"uid\": \"f321438b-393f-4e0e-a3d6-ae89c1e0f6f1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-bzhwg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"conformance1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-bzhwg\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-bzhwg\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-29T02:30:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-29T02:30:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-29T02:30:45Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-29T02:30:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://7fb3db7a264350ccec6f8074160e0c1e46b0229fb9a12cc94936a828b6586c62\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-29T02:30:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.1.89\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.0.92\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-29T02:30:43Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 29 02:30:48.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 replace -f - --namespace=kubectl-8870'
Jul 29 02:30:48.746: INFO: stderr: ""
Jul 29 02:30:48.746: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jul 29 02:30:48.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete pods e2e-test-nginx-pod --namespace=kubectl-8870'
Jul 29 02:30:54.067: INFO: stderr: ""
Jul 29 02:30:54.067: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:30:54.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8870" for this suite.
Jul 29 02:31:00.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:31:00.239: INFO: namespace kubectl-8870 deletion completed in 6.163505942s

• [SLOW TEST:17.305 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:31:00.240: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0729 02:31:06.529660      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 29 02:31:06.529: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:31:06.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8601" for this suite.
Jul 29 02:31:14.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:31:14.807: INFO: namespace gc-8601 deletion completed in 8.265668247s

• [SLOW TEST:14.567 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:31:14.809: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 29 02:31:14.890: INFO: Waiting up to 5m0s for pod "pod-c4f33f74-8792-4764-a06b-d24115ce07a4" in namespace "emptydir-8923" to be "success or failure"
Jul 29 02:31:14.894: INFO: Pod "pod-c4f33f74-8792-4764-a06b-d24115ce07a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566041ms
Jul 29 02:31:16.899: INFO: Pod "pod-c4f33f74-8792-4764-a06b-d24115ce07a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009134623s
Jul 29 02:31:18.904: INFO: Pod "pod-c4f33f74-8792-4764-a06b-d24115ce07a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013451517s
STEP: Saw pod success
Jul 29 02:31:18.904: INFO: Pod "pod-c4f33f74-8792-4764-a06b-d24115ce07a4" satisfied condition "success or failure"
Jul 29 02:31:18.908: INFO: Trying to get logs from node conformance0 pod pod-c4f33f74-8792-4764-a06b-d24115ce07a4 container test-container: <nil>
STEP: delete the pod
Jul 29 02:31:18.952: INFO: Waiting for pod pod-c4f33f74-8792-4764-a06b-d24115ce07a4 to disappear
Jul 29 02:31:18.957: INFO: Pod pod-c4f33f74-8792-4764-a06b-d24115ce07a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:31:18.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8923" for this suite.
Jul 29 02:31:24.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:31:25.147: INFO: namespace emptydir-8923 deletion completed in 6.184186372s

• [SLOW TEST:10.338 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:31:25.151: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 29 02:31:25.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-3572'
Jul 29 02:31:25.386: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 29 02:31:25.386: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jul 29 02:31:27.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3572'
Jul 29 02:31:27.643: INFO: stderr: ""
Jul 29 02:31:27.643: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:31:27.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3572" for this suite.
Jul 29 02:31:49.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:31:49.888: INFO: namespace kubectl-3572 deletion completed in 22.235047617s

• [SLOW TEST:24.737 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:31:49.888: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4971
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 29 02:31:49.935: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 29 02:32:16.089: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.123:8080/dial?request=hostName&protocol=udp&host=10.244.0.101&port=8081&tries=1'] Namespace:pod-network-test-4971 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:32:16.089: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:32:16.547: INFO: Waiting for endpoints: map[]
Jul 29 02:32:16.553: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.123:8080/dial?request=hostName&protocol=udp&host=10.244.1.122&port=8081&tries=1'] Namespace:pod-network-test-4971 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:32:16.553: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:32:16.947: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:32:16.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4971" for this suite.
Jul 29 02:32:40.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:32:41.113: INFO: namespace pod-network-test-4971 deletion completed in 24.158200962s

• [SLOW TEST:51.225 seconds]
[sig-network] Networking
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:32:41.117: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:32:45.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1104" for this suite.
Jul 29 02:33:29.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:33:29.393: INFO: namespace kubelet-test-1104 deletion completed in 44.180573615s

• [SLOW TEST:48.277 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:33:29.394: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-913
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-913 to expose endpoints map[]
Jul 29 02:33:29.481: INFO: Get endpoints failed (7.497386ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul 29 02:33:30.486: INFO: successfully validated that service multi-endpoint-test in namespace services-913 exposes endpoints map[] (1.011968761s elapsed)
STEP: Creating pod pod1 in namespace services-913
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-913 to expose endpoints map[pod1:[100]]
Jul 29 02:33:33.569: INFO: successfully validated that service multi-endpoint-test in namespace services-913 exposes endpoints map[pod1:[100]] (3.055054665s elapsed)
STEP: Creating pod pod2 in namespace services-913
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-913 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 29 02:33:36.649: INFO: successfully validated that service multi-endpoint-test in namespace services-913 exposes endpoints map[pod1:[100] pod2:[101]] (3.066885052s elapsed)
STEP: Deleting pod pod1 in namespace services-913
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-913 to expose endpoints map[pod2:[101]]
Jul 29 02:33:36.722: INFO: successfully validated that service multi-endpoint-test in namespace services-913 exposes endpoints map[pod2:[101]] (39.297915ms elapsed)
STEP: Deleting pod pod2 in namespace services-913
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-913 to expose endpoints map[]
Jul 29 02:33:36.758: INFO: successfully validated that service multi-endpoint-test in namespace services-913 exposes endpoints map[] (20.548772ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:33:36.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-913" for this suite.
Jul 29 02:33:42.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:33:43.021: INFO: namespace services-913 deletion completed in 6.188005031s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:13.627 seconds]
[sig-network] Services
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:33:43.023: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-0888886e-f7a0-44e2-b1ea-8a5ae82645e6
STEP: Creating configMap with name cm-test-opt-upd-e8cba0ff-e4ec-4e22-a37a-ec21612edb0b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0888886e-f7a0-44e2-b1ea-8a5ae82645e6
STEP: Updating configmap cm-test-opt-upd-e8cba0ff-e4ec-4e22-a37a-ec21612edb0b
STEP: Creating configMap with name cm-test-opt-create-a6fc9f36-80ee-4552-acfc-5f7a7e24ccfb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:33:51.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8351" for this suite.
Jul 29 02:34:13.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:34:13.433: INFO: namespace projected-8351 deletion completed in 22.185754684s

• [SLOW TEST:30.410 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:34:13.433: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:34:13.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7583" for this suite.
Jul 29 02:34:19.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:34:19.678: INFO: namespace services-7583 deletion completed in 6.18226934s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.245 seconds]
[sig-network] Services
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:34:19.679: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-95f937ba-3b5c-4658-a426-42560f5146d4
STEP: Creating a pod to test consume configMaps
Jul 29 02:34:19.757: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac083bef-62ab-40ad-b2e0-07471939360e" in namespace "projected-4858" to be "success or failure"
Jul 29 02:34:19.765: INFO: Pod "pod-projected-configmaps-ac083bef-62ab-40ad-b2e0-07471939360e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.491603ms
Jul 29 02:34:21.770: INFO: Pod "pod-projected-configmaps-ac083bef-62ab-40ad-b2e0-07471939360e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013130245s
Jul 29 02:34:23.775: INFO: Pod "pod-projected-configmaps-ac083bef-62ab-40ad-b2e0-07471939360e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018854272s
STEP: Saw pod success
Jul 29 02:34:23.776: INFO: Pod "pod-projected-configmaps-ac083bef-62ab-40ad-b2e0-07471939360e" satisfied condition "success or failure"
Jul 29 02:34:23.780: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-ac083bef-62ab-40ad-b2e0-07471939360e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 02:34:23.813: INFO: Waiting for pod pod-projected-configmaps-ac083bef-62ab-40ad-b2e0-07471939360e to disappear
Jul 29 02:34:23.823: INFO: Pod pod-projected-configmaps-ac083bef-62ab-40ad-b2e0-07471939360e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:34:23.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4858" for this suite.
Jul 29 02:34:29.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:34:29.983: INFO: namespace projected-4858 deletion completed in 6.153847576s

• [SLOW TEST:10.304 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:34:29.984: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 29 02:34:30.075: INFO: Waiting up to 5m0s for pod "pod-6346d89d-1fa9-4281-b028-4bcde90f14ea" in namespace "emptydir-8481" to be "success or failure"
Jul 29 02:34:30.081: INFO: Pod "pod-6346d89d-1fa9-4281-b028-4bcde90f14ea": Phase="Pending", Reason="", readiness=false. Elapsed: 5.56352ms
Jul 29 02:34:32.086: INFO: Pod "pod-6346d89d-1fa9-4281-b028-4bcde90f14ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010448506s
Jul 29 02:34:34.091: INFO: Pod "pod-6346d89d-1fa9-4281-b028-4bcde90f14ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015674563s
STEP: Saw pod success
Jul 29 02:34:34.091: INFO: Pod "pod-6346d89d-1fa9-4281-b028-4bcde90f14ea" satisfied condition "success or failure"
Jul 29 02:34:34.095: INFO: Trying to get logs from node conformance1 pod pod-6346d89d-1fa9-4281-b028-4bcde90f14ea container test-container: <nil>
STEP: delete the pod
Jul 29 02:34:34.133: INFO: Waiting for pod pod-6346d89d-1fa9-4281-b028-4bcde90f14ea to disappear
Jul 29 02:34:34.138: INFO: Pod pod-6346d89d-1fa9-4281-b028-4bcde90f14ea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:34:34.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8481" for this suite.
Jul 29 02:34:40.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:34:40.338: INFO: namespace emptydir-8481 deletion completed in 6.193555787s

• [SLOW TEST:10.355 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:34:40.348: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-fad4157c-cd24-4036-be03-1da5e70fb308
STEP: Creating a pod to test consume secrets
Jul 29 02:34:40.464: INFO: Waiting up to 5m0s for pod "pod-secrets-15678fcc-65e7-44d4-97ac-5082fd1c979c" in namespace "secrets-149" to be "success or failure"
Jul 29 02:34:40.482: INFO: Pod "pod-secrets-15678fcc-65e7-44d4-97ac-5082fd1c979c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.515233ms
Jul 29 02:34:42.493: INFO: Pod "pod-secrets-15678fcc-65e7-44d4-97ac-5082fd1c979c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029074363s
Jul 29 02:34:44.500: INFO: Pod "pod-secrets-15678fcc-65e7-44d4-97ac-5082fd1c979c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03525914s
Jul 29 02:34:46.505: INFO: Pod "pod-secrets-15678fcc-65e7-44d4-97ac-5082fd1c979c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041120403s
STEP: Saw pod success
Jul 29 02:34:46.506: INFO: Pod "pod-secrets-15678fcc-65e7-44d4-97ac-5082fd1c979c" satisfied condition "success or failure"
Jul 29 02:34:46.511: INFO: Trying to get logs from node conformance1 pod pod-secrets-15678fcc-65e7-44d4-97ac-5082fd1c979c container secret-volume-test: <nil>
STEP: delete the pod
Jul 29 02:34:46.564: INFO: Waiting for pod pod-secrets-15678fcc-65e7-44d4-97ac-5082fd1c979c to disappear
Jul 29 02:34:46.572: INFO: Pod pod-secrets-15678fcc-65e7-44d4-97ac-5082fd1c979c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:34:46.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-149" for this suite.
Jul 29 02:34:52.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:34:52.748: INFO: namespace secrets-149 deletion completed in 6.168175047s

• [SLOW TEST:12.402 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:34:52.750: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 29 02:35:00.878: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:00.878: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:01.180: INFO: Exec stderr: ""
Jul 29 02:35:01.181: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:01.181: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:01.544: INFO: Exec stderr: ""
Jul 29 02:35:01.544: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:01.544: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:01.860: INFO: Exec stderr: ""
Jul 29 02:35:01.861: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:01.861: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:02.225: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 29 02:35:02.226: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:02.226: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:02.560: INFO: Exec stderr: ""
Jul 29 02:35:02.561: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:02.561: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:02.928: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 29 02:35:02.928: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:02.928: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:03.241: INFO: Exec stderr: ""
Jul 29 02:35:03.241: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:03.241: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:03.594: INFO: Exec stderr: ""
Jul 29 02:35:03.594: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:03.594: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:04.183: INFO: Exec stderr: ""
Jul 29 02:35:04.183: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-988 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:35:04.183: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:35:04.583: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:35:04.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-988" for this suite.
Jul 29 02:35:48.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:35:48.762: INFO: namespace e2e-kubelet-etc-hosts-988 deletion completed in 44.17300676s

• [SLOW TEST:56.012 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:35:48.765: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-zhlw
STEP: Creating a pod to test atomic-volume-subpath
Jul 29 02:35:48.861: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zhlw" in namespace "subpath-2063" to be "success or failure"
Jul 29 02:35:48.888: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Pending", Reason="", readiness=false. Elapsed: 27.49756ms
Jul 29 02:35:50.924: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063726055s
Jul 29 02:35:52.930: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 4.068959381s
Jul 29 02:35:54.939: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 6.078012714s
Jul 29 02:35:56.945: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 8.08440583s
Jul 29 02:35:58.950: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 10.089068701s
Jul 29 02:36:01.012: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 12.151066061s
Jul 29 02:36:03.017: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 14.156456497s
Jul 29 02:36:05.022: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 16.161127658s
Jul 29 02:36:07.027: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 18.165928256s
Jul 29 02:36:09.032: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 20.171719133s
Jul 29 02:36:11.038: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Running", Reason="", readiness=true. Elapsed: 22.177151375s
Jul 29 02:36:13.045: INFO: Pod "pod-subpath-test-projected-zhlw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.184163822s
STEP: Saw pod success
Jul 29 02:36:13.045: INFO: Pod "pod-subpath-test-projected-zhlw" satisfied condition "success or failure"
Jul 29 02:36:13.053: INFO: Trying to get logs from node conformance1 pod pod-subpath-test-projected-zhlw container test-container-subpath-projected-zhlw: <nil>
STEP: delete the pod
Jul 29 02:36:13.101: INFO: Waiting for pod pod-subpath-test-projected-zhlw to disappear
Jul 29 02:36:13.121: INFO: Pod pod-subpath-test-projected-zhlw no longer exists
STEP: Deleting pod pod-subpath-test-projected-zhlw
Jul 29 02:36:13.121: INFO: Deleting pod "pod-subpath-test-projected-zhlw" in namespace "subpath-2063"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:36:13.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2063" for this suite.
Jul 29 02:36:19.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:36:19.329: INFO: namespace subpath-2063 deletion completed in 6.186791225s

• [SLOW TEST:30.564 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:36:19.330: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 29 02:36:19.412: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1812,SelfLink:/api/v1/namespaces/watch-1812/configmaps/e2e-watch-test-watch-closed,UID:e02ce9bb-4ee2-45be-b4d2-42e128e30581,ResourceVersion:9800,Generation:0,CreationTimestamp:2019-07-29 02:36:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 29 02:36:19.412: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1812,SelfLink:/api/v1/namespaces/watch-1812/configmaps/e2e-watch-test-watch-closed,UID:e02ce9bb-4ee2-45be-b4d2-42e128e30581,ResourceVersion:9801,Generation:0,CreationTimestamp:2019-07-29 02:36:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 29 02:36:19.434: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1812,SelfLink:/api/v1/namespaces/watch-1812/configmaps/e2e-watch-test-watch-closed,UID:e02ce9bb-4ee2-45be-b4d2-42e128e30581,ResourceVersion:9802,Generation:0,CreationTimestamp:2019-07-29 02:36:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 29 02:36:19.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1812,SelfLink:/api/v1/namespaces/watch-1812/configmaps/e2e-watch-test-watch-closed,UID:e02ce9bb-4ee2-45be-b4d2-42e128e30581,ResourceVersion:9803,Generation:0,CreationTimestamp:2019-07-29 02:36:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:36:19.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1812" for this suite.
Jul 29 02:36:25.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:36:25.602: INFO: namespace watch-1812 deletion completed in 6.162148606s

• [SLOW TEST:6.272 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:36:25.609: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-bd2c2052-d7a7-4c6a-85cd-1e1515c11cba
STEP: Creating a pod to test consume secrets
Jul 29 02:36:25.687: INFO: Waiting up to 5m0s for pod "pod-secrets-c18cb182-f8e9-45b8-a966-c3efd9b7a94b" in namespace "secrets-4857" to be "success or failure"
Jul 29 02:36:25.692: INFO: Pod "pod-secrets-c18cb182-f8e9-45b8-a966-c3efd9b7a94b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102915ms
Jul 29 02:36:27.697: INFO: Pod "pod-secrets-c18cb182-f8e9-45b8-a966-c3efd9b7a94b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009506398s
Jul 29 02:36:29.702: INFO: Pod "pod-secrets-c18cb182-f8e9-45b8-a966-c3efd9b7a94b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014589985s
STEP: Saw pod success
Jul 29 02:36:29.702: INFO: Pod "pod-secrets-c18cb182-f8e9-45b8-a966-c3efd9b7a94b" satisfied condition "success or failure"
Jul 29 02:36:29.705: INFO: Trying to get logs from node conformance0 pod pod-secrets-c18cb182-f8e9-45b8-a966-c3efd9b7a94b container secret-volume-test: <nil>
STEP: delete the pod
Jul 29 02:36:29.739: INFO: Waiting for pod pod-secrets-c18cb182-f8e9-45b8-a966-c3efd9b7a94b to disappear
Jul 29 02:36:29.744: INFO: Pod pod-secrets-c18cb182-f8e9-45b8-a966-c3efd9b7a94b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:36:29.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4857" for this suite.
Jul 29 02:36:35.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:36:35.913: INFO: namespace secrets-4857 deletion completed in 6.163012813s

• [SLOW TEST:10.305 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:36:35.919: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5806
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 29 02:36:36.002: INFO: Found 0 stateful pods, waiting for 3
Jul 29 02:36:46.008: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 02:36:46.008: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 02:36:46.008: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 02:36:46.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-5806 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:36:46.736: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:36:46.736: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:36:46.736: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 29 02:36:46.783: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 29 02:36:56.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-5806 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:36:57.293: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 29 02:36:57.293: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 29 02:36:57.293: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Jul 29 02:37:17.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-5806 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:37:18.411: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:37:18.411: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:37:18.411: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 29 02:37:28.456: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 29 02:37:38.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-5806 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:37:38.970: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 29 02:37:38.970: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 29 02:37:38.970: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 29 02:37:49.004: INFO: Waiting for StatefulSet statefulset-5806/ss2 to complete update
Jul 29 02:37:49.004: INFO: Waiting for Pod statefulset-5806/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 29 02:37:49.004: INFO: Waiting for Pod statefulset-5806/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 29 02:37:59.015: INFO: Waiting for StatefulSet statefulset-5806/ss2 to complete update
Jul 29 02:37:59.015: INFO: Waiting for Pod statefulset-5806/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 29 02:38:09.014: INFO: Deleting all statefulset in ns statefulset-5806
Jul 29 02:38:09.027: INFO: Scaling statefulset ss2 to 0
Jul 29 02:38:29.054: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 02:38:29.057: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:38:29.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5806" for this suite.
Jul 29 02:38:37.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:38:37.298: INFO: namespace statefulset-5806 deletion completed in 8.212738821s

• [SLOW TEST:121.380 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:38:37.298: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6991.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6991.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 29 02:38:41.434: INFO: DNS probes using dns-6991/dns-test-01dbf824-b28c-4bcf-ba81-76781309b153 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:38:41.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6991" for this suite.
Jul 29 02:38:47.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:38:47.687: INFO: namespace dns-6991 deletion completed in 6.22460189s

• [SLOW TEST:10.389 seconds]
[sig-network] DNS
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:38:47.689: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 29 02:38:52.345: INFO: Successfully updated pod "annotationupdatee4531990-5957-438f-986b-74d9f561f4fb"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:38:56.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3890" for this suite.
Jul 29 02:39:18.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:39:18.561: INFO: namespace projected-3890 deletion completed in 22.171022588s

• [SLOW TEST:30.872 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:39:18.562: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:39:18.685: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1083366-401c-41c2-a51d-e78655b41199" in namespace "downward-api-2857" to be "success or failure"
Jul 29 02:39:18.697: INFO: Pod "downwardapi-volume-b1083366-401c-41c2-a51d-e78655b41199": Phase="Pending", Reason="", readiness=false. Elapsed: 11.496695ms
Jul 29 02:39:20.702: INFO: Pod "downwardapi-volume-b1083366-401c-41c2-a51d-e78655b41199": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017353352s
Jul 29 02:39:22.708: INFO: Pod "downwardapi-volume-b1083366-401c-41c2-a51d-e78655b41199": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022651533s
STEP: Saw pod success
Jul 29 02:39:22.708: INFO: Pod "downwardapi-volume-b1083366-401c-41c2-a51d-e78655b41199" satisfied condition "success or failure"
Jul 29 02:39:22.711: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-b1083366-401c-41c2-a51d-e78655b41199 container client-container: <nil>
STEP: delete the pod
Jul 29 02:39:22.740: INFO: Waiting for pod downwardapi-volume-b1083366-401c-41c2-a51d-e78655b41199 to disappear
Jul 29 02:39:22.747: INFO: Pod downwardapi-volume-b1083366-401c-41c2-a51d-e78655b41199 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:39:22.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2857" for this suite.
Jul 29 02:39:28.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:39:28.920: INFO: namespace downward-api-2857 deletion completed in 6.168480058s

• [SLOW TEST:10.359 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:39:28.922: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:39:34.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6768" for this suite.
Jul 29 02:39:56.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:39:56.233: INFO: namespace replication-controller-6768 deletion completed in 22.201725264s

• [SLOW TEST:27.311 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:39:56.234: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jul 29 02:39:56.314: INFO: Waiting up to 5m0s for pod "client-containers-c5325274-01b5-40e3-a14d-bfc5cf0addde" in namespace "containers-2823" to be "success or failure"
Jul 29 02:39:56.330: INFO: Pod "client-containers-c5325274-01b5-40e3-a14d-bfc5cf0addde": Phase="Pending", Reason="", readiness=false. Elapsed: 15.830381ms
Jul 29 02:39:58.335: INFO: Pod "client-containers-c5325274-01b5-40e3-a14d-bfc5cf0addde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021263933s
Jul 29 02:40:00.341: INFO: Pod "client-containers-c5325274-01b5-40e3-a14d-bfc5cf0addde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026830701s
STEP: Saw pod success
Jul 29 02:40:00.341: INFO: Pod "client-containers-c5325274-01b5-40e3-a14d-bfc5cf0addde" satisfied condition "success or failure"
Jul 29 02:40:00.345: INFO: Trying to get logs from node conformance0 pod client-containers-c5325274-01b5-40e3-a14d-bfc5cf0addde container test-container: <nil>
STEP: delete the pod
Jul 29 02:40:00.391: INFO: Waiting for pod client-containers-c5325274-01b5-40e3-a14d-bfc5cf0addde to disappear
Jul 29 02:40:00.403: INFO: Pod client-containers-c5325274-01b5-40e3-a14d-bfc5cf0addde no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:40:00.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2823" for this suite.
Jul 29 02:40:06.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:40:06.565: INFO: namespace containers-2823 deletion completed in 6.156615014s

• [SLOW TEST:10.331 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:40:06.566: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:40:06.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7cf6a3ed-ff9c-4cb6-9577-087cfe93aa73" in namespace "downward-api-5469" to be "success or failure"
Jul 29 02:40:06.640: INFO: Pod "downwardapi-volume-7cf6a3ed-ff9c-4cb6-9577-087cfe93aa73": Phase="Pending", Reason="", readiness=false. Elapsed: 8.49047ms
Jul 29 02:40:08.645: INFO: Pod "downwardapi-volume-7cf6a3ed-ff9c-4cb6-9577-087cfe93aa73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014017989s
Jul 29 02:40:10.664: INFO: Pod "downwardapi-volume-7cf6a3ed-ff9c-4cb6-9577-087cfe93aa73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032951919s
STEP: Saw pod success
Jul 29 02:40:10.664: INFO: Pod "downwardapi-volume-7cf6a3ed-ff9c-4cb6-9577-087cfe93aa73" satisfied condition "success or failure"
Jul 29 02:40:10.669: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-7cf6a3ed-ff9c-4cb6-9577-087cfe93aa73 container client-container: <nil>
STEP: delete the pod
Jul 29 02:40:10.777: INFO: Waiting for pod downwardapi-volume-7cf6a3ed-ff9c-4cb6-9577-087cfe93aa73 to disappear
Jul 29 02:40:10.793: INFO: Pod downwardapi-volume-7cf6a3ed-ff9c-4cb6-9577-087cfe93aa73 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:40:10.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5469" for this suite.
Jul 29 02:40:16.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:40:17.019: INFO: namespace downward-api-5469 deletion completed in 6.220358553s

• [SLOW TEST:10.453 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:40:17.020: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 29 02:40:17.128: INFO: Waiting up to 5m0s for pod "pod-9d5c15bb-cd1a-48b9-a5d4-27f7b81de76b" in namespace "emptydir-7643" to be "success or failure"
Jul 29 02:40:17.139: INFO: Pod "pod-9d5c15bb-cd1a-48b9-a5d4-27f7b81de76b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.839849ms
Jul 29 02:40:19.144: INFO: Pod "pod-9d5c15bb-cd1a-48b9-a5d4-27f7b81de76b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015844185s
Jul 29 02:40:21.150: INFO: Pod "pod-9d5c15bb-cd1a-48b9-a5d4-27f7b81de76b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021942259s
STEP: Saw pod success
Jul 29 02:40:21.150: INFO: Pod "pod-9d5c15bb-cd1a-48b9-a5d4-27f7b81de76b" satisfied condition "success or failure"
Jul 29 02:40:21.154: INFO: Trying to get logs from node conformance1 pod pod-9d5c15bb-cd1a-48b9-a5d4-27f7b81de76b container test-container: <nil>
STEP: delete the pod
Jul 29 02:40:21.192: INFO: Waiting for pod pod-9d5c15bb-cd1a-48b9-a5d4-27f7b81de76b to disappear
Jul 29 02:40:21.199: INFO: Pod pod-9d5c15bb-cd1a-48b9-a5d4-27f7b81de76b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:40:21.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7643" for this suite.
Jul 29 02:40:27.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:40:27.393: INFO: namespace emptydir-7643 deletion completed in 6.187268372s

• [SLOW TEST:10.373 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:40:27.394: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 29 02:40:31.486: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-e67b4f14-2818-4f17-9550-85760629581c,GenerateName:,Namespace:events-5771,SelfLink:/api/v1/namespaces/events-5771/pods/send-events-e67b4f14-2818-4f17-9550-85760629581c,UID:8d452562-f260-4dce-b747-c8d361a95772,ResourceVersion:10497,Generation:0,CreationTimestamp:2019-07-29 02:40:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 454091500,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rml2f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rml2f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-rml2f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb6b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb6b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:40:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:40:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:40:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 02:40:27 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.89,PodIP:10.244.0.113,StartTime:2019-07-29 02:40:27 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-29 02:40:29 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://da9ec4a73567e7a093e67ef9de49a141642f4dfc46e501e9630d11f152427c36}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul 29 02:40:33.492: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 29 02:40:35.497: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:40:35.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5771" for this suite.
Jul 29 02:41:15.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:41:15.735: INFO: namespace events-5771 deletion completed in 40.190946268s

• [SLOW TEST:48.341 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:41:15.742: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 29 02:41:15.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-2557'
Jul 29 02:41:16.374: INFO: stderr: ""
Jul 29 02:41:16.374: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 29 02:41:16.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2557'
Jul 29 02:41:16.620: INFO: stderr: ""
Jul 29 02:41:16.620: INFO: stdout: "update-demo-nautilus-572wk update-demo-nautilus-p6b58 "
Jul 29 02:41:16.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-572wk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2557'
Jul 29 02:41:16.786: INFO: stderr: ""
Jul 29 02:41:16.786: INFO: stdout: ""
Jul 29 02:41:16.786: INFO: update-demo-nautilus-572wk is created but not running
Jul 29 02:41:21.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2557'
Jul 29 02:41:21.930: INFO: stderr: ""
Jul 29 02:41:21.930: INFO: stdout: "update-demo-nautilus-572wk update-demo-nautilus-p6b58 "
Jul 29 02:41:21.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-572wk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2557'
Jul 29 02:41:22.088: INFO: stderr: ""
Jul 29 02:41:22.088: INFO: stdout: "true"
Jul 29 02:41:22.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-572wk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2557'
Jul 29 02:41:22.236: INFO: stderr: ""
Jul 29 02:41:22.236: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 02:41:22.236: INFO: validating pod update-demo-nautilus-572wk
Jul 29 02:41:22.269: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 02:41:22.269: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 02:41:22.269: INFO: update-demo-nautilus-572wk is verified up and running
Jul 29 02:41:22.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-p6b58 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2557'
Jul 29 02:41:22.415: INFO: stderr: ""
Jul 29 02:41:22.415: INFO: stdout: "true"
Jul 29 02:41:22.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-p6b58 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2557'
Jul 29 02:41:22.564: INFO: stderr: ""
Jul 29 02:41:22.564: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 02:41:22.564: INFO: validating pod update-demo-nautilus-p6b58
Jul 29 02:41:22.576: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 02:41:22.576: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 02:41:22.576: INFO: update-demo-nautilus-p6b58 is verified up and running
STEP: using delete to clean up resources
Jul 29 02:41:22.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-2557'
Jul 29 02:41:22.749: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 02:41:22.749: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 29 02:41:22.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2557'
Jul 29 02:41:22.932: INFO: stderr: "No resources found.\n"
Jul 29 02:41:22.932: INFO: stdout: ""
Jul 29 02:41:22.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -l name=update-demo --namespace=kubectl-2557 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 29 02:41:23.124: INFO: stderr: ""
Jul 29 02:41:23.124: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:41:23.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2557" for this suite.
Jul 29 02:41:45.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:41:45.343: INFO: namespace kubectl-2557 deletion completed in 22.2117944s

• [SLOW TEST:29.602 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:41:45.352: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-a69138fc-71d8-49df-9509-67c2e90e71ab
STEP: Creating a pod to test consume configMaps
Jul 29 02:41:45.460: INFO: Waiting up to 5m0s for pod "pod-configmaps-640813b8-b089-4154-b694-03323e0cfa0e" in namespace "configmap-5571" to be "success or failure"
Jul 29 02:41:45.475: INFO: Pod "pod-configmaps-640813b8-b089-4154-b694-03323e0cfa0e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.421816ms
Jul 29 02:41:47.483: INFO: Pod "pod-configmaps-640813b8-b089-4154-b694-03323e0cfa0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022285794s
Jul 29 02:41:49.488: INFO: Pod "pod-configmaps-640813b8-b089-4154-b694-03323e0cfa0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027553513s
Jul 29 02:41:51.494: INFO: Pod "pod-configmaps-640813b8-b089-4154-b694-03323e0cfa0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033748845s
STEP: Saw pod success
Jul 29 02:41:51.494: INFO: Pod "pod-configmaps-640813b8-b089-4154-b694-03323e0cfa0e" satisfied condition "success or failure"
Jul 29 02:41:51.499: INFO: Trying to get logs from node conformance0 pod pod-configmaps-640813b8-b089-4154-b694-03323e0cfa0e container configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 02:41:51.537: INFO: Waiting for pod pod-configmaps-640813b8-b089-4154-b694-03323e0cfa0e to disappear
Jul 29 02:41:51.542: INFO: Pod pod-configmaps-640813b8-b089-4154-b694-03323e0cfa0e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:41:51.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5571" for this suite.
Jul 29 02:41:57.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:41:57.766: INFO: namespace configmap-5571 deletion completed in 6.216300567s

• [SLOW TEST:12.415 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:41:57.767: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 29 02:41:57.875: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 29 02:41:57.888: INFO: Waiting for terminating namespaces to be deleted...
Jul 29 02:41:57.893: INFO: 
Logging pods the kubelet thinks is on node conformance0 before test
Jul 29 02:41:57.909: INFO: kube-flannel-ds-q9cx6 from kube-system started at 2019-07-29 01:33:59 +0000 UTC (1 container statuses recorded)
Jul 29 02:41:57.909: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 29 02:41:57.909: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-29 01:38:17 +0000 UTC (1 container statuses recorded)
Jul 29 02:41:57.909: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 29 02:41:57.909: INFO: nirmata-cni-installer-gstxd from nirmata started at 2019-07-29 01:34:09 +0000 UTC (1 container statuses recorded)
Jul 29 02:41:57.909: INFO: 	Container install-cni ready: true, restart count 0
Jul 29 02:41:57.910: INFO: haproxy-ingress-98858d4d6-fzcg2 from ingress-haproxy started at 2019-07-29 01:34:16 +0000 UTC (1 container statuses recorded)
Jul 29 02:41:57.910: INFO: 	Container haproxy-ingress ready: true, restart count 0
Jul 29 02:41:57.910: INFO: ingress-default-backend-6bd67fff4c-vzlm8 from ingress-haproxy started at 2019-07-29 01:34:15 +0000 UTC (1 container statuses recorded)
Jul 29 02:41:57.910: INFO: 	Container ingress-default-backend ready: true, restart count 0
Jul 29 02:41:57.910: INFO: sonobuoy-e2e-job-feb2cf4d6c56458c from heptio-sonobuoy started at 2019-07-29 01:38:22 +0000 UTC (2 container statuses recorded)
Jul 29 02:41:57.910: INFO: 	Container e2e ready: true, restart count 0
Jul 29 02:41:57.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 02:41:57.910: INFO: sonobuoy-systemd-logs-daemon-set-5e30946e92f64e9d-v422w from heptio-sonobuoy started at 2019-07-29 01:38:22 +0000 UTC (2 container statuses recorded)
Jul 29 02:41:57.910: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 29 02:41:57.910: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 29 02:41:57.910: INFO: 
Logging pods the kubelet thinks is on node conformance1 before test
Jul 29 02:41:57.927: INFO: nirmata-kube-controller-7c556b44bc-b5rnr from nirmata started at 2019-07-29 01:33:36 +0000 UTC (1 container statuses recorded)
Jul 29 02:41:57.927: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
Jul 29 02:41:57.927: INFO: sonobuoy-systemd-logs-daemon-set-5e30946e92f64e9d-ljfqt from heptio-sonobuoy started at 2019-07-29 01:38:22 +0000 UTC (2 container statuses recorded)
Jul 29 02:41:57.927: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 29 02:41:57.927: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 29 02:41:57.927: INFO: kube-dns-6475b5b555-kv5d8 from kube-system started at 2019-07-29 01:33:36 +0000 UTC (3 container statuses recorded)
Jul 29 02:41:57.927: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 29 02:41:57.927: INFO: 	Container kubedns ready: true, restart count 0
Jul 29 02:41:57.927: INFO: 	Container sidecar ready: true, restart count 0
Jul 29 02:41:57.927: INFO: metrics-server-8454c7c9c6-htc58 from kube-system started at 2019-07-29 01:33:36 +0000 UTC (1 container statuses recorded)
Jul 29 02:41:57.927: INFO: 	Container metrics-server ready: true, restart count 0
Jul 29 02:41:57.927: INFO: kube-flannel-ds-gmpdt from kube-system started at 2019-07-29 01:33:20 +0000 UTC (1 container statuses recorded)
Jul 29 02:41:57.927: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 29 02:41:57.927: INFO: nirmata-cni-installer-swqlb from nirmata started at 2019-07-29 01:33:34 +0000 UTC (1 container statuses recorded)
Jul 29 02:41:57.927: INFO: 	Container install-cni ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node conformance0
STEP: verifying the node has the label node conformance1
Jul 29 02:41:58.071: INFO: Pod sonobuoy requesting resource cpu=0m on Node conformance0
Jul 29 02:41:58.071: INFO: Pod sonobuoy-e2e-job-feb2cf4d6c56458c requesting resource cpu=0m on Node conformance0
Jul 29 02:41:58.071: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e30946e92f64e9d-ljfqt requesting resource cpu=0m on Node conformance1
Jul 29 02:41:58.071: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e30946e92f64e9d-v422w requesting resource cpu=0m on Node conformance0
Jul 29 02:41:58.071: INFO: Pod haproxy-ingress-98858d4d6-fzcg2 requesting resource cpu=0m on Node conformance0
Jul 29 02:41:58.071: INFO: Pod ingress-default-backend-6bd67fff4c-vzlm8 requesting resource cpu=0m on Node conformance0
Jul 29 02:41:58.071: INFO: Pod kube-dns-6475b5b555-kv5d8 requesting resource cpu=260m on Node conformance1
Jul 29 02:41:58.071: INFO: Pod kube-flannel-ds-gmpdt requesting resource cpu=0m on Node conformance1
Jul 29 02:41:58.071: INFO: Pod kube-flannel-ds-q9cx6 requesting resource cpu=0m on Node conformance0
Jul 29 02:41:58.071: INFO: Pod metrics-server-8454c7c9c6-htc58 requesting resource cpu=0m on Node conformance1
Jul 29 02:41:58.071: INFO: Pod nirmata-cni-installer-gstxd requesting resource cpu=0m on Node conformance0
Jul 29 02:41:58.071: INFO: Pod nirmata-cni-installer-swqlb requesting resource cpu=0m on Node conformance1
Jul 29 02:41:58.071: INFO: Pod nirmata-kube-controller-7c556b44bc-b5rnr requesting resource cpu=0m on Node conformance1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db8f2a40-7b9f-4067-844b-e3005325a7a6.15b5c083da8f9e38], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3053/filler-pod-db8f2a40-7b9f-4067-844b-e3005325a7a6 to conformance1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db8f2a40-7b9f-4067-844b-e3005325a7a6.15b5c0843b9954e8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db8f2a40-7b9f-4067-844b-e3005325a7a6.15b5c0844ea3bc54], Reason = [Created], Message = [Created container filler-pod-db8f2a40-7b9f-4067-844b-e3005325a7a6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db8f2a40-7b9f-4067-844b-e3005325a7a6.15b5c0846964212b], Reason = [Started], Message = [Started container filler-pod-db8f2a40-7b9f-4067-844b-e3005325a7a6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa82d4d8-5afa-499f-861c-57252d03d651.15b5c083d8d6fcc9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3053/filler-pod-fa82d4d8-5afa-499f-861c-57252d03d651 to conformance0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa82d4d8-5afa-499f-861c-57252d03d651.15b5c0844c2dea17], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa82d4d8-5afa-499f-861c-57252d03d651.15b5c08450d2732e], Reason = [Created], Message = [Created container filler-pod-fa82d4d8-5afa-499f-861c-57252d03d651]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa82d4d8-5afa-499f-861c-57252d03d651.15b5c08469e89217], Reason = [Started], Message = [Started container filler-pod-fa82d4d8-5afa-499f-861c-57252d03d651]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b5c084caf955c2], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node conformance0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node conformance1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:42:03.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3053" for this suite.
Jul 29 02:42:09.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:42:09.568: INFO: namespace sched-pred-3053 deletion completed in 6.332980278s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.802 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:42:09.574: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:42:09.671: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 29 02:42:09.815: INFO: Number of nodes with available pods: 0
Jul 29 02:42:09.815: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:42:10.827: INFO: Number of nodes with available pods: 0
Jul 29 02:42:10.828: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:42:11.834: INFO: Number of nodes with available pods: 0
Jul 29 02:42:11.834: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:42:12.828: INFO: Number of nodes with available pods: 2
Jul 29 02:42:12.828: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 29 02:42:12.884: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:12.884: INFO: Wrong image for pod: daemon-set-hbrrp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:13.902: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:13.902: INFO: Wrong image for pod: daemon-set-hbrrp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:14.902: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:14.902: INFO: Wrong image for pod: daemon-set-hbrrp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:15.902: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:15.902: INFO: Wrong image for pod: daemon-set-hbrrp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:15.902: INFO: Pod daemon-set-hbrrp is not available
Jul 29 02:42:16.902: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:16.903: INFO: Pod daemon-set-mll5b is not available
Jul 29 02:42:17.909: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:17.909: INFO: Pod daemon-set-mll5b is not available
Jul 29 02:42:18.902: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:19.903: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:19.903: INFO: Pod daemon-set-68qvc is not available
Jul 29 02:42:20.913: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:20.913: INFO: Pod daemon-set-68qvc is not available
Jul 29 02:42:21.902: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:21.902: INFO: Pod daemon-set-68qvc is not available
Jul 29 02:42:22.901: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:22.902: INFO: Pod daemon-set-68qvc is not available
Jul 29 02:42:23.902: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:23.902: INFO: Pod daemon-set-68qvc is not available
Jul 29 02:42:24.902: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:24.903: INFO: Pod daemon-set-68qvc is not available
Jul 29 02:42:25.901: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:25.902: INFO: Pod daemon-set-68qvc is not available
Jul 29 02:42:26.904: INFO: Wrong image for pod: daemon-set-68qvc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 29 02:42:26.904: INFO: Pod daemon-set-68qvc is not available
Jul 29 02:42:27.902: INFO: Pod daemon-set-5t997 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 29 02:42:27.923: INFO: Number of nodes with available pods: 1
Jul 29 02:42:27.923: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:42:28.934: INFO: Number of nodes with available pods: 1
Jul 29 02:42:28.934: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:42:29.934: INFO: Number of nodes with available pods: 1
Jul 29 02:42:29.934: INFO: Node conformance0 is running more than one daemon pod
Jul 29 02:42:30.945: INFO: Number of nodes with available pods: 2
Jul 29 02:42:30.946: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9301, will wait for the garbage collector to delete the pods
Jul 29 02:42:31.127: INFO: Deleting DaemonSet.extensions daemon-set took: 12.387252ms
Jul 29 02:42:31.529: INFO: Terminating DaemonSet.extensions daemon-set pods took: 401.522972ms
Jul 29 02:42:44.135: INFO: Number of nodes with available pods: 0
Jul 29 02:42:44.135: INFO: Number of running nodes: 0, number of available pods: 0
Jul 29 02:42:44.141: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9301/daemonsets","resourceVersion":"10788"},"items":null}

Jul 29 02:42:44.149: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9301/pods","resourceVersion":"10788"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:42:44.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9301" for this suite.
Jul 29 02:42:50.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:42:50.380: INFO: namespace daemonsets-9301 deletion completed in 6.208214425s

• [SLOW TEST:40.807 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:42:50.381: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 29 02:42:54.505: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:42:54.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3353" for this suite.
Jul 29 02:43:00.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:43:00.794: INFO: namespace container-runtime-3353 deletion completed in 6.253899417s

• [SLOW TEST:10.413 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:43:00.795: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-b82f7487-6ae2-4184-bfa6-84bad9e15632 in namespace container-probe-9667
Jul 29 02:43:04.966: INFO: Started pod liveness-b82f7487-6ae2-4184-bfa6-84bad9e15632 in namespace container-probe-9667
STEP: checking the pod's current state and verifying that restartCount is present
Jul 29 02:43:04.970: INFO: Initial restart count of pod liveness-b82f7487-6ae2-4184-bfa6-84bad9e15632 is 0
Jul 29 02:43:23.039: INFO: Restart count of pod container-probe-9667/liveness-b82f7487-6ae2-4184-bfa6-84bad9e15632 is now 1 (18.069052151s elapsed)
Jul 29 02:43:43.110: INFO: Restart count of pod container-probe-9667/liveness-b82f7487-6ae2-4184-bfa6-84bad9e15632 is now 2 (38.13999209s elapsed)
Jul 29 02:44:05.318: INFO: Restart count of pod container-probe-9667/liveness-b82f7487-6ae2-4184-bfa6-84bad9e15632 is now 3 (1m0.348142751s elapsed)
Jul 29 02:44:23.372: INFO: Restart count of pod container-probe-9667/liveness-b82f7487-6ae2-4184-bfa6-84bad9e15632 is now 4 (1m18.40204967s elapsed)
Jul 29 02:45:35.657: INFO: Restart count of pod container-probe-9667/liveness-b82f7487-6ae2-4184-bfa6-84bad9e15632 is now 5 (2m30.687145125s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:45:35.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9667" for this suite.
Jul 29 02:45:41.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:45:41.929: INFO: namespace container-probe-9667 deletion completed in 6.233390168s

• [SLOW TEST:161.135 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:45:41.932: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:45:42.068: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 29 02:45:44.154: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:45:44.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9056" for this suite.
Jul 29 02:45:50.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:45:50.428: INFO: namespace replication-controller-9056 deletion completed in 6.251089646s

• [SLOW TEST:8.496 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:45:50.435: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:45:50.621: INFO: Waiting up to 5m0s for pod "downwardapi-volume-454b63b1-2678-4d5a-bb89-335c74c9abe5" in namespace "projected-1421" to be "success or failure"
Jul 29 02:45:50.631: INFO: Pod "downwardapi-volume-454b63b1-2678-4d5a-bb89-335c74c9abe5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.866355ms
Jul 29 02:45:52.637: INFO: Pod "downwardapi-volume-454b63b1-2678-4d5a-bb89-335c74c9abe5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015688587s
Jul 29 02:45:54.643: INFO: Pod "downwardapi-volume-454b63b1-2678-4d5a-bb89-335c74c9abe5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02174515s
STEP: Saw pod success
Jul 29 02:45:54.643: INFO: Pod "downwardapi-volume-454b63b1-2678-4d5a-bb89-335c74c9abe5" satisfied condition "success or failure"
Jul 29 02:45:54.653: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-454b63b1-2678-4d5a-bb89-335c74c9abe5 container client-container: <nil>
STEP: delete the pod
Jul 29 02:45:54.837: INFO: Waiting for pod downwardapi-volume-454b63b1-2678-4d5a-bb89-335c74c9abe5 to disappear
Jul 29 02:45:54.849: INFO: Pod downwardapi-volume-454b63b1-2678-4d5a-bb89-335c74c9abe5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:45:54.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1421" for this suite.
Jul 29 02:46:00.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:46:01.056: INFO: namespace projected-1421 deletion completed in 6.198231516s

• [SLOW TEST:10.621 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:46:01.064: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 29 02:46:01.124: INFO: PodSpec: initContainers in spec.initContainers
Jul 29 02:46:49.537: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-06797056-c40e-4d3f-8942-9ac61f114727", GenerateName:"", Namespace:"init-container-3382", SelfLink:"/api/v1/namespaces/init-container-3382/pods/pod-init-06797056-c40e-4d3f-8942-9ac61f114727", UID:"e208a47f-f7b8-4d48-bc55-052a59e5de0f", ResourceVersion:"11122", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63699965161, loc:(*time.Location)(0x80c0a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"124866289"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-rrsm4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0024e2000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rrsm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rrsm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rrsm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0026b20b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"conformance0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0024644e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0026b2140)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0026b2160)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0026b2168), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0026b216c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699965161, loc:(*time.Location)(0x80c0a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699965161, loc:(*time.Location)(0x80c0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699965161, loc:(*time.Location)(0x80c0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699965161, loc:(*time.Location)(0x80c0a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.1.213", PodIP:"10.244.1.146", StartTime:(*v1.Time)(0xc001b5b560), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001cc0e00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001cc0ee0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://c4783901c9f3bfc679909edf3650cdf558e87f317e0f11d767bdd766e5e83dfd"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b5b5a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b5b580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:46:49.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3382" for this suite.
Jul 29 02:47:11.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:47:11.774: INFO: namespace init-container-3382 deletion completed in 22.228195283s

• [SLOW TEST:70.710 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:47:11.775: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-89360d34-63e2-45e7-a846-622dd2b5de08
STEP: Creating a pod to test consume configMaps
Jul 29 02:47:11.852: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3354c57b-9afc-42c9-9cb0-c09ef9a162a8" in namespace "projected-177" to be "success or failure"
Jul 29 02:47:11.885: INFO: Pod "pod-projected-configmaps-3354c57b-9afc-42c9-9cb0-c09ef9a162a8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.627874ms
Jul 29 02:47:13.901: INFO: Pod "pod-projected-configmaps-3354c57b-9afc-42c9-9cb0-c09ef9a162a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049155605s
Jul 29 02:47:15.907: INFO: Pod "pod-projected-configmaps-3354c57b-9afc-42c9-9cb0-c09ef9a162a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05553426s
STEP: Saw pod success
Jul 29 02:47:15.908: INFO: Pod "pod-projected-configmaps-3354c57b-9afc-42c9-9cb0-c09ef9a162a8" satisfied condition "success or failure"
Jul 29 02:47:15.914: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-3354c57b-9afc-42c9-9cb0-c09ef9a162a8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 02:47:15.957: INFO: Waiting for pod pod-projected-configmaps-3354c57b-9afc-42c9-9cb0-c09ef9a162a8 to disappear
Jul 29 02:47:15.965: INFO: Pod pod-projected-configmaps-3354c57b-9afc-42c9-9cb0-c09ef9a162a8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:47:15.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-177" for this suite.
Jul 29 02:47:21.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:47:22.244: INFO: namespace projected-177 deletion completed in 6.27266685s

• [SLOW TEST:10.469 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:47:22.244: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 02:47:28.420: INFO: Waiting up to 5m0s for pod "client-envvars-da73aef5-171d-4c83-9975-633ee744fece" in namespace "pods-919" to be "success or failure"
Jul 29 02:47:28.477: INFO: Pod "client-envvars-da73aef5-171d-4c83-9975-633ee744fece": Phase="Pending", Reason="", readiness=false. Elapsed: 56.438413ms
Jul 29 02:47:30.498: INFO: Pod "client-envvars-da73aef5-171d-4c83-9975-633ee744fece": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077542581s
Jul 29 02:47:32.504: INFO: Pod "client-envvars-da73aef5-171d-4c83-9975-633ee744fece": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083875124s
Jul 29 02:47:34.510: INFO: Pod "client-envvars-da73aef5-171d-4c83-9975-633ee744fece": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089432923s
STEP: Saw pod success
Jul 29 02:47:34.510: INFO: Pod "client-envvars-da73aef5-171d-4c83-9975-633ee744fece" satisfied condition "success or failure"
Jul 29 02:47:34.515: INFO: Trying to get logs from node conformance1 pod client-envvars-da73aef5-171d-4c83-9975-633ee744fece container env3cont: <nil>
STEP: delete the pod
Jul 29 02:47:34.556: INFO: Waiting for pod client-envvars-da73aef5-171d-4c83-9975-633ee744fece to disappear
Jul 29 02:47:34.561: INFO: Pod client-envvars-da73aef5-171d-4c83-9975-633ee744fece no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:47:34.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-919" for this suite.
Jul 29 02:48:20.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:48:20.774: INFO: namespace pods-919 deletion completed in 46.203738096s

• [SLOW TEST:58.530 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:48:20.775: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5583.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5583.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5583.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5583.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 29 02:48:24.934: INFO: DNS probes using dns-test-16cd7f8b-a0a7-4098-be6c-62c50f22b72f succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5583.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5583.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5583.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5583.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 29 02:48:29.047: INFO: DNS probes using dns-test-e3c4cf19-7633-4ae0-86a7-e88234fd5f9e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5583.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5583.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5583.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5583.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 29 02:48:35.235: INFO: DNS probes using dns-test-63dd9880-cf79-4020-86fe-0172cc739346 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:48:35.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5583" for this suite.
Jul 29 02:48:41.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:48:41.554: INFO: namespace dns-5583 deletion completed in 6.220503143s

• [SLOW TEST:20.779 seconds]
[sig-network] DNS
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:48:41.555: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-f42dc75d-df12-4a3f-b5f7-ea96107c760e
STEP: Creating a pod to test consume configMaps
Jul 29 02:48:41.639: INFO: Waiting up to 5m0s for pod "pod-configmaps-b038ca3e-1721-401a-8f49-3c1455a27535" in namespace "configmap-3771" to be "success or failure"
Jul 29 02:48:41.675: INFO: Pod "pod-configmaps-b038ca3e-1721-401a-8f49-3c1455a27535": Phase="Pending", Reason="", readiness=false. Elapsed: 36.33127ms
Jul 29 02:48:43.683: INFO: Pod "pod-configmaps-b038ca3e-1721-401a-8f49-3c1455a27535": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043938448s
Jul 29 02:48:45.689: INFO: Pod "pod-configmaps-b038ca3e-1721-401a-8f49-3c1455a27535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05050166s
STEP: Saw pod success
Jul 29 02:48:45.690: INFO: Pod "pod-configmaps-b038ca3e-1721-401a-8f49-3c1455a27535" satisfied condition "success or failure"
Jul 29 02:48:45.697: INFO: Trying to get logs from node conformance1 pod pod-configmaps-b038ca3e-1721-401a-8f49-3c1455a27535 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 02:48:45.733: INFO: Waiting for pod pod-configmaps-b038ca3e-1721-401a-8f49-3c1455a27535 to disappear
Jul 29 02:48:45.750: INFO: Pod pod-configmaps-b038ca3e-1721-401a-8f49-3c1455a27535 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:48:45.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3771" for this suite.
Jul 29 02:48:51.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:48:52.043: INFO: namespace configmap-3771 deletion completed in 6.284671586s

• [SLOW TEST:10.488 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:48:52.046: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3648
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 29 02:48:52.158: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 29 02:49:16.354: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.152:8080/dial?request=hostName&protocol=http&host=10.244.1.151&port=8080&tries=1'] Namespace:pod-network-test-3648 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:49:16.354: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:49:16.714: INFO: Waiting for endpoints: map[]
Jul 29 02:49:16.719: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.152:8080/dial?request=hostName&protocol=http&host=10.244.0.124&port=8080&tries=1'] Namespace:pod-network-test-3648 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 29 02:49:16.719: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
Jul 29 02:49:17.149: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:49:17.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3648" for this suite.
Jul 29 02:49:41.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:49:41.388: INFO: namespace pod-network-test-3648 deletion completed in 24.229650699s

• [SLOW TEST:49.342 seconds]
[sig-network] Networking
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:49:41.391: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-327
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-327
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-327
Jul 29 02:49:41.507: INFO: Found 0 stateful pods, waiting for 1
Jul 29 02:49:51.513: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 29 02:49:51.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:49:52.257: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:49:52.257: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:49:52.257: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 29 02:49:52.264: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 02:49:52.264: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 02:49:52.297: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999996583s
Jul 29 02:49:53.303: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.9942479s
Jul 29 02:49:54.308: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988210528s
Jul 29 02:49:55.314: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982633824s
Jul 29 02:49:56.320: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976706722s
Jul 29 02:49:57.326: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970510049s
Jul 29 02:49:58.332: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964703996s
Jul 29 02:49:59.338: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.958866005s
Jul 29 02:50:00.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.952327584s
Jul 29 02:50:01.353: INFO: Verifying statefulset ss doesn't scale past 1 for another 944.147218ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-327
Jul 29 02:50:02.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:50:02.851: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 29 02:50:02.851: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 29 02:50:02.851: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 29 02:50:02.858: INFO: Found 1 stateful pods, waiting for 3
Jul 29 02:50:12.864: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 02:50:12.864: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 02:50:12.864: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 29 02:50:12.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:50:13.422: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:50:13.422: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:50:13.422: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 29 02:50:13.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:50:13.935: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:50:13.935: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:50:13.935: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 29 02:50:13.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 29 02:50:14.439: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 29 02:50:14.440: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 29 02:50:14.440: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 29 02:50:14.440: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 02:50:14.445: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 29 02:50:24.458: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 02:50:24.458: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 02:50:24.458: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 02:50:24.487: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999990899s
Jul 29 02:50:25.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990494726s
Jul 29 02:50:26.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985514084s
Jul 29 02:50:27.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980074697s
Jul 29 02:50:28.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.973937785s
Jul 29 02:50:29.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966759534s
Jul 29 02:50:30.538: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.947707827s
Jul 29 02:50:31.544: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.939525506s
Jul 29 02:50:32.551: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933140962s
Jul 29 02:50:33.558: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.545456ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-327
Jul 29 02:50:34.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:50:35.034: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 29 02:50:35.034: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 29 02:50:35.034: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 29 02:50:35.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:50:35.596: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 29 02:50:35.596: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 29 02:50:35.596: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 29 02:50:35.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:50:36.188: INFO: rc: 126
Jul 29 02:50:36.188: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> OCI runtime exec failed: exec failed: cannot exec a container that has stopped: unknown
 command terminated with exit code 126
 [] <nil> 0xc0022049f0 exit status 126 <nil> <nil> true [0xc0023778c0 0xc0023778d8 0xc0023778f0] [0xc0023778c0 0xc0023778d8 0xc0023778f0] [0xc0023778d0 0xc0023778e8] [0x9d17b0 0x9d17b0] 0xc00210a240 <nil>}:
Command stdout:
OCI runtime exec failed: exec failed: cannot exec a container that has stopped: unknown

stderr:
command terminated with exit code 126

error:
exit status 126
Jul 29 02:50:46.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:50:46.337: INFO: rc: 1
Jul 29 02:50:46.337: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002204de0 exit status 1 <nil> <nil> true [0xc0023778f8 0xc002377910 0xc002377928] [0xc0023778f8 0xc002377910 0xc002377928] [0xc002377908 0xc002377920] [0x9d17b0 0x9d17b0] 0xc00210a720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:50:56.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:50:56.514: INFO: rc: 1
Jul 29 02:50:56.514: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001decb40 exit status 1 <nil> <nil> true [0xc00362b9b8 0xc00362ba10 0xc00362ba50] [0xc00362b9b8 0xc00362ba10 0xc00362ba50] [0xc00362b9f8 0xc00362ba48] [0x9d17b0 0x9d17b0] 0xc001c41140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:51:06.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:51:06.671: INFO: rc: 1
Jul 29 02:51:06.671: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3e330 exit status 1 <nil> <nil> true [0xc00362a008 0xc00362a040 0xc00362a070] [0xc00362a008 0xc00362a040 0xc00362a070] [0xc00362a038 0xc00362a068] [0x9d17b0 0x9d17b0] 0xc0036bc060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:51:16.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:51:16.825: INFO: rc: 1
Jul 29 02:51:16.825: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3e6c0 exit status 1 <nil> <nil> true [0xc00362a078 0xc00362a0b0 0xc00362a0e8] [0xc00362a078 0xc00362a0b0 0xc00362a0e8] [0xc00362a0a8 0xc00362a0c8] [0x9d17b0 0x9d17b0] 0xc0036bc420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:51:26.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:51:26.974: INFO: rc: 1
Jul 29 02:51:26.974: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3ea50 exit status 1 <nil> <nil> true [0xc00362a0f0 0xc00362a120 0xc00362a150] [0xc00362a0f0 0xc00362a120 0xc00362a150] [0xc00362a110 0xc00362a148] [0x9d17b0 0x9d17b0] 0xc0036bcb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:51:36.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:51:37.117: INFO: rc: 1
Jul 29 02:51:37.117: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3ede0 exit status 1 <nil> <nil> true [0xc00362a178 0xc00362a1c8 0xc00362a200] [0xc00362a178 0xc00362a1c8 0xc00362a200] [0xc00362a1a8 0xc00362a1e8] [0x9d17b0 0x9d17b0] 0xc0036bd620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:51:47.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:51:47.285: INFO: rc: 1
Jul 29 02:51:47.285: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3f1a0 exit status 1 <nil> <nil> true [0xc00362a208 0xc00362a248 0xc00362a2b0] [0xc00362a208 0xc00362a248 0xc00362a2b0] [0xc00362a230 0xc00362a2a8] [0x9d17b0 0x9d17b0] 0xc0036bdc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:51:57.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:51:57.534: INFO: rc: 1
Jul 29 02:51:57.534: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002004360 exit status 1 <nil> <nil> true [0xc002376030 0xc0023761d8 0xc002376220] [0xc002376030 0xc0023761d8 0xc002376220] [0xc0023761a0 0xc0023761f8] [0x9d17b0 0x9d17b0] 0xc00292ca20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:52:07.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:52:07.708: INFO: rc: 1
Jul 29 02:52:07.708: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3f9e0 exit status 1 <nil> <nil> true [0xc00362a2b8 0xc00362a2f0 0xc00362a310] [0xc00362a2b8 0xc00362a2f0 0xc00362a310] [0xc00362a2d8 0xc00362a300] [0x9d17b0 0x9d17b0] 0xc00216ae40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:52:17.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:52:17.874: INFO: rc: 1
Jul 29 02:52:17.874: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3fda0 exit status 1 <nil> <nil> true [0xc00362a348 0xc00362a390 0xc00362a3d0] [0xc00362a348 0xc00362a390 0xc00362a3d0] [0xc00362a370 0xc00362a3c8] [0x9d17b0 0x9d17b0] 0xc002096420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:52:27.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:52:28.040: INFO: rc: 1
Jul 29 02:52:28.040: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002036300 exit status 1 <nil> <nil> true [0xc00362a3d8 0xc00362a400 0xc00362a418] [0xc00362a3d8 0xc00362a400 0xc00362a418] [0xc00362a3e8 0xc00362a410] [0x9d17b0 0x9d17b0] 0xc0020974a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:52:38.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:52:38.185: INFO: rc: 1
Jul 29 02:52:38.185: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020366c0 exit status 1 <nil> <nil> true [0xc00362a420 0xc00362a438 0xc00362a478] [0xc00362a420 0xc00362a438 0xc00362a478] [0xc00362a430 0xc00362a458] [0x9d17b0 0x9d17b0] 0xc0023a2f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:52:48.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:52:48.349: INFO: rc: 1
Jul 29 02:52:48.349: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002036a50 exit status 1 <nil> <nil> true [0xc00362a490 0xc00362a4d8 0xc00362a508] [0xc00362a490 0xc00362a4d8 0xc00362a508] [0xc00362a4b8 0xc00362a4f8] [0x9d17b0 0x9d17b0] 0xc0024a2900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:52:58.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:52:58.512: INFO: rc: 1
Jul 29 02:52:58.512: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002036de0 exit status 1 <nil> <nil> true [0xc00362a538 0xc00362a5c8 0xc00362a618] [0xc00362a538 0xc00362a5c8 0xc00362a618] [0xc00362a5c0 0xc00362a5f0] [0x9d17b0 0x9d17b0] 0xc00226c4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:53:08.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:53:08.663: INFO: rc: 1
Jul 29 02:53:08.663: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3e360 exit status 1 <nil> <nil> true [0xc00362a008 0xc00362a040 0xc00362a070] [0xc00362a008 0xc00362a040 0xc00362a070] [0xc00362a038 0xc00362a068] [0x9d17b0 0x9d17b0] 0xc0024a32c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:53:18.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:53:18.814: INFO: rc: 1
Jul 29 02:53:18.814: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020364e0 exit status 1 <nil> <nil> true [0xc002376030 0xc0023761d8 0xc002376220] [0xc002376030 0xc0023761d8 0xc002376220] [0xc0023761a0 0xc0023761f8] [0x9d17b0 0x9d17b0] 0xc0023a36e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:53:28.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:53:28.976: INFO: rc: 1
Jul 29 02:53:28.976: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020368a0 exit status 1 <nil> <nil> true [0xc002376258 0xc002376348 0xc002376478] [0xc002376258 0xc002376348 0xc002376478] [0xc0023762c0 0xc002376400] [0x9d17b0 0x9d17b0] 0xc002096d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:53:38.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:53:39.134: INFO: rc: 1
Jul 29 02:53:39.134: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002036c60 exit status 1 <nil> <nil> true [0xc002376518 0xc0023765a0 0xc002376638] [0xc002376518 0xc0023765a0 0xc002376638] [0xc002376570 0xc002376620] [0x9d17b0 0x9d17b0] 0xc00216a060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:53:49.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:53:49.297: INFO: rc: 1
Jul 29 02:53:49.298: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002037020 exit status 1 <nil> <nil> true [0xc002376640 0xc002376680 0xc0023766c0] [0xc002376640 0xc002376680 0xc0023766c0] [0xc002376650 0xc0023766b0] [0x9d17b0 0x9d17b0] 0xc00216b3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:53:59.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:53:59.472: INFO: rc: 1
Jul 29 02:53:59.472: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020373b0 exit status 1 <nil> <nil> true [0xc0023766d0 0xc002376718 0xc002376768] [0xc0023766d0 0xc002376718 0xc002376768] [0xc002376710 0xc002376730] [0x9d17b0 0x9d17b0] 0xc0036bc1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:54:09.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:54:09.632: INFO: rc: 1
Jul 29 02:54:09.633: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3e720 exit status 1 <nil> <nil> true [0xc00362a078 0xc00362a0b0 0xc00362a0e8] [0xc00362a078 0xc00362a0b0 0xc00362a0e8] [0xc00362a0a8 0xc00362a0c8] [0x9d17b0 0x9d17b0] 0xc001ad9320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:54:19.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:54:19.802: INFO: rc: 1
Jul 29 02:54:19.802: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3eae0 exit status 1 <nil> <nil> true [0xc00362a0f0 0xc00362a120 0xc00362a150] [0xc00362a0f0 0xc00362a120 0xc00362a150] [0xc00362a110 0xc00362a148] [0x9d17b0 0x9d17b0] 0xc00292c360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:54:29.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:54:29.962: INFO: rc: 1
Jul 29 02:54:29.962: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3eea0 exit status 1 <nil> <nil> true [0xc00362a178 0xc00362a1c8 0xc00362a200] [0xc00362a178 0xc00362a1c8 0xc00362a200] [0xc00362a1a8 0xc00362a1e8] [0x9d17b0 0x9d17b0] 0xc00292cc60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:54:39.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:54:40.192: INFO: rc: 1
Jul 29 02:54:40.193: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002037740 exit status 1 <nil> <nil> true [0xc0023767a0 0xc0023767f8 0xc002376838] [0xc0023767a0 0xc0023767f8 0xc002376838] [0xc0023767e0 0xc002376820] [0x9d17b0 0x9d17b0] 0xc0036bc660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:54:50.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:54:50.431: INFO: rc: 1
Jul 29 02:54:50.431: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002037ad0 exit status 1 <nil> <nil> true [0xc002376858 0xc002376880 0xc002376898] [0xc002376858 0xc002376880 0xc002376898] [0xc002376878 0xc002376890] [0x9d17b0 0x9d17b0] 0xc0036bcf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:55:00.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:55:00.582: INFO: rc: 1
Jul 29 02:55:00.582: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002037e60 exit status 1 <nil> <nil> true [0xc0023768b0 0xc002376900 0xc002376958] [0xc0023768b0 0xc002376900 0xc002376958] [0xc0023768e8 0xc002376938] [0x9d17b0 0x9d17b0] 0xc0036bd800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:55:10.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:55:10.738: INFO: rc: 1
Jul 29 02:55:10.738: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020364b0 exit status 1 <nil> <nil> true [0xc002376160 0xc0023761f0 0xc002376258] [0xc002376160 0xc0023761f0 0xc002376258] [0xc0023761d8 0xc002376220] [0x9d17b0 0x9d17b0] 0xc001ad8960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:55:20.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:55:20.899: INFO: rc: 1
Jul 29 02:55:20.899: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3e330 exit status 1 <nil> <nil> true [0xc00362a000 0xc00362a038 0xc00362a068] [0xc00362a000 0xc00362a038 0xc00362a068] [0xc00362a030 0xc00362a050] [0x9d17b0 0x9d17b0] 0xc00216ae40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:55:30.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:55:31.065: INFO: rc: 1
Jul 29 02:55:31.065: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b3e6f0 exit status 1 <nil> <nil> true [0xc00362a070 0xc00362a0a8 0xc00362a0c8] [0xc00362a070 0xc00362a0a8 0xc00362a0c8] [0xc00362a090 0xc00362a0b8] [0x9d17b0 0x9d17b0] 0xc002096420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 29 02:55:41.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 exec --namespace=statefulset-327 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 29 02:55:41.225: INFO: rc: 1
Jul 29 02:55:41.226: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Jul 29 02:55:41.226: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 29 02:55:41.248: INFO: Deleting all statefulset in ns statefulset-327
Jul 29 02:55:41.255: INFO: Scaling statefulset ss to 0
Jul 29 02:55:41.272: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 02:55:41.277: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:55:41.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-327" for this suite.
Jul 29 02:55:47.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:55:47.528: INFO: namespace statefulset-327 deletion completed in 6.200348299s

• [SLOW TEST:366.137 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:55:47.529: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul 29 02:55:51.683: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-518829307 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 29 02:56:06.835: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:56:06.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3419" for this suite.
Jul 29 02:56:12.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:56:13.129: INFO: namespace pods-3419 deletion completed in 6.276657798s

• [SLOW TEST:25.601 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:56:13.130: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-44f68431-8bc7-4938-9f08-fe131f6bf806
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-44f68431-8bc7-4938-9f08-fe131f6bf806
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:56:19.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7978" for this suite.
Jul 29 02:56:41.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:56:41.686: INFO: namespace configmap-7978 deletion completed in 22.224861995s

• [SLOW TEST:28.557 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:56:41.689: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 29 02:56:41.763: INFO: Waiting up to 5m0s for pod "pod-c8388b1e-2bb7-48f3-af08-93dd431c38c6" in namespace "emptydir-5220" to be "success or failure"
Jul 29 02:56:41.770: INFO: Pod "pod-c8388b1e-2bb7-48f3-af08-93dd431c38c6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.170378ms
Jul 29 02:56:43.776: INFO: Pod "pod-c8388b1e-2bb7-48f3-af08-93dd431c38c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013067918s
Jul 29 02:56:45.782: INFO: Pod "pod-c8388b1e-2bb7-48f3-af08-93dd431c38c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018750405s
STEP: Saw pod success
Jul 29 02:56:45.782: INFO: Pod "pod-c8388b1e-2bb7-48f3-af08-93dd431c38c6" satisfied condition "success or failure"
Jul 29 02:56:45.787: INFO: Trying to get logs from node conformance1 pod pod-c8388b1e-2bb7-48f3-af08-93dd431c38c6 container test-container: <nil>
STEP: delete the pod
Jul 29 02:56:45.836: INFO: Waiting for pod pod-c8388b1e-2bb7-48f3-af08-93dd431c38c6 to disappear
Jul 29 02:56:45.842: INFO: Pod pod-c8388b1e-2bb7-48f3-af08-93dd431c38c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:56:45.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5220" for this suite.
Jul 29 02:56:51.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:56:52.076: INFO: namespace emptydir-5220 deletion completed in 6.227170914s

• [SLOW TEST:10.388 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:56:52.077: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:56:52.214: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cfa25e7c-58b3-4297-b1e5-266760ca9a63" in namespace "projected-8614" to be "success or failure"
Jul 29 02:56:52.239: INFO: Pod "downwardapi-volume-cfa25e7c-58b3-4297-b1e5-266760ca9a63": Phase="Pending", Reason="", readiness=false. Elapsed: 24.23512ms
Jul 29 02:56:54.247: INFO: Pod "downwardapi-volume-cfa25e7c-58b3-4297-b1e5-266760ca9a63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032393833s
Jul 29 02:56:56.254: INFO: Pod "downwardapi-volume-cfa25e7c-58b3-4297-b1e5-266760ca9a63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038632357s
STEP: Saw pod success
Jul 29 02:56:56.254: INFO: Pod "downwardapi-volume-cfa25e7c-58b3-4297-b1e5-266760ca9a63" satisfied condition "success or failure"
Jul 29 02:56:56.259: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-cfa25e7c-58b3-4297-b1e5-266760ca9a63 container client-container: <nil>
STEP: delete the pod
Jul 29 02:56:56.298: INFO: Waiting for pod downwardapi-volume-cfa25e7c-58b3-4297-b1e5-266760ca9a63 to disappear
Jul 29 02:56:56.305: INFO: Pod downwardapi-volume-cfa25e7c-58b3-4297-b1e5-266760ca9a63 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:56:56.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8614" for this suite.
Jul 29 02:57:02.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:57:02.547: INFO: namespace projected-8614 deletion completed in 6.235063048s

• [SLOW TEST:10.470 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:57:02.550: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 02:57:02.698: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af27ff71-8e13-4921-9c0c-9e459f06a202" in namespace "downward-api-9602" to be "success or failure"
Jul 29 02:57:02.704: INFO: Pod "downwardapi-volume-af27ff71-8e13-4921-9c0c-9e459f06a202": Phase="Pending", Reason="", readiness=false. Elapsed: 5.434483ms
Jul 29 02:57:04.709: INFO: Pod "downwardapi-volume-af27ff71-8e13-4921-9c0c-9e459f06a202": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010877359s
Jul 29 02:57:06.715: INFO: Pod "downwardapi-volume-af27ff71-8e13-4921-9c0c-9e459f06a202": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016983261s
STEP: Saw pod success
Jul 29 02:57:06.715: INFO: Pod "downwardapi-volume-af27ff71-8e13-4921-9c0c-9e459f06a202" satisfied condition "success or failure"
Jul 29 02:57:06.720: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-af27ff71-8e13-4921-9c0c-9e459f06a202 container client-container: <nil>
STEP: delete the pod
Jul 29 02:57:06.763: INFO: Waiting for pod downwardapi-volume-af27ff71-8e13-4921-9c0c-9e459f06a202 to disappear
Jul 29 02:57:06.773: INFO: Pod downwardapi-volume-af27ff71-8e13-4921-9c0c-9e459f06a202 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:57:06.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9602" for this suite.
Jul 29 02:57:12.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:57:13.066: INFO: namespace downward-api-9602 deletion completed in 6.283381873s

• [SLOW TEST:10.516 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:57:13.068: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 29 02:57:13.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5857'
Jul 29 02:57:13.313: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 29 02:57:13.313: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul 29 02:57:13.327: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jul 29 02:57:13.358: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul 29 02:57:13.387: INFO: scanned /root for discovery docs: <nil>
Jul 29 02:57:13.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5857'
Jul 29 02:57:29.426: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 29 02:57:29.426: INFO: stdout: "Created e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06\nScaling up e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul 29 02:57:29.426: INFO: stdout: "Created e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06\nScaling up e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 29 02:57:29.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5857'
Jul 29 02:57:29.577: INFO: stderr: ""
Jul 29 02:57:29.578: INFO: stdout: "e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06-l6qmf "
Jul 29 02:57:29.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06-l6qmf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5857'
Jul 29 02:57:29.775: INFO: stderr: ""
Jul 29 02:57:29.775: INFO: stdout: "true"
Jul 29 02:57:29.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06-l6qmf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5857'
Jul 29 02:57:29.916: INFO: stderr: ""
Jul 29 02:57:29.916: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul 29 02:57:29.916: INFO: e2e-test-nginx-rc-e657b5959f43385db099eaa3d269bb06-l6qmf is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jul 29 02:57:29.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete rc e2e-test-nginx-rc --namespace=kubectl-5857'
Jul 29 02:57:30.112: INFO: stderr: ""
Jul 29 02:57:30.112: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:57:30.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5857" for this suite.
Jul 29 02:57:52.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:57:52.362: INFO: namespace kubectl-5857 deletion completed in 22.2325349s

• [SLOW TEST:39.295 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:57:52.377: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 29 02:57:55.493: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:57:55.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6241" for this suite.
Jul 29 02:58:01.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:58:01.736: INFO: namespace container-runtime-6241 deletion completed in 6.203406209s

• [SLOW TEST:9.359 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:58:01.738: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-aef84d22-3002-49ce-bd1d-a5bd693b64b0
STEP: Creating a pod to test consume configMaps
Jul 29 02:58:01.821: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bbdc3490-ce3e-4597-8f30-450726ede297" in namespace "projected-9354" to be "success or failure"
Jul 29 02:58:01.846: INFO: Pod "pod-projected-configmaps-bbdc3490-ce3e-4597-8f30-450726ede297": Phase="Pending", Reason="", readiness=false. Elapsed: 24.794022ms
Jul 29 02:58:03.851: INFO: Pod "pod-projected-configmaps-bbdc3490-ce3e-4597-8f30-450726ede297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030182415s
Jul 29 02:58:05.861: INFO: Pod "pod-projected-configmaps-bbdc3490-ce3e-4597-8f30-450726ede297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039972379s
STEP: Saw pod success
Jul 29 02:58:05.861: INFO: Pod "pod-projected-configmaps-bbdc3490-ce3e-4597-8f30-450726ede297" satisfied condition "success or failure"
Jul 29 02:58:05.866: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-bbdc3490-ce3e-4597-8f30-450726ede297 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 02:58:05.910: INFO: Waiting for pod pod-projected-configmaps-bbdc3490-ce3e-4597-8f30-450726ede297 to disappear
Jul 29 02:58:05.922: INFO: Pod pod-projected-configmaps-bbdc3490-ce3e-4597-8f30-450726ede297 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:58:05.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9354" for this suite.
Jul 29 02:58:12.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:58:12.215: INFO: namespace projected-9354 deletion completed in 6.233598971s

• [SLOW TEST:10.477 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:58:12.215: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jul 29 02:58:12.290: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 29 02:58:12.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-7599'
Jul 29 02:58:12.731: INFO: stderr: ""
Jul 29 02:58:12.731: INFO: stdout: "service/redis-slave created\n"
Jul 29 02:58:12.731: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 29 02:58:12.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-7599'
Jul 29 02:58:13.157: INFO: stderr: ""
Jul 29 02:58:13.157: INFO: stdout: "service/redis-master created\n"
Jul 29 02:58:13.158: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 29 02:58:13.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-7599'
Jul 29 02:58:13.663: INFO: stderr: ""
Jul 29 02:58:13.663: INFO: stdout: "service/frontend created\n"
Jul 29 02:58:13.663: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 29 02:58:13.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-7599'
Jul 29 02:58:14.149: INFO: stderr: ""
Jul 29 02:58:14.149: INFO: stdout: "deployment.apps/frontend created\n"
Jul 29 02:58:14.150: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 29 02:58:14.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-7599'
Jul 29 02:58:14.689: INFO: stderr: ""
Jul 29 02:58:14.689: INFO: stdout: "deployment.apps/redis-master created\n"
Jul 29 02:58:14.690: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 29 02:58:14.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-7599'
Jul 29 02:58:15.168: INFO: stderr: ""
Jul 29 02:58:15.168: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul 29 02:58:15.168: INFO: Waiting for all frontend pods to be Running.
Jul 29 02:58:25.220: INFO: Waiting for frontend to serve content.
Jul 29 02:58:25.769: INFO: Trying to add a new entry to the guestbook.
Jul 29 02:58:25.809: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 29 02:58:25.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-7599'
Jul 29 02:58:26.048: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 02:58:26.048: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 29 02:58:26.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-7599'
Jul 29 02:58:26.263: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 02:58:26.263: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 29 02:58:26.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-7599'
Jul 29 02:58:26.481: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 02:58:26.481: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 29 02:58:26.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-7599'
Jul 29 02:58:26.641: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 02:58:26.641: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 29 02:58:26.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-7599'
Jul 29 02:58:26.813: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 02:58:26.813: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 29 02:58:26.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-7599'
Jul 29 02:58:27.185: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 02:58:27.185: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:58:27.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7599" for this suite.
Jul 29 02:59:13.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:59:13.477: INFO: namespace kubectl-7599 deletion completed in 46.274503002s

• [SLOW TEST:61.261 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:59:13.477: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-5612/secret-test-8c7a4647-f01e-46c8-951f-3a59de1cff85
STEP: Creating a pod to test consume secrets
Jul 29 02:59:13.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-a55a68c2-1876-43bf-8434-592c6f31f137" in namespace "secrets-5612" to be "success or failure"
Jul 29 02:59:13.559: INFO: Pod "pod-configmaps-a55a68c2-1876-43bf-8434-592c6f31f137": Phase="Pending", Reason="", readiness=false. Elapsed: 9.409346ms
Jul 29 02:59:15.564: INFO: Pod "pod-configmaps-a55a68c2-1876-43bf-8434-592c6f31f137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014505756s
Jul 29 02:59:17.572: INFO: Pod "pod-configmaps-a55a68c2-1876-43bf-8434-592c6f31f137": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022031373s
STEP: Saw pod success
Jul 29 02:59:17.572: INFO: Pod "pod-configmaps-a55a68c2-1876-43bf-8434-592c6f31f137" satisfied condition "success or failure"
Jul 29 02:59:17.577: INFO: Trying to get logs from node conformance1 pod pod-configmaps-a55a68c2-1876-43bf-8434-592c6f31f137 container env-test: <nil>
STEP: delete the pod
Jul 29 02:59:17.705: INFO: Waiting for pod pod-configmaps-a55a68c2-1876-43bf-8434-592c6f31f137 to disappear
Jul 29 02:59:17.722: INFO: Pod pod-configmaps-a55a68c2-1876-43bf-8434-592c6f31f137 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:59:17.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5612" for this suite.
Jul 29 02:59:23.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:59:23.938: INFO: namespace secrets-5612 deletion completed in 6.208585865s

• [SLOW TEST:10.461 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:59:23.940: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-7e5409b9-8c65-4870-9770-4d6025d2660e
STEP: Creating a pod to test consume configMaps
Jul 29 02:59:24.053: INFO: Waiting up to 5m0s for pod "pod-configmaps-25609ccc-ddf0-4df9-8437-66326ec0cea2" in namespace "configmap-506" to be "success or failure"
Jul 29 02:59:24.077: INFO: Pod "pod-configmaps-25609ccc-ddf0-4df9-8437-66326ec0cea2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.733839ms
Jul 29 02:59:26.085: INFO: Pod "pod-configmaps-25609ccc-ddf0-4df9-8437-66326ec0cea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031679199s
Jul 29 02:59:28.094: INFO: Pod "pod-configmaps-25609ccc-ddf0-4df9-8437-66326ec0cea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040525368s
STEP: Saw pod success
Jul 29 02:59:28.094: INFO: Pod "pod-configmaps-25609ccc-ddf0-4df9-8437-66326ec0cea2" satisfied condition "success or failure"
Jul 29 02:59:28.101: INFO: Trying to get logs from node conformance1 pod pod-configmaps-25609ccc-ddf0-4df9-8437-66326ec0cea2 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 02:59:28.151: INFO: Waiting for pod pod-configmaps-25609ccc-ddf0-4df9-8437-66326ec0cea2 to disappear
Jul 29 02:59:28.157: INFO: Pod pod-configmaps-25609ccc-ddf0-4df9-8437-66326ec0cea2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:59:28.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-506" for this suite.
Jul 29 02:59:34.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:59:34.358: INFO: namespace configmap-506 deletion completed in 6.18979763s

• [SLOW TEST:10.418 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:59:34.368: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-290f0d4f-83d7-4b34-8c21-6623dc317ea8
STEP: Creating a pod to test consume secrets
Jul 29 02:59:34.448: INFO: Waiting up to 5m0s for pod "pod-secrets-44899507-5e48-476b-a73a-73d8506928ae" in namespace "secrets-2609" to be "success or failure"
Jul 29 02:59:34.466: INFO: Pod "pod-secrets-44899507-5e48-476b-a73a-73d8506928ae": Phase="Pending", Reason="", readiness=false. Elapsed: 17.473369ms
Jul 29 02:59:36.473: INFO: Pod "pod-secrets-44899507-5e48-476b-a73a-73d8506928ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023800941s
Jul 29 02:59:38.480: INFO: Pod "pod-secrets-44899507-5e48-476b-a73a-73d8506928ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030828032s
STEP: Saw pod success
Jul 29 02:59:38.480: INFO: Pod "pod-secrets-44899507-5e48-476b-a73a-73d8506928ae" satisfied condition "success or failure"
Jul 29 02:59:38.485: INFO: Trying to get logs from node conformance0 pod pod-secrets-44899507-5e48-476b-a73a-73d8506928ae container secret-volume-test: <nil>
STEP: delete the pod
Jul 29 02:59:38.524: INFO: Waiting for pod pod-secrets-44899507-5e48-476b-a73a-73d8506928ae to disappear
Jul 29 02:59:38.530: INFO: Pod pod-secrets-44899507-5e48-476b-a73a-73d8506928ae no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:59:38.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2609" for this suite.
Jul 29 02:59:44.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:59:44.822: INFO: namespace secrets-2609 deletion completed in 6.286243452s

• [SLOW TEST:10.455 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:59:44.828: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 29 02:59:44.935: INFO: Waiting up to 5m0s for pod "pod-4dfd1706-915c-453a-aaa0-c6fc21abd7e4" in namespace "emptydir-3355" to be "success or failure"
Jul 29 02:59:44.948: INFO: Pod "pod-4dfd1706-915c-453a-aaa0-c6fc21abd7e4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.715631ms
Jul 29 02:59:46.961: INFO: Pod "pod-4dfd1706-915c-453a-aaa0-c6fc21abd7e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025356631s
Jul 29 02:59:48.967: INFO: Pod "pod-4dfd1706-915c-453a-aaa0-c6fc21abd7e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03120265s
STEP: Saw pod success
Jul 29 02:59:48.967: INFO: Pod "pod-4dfd1706-915c-453a-aaa0-c6fc21abd7e4" satisfied condition "success or failure"
Jul 29 02:59:48.971: INFO: Trying to get logs from node conformance1 pod pod-4dfd1706-915c-453a-aaa0-c6fc21abd7e4 container test-container: <nil>
STEP: delete the pod
Jul 29 02:59:49.014: INFO: Waiting for pod pod-4dfd1706-915c-453a-aaa0-c6fc21abd7e4 to disappear
Jul 29 02:59:49.022: INFO: Pod pod-4dfd1706-915c-453a-aaa0-c6fc21abd7e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:59:49.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3355" for this suite.
Jul 29 02:59:55.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 02:59:55.403: INFO: namespace emptydir-3355 deletion completed in 6.371428031s

• [SLOW TEST:10.576 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 02:59:55.404: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jul 29 02:59:55.509: INFO: Waiting up to 5m0s for pod "var-expansion-e3284a45-d0a3-4546-83e7-3e36bf354f05" in namespace "var-expansion-9937" to be "success or failure"
Jul 29 02:59:55.530: INFO: Pod "var-expansion-e3284a45-d0a3-4546-83e7-3e36bf354f05": Phase="Pending", Reason="", readiness=false. Elapsed: 21.28256ms
Jul 29 02:59:57.537: INFO: Pod "var-expansion-e3284a45-d0a3-4546-83e7-3e36bf354f05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027530285s
Jul 29 02:59:59.542: INFO: Pod "var-expansion-e3284a45-d0a3-4546-83e7-3e36bf354f05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032791351s
STEP: Saw pod success
Jul 29 02:59:59.542: INFO: Pod "var-expansion-e3284a45-d0a3-4546-83e7-3e36bf354f05" satisfied condition "success or failure"
Jul 29 02:59:59.546: INFO: Trying to get logs from node conformance0 pod var-expansion-e3284a45-d0a3-4546-83e7-3e36bf354f05 container dapi-container: <nil>
STEP: delete the pod
Jul 29 02:59:59.585: INFO: Waiting for pod var-expansion-e3284a45-d0a3-4546-83e7-3e36bf354f05 to disappear
Jul 29 02:59:59.593: INFO: Pod var-expansion-e3284a45-d0a3-4546-83e7-3e36bf354f05 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 02:59:59.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9937" for this suite.
Jul 29 03:00:05.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:00:05.818: INFO: namespace var-expansion-9937 deletion completed in 6.216829803s

• [SLOW TEST:10.414 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:00:05.819: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 03:00:05.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6fd8bdc0-4eb4-41f0-9ed8-fdf613b53153" in namespace "projected-7325" to be "success or failure"
Jul 29 03:00:05.946: INFO: Pod "downwardapi-volume-6fd8bdc0-4eb4-41f0-9ed8-fdf613b53153": Phase="Pending", Reason="", readiness=false. Elapsed: 35.729067ms
Jul 29 03:00:07.971: INFO: Pod "downwardapi-volume-6fd8bdc0-4eb4-41f0-9ed8-fdf613b53153": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061419681s
Jul 29 03:00:09.977: INFO: Pod "downwardapi-volume-6fd8bdc0-4eb4-41f0-9ed8-fdf613b53153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067151047s
STEP: Saw pod success
Jul 29 03:00:09.977: INFO: Pod "downwardapi-volume-6fd8bdc0-4eb4-41f0-9ed8-fdf613b53153" satisfied condition "success or failure"
Jul 29 03:00:09.982: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-6fd8bdc0-4eb4-41f0-9ed8-fdf613b53153 container client-container: <nil>
STEP: delete the pod
Jul 29 03:00:10.019: INFO: Waiting for pod downwardapi-volume-6fd8bdc0-4eb4-41f0-9ed8-fdf613b53153 to disappear
Jul 29 03:00:10.035: INFO: Pod downwardapi-volume-6fd8bdc0-4eb4-41f0-9ed8-fdf613b53153 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:00:10.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7325" for this suite.
Jul 29 03:00:16.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:00:16.283: INFO: namespace projected-7325 deletion completed in 6.224520596s

• [SLOW TEST:10.464 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:00:16.285: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 03:00:16.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15894c39-6a94-4a0d-8094-af6f1a3a79d0" in namespace "downward-api-3365" to be "success or failure"
Jul 29 03:00:16.385: INFO: Pod "downwardapi-volume-15894c39-6a94-4a0d-8094-af6f1a3a79d0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.683724ms
Jul 29 03:00:18.391: INFO: Pod "downwardapi-volume-15894c39-6a94-4a0d-8094-af6f1a3a79d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023824051s
Jul 29 03:00:20.397: INFO: Pod "downwardapi-volume-15894c39-6a94-4a0d-8094-af6f1a3a79d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029809822s
STEP: Saw pod success
Jul 29 03:00:20.397: INFO: Pod "downwardapi-volume-15894c39-6a94-4a0d-8094-af6f1a3a79d0" satisfied condition "success or failure"
Jul 29 03:00:20.402: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-15894c39-6a94-4a0d-8094-af6f1a3a79d0 container client-container: <nil>
STEP: delete the pod
Jul 29 03:00:20.470: INFO: Waiting for pod downwardapi-volume-15894c39-6a94-4a0d-8094-af6f1a3a79d0 to disappear
Jul 29 03:00:20.476: INFO: Pod downwardapi-volume-15894c39-6a94-4a0d-8094-af6f1a3a79d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:00:20.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3365" for this suite.
Jul 29 03:00:26.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:00:26.715: INFO: namespace downward-api-3365 deletion completed in 6.231896937s

• [SLOW TEST:10.430 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:00:26.720: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 29 03:00:29.896: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:00:29.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2599" for this suite.
Jul 29 03:00:35.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:00:36.168: INFO: namespace container-runtime-2599 deletion completed in 6.226161114s

• [SLOW TEST:9.448 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:00:36.169: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 03:00:36.241: INFO: Creating ReplicaSet my-hostname-basic-3da29ddd-8d39-4613-b60f-a0b07a1dc75e
Jul 29 03:00:36.258: INFO: Pod name my-hostname-basic-3da29ddd-8d39-4613-b60f-a0b07a1dc75e: Found 0 pods out of 1
Jul 29 03:00:41.265: INFO: Pod name my-hostname-basic-3da29ddd-8d39-4613-b60f-a0b07a1dc75e: Found 1 pods out of 1
Jul 29 03:00:41.265: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3da29ddd-8d39-4613-b60f-a0b07a1dc75e" is running
Jul 29 03:00:41.270: INFO: Pod "my-hostname-basic-3da29ddd-8d39-4613-b60f-a0b07a1dc75e-j92st" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-29 03:00:36 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-29 03:00:40 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-29 03:00:40 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-29 03:00:36 +0000 UTC Reason: Message:}])
Jul 29 03:00:41.270: INFO: Trying to dial the pod
Jul 29 03:00:46.293: INFO: Controller my-hostname-basic-3da29ddd-8d39-4613-b60f-a0b07a1dc75e: Got expected result from replica 1 [my-hostname-basic-3da29ddd-8d39-4613-b60f-a0b07a1dc75e-j92st]: "my-hostname-basic-3da29ddd-8d39-4613-b60f-a0b07a1dc75e-j92st", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:00:46.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2400" for this suite.
Jul 29 03:00:52.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:00:52.514: INFO: namespace replicaset-2400 deletion completed in 6.214177587s

• [SLOW TEST:16.345 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:00:52.514: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jul 29 03:00:52.603: INFO: Waiting up to 5m0s for pod "var-expansion-32ccd14a-d901-4e5e-a3d2-0972081e409c" in namespace "var-expansion-9682" to be "success or failure"
Jul 29 03:00:52.617: INFO: Pod "var-expansion-32ccd14a-d901-4e5e-a3d2-0972081e409c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.325685ms
Jul 29 03:00:54.624: INFO: Pod "var-expansion-32ccd14a-d901-4e5e-a3d2-0972081e409c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021250014s
Jul 29 03:00:56.630: INFO: Pod "var-expansion-32ccd14a-d901-4e5e-a3d2-0972081e409c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026426979s
STEP: Saw pod success
Jul 29 03:00:56.630: INFO: Pod "var-expansion-32ccd14a-d901-4e5e-a3d2-0972081e409c" satisfied condition "success or failure"
Jul 29 03:00:56.634: INFO: Trying to get logs from node conformance1 pod var-expansion-32ccd14a-d901-4e5e-a3d2-0972081e409c container dapi-container: <nil>
STEP: delete the pod
Jul 29 03:00:56.679: INFO: Waiting for pod var-expansion-32ccd14a-d901-4e5e-a3d2-0972081e409c to disappear
Jul 29 03:00:56.684: INFO: Pod var-expansion-32ccd14a-d901-4e5e-a3d2-0972081e409c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:00:56.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9682" for this suite.
Jul 29 03:01:02.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:01:02.936: INFO: namespace var-expansion-9682 deletion completed in 6.245107389s

• [SLOW TEST:10.422 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:01:02.937: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e653d306-a7ef-4730-a88c-66571ba8d64b
STEP: Creating a pod to test consume configMaps
Jul 29 03:01:03.078: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cd182e93-05f9-4512-9600-dbcce80fdff5" in namespace "projected-6336" to be "success or failure"
Jul 29 03:01:03.087: INFO: Pod "pod-projected-configmaps-cd182e93-05f9-4512-9600-dbcce80fdff5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109896ms
Jul 29 03:01:05.092: INFO: Pod "pod-projected-configmaps-cd182e93-05f9-4512-9600-dbcce80fdff5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013672972s
Jul 29 03:01:07.098: INFO: Pod "pod-projected-configmaps-cd182e93-05f9-4512-9600-dbcce80fdff5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019335753s
STEP: Saw pod success
Jul 29 03:01:07.098: INFO: Pod "pod-projected-configmaps-cd182e93-05f9-4512-9600-dbcce80fdff5" satisfied condition "success or failure"
Jul 29 03:01:07.104: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-cd182e93-05f9-4512-9600-dbcce80fdff5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 03:01:07.139: INFO: Waiting for pod pod-projected-configmaps-cd182e93-05f9-4512-9600-dbcce80fdff5 to disappear
Jul 29 03:01:07.146: INFO: Pod pod-projected-configmaps-cd182e93-05f9-4512-9600-dbcce80fdff5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:01:07.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6336" for this suite.
Jul 29 03:01:13.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:01:13.345: INFO: namespace projected-6336 deletion completed in 6.191864053s

• [SLOW TEST:10.409 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:01:13.346: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 29 03:01:21.495: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:21.500: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:23.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:23.506: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:25.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:25.515: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:27.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:27.507: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:29.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:29.506: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:31.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:31.506: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:33.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:33.510: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:35.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:35.509: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:37.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:37.506: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:39.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:39.506: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:41.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:41.507: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:43.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:43.505: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:45.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:45.505: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 03:01:47.501: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 03:01:47.507: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:01:47.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3964" for this suite.
Jul 29 03:02:11.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:02:11.713: INFO: namespace container-lifecycle-hook-3964 deletion completed in 24.198280756s

• [SLOW TEST:58.368 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:02:11.719: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:03:11.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8953" for this suite.
Jul 29 03:03:33.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:03:34.054: INFO: namespace container-probe-8953 deletion completed in 22.22733055s

• [SLOW TEST:82.336 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:03:34.055: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-f6ac023e-f29c-4fe3-a8b9-16b27ecf810a
STEP: Creating a pod to test consume secrets
Jul 29 03:03:34.146: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3ec70302-bb8e-4a00-b782-9e0b95866e43" in namespace "projected-4062" to be "success or failure"
Jul 29 03:03:34.164: INFO: Pod "pod-projected-secrets-3ec70302-bb8e-4a00-b782-9e0b95866e43": Phase="Pending", Reason="", readiness=false. Elapsed: 18.174854ms
Jul 29 03:03:36.171: INFO: Pod "pod-projected-secrets-3ec70302-bb8e-4a00-b782-9e0b95866e43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025024411s
Jul 29 03:03:38.176: INFO: Pod "pod-projected-secrets-3ec70302-bb8e-4a00-b782-9e0b95866e43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030212227s
STEP: Saw pod success
Jul 29 03:03:38.176: INFO: Pod "pod-projected-secrets-3ec70302-bb8e-4a00-b782-9e0b95866e43" satisfied condition "success or failure"
Jul 29 03:03:38.181: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-3ec70302-bb8e-4a00-b782-9e0b95866e43 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 29 03:03:38.220: INFO: Waiting for pod pod-projected-secrets-3ec70302-bb8e-4a00-b782-9e0b95866e43 to disappear
Jul 29 03:03:38.226: INFO: Pod pod-projected-secrets-3ec70302-bb8e-4a00-b782-9e0b95866e43 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:03:38.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4062" for this suite.
Jul 29 03:03:44.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:03:44.476: INFO: namespace projected-4062 deletion completed in 6.241981643s

• [SLOW TEST:10.422 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:03:44.478: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jul 29 03:03:44.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-7297'
Jul 29 03:03:45.166: INFO: stderr: ""
Jul 29 03:03:45.166: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 29 03:03:45.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7297'
Jul 29 03:03:45.345: INFO: stderr: ""
Jul 29 03:03:45.345: INFO: stdout: "update-demo-nautilus-mg8tx update-demo-nautilus-pk6ct "
Jul 29 03:03:45.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-mg8tx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7297'
Jul 29 03:03:45.539: INFO: stderr: ""
Jul 29 03:03:45.539: INFO: stdout: ""
Jul 29 03:03:45.539: INFO: update-demo-nautilus-mg8tx is created but not running
Jul 29 03:03:50.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7297'
Jul 29 03:03:50.689: INFO: stderr: ""
Jul 29 03:03:50.689: INFO: stdout: "update-demo-nautilus-mg8tx update-demo-nautilus-pk6ct "
Jul 29 03:03:50.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-mg8tx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7297'
Jul 29 03:03:50.830: INFO: stderr: ""
Jul 29 03:03:50.830: INFO: stdout: "true"
Jul 29 03:03:50.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-mg8tx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7297'
Jul 29 03:03:50.988: INFO: stderr: ""
Jul 29 03:03:50.988: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 03:03:50.988: INFO: validating pod update-demo-nautilus-mg8tx
Jul 29 03:03:50.996: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 03:03:50.996: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 03:03:50.996: INFO: update-demo-nautilus-mg8tx is verified up and running
Jul 29 03:03:50.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-pk6ct -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7297'
Jul 29 03:03:51.135: INFO: stderr: ""
Jul 29 03:03:51.136: INFO: stdout: "true"
Jul 29 03:03:51.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-pk6ct -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7297'
Jul 29 03:03:51.285: INFO: stderr: ""
Jul 29 03:03:51.285: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 03:03:51.285: INFO: validating pod update-demo-nautilus-pk6ct
Jul 29 03:03:51.294: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 03:03:51.294: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 03:03:51.294: INFO: update-demo-nautilus-pk6ct is verified up and running
STEP: rolling-update to new replication controller
Jul 29 03:03:51.304: INFO: scanned /root for discovery docs: <nil>
Jul 29 03:03:51.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7297'
Jul 29 03:04:14.243: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 29 03:04:14.243: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 29 03:04:14.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7297'
Jul 29 03:04:14.402: INFO: stderr: ""
Jul 29 03:04:14.402: INFO: stdout: "update-demo-kitten-qh6cv update-demo-kitten-rxgjp "
Jul 29 03:04:14.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-kitten-qh6cv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7297'
Jul 29 03:04:14.548: INFO: stderr: ""
Jul 29 03:04:14.548: INFO: stdout: "true"
Jul 29 03:04:14.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-kitten-qh6cv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7297'
Jul 29 03:04:14.711: INFO: stderr: ""
Jul 29 03:04:14.712: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 29 03:04:14.712: INFO: validating pod update-demo-kitten-qh6cv
Jul 29 03:04:14.747: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 29 03:04:14.747: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 29 03:04:14.747: INFO: update-demo-kitten-qh6cv is verified up and running
Jul 29 03:04:14.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-kitten-rxgjp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7297'
Jul 29 03:04:14.891: INFO: stderr: ""
Jul 29 03:04:14.891: INFO: stdout: "true"
Jul 29 03:04:14.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-kitten-rxgjp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7297'
Jul 29 03:04:15.042: INFO: stderr: ""
Jul 29 03:04:15.042: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 29 03:04:15.042: INFO: validating pod update-demo-kitten-rxgjp
Jul 29 03:04:15.085: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 29 03:04:15.085: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 29 03:04:15.085: INFO: update-demo-kitten-rxgjp is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:04:15.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7297" for this suite.
Jul 29 03:04:39.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:04:39.355: INFO: namespace kubectl-7297 deletion completed in 24.262196807s

• [SLOW TEST:54.877 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:04:39.371: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 29 03:04:39.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-5723'
Jul 29 03:04:39.843: INFO: stderr: ""
Jul 29 03:04:39.843: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 29 03:04:39.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5723'
Jul 29 03:04:40.044: INFO: stderr: ""
Jul 29 03:04:40.045: INFO: stdout: "update-demo-nautilus-jxz7w update-demo-nautilus-lzpjh "
Jul 29 03:04:40.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-jxz7w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:40.208: INFO: stderr: ""
Jul 29 03:04:40.208: INFO: stdout: ""
Jul 29 03:04:40.208: INFO: update-demo-nautilus-jxz7w is created but not running
Jul 29 03:04:45.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5723'
Jul 29 03:04:45.381: INFO: stderr: ""
Jul 29 03:04:45.381: INFO: stdout: "update-demo-nautilus-jxz7w update-demo-nautilus-lzpjh "
Jul 29 03:04:45.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-jxz7w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:45.625: INFO: stderr: ""
Jul 29 03:04:45.625: INFO: stdout: "true"
Jul 29 03:04:45.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-jxz7w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:45.791: INFO: stderr: ""
Jul 29 03:04:45.791: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 03:04:45.791: INFO: validating pod update-demo-nautilus-jxz7w
Jul 29 03:04:45.804: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 03:04:45.804: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 03:04:45.804: INFO: update-demo-nautilus-jxz7w is verified up and running
Jul 29 03:04:45.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-lzpjh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:45.976: INFO: stderr: ""
Jul 29 03:04:45.976: INFO: stdout: "true"
Jul 29 03:04:45.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-lzpjh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:46.123: INFO: stderr: ""
Jul 29 03:04:46.123: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 03:04:46.123: INFO: validating pod update-demo-nautilus-lzpjh
Jul 29 03:04:46.130: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 03:04:46.131: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 03:04:46.131: INFO: update-demo-nautilus-lzpjh is verified up and running
STEP: scaling down the replication controller
Jul 29 03:04:46.139: INFO: scanned /root for discovery docs: <nil>
Jul 29 03:04:46.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5723'
Jul 29 03:04:47.411: INFO: stderr: ""
Jul 29 03:04:47.411: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 29 03:04:47.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5723'
Jul 29 03:04:47.575: INFO: stderr: ""
Jul 29 03:04:47.575: INFO: stdout: "update-demo-nautilus-jxz7w update-demo-nautilus-lzpjh "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 29 03:04:52.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5723'
Jul 29 03:04:52.734: INFO: stderr: ""
Jul 29 03:04:52.734: INFO: stdout: "update-demo-nautilus-lzpjh "
Jul 29 03:04:52.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-lzpjh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:52.904: INFO: stderr: ""
Jul 29 03:04:52.904: INFO: stdout: "true"
Jul 29 03:04:52.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-lzpjh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:53.063: INFO: stderr: ""
Jul 29 03:04:53.063: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 03:04:53.063: INFO: validating pod update-demo-nautilus-lzpjh
Jul 29 03:04:53.069: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 03:04:53.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 03:04:53.069: INFO: update-demo-nautilus-lzpjh is verified up and running
STEP: scaling up the replication controller
Jul 29 03:04:53.077: INFO: scanned /root for discovery docs: <nil>
Jul 29 03:04:53.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5723'
Jul 29 03:04:54.296: INFO: stderr: ""
Jul 29 03:04:54.296: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 29 03:04:54.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5723'
Jul 29 03:04:54.445: INFO: stderr: ""
Jul 29 03:04:54.445: INFO: stdout: "update-demo-nautilus-lzpjh update-demo-nautilus-vwhwh "
Jul 29 03:04:54.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-lzpjh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:54.604: INFO: stderr: ""
Jul 29 03:04:54.604: INFO: stdout: "true"
Jul 29 03:04:54.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-lzpjh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:54.763: INFO: stderr: ""
Jul 29 03:04:54.764: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 03:04:54.764: INFO: validating pod update-demo-nautilus-lzpjh
Jul 29 03:04:54.770: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 03:04:54.770: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 03:04:54.770: INFO: update-demo-nautilus-lzpjh is verified up and running
Jul 29 03:04:54.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-vwhwh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:04:54.929: INFO: stderr: ""
Jul 29 03:04:54.929: INFO: stdout: ""
Jul 29 03:04:54.929: INFO: update-demo-nautilus-vwhwh is created but not running
Jul 29 03:04:59.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5723'
Jul 29 03:05:00.110: INFO: stderr: ""
Jul 29 03:05:00.110: INFO: stdout: "update-demo-nautilus-lzpjh update-demo-nautilus-vwhwh "
Jul 29 03:05:00.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-lzpjh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:05:00.265: INFO: stderr: ""
Jul 29 03:05:00.265: INFO: stdout: "true"
Jul 29 03:05:00.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-lzpjh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:05:00.419: INFO: stderr: ""
Jul 29 03:05:00.419: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 03:05:00.419: INFO: validating pod update-demo-nautilus-lzpjh
Jul 29 03:05:00.425: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 03:05:00.425: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 03:05:00.425: INFO: update-demo-nautilus-lzpjh is verified up and running
Jul 29 03:05:00.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-vwhwh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:05:00.565: INFO: stderr: ""
Jul 29 03:05:00.566: INFO: stdout: "true"
Jul 29 03:05:00.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods update-demo-nautilus-vwhwh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5723'
Jul 29 03:05:00.737: INFO: stderr: ""
Jul 29 03:05:00.737: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 29 03:05:00.737: INFO: validating pod update-demo-nautilus-vwhwh
Jul 29 03:05:00.758: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 03:05:00.758: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 03:05:00.758: INFO: update-demo-nautilus-vwhwh is verified up and running
STEP: using delete to clean up resources
Jul 29 03:05:00.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-5723'
Jul 29 03:05:00.931: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 03:05:00.931: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 29 03:05:00.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5723'
Jul 29 03:05:01.119: INFO: stderr: "No resources found.\n"
Jul 29 03:05:01.119: INFO: stdout: ""
Jul 29 03:05:01.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -l name=update-demo --namespace=kubectl-5723 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 29 03:05:01.299: INFO: stderr: ""
Jul 29 03:05:01.299: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:05:01.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5723" for this suite.
Jul 29 03:05:25.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:05:25.524: INFO: namespace kubectl-5723 deletion completed in 24.215167462s

• [SLOW TEST:46.154 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:05:25.528: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 29 03:05:25.599: INFO: Waiting up to 5m0s for pod "pod-13e16381-06a0-4034-b89f-70f6af6709ff" in namespace "emptydir-4954" to be "success or failure"
Jul 29 03:05:25.715: INFO: Pod "pod-13e16381-06a0-4034-b89f-70f6af6709ff": Phase="Pending", Reason="", readiness=false. Elapsed: 115.89905ms
Jul 29 03:05:27.721: INFO: Pod "pod-13e16381-06a0-4034-b89f-70f6af6709ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121963906s
Jul 29 03:05:29.728: INFO: Pod "pod-13e16381-06a0-4034-b89f-70f6af6709ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.129057918s
STEP: Saw pod success
Jul 29 03:05:29.728: INFO: Pod "pod-13e16381-06a0-4034-b89f-70f6af6709ff" satisfied condition "success or failure"
Jul 29 03:05:29.733: INFO: Trying to get logs from node conformance0 pod pod-13e16381-06a0-4034-b89f-70f6af6709ff container test-container: <nil>
STEP: delete the pod
Jul 29 03:05:29.768: INFO: Waiting for pod pod-13e16381-06a0-4034-b89f-70f6af6709ff to disappear
Jul 29 03:05:29.777: INFO: Pod pod-13e16381-06a0-4034-b89f-70f6af6709ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:05:29.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4954" for this suite.
Jul 29 03:05:35.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:05:35.982: INFO: namespace emptydir-4954 deletion completed in 6.186984508s

• [SLOW TEST:10.455 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:05:35.983: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0729 03:06:16.116135      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 29 03:06:16.116: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:06:16.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3556" for this suite.
Jul 29 03:06:24.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:06:24.979: INFO: namespace gc-3556 deletion completed in 8.856634009s

• [SLOW TEST:48.996 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:06:24.985: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jul 29 03:06:25.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-570'
Jul 29 03:06:25.875: INFO: stderr: ""
Jul 29 03:06:25.875: INFO: stdout: "pod/pause created\n"
Jul 29 03:06:25.875: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 29 03:06:25.875: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-570" to be "running and ready"
Jul 29 03:06:25.889: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.134512ms
Jul 29 03:06:27.895: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020098704s
Jul 29 03:06:29.901: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.025788046s
Jul 29 03:06:29.901: INFO: Pod "pause" satisfied condition "running and ready"
Jul 29 03:06:29.901: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 29 03:06:29.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 label pods pause testing-label=testing-label-value --namespace=kubectl-570'
Jul 29 03:06:30.095: INFO: stderr: ""
Jul 29 03:06:30.095: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 29 03:06:30.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pod pause -L testing-label --namespace=kubectl-570'
Jul 29 03:06:30.244: INFO: stderr: ""
Jul 29 03:06:30.244: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 29 03:06:30.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 label pods pause testing-label- --namespace=kubectl-570'
Jul 29 03:06:30.414: INFO: stderr: ""
Jul 29 03:06:30.414: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 29 03:06:30.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pod pause -L testing-label --namespace=kubectl-570'
Jul 29 03:06:30.576: INFO: stderr: ""
Jul 29 03:06:30.576: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jul 29 03:06:30.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-570'
Jul 29 03:06:30.748: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 03:06:30.748: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 29 03:06:30.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get rc,svc -l name=pause --no-headers --namespace=kubectl-570'
Jul 29 03:06:30.954: INFO: stderr: "No resources found.\n"
Jul 29 03:06:30.954: INFO: stdout: ""
Jul 29 03:06:30.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -l name=pause --namespace=kubectl-570 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 29 03:06:31.123: INFO: stderr: ""
Jul 29 03:06:31.123: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:06:31.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-570" for this suite.
Jul 29 03:06:37.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:06:37.418: INFO: namespace kubectl-570 deletion completed in 6.288357552s

• [SLOW TEST:12.434 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:06:37.419: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 29 03:06:37.502: INFO: Waiting up to 5m0s for pod "pod-8f61bd0c-8243-4610-bdd0-a2706a42ecd1" in namespace "emptydir-4294" to be "success or failure"
Jul 29 03:06:37.511: INFO: Pod "pod-8f61bd0c-8243-4610-bdd0-a2706a42ecd1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.416103ms
Jul 29 03:06:39.517: INFO: Pod "pod-8f61bd0c-8243-4610-bdd0-a2706a42ecd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014493165s
Jul 29 03:06:41.524: INFO: Pod "pod-8f61bd0c-8243-4610-bdd0-a2706a42ecd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02059126s
STEP: Saw pod success
Jul 29 03:06:41.524: INFO: Pod "pod-8f61bd0c-8243-4610-bdd0-a2706a42ecd1" satisfied condition "success or failure"
Jul 29 03:06:41.529: INFO: Trying to get logs from node conformance0 pod pod-8f61bd0c-8243-4610-bdd0-a2706a42ecd1 container test-container: <nil>
STEP: delete the pod
Jul 29 03:06:41.563: INFO: Waiting for pod pod-8f61bd0c-8243-4610-bdd0-a2706a42ecd1 to disappear
Jul 29 03:06:41.570: INFO: Pod pod-8f61bd0c-8243-4610-bdd0-a2706a42ecd1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:06:41.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4294" for this suite.
Jul 29 03:06:47.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:06:47.795: INFO: namespace emptydir-4294 deletion completed in 6.219040598s

• [SLOW TEST:10.377 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:06:47.806: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 29 03:06:47.880: INFO: Waiting up to 5m0s for pod "pod-c2007157-646d-40f2-b18c-0568aeb9867a" in namespace "emptydir-3965" to be "success or failure"
Jul 29 03:06:47.885: INFO: Pod "pod-c2007157-646d-40f2-b18c-0568aeb9867a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607308ms
Jul 29 03:06:49.891: INFO: Pod "pod-c2007157-646d-40f2-b18c-0568aeb9867a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01104556s
Jul 29 03:06:51.901: INFO: Pod "pod-c2007157-646d-40f2-b18c-0568aeb9867a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020832339s
STEP: Saw pod success
Jul 29 03:06:51.901: INFO: Pod "pod-c2007157-646d-40f2-b18c-0568aeb9867a" satisfied condition "success or failure"
Jul 29 03:06:51.915: INFO: Trying to get logs from node conformance1 pod pod-c2007157-646d-40f2-b18c-0568aeb9867a container test-container: <nil>
STEP: delete the pod
Jul 29 03:06:51.954: INFO: Waiting for pod pod-c2007157-646d-40f2-b18c-0568aeb9867a to disappear
Jul 29 03:06:51.960: INFO: Pod pod-c2007157-646d-40f2-b18c-0568aeb9867a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:06:51.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3965" for this suite.
Jul 29 03:06:57.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:06:58.234: INFO: namespace emptydir-3965 deletion completed in 6.266006452s

• [SLOW TEST:10.428 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:06:58.239: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 03:06:58.351: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 29 03:06:58.377: INFO: Number of nodes with available pods: 0
Jul 29 03:06:58.378: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 29 03:06:58.428: INFO: Number of nodes with available pods: 0
Jul 29 03:06:58.428: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:06:59.436: INFO: Number of nodes with available pods: 0
Jul 29 03:06:59.436: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:00.434: INFO: Number of nodes with available pods: 0
Jul 29 03:07:00.434: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:01.434: INFO: Number of nodes with available pods: 1
Jul 29 03:07:01.434: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 29 03:07:01.458: INFO: Number of nodes with available pods: 1
Jul 29 03:07:01.458: INFO: Number of running nodes: 0, number of available pods: 1
Jul 29 03:07:02.465: INFO: Number of nodes with available pods: 0
Jul 29 03:07:02.465: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 29 03:07:02.486: INFO: Number of nodes with available pods: 0
Jul 29 03:07:02.486: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:03.492: INFO: Number of nodes with available pods: 0
Jul 29 03:07:03.492: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:04.492: INFO: Number of nodes with available pods: 0
Jul 29 03:07:04.492: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:05.496: INFO: Number of nodes with available pods: 0
Jul 29 03:07:05.497: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:06.494: INFO: Number of nodes with available pods: 0
Jul 29 03:07:06.494: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:07.817: INFO: Number of nodes with available pods: 0
Jul 29 03:07:07.817: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:08.493: INFO: Number of nodes with available pods: 0
Jul 29 03:07:08.494: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:09.493: INFO: Number of nodes with available pods: 0
Jul 29 03:07:09.493: INFO: Node conformance0 is running more than one daemon pod
Jul 29 03:07:10.493: INFO: Number of nodes with available pods: 1
Jul 29 03:07:10.493: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4537, will wait for the garbage collector to delete the pods
Jul 29 03:07:10.587: INFO: Deleting DaemonSet.extensions daemon-set took: 29.780425ms
Jul 29 03:07:10.888: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.455985ms
Jul 29 03:07:17.496: INFO: Number of nodes with available pods: 0
Jul 29 03:07:17.496: INFO: Number of running nodes: 0, number of available pods: 0
Jul 29 03:07:17.499: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4537/daemonsets","resourceVersion":"13534"},"items":null}

Jul 29 03:07:17.504: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4537/pods","resourceVersion":"13534"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:07:17.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4537" for this suite.
Jul 29 03:07:23.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:07:23.762: INFO: namespace daemonsets-4537 deletion completed in 6.217703237s

• [SLOW TEST:25.524 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:07:23.763: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jul 29 03:07:23.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 api-versions'
Jul 29 03:07:24.002: INFO: stderr: ""
Jul 29 03:07:24.002: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:07:24.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-487" for this suite.
Jul 29 03:07:30.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:07:30.203: INFO: namespace kubectl-487 deletion completed in 6.185464948s

• [SLOW TEST:6.440 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:07:30.203: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 29 03:07:30.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-854'
Jul 29 03:07:30.449: INFO: stderr: ""
Jul 29 03:07:30.449: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jul 29 03:07:30.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete pods e2e-test-nginx-pod --namespace=kubectl-854'
Jul 29 03:07:33.502: INFO: stderr: ""
Jul 29 03:07:33.502: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:07:33.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-854" for this suite.
Jul 29 03:07:39.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:07:39.803: INFO: namespace kubectl-854 deletion completed in 6.293712097s

• [SLOW TEST:9.600 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:07:39.807: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-265e9e64-8c51-4171-b355-726dc3303e91
STEP: Creating a pod to test consume secrets
Jul 29 03:07:39.999: INFO: Waiting up to 5m0s for pod "pod-secrets-6592138e-9147-4912-9150-9537c6008a36" in namespace "secrets-1986" to be "success or failure"
Jul 29 03:07:40.016: INFO: Pod "pod-secrets-6592138e-9147-4912-9150-9537c6008a36": Phase="Pending", Reason="", readiness=false. Elapsed: 16.851574ms
Jul 29 03:07:42.030: INFO: Pod "pod-secrets-6592138e-9147-4912-9150-9537c6008a36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031346372s
Jul 29 03:07:44.040: INFO: Pod "pod-secrets-6592138e-9147-4912-9150-9537c6008a36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041682461s
STEP: Saw pod success
Jul 29 03:07:44.041: INFO: Pod "pod-secrets-6592138e-9147-4912-9150-9537c6008a36" satisfied condition "success or failure"
Jul 29 03:07:44.046: INFO: Trying to get logs from node conformance1 pod pod-secrets-6592138e-9147-4912-9150-9537c6008a36 container secret-volume-test: <nil>
STEP: delete the pod
Jul 29 03:07:44.088: INFO: Waiting for pod pod-secrets-6592138e-9147-4912-9150-9537c6008a36 to disappear
Jul 29 03:07:44.096: INFO: Pod pod-secrets-6592138e-9147-4912-9150-9537c6008a36 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:07:44.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1986" for this suite.
Jul 29 03:07:50.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:07:50.365: INFO: namespace secrets-1986 deletion completed in 6.262790024s
STEP: Destroying namespace "secret-namespace-3150" for this suite.
Jul 29 03:07:56.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:07:56.593: INFO: namespace secret-namespace-3150 deletion completed in 6.228205364s

• [SLOW TEST:16.786 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:07:56.595: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-1493/configmap-test-a0828ac1-19dd-4e5c-ba4e-a14e9e70e1dd
STEP: Creating a pod to test consume configMaps
Jul 29 03:07:56.687: INFO: Waiting up to 5m0s for pod "pod-configmaps-c155abca-f893-431d-9219-1493add3413a" in namespace "configmap-1493" to be "success or failure"
Jul 29 03:07:56.712: INFO: Pod "pod-configmaps-c155abca-f893-431d-9219-1493add3413a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.486497ms
Jul 29 03:07:58.718: INFO: Pod "pod-configmaps-c155abca-f893-431d-9219-1493add3413a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030574423s
Jul 29 03:08:00.723: INFO: Pod "pod-configmaps-c155abca-f893-431d-9219-1493add3413a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035604042s
STEP: Saw pod success
Jul 29 03:08:00.723: INFO: Pod "pod-configmaps-c155abca-f893-431d-9219-1493add3413a" satisfied condition "success or failure"
Jul 29 03:08:00.726: INFO: Trying to get logs from node conformance0 pod pod-configmaps-c155abca-f893-431d-9219-1493add3413a container env-test: <nil>
STEP: delete the pod
Jul 29 03:08:00.769: INFO: Waiting for pod pod-configmaps-c155abca-f893-431d-9219-1493add3413a to disappear
Jul 29 03:08:00.773: INFO: Pod pod-configmaps-c155abca-f893-431d-9219-1493add3413a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:08:00.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1493" for this suite.
Jul 29 03:08:06.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:08:07.030: INFO: namespace configmap-1493 deletion completed in 6.230434214s

• [SLOW TEST:10.435 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:08:07.032: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-006e9916-35f5-4cb5-acee-7621c7f55054 in namespace container-probe-9749
Jul 29 03:08:11.124: INFO: Started pod busybox-006e9916-35f5-4cb5-acee-7621c7f55054 in namespace container-probe-9749
STEP: checking the pod's current state and verifying that restartCount is present
Jul 29 03:08:11.129: INFO: Initial restart count of pod busybox-006e9916-35f5-4cb5-acee-7621c7f55054 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:12:11.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9749" for this suite.
Jul 29 03:12:17.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:12:18.091: INFO: namespace container-probe-9749 deletion completed in 6.250516784s

• [SLOW TEST:251.059 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:12:18.093: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5943
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5943
STEP: Creating statefulset with conflicting port in namespace statefulset-5943
STEP: Waiting until pod test-pod will start running in namespace statefulset-5943
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5943
Jul 29 03:12:22.306: INFO: Observed stateful pod in namespace: statefulset-5943, name: ss-0, uid: 8c24a3fe-dfdf-4b86-adfe-935fe0670330, status phase: Pending. Waiting for statefulset controller to delete.
Jul 29 03:12:22.678: INFO: Observed stateful pod in namespace: statefulset-5943, name: ss-0, uid: 8c24a3fe-dfdf-4b86-adfe-935fe0670330, status phase: Failed. Waiting for statefulset controller to delete.
Jul 29 03:12:22.687: INFO: Observed stateful pod in namespace: statefulset-5943, name: ss-0, uid: 8c24a3fe-dfdf-4b86-adfe-935fe0670330, status phase: Failed. Waiting for statefulset controller to delete.
Jul 29 03:12:22.697: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5943
STEP: Removing pod with conflicting port in namespace statefulset-5943
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5943 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 29 03:12:26.759: INFO: Deleting all statefulset in ns statefulset-5943
Jul 29 03:12:26.764: INFO: Scaling statefulset ss to 0
Jul 29 03:12:36.797: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 03:12:36.801: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:12:36.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5943" for this suite.
Jul 29 03:12:42.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:12:43.080: INFO: namespace statefulset-5943 deletion completed in 6.254366954s

• [SLOW TEST:24.987 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:12:43.081: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 29 03:12:43.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8408'
Jul 29 03:12:43.314: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 29 03:12:43.314: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jul 29 03:12:43.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete jobs e2e-test-nginx-job --namespace=kubectl-8408'
Jul 29 03:12:43.551: INFO: stderr: ""
Jul 29 03:12:43.551: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:12:43.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8408" for this suite.
Jul 29 03:12:49.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:12:49.737: INFO: namespace kubectl-8408 deletion completed in 6.177758568s

• [SLOW TEST:6.656 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:12:49.738: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 29 03:12:49.835: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1210,SelfLink:/api/v1/namespaces/watch-1210/configmaps/e2e-watch-test-label-changed,UID:ee4490a0-c966-4247-a54c-688bbbdd24be,ResourceVersion:13997,Generation:0,CreationTimestamp:2019-07-29 03:12:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 29 03:12:49.835: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1210,SelfLink:/api/v1/namespaces/watch-1210/configmaps/e2e-watch-test-label-changed,UID:ee4490a0-c966-4247-a54c-688bbbdd24be,ResourceVersion:13998,Generation:0,CreationTimestamp:2019-07-29 03:12:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 29 03:12:49.836: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1210,SelfLink:/api/v1/namespaces/watch-1210/configmaps/e2e-watch-test-label-changed,UID:ee4490a0-c966-4247-a54c-688bbbdd24be,ResourceVersion:13999,Generation:0,CreationTimestamp:2019-07-29 03:12:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 29 03:12:59.886: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1210,SelfLink:/api/v1/namespaces/watch-1210/configmaps/e2e-watch-test-label-changed,UID:ee4490a0-c966-4247-a54c-688bbbdd24be,ResourceVersion:14005,Generation:0,CreationTimestamp:2019-07-29 03:12:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 29 03:12:59.886: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1210,SelfLink:/api/v1/namespaces/watch-1210/configmaps/e2e-watch-test-label-changed,UID:ee4490a0-c966-4247-a54c-688bbbdd24be,ResourceVersion:14006,Generation:0,CreationTimestamp:2019-07-29 03:12:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 29 03:12:59.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1210,SelfLink:/api/v1/namespaces/watch-1210/configmaps/e2e-watch-test-label-changed,UID:ee4490a0-c966-4247-a54c-688bbbdd24be,ResourceVersion:14007,Generation:0,CreationTimestamp:2019-07-29 03:12:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:12:59.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1210" for this suite.
Jul 29 03:13:05.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:13:06.060: INFO: namespace watch-1210 deletion completed in 6.168135295s

• [SLOW TEST:16.322 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:13:06.070: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2l7qg in namespace proxy-4475
I0729 03:13:06.152396      14 runners.go:180] Created replication controller with name: proxy-service-2l7qg, namespace: proxy-4475, replica count: 1
I0729 03:13:07.205256      14 runners.go:180] proxy-service-2l7qg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 03:13:08.205601      14 runners.go:180] proxy-service-2l7qg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 03:13:09.205976      14 runners.go:180] proxy-service-2l7qg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0729 03:13:10.206333      14 runners.go:180] proxy-service-2l7qg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0729 03:13:11.206773      14 runners.go:180] proxy-service-2l7qg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0729 03:13:12.207202      14 runners.go:180] proxy-service-2l7qg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 03:13:12.211: INFO: setup took 6.088428431s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 29 03:13:12.225: INFO: (0) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 13.211508ms)
Jul 29 03:13:12.236: INFO: (0) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 24.657415ms)
Jul 29 03:13:12.243: INFO: (0) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 29.839429ms)
Jul 29 03:13:12.244: INFO: (0) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 29.293527ms)
Jul 29 03:13:12.249: INFO: (0) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 36.484516ms)
Jul 29 03:13:12.250: INFO: (0) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 36.524743ms)
Jul 29 03:13:12.257: INFO: (0) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 45.132954ms)
Jul 29 03:13:12.258: INFO: (0) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 44.15493ms)
Jul 29 03:13:12.258: INFO: (0) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 45.66451ms)
Jul 29 03:13:12.262: INFO: (0) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 49.975006ms)
Jul 29 03:13:12.274: INFO: (0) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 60.856953ms)
Jul 29 03:13:12.274: INFO: (0) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 59.872252ms)
Jul 29 03:13:12.275: INFO: (0) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 60.87686ms)
Jul 29 03:13:12.284: INFO: (0) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 71.100664ms)
Jul 29 03:13:12.287: INFO: (0) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 72.183375ms)
Jul 29 03:13:12.287: INFO: (0) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 72.496628ms)
Jul 29 03:13:12.295: INFO: (1) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 8.183976ms)
Jul 29 03:13:12.296: INFO: (1) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 9.384284ms)
Jul 29 03:13:12.310: INFO: (1) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 22.729759ms)
Jul 29 03:13:12.314: INFO: (1) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 26.372814ms)
Jul 29 03:13:12.315: INFO: (1) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 26.415856ms)
Jul 29 03:13:12.315: INFO: (1) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 26.929185ms)
Jul 29 03:13:12.315: INFO: (1) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 27.378723ms)
Jul 29 03:13:12.316: INFO: (1) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 28.200102ms)
Jul 29 03:13:12.316: INFO: (1) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 28.157801ms)
Jul 29 03:13:12.318: INFO: (1) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 30.386593ms)
Jul 29 03:13:12.318: INFO: (1) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 29.971475ms)
Jul 29 03:13:12.318: INFO: (1) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 30.463523ms)
Jul 29 03:13:12.319: INFO: (1) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 30.665922ms)
Jul 29 03:13:12.319: INFO: (1) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 30.711066ms)
Jul 29 03:13:12.320: INFO: (1) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 32.352444ms)
Jul 29 03:13:12.321: INFO: (1) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 32.494712ms)
Jul 29 03:13:12.337: INFO: (2) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 15.973862ms)
Jul 29 03:13:12.337: INFO: (2) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 15.246125ms)
Jul 29 03:13:12.337: INFO: (2) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 14.967178ms)
Jul 29 03:13:12.340: INFO: (2) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 16.965195ms)
Jul 29 03:13:12.341: INFO: (2) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 19.023717ms)
Jul 29 03:13:12.342: INFO: (2) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 19.128771ms)
Jul 29 03:13:12.344: INFO: (2) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 21.342113ms)
Jul 29 03:13:12.345: INFO: (2) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 22.901094ms)
Jul 29 03:13:12.347: INFO: (2) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 25.478425ms)
Jul 29 03:13:12.348: INFO: (2) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 25.011611ms)
Jul 29 03:13:12.350: INFO: (2) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 28.14023ms)
Jul 29 03:13:12.350: INFO: (2) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 26.952585ms)
Jul 29 03:13:12.351: INFO: (2) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 27.740462ms)
Jul 29 03:13:12.352: INFO: (2) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 29.879762ms)
Jul 29 03:13:12.353: INFO: (2) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 31.441883ms)
Jul 29 03:13:12.353: INFO: (2) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 28.774239ms)
Jul 29 03:13:12.359: INFO: (3) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 6.15194ms)
Jul 29 03:13:12.377: INFO: (3) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 22.383813ms)
Jul 29 03:13:12.378: INFO: (3) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 23.871355ms)
Jul 29 03:13:12.378: INFO: (3) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 24.229611ms)
Jul 29 03:13:12.380: INFO: (3) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 26.160369ms)
Jul 29 03:13:12.383: INFO: (3) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 28.979666ms)
Jul 29 03:13:12.383: INFO: (3) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 29.897684ms)
Jul 29 03:13:12.384: INFO: (3) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 28.879235ms)
Jul 29 03:13:12.384: INFO: (3) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 29.723934ms)
Jul 29 03:13:12.384: INFO: (3) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 30.74915ms)
Jul 29 03:13:12.385: INFO: (3) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 30.491803ms)
Jul 29 03:13:12.385: INFO: (3) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 30.735765ms)
Jul 29 03:13:12.386: INFO: (3) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 31.388581ms)
Jul 29 03:13:12.386: INFO: (3) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 31.603639ms)
Jul 29 03:13:12.387: INFO: (3) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 32.24652ms)
Jul 29 03:13:12.387: INFO: (3) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 32.458896ms)
Jul 29 03:13:12.398: INFO: (4) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 10.336171ms)
Jul 29 03:13:12.398: INFO: (4) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 10.490648ms)
Jul 29 03:13:12.398: INFO: (4) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 11.128013ms)
Jul 29 03:13:12.401: INFO: (4) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 12.981056ms)
Jul 29 03:13:12.403: INFO: (4) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 14.684901ms)
Jul 29 03:13:12.403: INFO: (4) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 15.241281ms)
Jul 29 03:13:12.409: INFO: (4) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 20.991884ms)
Jul 29 03:13:12.410: INFO: (4) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 22.037142ms)
Jul 29 03:13:12.415: INFO: (4) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 26.556005ms)
Jul 29 03:13:12.417: INFO: (4) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 28.607939ms)
Jul 29 03:13:12.417: INFO: (4) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 28.395766ms)
Jul 29 03:13:12.418: INFO: (4) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 28.838927ms)
Jul 29 03:13:12.419: INFO: (4) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 29.870763ms)
Jul 29 03:13:12.420: INFO: (4) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 31.31141ms)
Jul 29 03:13:12.421: INFO: (4) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 31.465431ms)
Jul 29 03:13:12.421: INFO: (4) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 32.40777ms)
Jul 29 03:13:12.439: INFO: (5) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 17.32966ms)
Jul 29 03:13:12.440: INFO: (5) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 18.297545ms)
Jul 29 03:13:12.440: INFO: (5) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 18.392182ms)
Jul 29 03:13:12.440: INFO: (5) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 17.677027ms)
Jul 29 03:13:12.448: INFO: (5) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 25.766963ms)
Jul 29 03:13:12.453: INFO: (5) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 30.956261ms)
Jul 29 03:13:12.453: INFO: (5) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 31.284328ms)
Jul 29 03:13:12.453: INFO: (5) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 31.24992ms)
Jul 29 03:13:12.454: INFO: (5) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 31.923102ms)
Jul 29 03:13:12.455: INFO: (5) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 31.910811ms)
Jul 29 03:13:12.455: INFO: (5) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 32.355099ms)
Jul 29 03:13:12.455: INFO: (5) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 33.040318ms)
Jul 29 03:13:12.456: INFO: (5) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 33.964416ms)
Jul 29 03:13:12.456: INFO: (5) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 33.84383ms)
Jul 29 03:13:12.457: INFO: (5) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 34.697016ms)
Jul 29 03:13:12.458: INFO: (5) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 36.201638ms)
Jul 29 03:13:12.464: INFO: (6) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 5.038803ms)
Jul 29 03:13:12.466: INFO: (6) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 7.614857ms)
Jul 29 03:13:12.467: INFO: (6) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 8.857965ms)
Jul 29 03:13:12.474: INFO: (6) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 14.841146ms)
Jul 29 03:13:12.481: INFO: (6) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 20.968264ms)
Jul 29 03:13:12.481: INFO: (6) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 21.827559ms)
Jul 29 03:13:12.482: INFO: (6) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 22.248281ms)
Jul 29 03:13:12.482: INFO: (6) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 22.62009ms)
Jul 29 03:13:12.483: INFO: (6) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 23.699219ms)
Jul 29 03:13:12.485: INFO: (6) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 25.594498ms)
Jul 29 03:13:12.486: INFO: (6) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 27.181831ms)
Jul 29 03:13:12.487: INFO: (6) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 27.08606ms)
Jul 29 03:13:12.488: INFO: (6) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 28.716691ms)
Jul 29 03:13:12.488: INFO: (6) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 28.510485ms)
Jul 29 03:13:12.489: INFO: (6) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 29.45745ms)
Jul 29 03:13:12.489: INFO: (6) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 29.723899ms)
Jul 29 03:13:12.500: INFO: (7) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 10.077318ms)
Jul 29 03:13:12.501: INFO: (7) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 11.010364ms)
Jul 29 03:13:12.501: INFO: (7) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 10.551787ms)
Jul 29 03:13:12.510: INFO: (7) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 20.221164ms)
Jul 29 03:13:12.514: INFO: (7) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 21.883819ms)
Jul 29 03:13:12.516: INFO: (7) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 23.408143ms)
Jul 29 03:13:12.518: INFO: (7) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 26.972654ms)
Jul 29 03:13:12.518: INFO: (7) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 28.73217ms)
Jul 29 03:13:12.518: INFO: (7) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 25.26629ms)
Jul 29 03:13:12.519: INFO: (7) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 26.331248ms)
Jul 29 03:13:12.520: INFO: (7) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 27.671055ms)
Jul 29 03:13:12.520: INFO: (7) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 29.635453ms)
Jul 29 03:13:12.521: INFO: (7) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 30.09805ms)
Jul 29 03:13:12.522: INFO: (7) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 31.879454ms)
Jul 29 03:13:12.524: INFO: (7) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 30.857719ms)
Jul 29 03:13:12.525: INFO: (7) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 31.66958ms)
Jul 29 03:13:12.544: INFO: (8) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 18.819759ms)
Jul 29 03:13:12.544: INFO: (8) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 19.440039ms)
Jul 29 03:13:12.544: INFO: (8) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 19.293652ms)
Jul 29 03:13:12.544: INFO: (8) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 18.957532ms)
Jul 29 03:13:12.546: INFO: (8) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 20.324199ms)
Jul 29 03:13:12.546: INFO: (8) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 20.832459ms)
Jul 29 03:13:12.552: INFO: (8) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 26.596879ms)
Jul 29 03:13:12.554: INFO: (8) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 28.276857ms)
Jul 29 03:13:12.554: INFO: (8) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 28.371463ms)
Jul 29 03:13:12.554: INFO: (8) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 28.992755ms)
Jul 29 03:13:12.556: INFO: (8) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 29.643394ms)
Jul 29 03:13:12.557: INFO: (8) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 31.295433ms)
Jul 29 03:13:12.558: INFO: (8) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 31.40884ms)
Jul 29 03:13:12.558: INFO: (8) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 32.431035ms)
Jul 29 03:13:12.558: INFO: (8) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 32.508391ms)
Jul 29 03:13:12.558: INFO: (8) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 32.755898ms)
Jul 29 03:13:12.576: INFO: (9) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 16.829982ms)
Jul 29 03:13:12.578: INFO: (9) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 19.075449ms)
Jul 29 03:13:12.578: INFO: (9) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 19.325279ms)
Jul 29 03:13:12.578: INFO: (9) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 18.829451ms)
Jul 29 03:13:12.578: INFO: (9) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 19.47803ms)
Jul 29 03:13:12.578: INFO: (9) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 18.974928ms)
Jul 29 03:13:12.579: INFO: (9) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 20.130864ms)
Jul 29 03:13:12.579: INFO: (9) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 19.916123ms)
Jul 29 03:13:12.579: INFO: (9) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 19.863172ms)
Jul 29 03:13:12.579: INFO: (9) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 20.755993ms)
Jul 29 03:13:12.584: INFO: (9) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 25.407647ms)
Jul 29 03:13:12.587: INFO: (9) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 27.293332ms)
Jul 29 03:13:12.587: INFO: (9) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 29.025265ms)
Jul 29 03:13:12.588: INFO: (9) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 29.824005ms)
Jul 29 03:13:12.590: INFO: (9) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 30.619654ms)
Jul 29 03:13:12.590: INFO: (9) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 30.908772ms)
Jul 29 03:13:12.599: INFO: (10) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 8.821361ms)
Jul 29 03:13:12.603: INFO: (10) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 12.144608ms)
Jul 29 03:13:12.604: INFO: (10) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 13.09181ms)
Jul 29 03:13:12.608: INFO: (10) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 16.624575ms)
Jul 29 03:13:12.609: INFO: (10) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 18.302396ms)
Jul 29 03:13:12.614: INFO: (10) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 22.359802ms)
Jul 29 03:13:12.614: INFO: (10) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 23.279484ms)
Jul 29 03:13:12.617: INFO: (10) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 25.984758ms)
Jul 29 03:13:12.618: INFO: (10) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 26.819059ms)
Jul 29 03:13:12.620: INFO: (10) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 28.884904ms)
Jul 29 03:13:12.621: INFO: (10) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 29.23172ms)
Jul 29 03:13:12.621: INFO: (10) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 28.993647ms)
Jul 29 03:13:12.623: INFO: (10) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 31.428607ms)
Jul 29 03:13:12.624: INFO: (10) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 32.979741ms)
Jul 29 03:13:12.624: INFO: (10) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 32.903175ms)
Jul 29 03:13:12.625: INFO: (10) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 33.218021ms)
Jul 29 03:13:12.633: INFO: (11) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 7.424947ms)
Jul 29 03:13:12.641: INFO: (11) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 14.782947ms)
Jul 29 03:13:12.646: INFO: (11) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 19.983549ms)
Jul 29 03:13:12.646: INFO: (11) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 20.211797ms)
Jul 29 03:13:12.646: INFO: (11) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 20.907361ms)
Jul 29 03:13:12.650: INFO: (11) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 23.210918ms)
Jul 29 03:13:12.650: INFO: (11) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 25.117564ms)
Jul 29 03:13:12.650: INFO: (11) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 23.194988ms)
Jul 29 03:13:12.654: INFO: (11) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 26.274612ms)
Jul 29 03:13:12.654: INFO: (11) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 26.092028ms)
Jul 29 03:13:12.654: INFO: (11) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 25.75483ms)
Jul 29 03:13:12.656: INFO: (11) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 27.324092ms)
Jul 29 03:13:12.658: INFO: (11) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 29.936608ms)
Jul 29 03:13:12.658: INFO: (11) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 31.306008ms)
Jul 29 03:13:12.658: INFO: (11) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 29.655711ms)
Jul 29 03:13:12.658: INFO: (11) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 29.510536ms)
Jul 29 03:13:12.671: INFO: (12) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 12.592688ms)
Jul 29 03:13:12.672: INFO: (12) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 13.819795ms)
Jul 29 03:13:12.674: INFO: (12) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 15.612901ms)
Jul 29 03:13:12.682: INFO: (12) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 22.962549ms)
Jul 29 03:13:12.682: INFO: (12) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 21.194837ms)
Jul 29 03:13:12.683: INFO: (12) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 23.655926ms)
Jul 29 03:13:12.684: INFO: (12) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 25.805592ms)
Jul 29 03:13:12.686: INFO: (12) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 26.233976ms)
Jul 29 03:13:12.686: INFO: (12) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 27.023425ms)
Jul 29 03:13:12.686: INFO: (12) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 26.833575ms)
Jul 29 03:13:12.689: INFO: (12) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 28.764746ms)
Jul 29 03:13:12.689: INFO: (12) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 29.202016ms)
Jul 29 03:13:12.691: INFO: (12) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 30.731157ms)
Jul 29 03:13:12.692: INFO: (12) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 32.771791ms)
Jul 29 03:13:12.692: INFO: (12) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 32.186787ms)
Jul 29 03:13:12.693: INFO: (12) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 32.474436ms)
Jul 29 03:13:12.701: INFO: (13) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 7.988536ms)
Jul 29 03:13:12.710: INFO: (13) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 15.875681ms)
Jul 29 03:13:12.713: INFO: (13) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 18.872158ms)
Jul 29 03:13:12.716: INFO: (13) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 21.481404ms)
Jul 29 03:13:12.717: INFO: (13) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 23.513409ms)
Jul 29 03:13:12.718: INFO: (13) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 24.943249ms)
Jul 29 03:13:12.719: INFO: (13) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 24.765089ms)
Jul 29 03:13:12.718: INFO: (13) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 25.026108ms)
Jul 29 03:13:12.726: INFO: (13) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 30.938933ms)
Jul 29 03:13:12.726: INFO: (13) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 31.110628ms)
Jul 29 03:13:12.726: INFO: (13) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 31.466036ms)
Jul 29 03:13:12.726: INFO: (13) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 30.856628ms)
Jul 29 03:13:12.726: INFO: (13) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 32.012896ms)
Jul 29 03:13:12.727: INFO: (13) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 31.225406ms)
Jul 29 03:13:12.728: INFO: (13) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 33.253594ms)
Jul 29 03:13:12.728: INFO: (13) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 33.101551ms)
Jul 29 03:13:12.734: INFO: (14) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 5.51848ms)
Jul 29 03:13:12.743: INFO: (14) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 14.070576ms)
Jul 29 03:13:12.749: INFO: (14) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 18.467853ms)
Jul 29 03:13:12.751: INFO: (14) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 21.32287ms)
Jul 29 03:13:12.751: INFO: (14) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 21.544898ms)
Jul 29 03:13:12.754: INFO: (14) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 23.507847ms)
Jul 29 03:13:12.755: INFO: (14) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 25.127134ms)
Jul 29 03:13:12.755: INFO: (14) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 24.533761ms)
Jul 29 03:13:12.755: INFO: (14) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 24.475418ms)
Jul 29 03:13:12.759: INFO: (14) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 28.243822ms)
Jul 29 03:13:12.761: INFO: (14) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 30.099377ms)
Jul 29 03:13:12.762: INFO: (14) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 31.143182ms)
Jul 29 03:13:12.763: INFO: (14) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 31.465645ms)
Jul 29 03:13:12.763: INFO: (14) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 33.485368ms)
Jul 29 03:13:12.764: INFO: (14) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 35.729668ms)
Jul 29 03:13:12.765: INFO: (14) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 35.353746ms)
Jul 29 03:13:12.783: INFO: (15) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 17.801943ms)
Jul 29 03:13:12.784: INFO: (15) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 17.349115ms)
Jul 29 03:13:12.785: INFO: (15) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 19.629388ms)
Jul 29 03:13:12.788: INFO: (15) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 21.565237ms)
Jul 29 03:13:12.789: INFO: (15) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 23.238329ms)
Jul 29 03:13:12.791: INFO: (15) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 24.73411ms)
Jul 29 03:13:12.791: INFO: (15) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 25.227885ms)
Jul 29 03:13:12.792: INFO: (15) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 26.482529ms)
Jul 29 03:13:12.794: INFO: (15) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 28.985458ms)
Jul 29 03:13:12.796: INFO: (15) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 30.059849ms)
Jul 29 03:13:12.797: INFO: (15) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 31.346506ms)
Jul 29 03:13:12.797: INFO: (15) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 32.312126ms)
Jul 29 03:13:12.798: INFO: (15) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 31.3554ms)
Jul 29 03:13:12.798: INFO: (15) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 32.112598ms)
Jul 29 03:13:12.799: INFO: (15) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 33.849131ms)
Jul 29 03:13:12.800: INFO: (15) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 34.043942ms)
Jul 29 03:13:12.807: INFO: (16) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 6.834467ms)
Jul 29 03:13:12.807: INFO: (16) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 7.256952ms)
Jul 29 03:13:12.816: INFO: (16) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 15.573214ms)
Jul 29 03:13:12.817: INFO: (16) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 15.745582ms)
Jul 29 03:13:12.819: INFO: (16) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 17.854767ms)
Jul 29 03:13:12.822: INFO: (16) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 20.971113ms)
Jul 29 03:13:12.823: INFO: (16) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 22.601014ms)
Jul 29 03:13:12.823: INFO: (16) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 22.52379ms)
Jul 29 03:13:12.823: INFO: (16) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 22.213255ms)
Jul 29 03:13:12.824: INFO: (16) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 23.140386ms)
Jul 29 03:13:12.826: INFO: (16) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 25.418143ms)
Jul 29 03:13:12.830: INFO: (16) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 28.986108ms)
Jul 29 03:13:12.832: INFO: (16) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 31.095518ms)
Jul 29 03:13:12.832: INFO: (16) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 30.87549ms)
Jul 29 03:13:12.836: INFO: (16) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 35.442511ms)
Jul 29 03:13:12.836: INFO: (16) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 35.140112ms)
Jul 29 03:13:12.849: INFO: (17) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 11.673446ms)
Jul 29 03:13:12.851: INFO: (17) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 13.313733ms)
Jul 29 03:13:12.851: INFO: (17) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 14.279977ms)
Jul 29 03:13:12.852: INFO: (17) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 14.289946ms)
Jul 29 03:13:12.853: INFO: (17) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 14.766814ms)
Jul 29 03:13:12.859: INFO: (17) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 20.327402ms)
Jul 29 03:13:12.860: INFO: (17) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 21.847013ms)
Jul 29 03:13:12.866: INFO: (17) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 26.926344ms)
Jul 29 03:13:12.867: INFO: (17) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 29.742686ms)
Jul 29 03:13:12.868: INFO: (17) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 29.404829ms)
Jul 29 03:13:12.868: INFO: (17) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 29.027878ms)
Jul 29 03:13:12.869: INFO: (17) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 30.267471ms)
Jul 29 03:13:12.869: INFO: (17) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 31.075272ms)
Jul 29 03:13:12.870: INFO: (17) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 33.428574ms)
Jul 29 03:13:12.870: INFO: (17) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 31.689854ms)
Jul 29 03:13:12.870: INFO: (17) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 31.476319ms)
Jul 29 03:13:12.877: INFO: (18) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 6.286372ms)
Jul 29 03:13:12.882: INFO: (18) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 11.973577ms)
Jul 29 03:13:12.890: INFO: (18) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 18.443366ms)
Jul 29 03:13:12.893: INFO: (18) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 22.360516ms)
Jul 29 03:13:12.894: INFO: (18) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 22.692714ms)
Jul 29 03:13:12.895: INFO: (18) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 23.909357ms)
Jul 29 03:13:12.897: INFO: (18) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 24.914504ms)
Jul 29 03:13:12.897: INFO: (18) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 25.906845ms)
Jul 29 03:13:12.897: INFO: (18) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 26.868874ms)
Jul 29 03:13:12.898: INFO: (18) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 27.6223ms)
Jul 29 03:13:12.898: INFO: (18) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 26.831564ms)
Jul 29 03:13:12.900: INFO: (18) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 28.406264ms)
Jul 29 03:13:12.900: INFO: (18) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 28.153725ms)
Jul 29 03:13:12.900: INFO: (18) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 28.703074ms)
Jul 29 03:13:12.900: INFO: (18) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 28.801766ms)
Jul 29 03:13:12.902: INFO: (18) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 31.110427ms)
Jul 29 03:13:12.914: INFO: (19) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:462/proxy/: tls qux (200; 11.86101ms)
Jul 29 03:13:12.919: INFO: (19) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g/proxy/rewriteme">test</a> (200; 16.332399ms)
Jul 29 03:13:12.919: INFO: (19) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">test<... (200; 16.390476ms)
Jul 29 03:13:12.922: INFO: (19) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:443/proxy/tlsrewritem... (200; 18.940437ms)
Jul 29 03:13:12.923: INFO: (19) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:1080/proxy/rewriteme">... (200; 19.610775ms)
Jul 29 03:13:12.923: INFO: (19) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 19.862289ms)
Jul 29 03:13:12.923: INFO: (19) /api/v1/namespaces/proxy-4475/pods/https:proxy-service-2l7qg-gkb8g:460/proxy/: tls baz (200; 20.14028ms)
Jul 29 03:13:12.927: INFO: (19) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname2/proxy/: bar (200; 24.619916ms)
Jul 29 03:13:12.930: INFO: (19) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:160/proxy/: foo (200; 26.763399ms)
Jul 29 03:13:12.930: INFO: (19) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname1/proxy/: tls baz (200; 26.684139ms)
Jul 29 03:13:12.933: INFO: (19) /api/v1/namespaces/proxy-4475/pods/proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 30.085384ms)
Jul 29 03:13:12.935: INFO: (19) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname2/proxy/: bar (200; 31.355824ms)
Jul 29 03:13:12.935: INFO: (19) /api/v1/namespaces/proxy-4475/services/proxy-service-2l7qg:portname1/proxy/: foo (200; 32.669773ms)
Jul 29 03:13:12.936: INFO: (19) /api/v1/namespaces/proxy-4475/pods/http:proxy-service-2l7qg-gkb8g:162/proxy/: bar (200; 32.341822ms)
Jul 29 03:13:12.937: INFO: (19) /api/v1/namespaces/proxy-4475/services/http:proxy-service-2l7qg:portname1/proxy/: foo (200; 33.230778ms)
Jul 29 03:13:12.937: INFO: (19) /api/v1/namespaces/proxy-4475/services/https:proxy-service-2l7qg:tlsportname2/proxy/: tls qux (200; 34.610777ms)
STEP: deleting ReplicationController proxy-service-2l7qg in namespace proxy-4475, will wait for the garbage collector to delete the pods
Jul 29 03:13:13.008: INFO: Deleting ReplicationController proxy-service-2l7qg took: 15.827619ms
Jul 29 03:13:13.309: INFO: Terminating ReplicationController proxy-service-2l7qg pods took: 300.568652ms
[AfterEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:13:24.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4475" for this suite.
Jul 29 03:13:30.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:13:30.291: INFO: namespace proxy-4475 deletion completed in 6.1747266s

• [SLOW TEST:24.221 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:13:30.295: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-584a85c9-6f13-4de1-be6c-38a3856ce8d1
STEP: Creating a pod to test consume configMaps
Jul 29 03:13:30.361: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e4f43bf-f504-4efa-9694-b32994aca482" in namespace "configmap-934" to be "success or failure"
Jul 29 03:13:30.370: INFO: Pod "pod-configmaps-7e4f43bf-f504-4efa-9694-b32994aca482": Phase="Pending", Reason="", readiness=false. Elapsed: 8.892924ms
Jul 29 03:13:32.375: INFO: Pod "pod-configmaps-7e4f43bf-f504-4efa-9694-b32994aca482": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01379708s
Jul 29 03:13:34.380: INFO: Pod "pod-configmaps-7e4f43bf-f504-4efa-9694-b32994aca482": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018563671s
STEP: Saw pod success
Jul 29 03:13:34.380: INFO: Pod "pod-configmaps-7e4f43bf-f504-4efa-9694-b32994aca482" satisfied condition "success or failure"
Jul 29 03:13:34.384: INFO: Trying to get logs from node conformance1 pod pod-configmaps-7e4f43bf-f504-4efa-9694-b32994aca482 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 03:13:34.422: INFO: Waiting for pod pod-configmaps-7e4f43bf-f504-4efa-9694-b32994aca482 to disappear
Jul 29 03:13:34.426: INFO: Pod pod-configmaps-7e4f43bf-f504-4efa-9694-b32994aca482 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:13:34.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-934" for this suite.
Jul 29 03:13:40.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:13:40.615: INFO: namespace configmap-934 deletion completed in 6.182929514s

• [SLOW TEST:10.320 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:13:40.620: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-ca5e3e2b-eafc-414c-bd01-9950fa35a8b1
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:13:40.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9485" for this suite.
Jul 29 03:13:46.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:13:46.857: INFO: namespace configmap-9485 deletion completed in 6.169852897s

• [SLOW TEST:6.238 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:13:46.867: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0729 03:13:48.019866      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 29 03:13:48.020: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:13:48.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-586" for this suite.
Jul 29 03:13:54.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:13:54.211: INFO: namespace gc-586 deletion completed in 6.183566795s

• [SLOW TEST:7.345 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:13:54.213: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-sdt2
STEP: Creating a pod to test atomic-volume-subpath
Jul 29 03:13:54.285: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sdt2" in namespace "subpath-6918" to be "success or failure"
Jul 29 03:13:54.292: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.986225ms
Jul 29 03:13:56.297: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012003098s
Jul 29 03:13:58.302: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Running", Reason="", readiness=true. Elapsed: 4.01674686s
Jul 29 03:14:00.307: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Running", Reason="", readiness=true. Elapsed: 6.022210397s
Jul 29 03:14:02.313: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Running", Reason="", readiness=true. Elapsed: 8.027919046s
Jul 29 03:14:04.318: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Running", Reason="", readiness=true. Elapsed: 10.032613053s
Jul 29 03:14:06.323: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Running", Reason="", readiness=true. Elapsed: 12.037647438s
Jul 29 03:14:08.327: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Running", Reason="", readiness=true. Elapsed: 14.042415659s
Jul 29 03:14:10.332: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Running", Reason="", readiness=true. Elapsed: 16.047303099s
Jul 29 03:14:12.342: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Running", Reason="", readiness=true. Elapsed: 18.056705722s
Jul 29 03:14:14.347: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Running", Reason="", readiness=true. Elapsed: 20.062159564s
Jul 29 03:14:16.352: INFO: Pod "pod-subpath-test-secret-sdt2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.06714925s
STEP: Saw pod success
Jul 29 03:14:16.352: INFO: Pod "pod-subpath-test-secret-sdt2" satisfied condition "success or failure"
Jul 29 03:14:16.357: INFO: Trying to get logs from node conformance0 pod pod-subpath-test-secret-sdt2 container test-container-subpath-secret-sdt2: <nil>
STEP: delete the pod
Jul 29 03:14:16.384: INFO: Waiting for pod pod-subpath-test-secret-sdt2 to disappear
Jul 29 03:14:16.397: INFO: Pod pod-subpath-test-secret-sdt2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-sdt2
Jul 29 03:14:16.397: INFO: Deleting pod "pod-subpath-test-secret-sdt2" in namespace "subpath-6918"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:14:16.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6918" for this suite.
Jul 29 03:14:22.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:14:22.597: INFO: namespace subpath-6918 deletion completed in 6.188600596s

• [SLOW TEST:28.385 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:14:22.604: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:14:26.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4252" for this suite.
Jul 29 03:15:08.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:15:08.881: INFO: namespace kubelet-test-4252 deletion completed in 42.155657674s

• [SLOW TEST:46.278 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:15:08.886: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 03:15:08.948: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62a585a6-ea58-4a69-acbd-7a267f34612e" in namespace "projected-4797" to be "success or failure"
Jul 29 03:15:08.957: INFO: Pod "downwardapi-volume-62a585a6-ea58-4a69-acbd-7a267f34612e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.515966ms
Jul 29 03:15:10.963: INFO: Pod "downwardapi-volume-62a585a6-ea58-4a69-acbd-7a267f34612e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014866499s
Jul 29 03:15:12.969: INFO: Pod "downwardapi-volume-62a585a6-ea58-4a69-acbd-7a267f34612e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020895058s
STEP: Saw pod success
Jul 29 03:15:12.969: INFO: Pod "downwardapi-volume-62a585a6-ea58-4a69-acbd-7a267f34612e" satisfied condition "success or failure"
Jul 29 03:15:12.985: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-62a585a6-ea58-4a69-acbd-7a267f34612e container client-container: <nil>
STEP: delete the pod
Jul 29 03:15:13.026: INFO: Waiting for pod downwardapi-volume-62a585a6-ea58-4a69-acbd-7a267f34612e to disappear
Jul 29 03:15:13.035: INFO: Pod downwardapi-volume-62a585a6-ea58-4a69-acbd-7a267f34612e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:15:13.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4797" for this suite.
Jul 29 03:15:19.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:15:19.244: INFO: namespace projected-4797 deletion completed in 6.202228001s

• [SLOW TEST:10.359 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:15:19.246: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-c0b25e32-e822-4307-8636-0bf292cb9010
STEP: Creating a pod to test consume secrets
Jul 29 03:15:19.327: INFO: Waiting up to 5m0s for pod "pod-secrets-7c8af499-2fd2-496a-8cbc-8bc66db74b66" in namespace "secrets-8604" to be "success or failure"
Jul 29 03:15:19.332: INFO: Pod "pod-secrets-7c8af499-2fd2-496a-8cbc-8bc66db74b66": Phase="Pending", Reason="", readiness=false. Elapsed: 5.239188ms
Jul 29 03:15:21.338: INFO: Pod "pod-secrets-7c8af499-2fd2-496a-8cbc-8bc66db74b66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010610811s
Jul 29 03:15:23.347: INFO: Pod "pod-secrets-7c8af499-2fd2-496a-8cbc-8bc66db74b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020005625s
STEP: Saw pod success
Jul 29 03:15:23.347: INFO: Pod "pod-secrets-7c8af499-2fd2-496a-8cbc-8bc66db74b66" satisfied condition "success or failure"
Jul 29 03:15:23.356: INFO: Trying to get logs from node conformance0 pod pod-secrets-7c8af499-2fd2-496a-8cbc-8bc66db74b66 container secret-volume-test: <nil>
STEP: delete the pod
Jul 29 03:15:23.390: INFO: Waiting for pod pod-secrets-7c8af499-2fd2-496a-8cbc-8bc66db74b66 to disappear
Jul 29 03:15:23.396: INFO: Pod pod-secrets-7c8af499-2fd2-496a-8cbc-8bc66db74b66 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:15:23.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8604" for this suite.
Jul 29 03:15:29.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:15:29.582: INFO: namespace secrets-8604 deletion completed in 6.179096884s

• [SLOW TEST:10.336 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:15:29.583: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jul 29 03:15:29.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 create -f - --namespace=kubectl-7360'
Jul 29 03:15:30.537: INFO: stderr: ""
Jul 29 03:15:30.537: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jul 29 03:15:31.543: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 03:15:31.543: INFO: Found 0 / 1
Jul 29 03:15:32.542: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 03:15:32.542: INFO: Found 0 / 1
Jul 29 03:15:33.542: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 03:15:33.542: INFO: Found 0 / 1
Jul 29 03:15:34.542: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 03:15:34.542: INFO: Found 1 / 1
Jul 29 03:15:34.542: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 29 03:15:34.546: INFO: Selector matched 1 pods for map[app:redis]
Jul 29 03:15:34.547: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 29 03:15:34.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 logs redis-master-nd549 redis-master --namespace=kubectl-7360'
Jul 29 03:15:34.717: INFO: stderr: ""
Jul 29 03:15:34.717: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Jul 03:15:33.160 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Jul 03:15:33.160 # Server started, Redis version 3.2.12\n1:M 29 Jul 03:15:33.160 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Jul 03:15:33.160 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 29 03:15:34.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 log redis-master-nd549 redis-master --namespace=kubectl-7360 --tail=1'
Jul 29 03:15:34.874: INFO: stderr: ""
Jul 29 03:15:34.874: INFO: stdout: "1:M 29 Jul 03:15:33.160 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 29 03:15:34.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 log redis-master-nd549 redis-master --namespace=kubectl-7360 --limit-bytes=1'
Jul 29 03:15:35.025: INFO: stderr: ""
Jul 29 03:15:35.025: INFO: stdout: " "
STEP: exposing timestamps
Jul 29 03:15:35.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 log redis-master-nd549 redis-master --namespace=kubectl-7360 --tail=1 --timestamps'
Jul 29 03:15:35.182: INFO: stderr: ""
Jul 29 03:15:35.183: INFO: stdout: "2019-07-29T03:15:33.161008447Z 1:M 29 Jul 03:15:33.160 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 29 03:15:37.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 log redis-master-nd549 redis-master --namespace=kubectl-7360 --since=1s'
Jul 29 03:15:37.882: INFO: stderr: ""
Jul 29 03:15:37.882: INFO: stdout: ""
Jul 29 03:15:37.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 log redis-master-nd549 redis-master --namespace=kubectl-7360 --since=24h'
Jul 29 03:15:38.106: INFO: stderr: ""
Jul 29 03:15:38.106: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Jul 03:15:33.160 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Jul 03:15:33.160 # Server started, Redis version 3.2.12\n1:M 29 Jul 03:15:33.160 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Jul 03:15:33.160 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jul 29 03:15:38.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 delete --grace-period=0 --force -f - --namespace=kubectl-7360'
Jul 29 03:15:38.304: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 03:15:38.304: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul 29 03:15:38.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get rc,svc -l name=nginx --no-headers --namespace=kubectl-7360'
Jul 29 03:15:38.545: INFO: stderr: "No resources found.\n"
Jul 29 03:15:38.545: INFO: stdout: ""
Jul 29 03:15:38.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-518829307 get pods -l name=nginx --namespace=kubectl-7360 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 29 03:15:38.726: INFO: stderr: ""
Jul 29 03:15:38.726: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:15:38.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7360" for this suite.
Jul 29 03:15:44.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:15:44.921: INFO: namespace kubectl-7360 deletion completed in 6.188428289s

• [SLOW TEST:15.338 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:15:44.923: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 29 03:15:44.990: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98ab526c-b395-4d69-bfb2-f840273d4fad" in namespace "downward-api-82" to be "success or failure"
Jul 29 03:15:45.010: INFO: Pod "downwardapi-volume-98ab526c-b395-4d69-bfb2-f840273d4fad": Phase="Pending", Reason="", readiness=false. Elapsed: 19.96281ms
Jul 29 03:15:47.015: INFO: Pod "downwardapi-volume-98ab526c-b395-4d69-bfb2-f840273d4fad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024410095s
Jul 29 03:15:49.025: INFO: Pod "downwardapi-volume-98ab526c-b395-4d69-bfb2-f840273d4fad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034906747s
STEP: Saw pod success
Jul 29 03:15:49.026: INFO: Pod "downwardapi-volume-98ab526c-b395-4d69-bfb2-f840273d4fad" satisfied condition "success or failure"
Jul 29 03:15:49.033: INFO: Trying to get logs from node conformance1 pod downwardapi-volume-98ab526c-b395-4d69-bfb2-f840273d4fad container client-container: <nil>
STEP: delete the pod
Jul 29 03:15:49.086: INFO: Waiting for pod downwardapi-volume-98ab526c-b395-4d69-bfb2-f840273d4fad to disappear
Jul 29 03:15:49.097: INFO: Pod downwardapi-volume-98ab526c-b395-4d69-bfb2-f840273d4fad no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:15:49.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-82" for this suite.
Jul 29 03:15:55.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:15:55.323: INFO: namespace downward-api-82 deletion completed in 6.205444798s

• [SLOW TEST:10.401 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:15:55.328: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 29 03:15:55.396: INFO: Waiting up to 5m0s for pod "pod-a5ff9ef6-b626-45a1-b898-599ce0785296" in namespace "emptydir-1418" to be "success or failure"
Jul 29 03:15:55.406: INFO: Pod "pod-a5ff9ef6-b626-45a1-b898-599ce0785296": Phase="Pending", Reason="", readiness=false. Elapsed: 10.254106ms
Jul 29 03:15:57.411: INFO: Pod "pod-a5ff9ef6-b626-45a1-b898-599ce0785296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015058205s
Jul 29 03:15:59.469: INFO: Pod "pod-a5ff9ef6-b626-45a1-b898-599ce0785296": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073499678s
STEP: Saw pod success
Jul 29 03:15:59.470: INFO: Pod "pod-a5ff9ef6-b626-45a1-b898-599ce0785296" satisfied condition "success or failure"
Jul 29 03:15:59.475: INFO: Trying to get logs from node conformance1 pod pod-a5ff9ef6-b626-45a1-b898-599ce0785296 container test-container: <nil>
STEP: delete the pod
Jul 29 03:15:59.520: INFO: Waiting for pod pod-a5ff9ef6-b626-45a1-b898-599ce0785296 to disappear
Jul 29 03:15:59.529: INFO: Pod pod-a5ff9ef6-b626-45a1-b898-599ce0785296 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:15:59.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1418" for this suite.
Jul 29 03:16:05.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:16:05.714: INFO: namespace emptydir-1418 deletion completed in 6.178674266s

• [SLOW TEST:10.387 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:16:05.717: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-1be6edee-0959-4d78-97cd-b7b8ddea2ad8
STEP: Creating a pod to test consume secrets
Jul 29 03:16:05.792: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4631667-4e59-4fef-84dd-a91b994ac00f" in namespace "projected-9640" to be "success or failure"
Jul 29 03:16:05.804: INFO: Pod "pod-projected-secrets-f4631667-4e59-4fef-84dd-a91b994ac00f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048757ms
Jul 29 03:16:07.810: INFO: Pod "pod-projected-secrets-f4631667-4e59-4fef-84dd-a91b994ac00f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017254002s
Jul 29 03:16:09.816: INFO: Pod "pod-projected-secrets-f4631667-4e59-4fef-84dd-a91b994ac00f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023802278s
STEP: Saw pod success
Jul 29 03:16:09.816: INFO: Pod "pod-projected-secrets-f4631667-4e59-4fef-84dd-a91b994ac00f" satisfied condition "success or failure"
Jul 29 03:16:09.821: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-f4631667-4e59-4fef-84dd-a91b994ac00f container secret-volume-test: <nil>
STEP: delete the pod
Jul 29 03:16:09.852: INFO: Waiting for pod pod-projected-secrets-f4631667-4e59-4fef-84dd-a91b994ac00f to disappear
Jul 29 03:16:09.859: INFO: Pod pod-projected-secrets-f4631667-4e59-4fef-84dd-a91b994ac00f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:16:09.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9640" for this suite.
Jul 29 03:16:15.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:16:16.044: INFO: namespace projected-9640 deletion completed in 6.179698994s

• [SLOW TEST:10.327 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:16:16.050: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 29 03:16:24.218: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 29 03:16:24.225: INFO: Pod pod-with-poststart-http-hook still exists
Jul 29 03:16:26.225: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 29 03:16:26.229: INFO: Pod pod-with-poststart-http-hook still exists
Jul 29 03:16:28.225: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 29 03:16:28.230: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:16:28.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7856" for this suite.
Jul 29 03:16:50.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:16:50.397: INFO: namespace container-lifecycle-hook-7856 deletion completed in 22.160718682s

• [SLOW TEST:34.347 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:16:50.397: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-f10a41c6-3213-4efd-a8dd-1297c17d5302
Jul 29 03:16:50.469: INFO: Pod name my-hostname-basic-f10a41c6-3213-4efd-a8dd-1297c17d5302: Found 0 pods out of 1
Jul 29 03:16:55.475: INFO: Pod name my-hostname-basic-f10a41c6-3213-4efd-a8dd-1297c17d5302: Found 1 pods out of 1
Jul 29 03:16:55.475: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f10a41c6-3213-4efd-a8dd-1297c17d5302" are running
Jul 29 03:16:55.479: INFO: Pod "my-hostname-basic-f10a41c6-3213-4efd-a8dd-1297c17d5302-6qb6d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-29 03:16:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-29 03:16:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-29 03:16:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-29 03:16:50 +0000 UTC Reason: Message:}])
Jul 29 03:16:55.479: INFO: Trying to dial the pod
Jul 29 03:17:00.521: INFO: Controller my-hostname-basic-f10a41c6-3213-4efd-a8dd-1297c17d5302: Got expected result from replica 1 [my-hostname-basic-f10a41c6-3213-4efd-a8dd-1297c17d5302-6qb6d]: "my-hostname-basic-f10a41c6-3213-4efd-a8dd-1297c17d5302-6qb6d", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:17:00.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5945" for this suite.
Jul 29 03:17:06.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:17:06.723: INFO: namespace replication-controller-5945 deletion completed in 6.19582322s

• [SLOW TEST:16.326 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:17:06.724: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:17:32.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9812" for this suite.
Jul 29 03:17:38.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:17:39.096: INFO: namespace namespaces-9812 deletion completed in 6.168853538s
STEP: Destroying namespace "nsdeletetest-7748" for this suite.
Jul 29 03:17:39.101: INFO: Namespace nsdeletetest-7748 was already deleted
STEP: Destroying namespace "nsdeletetest-2320" for this suite.
Jul 29 03:17:45.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:17:45.286: INFO: namespace nsdeletetest-2320 deletion completed in 6.184848103s

• [SLOW TEST:38.563 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:17:45.291: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 29 03:17:45.348: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:17:51.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-904" for this suite.
Jul 29 03:18:15.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:18:15.413: INFO: namespace init-container-904 deletion completed in 24.175733102s

• [SLOW TEST:30.123 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:18:15.413: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 03:18:15.477: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 29 03:18:20.483: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 29 03:18:20.483: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Jul 29 03:18:24.542: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8307,SelfLink:/apis/apps/v1/namespaces/deployment-8307/deployments/test-cleanup-deployment,UID:8f303772-7e7c-461e-a12d-7a65ea1cd0cd,ResourceVersion:14684,Generation:1,CreationTimestamp:2019-07-29 03:18:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-29 03:18:20 +0000 UTC 2019-07-29 03:18:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-29 03:18:23 +0000 UTC 2019-07-29 03:18:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 29 03:18:24.547: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-8307,SelfLink:/apis/apps/v1/namespaces/deployment-8307/replicasets/test-cleanup-deployment-55bbcbc84c,UID:c320dfbd-37af-4bd0-a3ef-0c580ea1f947,ResourceVersion:14673,Generation:1,CreationTimestamp:2019-07-29 03:18:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 8f303772-7e7c-461e-a12d-7a65ea1cd0cd 0xc000d7d997 0xc000d7d998}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 29 03:18:24.554: INFO: Pod "test-cleanup-deployment-55bbcbc84c-k9zkt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-k9zkt,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-8307,SelfLink:/api/v1/namespaces/deployment-8307/pods/test-cleanup-deployment-55bbcbc84c-k9zkt,UID:550b3cb0-4fb0-45d2-a190-3b0f03dd24d9,ResourceVersion:14672,Generation:0,CreationTimestamp:2019-07-29 03:18:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c c320dfbd-37af-4bd0-a3ef-0c580ea1f947 0xc0007abae7 0xc0007abae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-889hn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-889hn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-889hn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007abf90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000267f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 03:18:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 03:18:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 03:18:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-29 03:18:20 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.213,PodIP:10.244.1.193,StartTime:2019-07-29 03:18:20 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-29 03:18:22 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://05ddfec28acb07968819898bef7294ee0d730aa2d4a2f26d15f1467d8a4b5e57}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:18:24.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8307" for this suite.
Jul 29 03:18:30.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:18:30.785: INFO: namespace deployment-8307 deletion completed in 6.226379785s

• [SLOW TEST:15.372 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:18:30.792: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 29 03:18:30.857: INFO: Waiting up to 5m0s for pod "downward-api-b26c5757-4e7b-4bee-8e67-da2e6aa17d15" in namespace "downward-api-6761" to be "success or failure"
Jul 29 03:18:30.865: INFO: Pod "downward-api-b26c5757-4e7b-4bee-8e67-da2e6aa17d15": Phase="Pending", Reason="", readiness=false. Elapsed: 8.647062ms
Jul 29 03:18:32.870: INFO: Pod "downward-api-b26c5757-4e7b-4bee-8e67-da2e6aa17d15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013649908s
Jul 29 03:18:34.875: INFO: Pod "downward-api-b26c5757-4e7b-4bee-8e67-da2e6aa17d15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018767971s
STEP: Saw pod success
Jul 29 03:18:34.876: INFO: Pod "downward-api-b26c5757-4e7b-4bee-8e67-da2e6aa17d15" satisfied condition "success or failure"
Jul 29 03:18:34.880: INFO: Trying to get logs from node conformance1 pod downward-api-b26c5757-4e7b-4bee-8e67-da2e6aa17d15 container dapi-container: <nil>
STEP: delete the pod
Jul 29 03:18:34.926: INFO: Waiting for pod downward-api-b26c5757-4e7b-4bee-8e67-da2e6aa17d15 to disappear
Jul 29 03:18:34.931: INFO: Pod downward-api-b26c5757-4e7b-4bee-8e67-da2e6aa17d15 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:18:34.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6761" for this suite.
Jul 29 03:18:40.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:18:41.112: INFO: namespace downward-api-6761 deletion completed in 6.174686165s

• [SLOW TEST:10.321 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:18:41.114: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-a1e1cd11-9db7-462d-b8b2-206fdf3b4f35
STEP: Creating secret with name s-test-opt-upd-c5d43c18-960b-458c-9d8f-7f31744cd2ec
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a1e1cd11-9db7-462d-b8b2-206fdf3b4f35
STEP: Updating secret s-test-opt-upd-c5d43c18-960b-458c-9d8f-7f31744cd2ec
STEP: Creating secret with name s-test-opt-create-685993fc-f74a-4d5e-af5e-53386a0ba04f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:18:51.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4042" for this suite.
Jul 29 03:19:13.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:19:13.630: INFO: namespace secrets-4042 deletion completed in 22.171536296s

• [SLOW TEST:32.516 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:19:13.637: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-5a1a85e5-129c-4bff-9eb6-531e61cbc126 in namespace container-probe-5597
Jul 29 03:19:17.735: INFO: Started pod busybox-5a1a85e5-129c-4bff-9eb6-531e61cbc126 in namespace container-probe-5597
STEP: checking the pod's current state and verifying that restartCount is present
Jul 29 03:19:17.740: INFO: Initial restart count of pod busybox-5a1a85e5-129c-4bff-9eb6-531e61cbc126 is 0
Jul 29 03:20:07.902: INFO: Restart count of pod container-probe-5597/busybox-5a1a85e5-129c-4bff-9eb6-531e61cbc126 is now 1 (50.161386828s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:20:07.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5597" for this suite.
Jul 29 03:20:13.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:20:14.136: INFO: namespace container-probe-5597 deletion completed in 6.202400187s

• [SLOW TEST:60.499 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:20:14.140: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 29 03:20:14.218: INFO: (0) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.354118ms)
Jul 29 03:20:14.225: INFO: (1) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.609834ms)
Jul 29 03:20:14.232: INFO: (2) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.53824ms)
Jul 29 03:20:14.239: INFO: (3) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.672387ms)
Jul 29 03:20:14.245: INFO: (4) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.466111ms)
Jul 29 03:20:14.252: INFO: (5) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.884839ms)
Jul 29 03:20:14.267: INFO: (6) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 14.681294ms)
Jul 29 03:20:14.276: INFO: (7) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.839825ms)
Jul 29 03:20:14.285: INFO: (8) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.653226ms)
Jul 29 03:20:14.293: INFO: (9) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.732772ms)
Jul 29 03:20:14.304: INFO: (10) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 11.070451ms)
Jul 29 03:20:14.312: INFO: (11) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.573591ms)
Jul 29 03:20:14.318: INFO: (12) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.361662ms)
Jul 29 03:20:14.326: INFO: (13) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.407395ms)
Jul 29 03:20:14.333: INFO: (14) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.766302ms)
Jul 29 03:20:14.339: INFO: (15) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.330159ms)
Jul 29 03:20:14.345: INFO: (16) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.027071ms)
Jul 29 03:20:14.353: INFO: (17) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.608838ms)
Jul 29 03:20:14.360: INFO: (18) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.815692ms)
Jul 29 03:20:14.368: INFO: (19) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.630913ms)
[AfterEach] version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:20:14.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1321" for this suite.
Jul 29 03:20:20.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:20:20.608: INFO: namespace proxy-1321 deletion completed in 6.234205519s

• [SLOW TEST:6.468 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:20:20.610: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 29 03:20:28.771: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:28.776: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:30.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:30.781: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:32.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:32.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:34.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:34.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:36.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:36.781: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:38.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:38.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:40.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:40.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:42.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:42.781: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:44.777: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:44.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:46.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:46.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:48.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:48.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:50.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:50.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:52.777: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:52.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:54.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:54.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:56.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:56.782: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 03:20:58.776: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 03:20:58.782: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:20:58.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1112" for this suite.
Jul 29 03:21:20.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:21:20.990: INFO: namespace container-lifecycle-hook-1112 deletion completed in 22.186858215s

• [SLOW TEST:60.381 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 29 03:21:20.998: INFO: >>> kubeConfig: /tmp/kubeconfig-518829307
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-5be05451-5a91-4899-8691-c69a5567e26d
STEP: Creating a pod to test consume configMaps
Jul 29 03:21:21.095: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a9b0b7bc-bb17-4cf8-8a08-b3eb3e654788" in namespace "projected-7842" to be "success or failure"
Jul 29 03:21:21.126: INFO: Pod "pod-projected-configmaps-a9b0b7bc-bb17-4cf8-8a08-b3eb3e654788": Phase="Pending", Reason="", readiness=false. Elapsed: 30.454547ms
Jul 29 03:21:23.142: INFO: Pod "pod-projected-configmaps-a9b0b7bc-bb17-4cf8-8a08-b3eb3e654788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047220052s
Jul 29 03:21:25.148: INFO: Pod "pod-projected-configmaps-a9b0b7bc-bb17-4cf8-8a08-b3eb3e654788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052627537s
STEP: Saw pod success
Jul 29 03:21:25.148: INFO: Pod "pod-projected-configmaps-a9b0b7bc-bb17-4cf8-8a08-b3eb3e654788" satisfied condition "success or failure"
Jul 29 03:21:25.153: INFO: Trying to get logs from node conformance1 pod pod-projected-configmaps-a9b0b7bc-bb17-4cf8-8a08-b3eb3e654788 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 29 03:21:25.190: INFO: Waiting for pod pod-projected-configmaps-a9b0b7bc-bb17-4cf8-8a08-b3eb3e654788 to disappear
Jul 29 03:21:25.201: INFO: Pod pod-projected-configmaps-a9b0b7bc-bb17-4cf8-8a08-b3eb3e654788 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 29 03:21:25.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7842" for this suite.
Jul 29 03:21:31.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 29 03:21:31.408: INFO: namespace projected-7842 deletion completed in 6.197983201s

• [SLOW TEST:10.411 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.1-beta.0.48+4485c6f18cee9a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSJul 29 03:21:31.409: INFO: Running AfterSuite actions on all nodes
Jul 29 03:21:31.437: INFO: Running AfterSuite actions on node 1
Jul 29 03:21:31.437: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 6175.325 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h43m3.303882228s
Test Suite Passed
