I0815 17:35:10.094882      19 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-136527297
I0815 17:35:10.095010      19 e2e.go:241] Starting e2e run "61cea873-2a88-4dee-8a86-f7691a8418b6" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1565890508 - Will randomize all specs
Will run 215 of 4411 specs

Aug 15 17:35:10.315: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:35:10.319: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 15 17:35:10.335: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 15 17:35:10.377: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 15 17:35:10.377: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Aug 15 17:35:10.377: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 15 17:35:10.386: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Aug 15 17:35:10.386: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-ds' (0 seconds elapsed)
Aug 15 17:35:10.386: INFO: e2e test version: v1.15.0
Aug 15 17:35:10.387: INFO: kube-apiserver version: v1.15.0
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:35:10.387: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename containers
Aug 15 17:35:10.438: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Aug 15 17:35:10.456: INFO: Waiting up to 5m0s for pod "client-containers-18164fe4-d5c7-4059-9a92-422187901fb1" in namespace "containers-1044" to be "success or failure"
Aug 15 17:35:10.461: INFO: Pod "client-containers-18164fe4-d5c7-4059-9a92-422187901fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377378ms
Aug 15 17:35:12.465: INFO: Pod "client-containers-18164fe4-d5c7-4059-9a92-422187901fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009361759s
Aug 15 17:35:14.470: INFO: Pod "client-containers-18164fe4-d5c7-4059-9a92-422187901fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014348514s
Aug 15 17:35:16.474: INFO: Pod "client-containers-18164fe4-d5c7-4059-9a92-422187901fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018050634s
Aug 15 17:35:18.482: INFO: Pod "client-containers-18164fe4-d5c7-4059-9a92-422187901fb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025514346s
STEP: Saw pod success
Aug 15 17:35:18.482: INFO: Pod "client-containers-18164fe4-d5c7-4059-9a92-422187901fb1" satisfied condition "success or failure"
Aug 15 17:35:18.488: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod client-containers-18164fe4-d5c7-4059-9a92-422187901fb1 container test-container: <nil>
STEP: delete the pod
Aug 15 17:35:18.521: INFO: Waiting for pod client-containers-18164fe4-d5c7-4059-9a92-422187901fb1 to disappear
Aug 15 17:35:18.524: INFO: Pod client-containers-18164fe4-d5c7-4059-9a92-422187901fb1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:35:18.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1044" for this suite.
Aug 15 17:35:24.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:35:24.720: INFO: namespace containers-1044 deletion completed in 6.192447285s

• [SLOW TEST:14.333 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:35:24.720: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 15 17:35:29.800: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:35:29.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7954" for this suite.
Aug 15 17:35:35.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:35:35.949: INFO: namespace container-runtime-7954 deletion completed in 6.121037181s

• [SLOW TEST:11.229 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:35:35.950: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-45488a0d-296b-45c3-952f-f25e026d5f91
STEP: Creating configMap with name cm-test-opt-upd-59f3e899-9aaf-45e6-b079-fdf17b013ea7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-45488a0d-296b-45c3-952f-f25e026d5f91
STEP: Updating configmap cm-test-opt-upd-59f3e899-9aaf-45e6-b079-fdf17b013ea7
STEP: Creating configMap with name cm-test-opt-create-77e21d1f-667b-4396-8980-12679be5f01b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:35:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6141" for this suite.
Aug 15 17:36:08.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:36:08.245: INFO: namespace projected-6141 deletion completed in 22.115314518s

• [SLOW TEST:32.295 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:36:08.246: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-d38b2ad6-88ef-43e3-9114-4cd04b5a5055
STEP: Creating a pod to test consume configMaps
Aug 15 17:36:08.301: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a7fcdf0-4a5a-4f04-af46-db0923bcfbd1" in namespace "configmap-4656" to be "success or failure"
Aug 15 17:36:08.308: INFO: Pod "pod-configmaps-5a7fcdf0-4a5a-4f04-af46-db0923bcfbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084333ms
Aug 15 17:36:10.312: INFO: Pod "pod-configmaps-5a7fcdf0-4a5a-4f04-af46-db0923bcfbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010630472s
Aug 15 17:36:12.316: INFO: Pod "pod-configmaps-5a7fcdf0-4a5a-4f04-af46-db0923bcfbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014762049s
Aug 15 17:36:14.323: INFO: Pod "pod-configmaps-5a7fcdf0-4a5a-4f04-af46-db0923bcfbd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021537685s
STEP: Saw pod success
Aug 15 17:36:14.323: INFO: Pod "pod-configmaps-5a7fcdf0-4a5a-4f04-af46-db0923bcfbd1" satisfied condition "success or failure"
Aug 15 17:36:14.327: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-configmaps-5a7fcdf0-4a5a-4f04-af46-db0923bcfbd1 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 17:36:14.350: INFO: Waiting for pod pod-configmaps-5a7fcdf0-4a5a-4f04-af46-db0923bcfbd1 to disappear
Aug 15 17:36:14.354: INFO: Pod pod-configmaps-5a7fcdf0-4a5a-4f04-af46-db0923bcfbd1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:36:14.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4656" for this suite.
Aug 15 17:36:20.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:36:20.478: INFO: namespace configmap-4656 deletion completed in 6.120120561s

• [SLOW TEST:12.233 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:36:20.479: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 15 17:36:20.514: INFO: namespace kubectl-9870
Aug 15 17:36:20.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-9870'
Aug 15 17:36:20.990: INFO: stderr: ""
Aug 15 17:36:20.991: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 15 17:36:21.995: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:36:21.995: INFO: Found 0 / 1
Aug 15 17:36:22.995: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:36:22.995: INFO: Found 0 / 1
Aug 15 17:36:23.995: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:36:23.995: INFO: Found 0 / 1
Aug 15 17:36:24.994: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:36:24.994: INFO: Found 0 / 1
Aug 15 17:36:25.995: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:36:25.995: INFO: Found 0 / 1
Aug 15 17:36:26.995: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:36:26.995: INFO: Found 1 / 1
Aug 15 17:36:26.995: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 15 17:36:26.997: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:36:26.997: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 15 17:36:26.997: INFO: wait on redis-master startup in kubectl-9870 
Aug 15 17:36:26.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 logs redis-master-4n2vh redis-master --namespace=kubectl-9870'
Aug 15 17:36:27.105: INFO: stderr: ""
Aug 15 17:36:27.105: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 15 Aug 17:36:25.709 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 15 Aug 17:36:25.709 # Server started, Redis version 3.2.12\n1:M 15 Aug 17:36:25.709 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 15 Aug 17:36:25.709 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug 15 17:36:27.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9870'
Aug 15 17:36:27.214: INFO: stderr: ""
Aug 15 17:36:27.214: INFO: stdout: "service/rm2 exposed\n"
Aug 15 17:36:27.226: INFO: Service rm2 in namespace kubectl-9870 found.
STEP: exposing service
Aug 15 17:36:29.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9870'
Aug 15 17:36:29.351: INFO: stderr: ""
Aug 15 17:36:29.351: INFO: stdout: "service/rm3 exposed\n"
Aug 15 17:36:29.358: INFO: Service rm3 in namespace kubectl-9870 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:36:31.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9870" for this suite.
Aug 15 17:36:53.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:36:53.487: INFO: namespace kubectl-9870 deletion completed in 22.11780622s

• [SLOW TEST:33.009 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:36:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 15 17:36:53.537: INFO: Waiting up to 5m0s for pod "pod-9d3d3b58-5513-4c8a-8481-6f7f9502bccb" in namespace "emptydir-5060" to be "success or failure"
Aug 15 17:36:53.540: INFO: Pod "pod-9d3d3b58-5513-4c8a-8481-6f7f9502bccb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.303033ms
Aug 15 17:36:55.544: INFO: Pod "pod-9d3d3b58-5513-4c8a-8481-6f7f9502bccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007247143s
Aug 15 17:36:57.549: INFO: Pod "pod-9d3d3b58-5513-4c8a-8481-6f7f9502bccb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011849667s
Aug 15 17:36:59.552: INFO: Pod "pod-9d3d3b58-5513-4c8a-8481-6f7f9502bccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015519316s
STEP: Saw pod success
Aug 15 17:36:59.552: INFO: Pod "pod-9d3d3b58-5513-4c8a-8481-6f7f9502bccb" satisfied condition "success or failure"
Aug 15 17:36:59.555: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-9d3d3b58-5513-4c8a-8481-6f7f9502bccb container test-container: <nil>
STEP: delete the pod
Aug 15 17:36:59.579: INFO: Waiting for pod pod-9d3d3b58-5513-4c8a-8481-6f7f9502bccb to disappear
Aug 15 17:36:59.582: INFO: Pod pod-9d3d3b58-5513-4c8a-8481-6f7f9502bccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:36:59.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5060" for this suite.
Aug 15 17:37:05.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:37:05.710: INFO: namespace emptydir-5060 deletion completed in 6.122590352s

• [SLOW TEST:12.222 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:37:05.710: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-7449/configmap-test-af02f3c2-52db-41ed-9cc3-9e5dcf71deb4
STEP: Creating a pod to test consume configMaps
Aug 15 17:37:05.759: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a36c43e-0af5-40ba-a530-5febe970f842" in namespace "configmap-7449" to be "success or failure"
Aug 15 17:37:05.763: INFO: Pod "pod-configmaps-2a36c43e-0af5-40ba-a530-5febe970f842": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109076ms
Aug 15 17:37:07.767: INFO: Pod "pod-configmaps-2a36c43e-0af5-40ba-a530-5febe970f842": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008326955s
Aug 15 17:37:09.771: INFO: Pod "pod-configmaps-2a36c43e-0af5-40ba-a530-5febe970f842": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012373674s
Aug 15 17:37:11.778: INFO: Pod "pod-configmaps-2a36c43e-0af5-40ba-a530-5febe970f842": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01917573s
STEP: Saw pod success
Aug 15 17:37:11.778: INFO: Pod "pod-configmaps-2a36c43e-0af5-40ba-a530-5febe970f842" satisfied condition "success or failure"
Aug 15 17:37:11.781: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-configmaps-2a36c43e-0af5-40ba-a530-5febe970f842 container env-test: <nil>
STEP: delete the pod
Aug 15 17:37:11.804: INFO: Waiting for pod pod-configmaps-2a36c43e-0af5-40ba-a530-5febe970f842 to disappear
Aug 15 17:37:11.808: INFO: Pod pod-configmaps-2a36c43e-0af5-40ba-a530-5febe970f842 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:37:11.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7449" for this suite.
Aug 15 17:37:17.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:37:17.927: INFO: namespace configmap-7449 deletion completed in 6.115253715s

• [SLOW TEST:12.217 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:37:17.928: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-771dbe9d-b394-4742-a6f7-e7bcf41783bd
STEP: Creating a pod to test consume secrets
Aug 15 17:37:17.985: INFO: Waiting up to 5m0s for pod "pod-secrets-a04bf974-f319-4b76-9a7b-4002e3ad9e36" in namespace "secrets-8071" to be "success or failure"
Aug 15 17:37:17.991: INFO: Pod "pod-secrets-a04bf974-f319-4b76-9a7b-4002e3ad9e36": Phase="Pending", Reason="", readiness=false. Elapsed: 5.263334ms
Aug 15 17:37:19.995: INFO: Pod "pod-secrets-a04bf974-f319-4b76-9a7b-4002e3ad9e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009526239s
Aug 15 17:37:21.999: INFO: Pod "pod-secrets-a04bf974-f319-4b76-9a7b-4002e3ad9e36": Phase="Running", Reason="", readiness=true. Elapsed: 4.013873386s
Aug 15 17:37:24.004: INFO: Pod "pod-secrets-a04bf974-f319-4b76-9a7b-4002e3ad9e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018974668s
STEP: Saw pod success
Aug 15 17:37:24.004: INFO: Pod "pod-secrets-a04bf974-f319-4b76-9a7b-4002e3ad9e36" satisfied condition "success or failure"
Aug 15 17:37:24.008: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-secrets-a04bf974-f319-4b76-9a7b-4002e3ad9e36 container secret-volume-test: <nil>
STEP: delete the pod
Aug 15 17:37:24.041: INFO: Waiting for pod pod-secrets-a04bf974-f319-4b76-9a7b-4002e3ad9e36 to disappear
Aug 15 17:37:24.045: INFO: Pod pod-secrets-a04bf974-f319-4b76-9a7b-4002e3ad9e36 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:37:24.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8071" for this suite.
Aug 15 17:37:30.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:37:30.164: INFO: namespace secrets-8071 deletion completed in 6.113955389s

• [SLOW TEST:12.237 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:37:30.164: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 17:37:30.204: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42d5dd1d-2ea8-48f2-ae48-f4ac999bf10c" in namespace "downward-api-2591" to be "success or failure"
Aug 15 17:37:30.212: INFO: Pod "downwardapi-volume-42d5dd1d-2ea8-48f2-ae48-f4ac999bf10c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.817893ms
Aug 15 17:37:32.216: INFO: Pod "downwardapi-volume-42d5dd1d-2ea8-48f2-ae48-f4ac999bf10c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012112303s
Aug 15 17:37:34.221: INFO: Pod "downwardapi-volume-42d5dd1d-2ea8-48f2-ae48-f4ac999bf10c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016153337s
STEP: Saw pod success
Aug 15 17:37:34.221: INFO: Pod "downwardapi-volume-42d5dd1d-2ea8-48f2-ae48-f4ac999bf10c" satisfied condition "success or failure"
Aug 15 17:37:34.223: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-42d5dd1d-2ea8-48f2-ae48-f4ac999bf10c container client-container: <nil>
STEP: delete the pod
Aug 15 17:37:34.245: INFO: Waiting for pod downwardapi-volume-42d5dd1d-2ea8-48f2-ae48-f4ac999bf10c to disappear
Aug 15 17:37:34.248: INFO: Pod downwardapi-volume-42d5dd1d-2ea8-48f2-ae48-f4ac999bf10c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:37:34.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2591" for this suite.
Aug 15 17:37:40.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:37:40.372: INFO: namespace downward-api-2591 deletion completed in 6.118954325s

• [SLOW TEST:10.207 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:37:40.372: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 15 17:37:40.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-8773'
Aug 15 17:37:40.677: INFO: stderr: ""
Aug 15 17:37:40.677: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 15 17:37:41.681: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:37:41.681: INFO: Found 0 / 1
Aug 15 17:37:42.682: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:37:42.682: INFO: Found 0 / 1
Aug 15 17:37:43.682: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:37:43.682: INFO: Found 0 / 1
Aug 15 17:37:44.682: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:37:44.682: INFO: Found 1 / 1
Aug 15 17:37:44.682: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 15 17:37:44.687: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:37:44.687: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 15 17:37:44.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 patch pod redis-master-vwdt2 --namespace=kubectl-8773 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 15 17:37:44.781: INFO: stderr: ""
Aug 15 17:37:44.781: INFO: stdout: "pod/redis-master-vwdt2 patched\n"
STEP: checking annotations
Aug 15 17:37:44.787: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:37:44.787: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:37:44.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8773" for this suite.
Aug 15 17:38:06.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:38:06.898: INFO: namespace kubectl-8773 deletion completed in 22.106754475s

• [SLOW TEST:26.526 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:38:06.898: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Aug 15 17:38:06.940: INFO: Waiting up to 5m0s for pod "var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439" in namespace "var-expansion-9422" to be "success or failure"
Aug 15 17:38:06.944: INFO: Pod "var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685175ms
Aug 15 17:38:08.949: INFO: Pod "var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008338758s
Aug 15 17:38:10.953: INFO: Pod "var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012957947s
Aug 15 17:38:12.958: INFO: Pod "var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018207977s
Aug 15 17:38:14.962: INFO: Pod "var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021839603s
STEP: Saw pod success
Aug 15 17:38:14.962: INFO: Pod "var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439" satisfied condition "success or failure"
Aug 15 17:38:14.965: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439 container dapi-container: <nil>
STEP: delete the pod
Aug 15 17:38:14.985: INFO: Waiting for pod var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439 to disappear
Aug 15 17:38:14.988: INFO: Pod var-expansion-6ad2b89a-72c6-4c2c-b2dd-6ebc6c4b8439 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:38:14.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9422" for this suite.
Aug 15 17:38:21.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:38:21.121: INFO: namespace var-expansion-9422 deletion completed in 6.129218575s

• [SLOW TEST:14.224 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:38:21.122: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1199
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 15 17:38:21.154: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 15 17:38:45.256: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.13:8080/dial?request=hostName&protocol=http&host=172.20.3.11&port=8080&tries=1'] Namespace:pod-network-test-1199 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 17:38:45.256: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:38:45.340: INFO: Waiting for endpoints: map[]
Aug 15 17:38:45.344: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.13:8080/dial?request=hostName&protocol=http&host=172.20.2.11&port=8080&tries=1'] Namespace:pod-network-test-1199 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 17:38:45.344: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:38:45.422: INFO: Waiting for endpoints: map[]
Aug 15 17:38:45.426: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.13:8080/dial?request=hostName&protocol=http&host=172.20.4.12&port=8080&tries=1'] Namespace:pod-network-test-1199 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 17:38:45.426: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:38:45.507: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:38:45.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1199" for this suite.
Aug 15 17:39:07.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:39:07.625: INFO: namespace pod-network-test-1199 deletion completed in 22.113992116s

• [SLOW TEST:46.504 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:39:07.626: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 17:39:07.671: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa0f2763-e060-4feb-ab97-582d789e7008" in namespace "projected-8887" to be "success or failure"
Aug 15 17:39:07.677: INFO: Pod "downwardapi-volume-fa0f2763-e060-4feb-ab97-582d789e7008": Phase="Pending", Reason="", readiness=false. Elapsed: 5.74354ms
Aug 15 17:39:09.683: INFO: Pod "downwardapi-volume-fa0f2763-e060-4feb-ab97-582d789e7008": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012068321s
STEP: Saw pod success
Aug 15 17:39:09.683: INFO: Pod "downwardapi-volume-fa0f2763-e060-4feb-ab97-582d789e7008" satisfied condition "success or failure"
Aug 15 17:39:09.688: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod downwardapi-volume-fa0f2763-e060-4feb-ab97-582d789e7008 container client-container: <nil>
STEP: delete the pod
Aug 15 17:39:09.714: INFO: Waiting for pod downwardapi-volume-fa0f2763-e060-4feb-ab97-582d789e7008 to disappear
Aug 15 17:39:09.717: INFO: Pod downwardapi-volume-fa0f2763-e060-4feb-ab97-582d789e7008 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:39:09.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8887" for this suite.
Aug 15 17:39:15.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:39:15.838: INFO: namespace projected-8887 deletion completed in 6.116483679s

• [SLOW TEST:8.212 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:39:15.838: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 17:39:15.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16dba170-fb5c-4a0e-b55c-c7287b4b8ce3" in namespace "projected-6263" to be "success or failure"
Aug 15 17:39:15.893: INFO: Pod "downwardapi-volume-16dba170-fb5c-4a0e-b55c-c7287b4b8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575861ms
Aug 15 17:39:17.899: INFO: Pod "downwardapi-volume-16dba170-fb5c-4a0e-b55c-c7287b4b8ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010178925s
Aug 15 17:39:19.904: INFO: Pod "downwardapi-volume-16dba170-fb5c-4a0e-b55c-c7287b4b8ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015018086s
STEP: Saw pod success
Aug 15 17:39:19.904: INFO: Pod "downwardapi-volume-16dba170-fb5c-4a0e-b55c-c7287b4b8ce3" satisfied condition "success or failure"
Aug 15 17:39:19.906: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-16dba170-fb5c-4a0e-b55c-c7287b4b8ce3 container client-container: <nil>
STEP: delete the pod
Aug 15 17:39:19.943: INFO: Waiting for pod downwardapi-volume-16dba170-fb5c-4a0e-b55c-c7287b4b8ce3 to disappear
Aug 15 17:39:19.947: INFO: Pod downwardapi-volume-16dba170-fb5c-4a0e-b55c-c7287b4b8ce3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:39:19.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6263" for this suite.
Aug 15 17:39:25.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:39:26.083: INFO: namespace projected-6263 deletion completed in 6.129058648s

• [SLOW TEST:10.245 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:39:26.084: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 17:39:26.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7618f782-f4db-40f3-9f68-9d0fc9ffb92c" in namespace "projected-8812" to be "success or failure"
Aug 15 17:39:26.138: INFO: Pod "downwardapi-volume-7618f782-f4db-40f3-9f68-9d0fc9ffb92c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.356636ms
Aug 15 17:39:28.143: INFO: Pod "downwardapi-volume-7618f782-f4db-40f3-9f68-9d0fc9ffb92c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010653597s
Aug 15 17:39:30.147: INFO: Pod "downwardapi-volume-7618f782-f4db-40f3-9f68-9d0fc9ffb92c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014794113s
STEP: Saw pod success
Aug 15 17:39:30.147: INFO: Pod "downwardapi-volume-7618f782-f4db-40f3-9f68-9d0fc9ffb92c" satisfied condition "success or failure"
Aug 15 17:39:30.150: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod downwardapi-volume-7618f782-f4db-40f3-9f68-9d0fc9ffb92c container client-container: <nil>
STEP: delete the pod
Aug 15 17:39:30.171: INFO: Waiting for pod downwardapi-volume-7618f782-f4db-40f3-9f68-9d0fc9ffb92c to disappear
Aug 15 17:39:30.176: INFO: Pod downwardapi-volume-7618f782-f4db-40f3-9f68-9d0fc9ffb92c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:39:30.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8812" for this suite.
Aug 15 17:39:36.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:39:36.291: INFO: namespace projected-8812 deletion completed in 6.107797533s

• [SLOW TEST:10.207 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:39:36.291: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 15 17:39:36.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8636'
Aug 15 17:39:36.434: INFO: stderr: ""
Aug 15 17:39:36.434: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Aug 15 17:39:36.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete pods e2e-test-nginx-pod --namespace=kubectl-8636'
Aug 15 17:39:41.056: INFO: stderr: ""
Aug 15 17:39:41.056: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:39:41.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8636" for this suite.
Aug 15 17:39:47.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:39:47.182: INFO: namespace kubectl-8636 deletion completed in 6.11970439s

• [SLOW TEST:10.890 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:39:47.182: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Aug 15 17:39:47.741: INFO: created pod pod-service-account-defaultsa
Aug 15 17:39:47.741: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 15 17:39:47.748: INFO: created pod pod-service-account-mountsa
Aug 15 17:39:47.748: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 15 17:39:47.761: INFO: created pod pod-service-account-nomountsa
Aug 15 17:39:47.761: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 15 17:39:47.782: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 15 17:39:47.782: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 15 17:39:47.795: INFO: created pod pod-service-account-mountsa-mountspec
Aug 15 17:39:47.795: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 15 17:39:47.818: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 15 17:39:47.819: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 15 17:39:47.835: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 15 17:39:47.835: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 15 17:39:47.845: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 15 17:39:47.845: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 15 17:39:47.856: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 15 17:39:47.856: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:39:47.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9440" for this suite.
Aug 15 17:39:53.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:39:53.997: INFO: namespace svcaccounts-9440 deletion completed in 6.127750195s

• [SLOW TEST:6.815 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:39:53.998: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 15 17:39:58.573: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e85c81da-13a5-4bec-ba9d-440e4e8304e9"
Aug 15 17:39:58.573: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e85c81da-13a5-4bec-ba9d-440e4e8304e9" in namespace "pods-9852" to be "terminated due to deadline exceeded"
Aug 15 17:39:58.577: INFO: Pod "pod-update-activedeadlineseconds-e85c81da-13a5-4bec-ba9d-440e4e8304e9": Phase="Running", Reason="", readiness=true. Elapsed: 3.880915ms
Aug 15 17:40:00.580: INFO: Pod "pod-update-activedeadlineseconds-e85c81da-13a5-4bec-ba9d-440e4e8304e9": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.007421558s
Aug 15 17:40:00.580: INFO: Pod "pod-update-activedeadlineseconds-e85c81da-13a5-4bec-ba9d-440e4e8304e9" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:40:00.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9852" for this suite.
Aug 15 17:40:06.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:40:06.697: INFO: namespace pods-9852 deletion completed in 6.110524546s

• [SLOW TEST:12.700 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:40:06.697: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 17:40:06.729: INFO: Creating deployment "test-recreate-deployment"
Aug 15 17:40:06.739: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 15 17:40:06.748: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 15 17:40:08.755: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 15 17:40:08.757: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701487606, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701487606, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701487606, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701487606, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 17:40:10.761: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 15 17:40:10.770: INFO: Updating deployment test-recreate-deployment
Aug 15 17:40:10.770: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 15 17:40:10.854: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-5403,SelfLink:/apis/apps/v1/namespaces/deployment-5403/deployments/test-recreate-deployment,UID:62030d0a-d14a-4b5a-b8b4-835ac6f876ae,ResourceVersion:9424,Generation:2,CreationTimestamp:2019-08-15 17:40:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-15 17:40:10 +0000 UTC 2019-08-15 17:40:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-15 17:40:10 +0000 UTC 2019-08-15 17:40:06 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug 15 17:40:10.857: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-5403,SelfLink:/apis/apps/v1/namespaces/deployment-5403/replicasets/test-recreate-deployment-5c8c9cc69d,UID:b00e5cfd-a25a-46bf-809e-aca109b9f06f,ResourceVersion:9422,Generation:1,CreationTimestamp:2019-08-15 17:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 62030d0a-d14a-4b5a-b8b4-835ac6f876ae 0xc00174f897 0xc00174f898}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 15 17:40:10.857: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 15 17:40:10.857: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-5403,SelfLink:/apis/apps/v1/namespaces/deployment-5403/replicasets/test-recreate-deployment-6df85df6b9,UID:888a16ee-d1ee-4b36-a85d-47269a6baf8b,ResourceVersion:9411,Generation:2,CreationTimestamp:2019-08-15 17:40:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 62030d0a-d14a-4b5a-b8b4-835ac6f876ae 0xc00174f957 0xc00174f958}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 15 17:40:10.860: INFO: Pod "test-recreate-deployment-5c8c9cc69d-f295h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-f295h,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-5403,SelfLink:/api/v1/namespaces/deployment-5403/pods/test-recreate-deployment-5c8c9cc69d-f295h,UID:68089b7f-0eed-475f-8f93-66a36709565e,ResourceVersion:9423,Generation:0,CreationTimestamp:2019-08-15 17:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d b00e5cfd-a25a-46bf-809e-aca109b9f06f 0xc0022e2227 0xc0022e2228}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wrd85 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wrd85,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wrd85 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022e2290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022e22b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 17:40:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 17:40:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 17:40:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 17:40:10 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.131,PodIP:,StartTime:2019-08-15 17:40:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:40:10.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5403" for this suite.
Aug 15 17:40:16.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:40:16.971: INFO: namespace deployment-5403 deletion completed in 6.107292382s

• [SLOW TEST:10.274 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:40:16.971: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-mtwl
STEP: Creating a pod to test atomic-volume-subpath
Aug 15 17:40:17.034: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mtwl" in namespace "subpath-8464" to be "success or failure"
Aug 15 17:40:17.044: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Pending", Reason="", readiness=false. Elapsed: 9.868901ms
Aug 15 17:40:19.048: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014146729s
Aug 15 17:40:21.051: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 4.017332465s
Aug 15 17:40:23.055: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 6.02110573s
Aug 15 17:40:25.059: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 8.025099279s
Aug 15 17:40:27.062: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 10.028642256s
Aug 15 17:40:29.066: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 12.03217209s
Aug 15 17:40:31.070: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 14.036175086s
Aug 15 17:40:33.074: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 16.040146606s
Aug 15 17:40:35.078: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 18.044241025s
Aug 15 17:40:37.082: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 20.048464399s
Aug 15 17:40:39.086: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Running", Reason="", readiness=true. Elapsed: 22.052589358s
Aug 15 17:40:41.091: INFO: Pod "pod-subpath-test-projected-mtwl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.057634876s
STEP: Saw pod success
Aug 15 17:40:41.091: INFO: Pod "pod-subpath-test-projected-mtwl" satisfied condition "success or failure"
Aug 15 17:40:41.095: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-subpath-test-projected-mtwl container test-container-subpath-projected-mtwl: <nil>
STEP: delete the pod
Aug 15 17:40:41.126: INFO: Waiting for pod pod-subpath-test-projected-mtwl to disappear
Aug 15 17:40:41.129: INFO: Pod pod-subpath-test-projected-mtwl no longer exists
STEP: Deleting pod pod-subpath-test-projected-mtwl
Aug 15 17:40:41.129: INFO: Deleting pod "pod-subpath-test-projected-mtwl" in namespace "subpath-8464"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:40:41.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8464" for this suite.
Aug 15 17:40:47.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:40:47.248: INFO: namespace subpath-8464 deletion completed in 6.111619003s

• [SLOW TEST:30.277 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:40:47.248: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 15 17:41:17.827: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:41:17.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0815 17:41:17.827083      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2658" for this suite.
Aug 15 17:41:23.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:41:23.946: INFO: namespace gc-2658 deletion completed in 6.115910156s

• [SLOW TEST:36.698 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:41:23.946: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 17:41:23.997: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4da9021-12c7-4c04-a05d-a884e779b027" in namespace "downward-api-6118" to be "success or failure"
Aug 15 17:41:24.001: INFO: Pod "downwardapi-volume-e4da9021-12c7-4c04-a05d-a884e779b027": Phase="Pending", Reason="", readiness=false. Elapsed: 4.147269ms
Aug 15 17:41:26.008: INFO: Pod "downwardapi-volume-e4da9021-12c7-4c04-a05d-a884e779b027": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010483778s
Aug 15 17:41:28.013: INFO: Pod "downwardapi-volume-e4da9021-12c7-4c04-a05d-a884e779b027": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015407073s
STEP: Saw pod success
Aug 15 17:41:28.013: INFO: Pod "downwardapi-volume-e4da9021-12c7-4c04-a05d-a884e779b027" satisfied condition "success or failure"
Aug 15 17:41:28.018: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-e4da9021-12c7-4c04-a05d-a884e779b027 container client-container: <nil>
STEP: delete the pod
Aug 15 17:41:28.046: INFO: Waiting for pod downwardapi-volume-e4da9021-12c7-4c04-a05d-a884e779b027 to disappear
Aug 15 17:41:28.049: INFO: Pod downwardapi-volume-e4da9021-12c7-4c04-a05d-a884e779b027 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:41:28.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6118" for this suite.
Aug 15 17:41:34.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:41:34.168: INFO: namespace downward-api-6118 deletion completed in 6.114074575s

• [SLOW TEST:10.222 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:41:34.168: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 17:41:34.211: INFO: Waiting up to 5m0s for pod "downwardapi-volume-214ed416-b9fe-4df7-b828-4cc1c78ad1d5" in namespace "projected-5677" to be "success or failure"
Aug 15 17:41:34.214: INFO: Pod "downwardapi-volume-214ed416-b9fe-4df7-b828-4cc1c78ad1d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918085ms
Aug 15 17:41:36.219: INFO: Pod "downwardapi-volume-214ed416-b9fe-4df7-b828-4cc1c78ad1d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007316469s
Aug 15 17:41:38.224: INFO: Pod "downwardapi-volume-214ed416-b9fe-4df7-b828-4cc1c78ad1d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012316109s
STEP: Saw pod success
Aug 15 17:41:38.224: INFO: Pod "downwardapi-volume-214ed416-b9fe-4df7-b828-4cc1c78ad1d5" satisfied condition "success or failure"
Aug 15 17:41:38.227: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod downwardapi-volume-214ed416-b9fe-4df7-b828-4cc1c78ad1d5 container client-container: <nil>
STEP: delete the pod
Aug 15 17:41:38.249: INFO: Waiting for pod downwardapi-volume-214ed416-b9fe-4df7-b828-4cc1c78ad1d5 to disappear
Aug 15 17:41:38.253: INFO: Pod downwardapi-volume-214ed416-b9fe-4df7-b828-4cc1c78ad1d5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:41:38.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5677" for this suite.
Aug 15 17:41:44.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:41:44.368: INFO: namespace projected-5677 deletion completed in 6.110053004s

• [SLOW TEST:10.200 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:41:44.369: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 17:41:44.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c03448ee-3e6d-4723-a491-2f0df0526ba1" in namespace "downward-api-5540" to be "success or failure"
Aug 15 17:41:44.420: INFO: Pod "downwardapi-volume-c03448ee-3e6d-4723-a491-2f0df0526ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.324094ms
Aug 15 17:41:46.424: INFO: Pod "downwardapi-volume-c03448ee-3e6d-4723-a491-2f0df0526ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009406962s
Aug 15 17:41:48.429: INFO: Pod "downwardapi-volume-c03448ee-3e6d-4723-a491-2f0df0526ba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01483449s
STEP: Saw pod success
Aug 15 17:41:48.430: INFO: Pod "downwardapi-volume-c03448ee-3e6d-4723-a491-2f0df0526ba1" satisfied condition "success or failure"
Aug 15 17:41:48.433: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod downwardapi-volume-c03448ee-3e6d-4723-a491-2f0df0526ba1 container client-container: <nil>
STEP: delete the pod
Aug 15 17:41:48.460: INFO: Waiting for pod downwardapi-volume-c03448ee-3e6d-4723-a491-2f0df0526ba1 to disappear
Aug 15 17:41:48.463: INFO: Pod downwardapi-volume-c03448ee-3e6d-4723-a491-2f0df0526ba1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:41:48.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5540" for this suite.
Aug 15 17:41:54.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:41:54.575: INFO: namespace downward-api-5540 deletion completed in 6.107303378s

• [SLOW TEST:10.206 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:41:54.575: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Aug 15 17:41:54.619: INFO: Waiting up to 5m0s for pod "var-expansion-7cbdb633-6d2e-405f-939a-8f5452c2ab29" in namespace "var-expansion-1647" to be "success or failure"
Aug 15 17:41:54.622: INFO: Pod "var-expansion-7cbdb633-6d2e-405f-939a-8f5452c2ab29": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259864ms
Aug 15 17:41:56.626: INFO: Pod "var-expansion-7cbdb633-6d2e-405f-939a-8f5452c2ab29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007471996s
Aug 15 17:41:58.631: INFO: Pod "var-expansion-7cbdb633-6d2e-405f-939a-8f5452c2ab29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012006918s
STEP: Saw pod success
Aug 15 17:41:58.631: INFO: Pod "var-expansion-7cbdb633-6d2e-405f-939a-8f5452c2ab29" satisfied condition "success or failure"
Aug 15 17:41:58.634: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod var-expansion-7cbdb633-6d2e-405f-939a-8f5452c2ab29 container dapi-container: <nil>
STEP: delete the pod
Aug 15 17:41:58.656: INFO: Waiting for pod var-expansion-7cbdb633-6d2e-405f-939a-8f5452c2ab29 to disappear
Aug 15 17:41:58.659: INFO: Pod var-expansion-7cbdb633-6d2e-405f-939a-8f5452c2ab29 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:41:58.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1647" for this suite.
Aug 15 17:42:04.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:42:04.774: INFO: namespace var-expansion-1647 deletion completed in 6.110783875s

• [SLOW TEST:10.200 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:42:04.775: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 15 17:42:04.823: INFO: Waiting up to 5m0s for pod "pod-9d3b214b-ffd6-403c-b38b-904cf36397e4" in namespace "emptydir-3556" to be "success or failure"
Aug 15 17:42:04.828: INFO: Pod "pod-9d3b214b-ffd6-403c-b38b-904cf36397e4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285031ms
Aug 15 17:42:06.832: INFO: Pod "pod-9d3b214b-ffd6-403c-b38b-904cf36397e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009179135s
STEP: Saw pod success
Aug 15 17:42:06.832: INFO: Pod "pod-9d3b214b-ffd6-403c-b38b-904cf36397e4" satisfied condition "success or failure"
Aug 15 17:42:06.835: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-9d3b214b-ffd6-403c-b38b-904cf36397e4 container test-container: <nil>
STEP: delete the pod
Aug 15 17:42:06.859: INFO: Waiting for pod pod-9d3b214b-ffd6-403c-b38b-904cf36397e4 to disappear
Aug 15 17:42:06.862: INFO: Pod pod-9d3b214b-ffd6-403c-b38b-904cf36397e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:42:06.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3556" for this suite.
Aug 15 17:42:12.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:42:12.995: INFO: namespace emptydir-3556 deletion completed in 6.128986397s

• [SLOW TEST:8.221 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:42:12.996: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 15 17:42:17.574: INFO: Successfully updated pod "pod-update-1983311a-7a0c-4e64-9d4a-1c166ffff930"
STEP: verifying the updated pod is in kubernetes
Aug 15 17:42:17.582: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:42:17.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1958" for this suite.
Aug 15 17:42:39.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:42:39.693: INFO: namespace pods-1958 deletion completed in 22.105631314s

• [SLOW TEST:26.697 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:42:39.693: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:43:39.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6471" for this suite.
Aug 15 17:44:01.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:44:01.891: INFO: namespace container-probe-6471 deletion completed in 22.14509495s

• [SLOW TEST:82.198 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:44:01.892: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 17:44:01.932: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:44:05.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1478" for this suite.
Aug 15 17:44:52.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:44:52.116: INFO: namespace pods-1478 deletion completed in 46.120462947s

• [SLOW TEST:50.224 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:44:52.116: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 15 17:44:52.153: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:45:03.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8590" for this suite.
Aug 15 17:45:09.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:45:10.103: INFO: namespace pods-8590 deletion completed in 6.126905565s

• [SLOW TEST:17.987 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:45:10.103: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-453
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 15 17:45:10.142: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 15 17:45:36.266: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.4.23 8081 | grep -v '^\s*$'] Namespace:pod-network-test-453 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 17:45:36.266: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:45:37.346: INFO: Found all expected endpoints: [netserver-0]
Aug 15 17:45:37.350: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.2.20 8081 | grep -v '^\s*$'] Namespace:pod-network-test-453 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 17:45:37.350: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:45:38.425: INFO: Found all expected endpoints: [netserver-1]
Aug 15 17:45:38.429: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.3.23 8081 | grep -v '^\s*$'] Namespace:pod-network-test-453 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 17:45:38.429: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:45:39.505: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:45:39.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-453" for this suite.
Aug 15 17:45:55.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:45:55.626: INFO: namespace pod-network-test-453 deletion completed in 16.116205057s

• [SLOW TEST:45.523 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:45:55.626: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-bb28c4e7-b07a-4c21-8ea3-bdf736d681c6
STEP: Creating a pod to test consume secrets
Aug 15 17:45:55.674: INFO: Waiting up to 5m0s for pod "pod-secrets-8e7953e3-2158-487f-93f6-d65a2a231293" in namespace "secrets-1515" to be "success or failure"
Aug 15 17:45:55.679: INFO: Pod "pod-secrets-8e7953e3-2158-487f-93f6-d65a2a231293": Phase="Pending", Reason="", readiness=false. Elapsed: 4.972633ms
Aug 15 17:45:57.683: INFO: Pod "pod-secrets-8e7953e3-2158-487f-93f6-d65a2a231293": Phase="Running", Reason="", readiness=true. Elapsed: 2.008651706s
Aug 15 17:45:59.687: INFO: Pod "pod-secrets-8e7953e3-2158-487f-93f6-d65a2a231293": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012888223s
STEP: Saw pod success
Aug 15 17:45:59.687: INFO: Pod "pod-secrets-8e7953e3-2158-487f-93f6-d65a2a231293" satisfied condition "success or failure"
Aug 15 17:45:59.690: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-secrets-8e7953e3-2158-487f-93f6-d65a2a231293 container secret-volume-test: <nil>
STEP: delete the pod
Aug 15 17:45:59.716: INFO: Waiting for pod pod-secrets-8e7953e3-2158-487f-93f6-d65a2a231293 to disappear
Aug 15 17:45:59.721: INFO: Pod pod-secrets-8e7953e3-2158-487f-93f6-d65a2a231293 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:45:59.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1515" for this suite.
Aug 15 17:46:05.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:46:05.850: INFO: namespace secrets-1515 deletion completed in 6.123787316s

• [SLOW TEST:10.224 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:46:05.851: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-76ee5bf7-27c5-4d74-82bd-6d377f9731ff
STEP: Creating a pod to test consume configMaps
Aug 15 17:46:05.903: INFO: Waiting up to 5m0s for pod "pod-configmaps-4eb25075-18c9-4dd1-9706-fc2f6136375f" in namespace "configmap-436" to be "success or failure"
Aug 15 17:46:05.909: INFO: Pod "pod-configmaps-4eb25075-18c9-4dd1-9706-fc2f6136375f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.551864ms
Aug 15 17:46:07.913: INFO: Pod "pod-configmaps-4eb25075-18c9-4dd1-9706-fc2f6136375f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010426681s
Aug 15 17:46:09.918: INFO: Pod "pod-configmaps-4eb25075-18c9-4dd1-9706-fc2f6136375f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015227646s
STEP: Saw pod success
Aug 15 17:46:09.918: INFO: Pod "pod-configmaps-4eb25075-18c9-4dd1-9706-fc2f6136375f" satisfied condition "success or failure"
Aug 15 17:46:09.922: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-configmaps-4eb25075-18c9-4dd1-9706-fc2f6136375f container configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 17:46:09.946: INFO: Waiting for pod pod-configmaps-4eb25075-18c9-4dd1-9706-fc2f6136375f to disappear
Aug 15 17:46:09.960: INFO: Pod pod-configmaps-4eb25075-18c9-4dd1-9706-fc2f6136375f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:46:09.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-436" for this suite.
Aug 15 17:46:15.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:46:16.072: INFO: namespace configmap-436 deletion completed in 6.107092056s

• [SLOW TEST:10.221 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:46:16.072: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 15 17:46:16.105: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:46:20.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-729" for this suite.
Aug 15 17:46:26.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:46:27.019: INFO: namespace init-container-729 deletion completed in 6.117022039s

• [SLOW TEST:10.947 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:46:27.019: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 15 17:46:27.064: INFO: Waiting up to 5m0s for pod "pod-afac27d0-a9a8-4742-b505-a05ffeca1b70" in namespace "emptydir-592" to be "success or failure"
Aug 15 17:46:27.069: INFO: Pod "pod-afac27d0-a9a8-4742-b505-a05ffeca1b70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.119844ms
Aug 15 17:46:29.074: INFO: Pod "pod-afac27d0-a9a8-4742-b505-a05ffeca1b70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009940377s
Aug 15 17:46:31.078: INFO: Pod "pod-afac27d0-a9a8-4742-b505-a05ffeca1b70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014062157s
STEP: Saw pod success
Aug 15 17:46:31.078: INFO: Pod "pod-afac27d0-a9a8-4742-b505-a05ffeca1b70" satisfied condition "success or failure"
Aug 15 17:46:31.081: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-afac27d0-a9a8-4742-b505-a05ffeca1b70 container test-container: <nil>
STEP: delete the pod
Aug 15 17:46:31.103: INFO: Waiting for pod pod-afac27d0-a9a8-4742-b505-a05ffeca1b70 to disappear
Aug 15 17:46:31.107: INFO: Pod pod-afac27d0-a9a8-4742-b505-a05ffeca1b70 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:46:31.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-592" for this suite.
Aug 15 17:46:37.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:46:37.216: INFO: namespace emptydir-592 deletion completed in 6.103418816s

• [SLOW TEST:10.197 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:46:37.217: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 15 17:46:37.284: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:37.284: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:37.287: INFO: Number of nodes with available pods: 0
Aug 15 17:46:37.287: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:46:38.291: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:38.291: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:38.294: INFO: Number of nodes with available pods: 0
Aug 15 17:46:38.294: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:46:39.292: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:39.292: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:39.295: INFO: Number of nodes with available pods: 0
Aug 15 17:46:39.295: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:46:40.292: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:40.292: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:40.295: INFO: Number of nodes with available pods: 3
Aug 15 17:46:40.295: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 15 17:46:40.311: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:40.311: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:40.314: INFO: Number of nodes with available pods: 2
Aug 15 17:46:40.314: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:46:41.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:41.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:41.322: INFO: Number of nodes with available pods: 2
Aug 15 17:46:41.322: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:46:42.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:42.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:42.322: INFO: Number of nodes with available pods: 2
Aug 15 17:46:42.322: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:46:43.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:43.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:43.322: INFO: Number of nodes with available pods: 2
Aug 15 17:46:43.322: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:46:44.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:44.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:44.323: INFO: Number of nodes with available pods: 2
Aug 15 17:46:44.323: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:46:45.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:45.320: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:45.324: INFO: Number of nodes with available pods: 2
Aug 15 17:46:45.324: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:46:46.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:46.319: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:46:46.323: INFO: Number of nodes with available pods: 3
Aug 15 17:46:46.323: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8923, will wait for the garbage collector to delete the pods
Aug 15 17:46:46.388: INFO: Deleting DaemonSet.extensions daemon-set took: 8.648361ms
Aug 15 17:46:46.789: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.357093ms
Aug 15 17:46:59.393: INFO: Number of nodes with available pods: 0
Aug 15 17:46:59.393: INFO: Number of running nodes: 0, number of available pods: 0
Aug 15 17:46:59.397: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8923/daemonsets","resourceVersion":"10756"},"items":null}

Aug 15 17:46:59.400: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8923/pods","resourceVersion":"10756"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:46:59.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8923" for this suite.
Aug 15 17:47:05.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:47:05.533: INFO: namespace daemonsets-8923 deletion completed in 6.113719583s

• [SLOW TEST:28.316 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:47:05.533: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-3ed52dae-49a5-4f50-a0fb-00ae7fc624e4
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:47:05.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7729" for this suite.
Aug 15 17:47:11.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:47:11.688: INFO: namespace configmap-7729 deletion completed in 6.117321275s

• [SLOW TEST:6.155 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:47:11.688: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6715
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 15 17:47:11.721: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 15 17:47:35.826: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.27:8080/dial?request=hostName&protocol=udp&host=172.20.2.24&port=8081&tries=1'] Namespace:pod-network-test-6715 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 17:47:35.826: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:47:35.913: INFO: Waiting for endpoints: map[]
Aug 15 17:47:35.917: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.27:8080/dial?request=hostName&protocol=udp&host=172.20.4.26&port=8081&tries=1'] Namespace:pod-network-test-6715 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 17:47:35.917: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:47:35.998: INFO: Waiting for endpoints: map[]
Aug 15 17:47:36.005: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.27:8080/dial?request=hostName&protocol=udp&host=172.20.3.28&port=8081&tries=1'] Namespace:pod-network-test-6715 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 17:47:36.005: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 17:47:36.088: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:47:36.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6715" for this suite.
Aug 15 17:47:58.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:47:58.212: INFO: namespace pod-network-test-6715 deletion completed in 22.118941565s

• [SLOW TEST:46.524 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:47:58.212: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-4295c670-643c-4b94-b4ef-ede28294cbda
Aug 15 17:47:58.270: INFO: Pod name my-hostname-basic-4295c670-643c-4b94-b4ef-ede28294cbda: Found 0 pods out of 1
Aug 15 17:48:03.275: INFO: Pod name my-hostname-basic-4295c670-643c-4b94-b4ef-ede28294cbda: Found 1 pods out of 1
Aug 15 17:48:03.275: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4295c670-643c-4b94-b4ef-ede28294cbda" are running
Aug 15 17:48:03.280: INFO: Pod "my-hostname-basic-4295c670-643c-4b94-b4ef-ede28294cbda-jqfw6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-15 17:47:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-15 17:48:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-15 17:48:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-15 17:47:58 +0000 UTC Reason: Message:}])
Aug 15 17:48:03.280: INFO: Trying to dial the pod
Aug 15 17:48:08.295: INFO: Controller my-hostname-basic-4295c670-643c-4b94-b4ef-ede28294cbda: Got expected result from replica 1 [my-hostname-basic-4295c670-643c-4b94-b4ef-ede28294cbda-jqfw6]: "my-hostname-basic-4295c670-643c-4b94-b4ef-ede28294cbda-jqfw6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:48:08.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8005" for this suite.
Aug 15 17:48:14.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:48:14.409: INFO: namespace replication-controller-8005 deletion completed in 6.106600067s

• [SLOW TEST:16.197 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:48:14.409: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Aug 15 17:48:14.450: INFO: Waiting up to 5m0s for pod "client-containers-e95ea9a8-fd35-41c0-8116-4ea6f5c4f48a" in namespace "containers-7245" to be "success or failure"
Aug 15 17:48:14.453: INFO: Pod "client-containers-e95ea9a8-fd35-41c0-8116-4ea6f5c4f48a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19621ms
Aug 15 17:48:16.457: INFO: Pod "client-containers-e95ea9a8-fd35-41c0-8116-4ea6f5c4f48a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006652008s
Aug 15 17:48:18.461: INFO: Pod "client-containers-e95ea9a8-fd35-41c0-8116-4ea6f5c4f48a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01063553s
STEP: Saw pod success
Aug 15 17:48:18.461: INFO: Pod "client-containers-e95ea9a8-fd35-41c0-8116-4ea6f5c4f48a" satisfied condition "success or failure"
Aug 15 17:48:18.463: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod client-containers-e95ea9a8-fd35-41c0-8116-4ea6f5c4f48a container test-container: <nil>
STEP: delete the pod
Aug 15 17:48:18.482: INFO: Waiting for pod client-containers-e95ea9a8-fd35-41c0-8116-4ea6f5c4f48a to disappear
Aug 15 17:48:18.487: INFO: Pod client-containers-e95ea9a8-fd35-41c0-8116-4ea6f5c4f48a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:48:18.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7245" for this suite.
Aug 15 17:48:24.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:48:24.612: INFO: namespace containers-7245 deletion completed in 6.120148209s

• [SLOW TEST:10.203 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:48:24.612: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 15 17:48:28.681: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-136527297 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 15 17:48:43.774: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:48:43.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7943" for this suite.
Aug 15 17:48:49.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:48:49.888: INFO: namespace pods-7943 deletion completed in 6.106904026s

• [SLOW TEST:25.276 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:48:49.889: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7223
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7223
STEP: Creating statefulset with conflicting port in namespace statefulset-7223
STEP: Waiting until pod test-pod will start running in namespace statefulset-7223
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7223
Aug 15 17:48:53.971: INFO: Observed stateful pod in namespace: statefulset-7223, name: ss-0, uid: dcca59f7-5118-46ce-aa45-e56196c21277, status phase: Pending. Waiting for statefulset controller to delete.
Aug 15 17:48:54.158: INFO: Observed stateful pod in namespace: statefulset-7223, name: ss-0, uid: dcca59f7-5118-46ce-aa45-e56196c21277, status phase: Failed. Waiting for statefulset controller to delete.
Aug 15 17:48:54.165: INFO: Observed stateful pod in namespace: statefulset-7223, name: ss-0, uid: dcca59f7-5118-46ce-aa45-e56196c21277, status phase: Failed. Waiting for statefulset controller to delete.
Aug 15 17:48:54.170: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7223
STEP: Removing pod with conflicting port in namespace statefulset-7223
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7223 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 15 17:49:00.209: INFO: Deleting all statefulset in ns statefulset-7223
Aug 15 17:49:00.212: INFO: Scaling statefulset ss to 0
Aug 15 17:49:10.234: INFO: Waiting for statefulset status.replicas updated to 0
Aug 15 17:49:10.237: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:49:10.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7223" for this suite.
Aug 15 17:49:16.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:49:16.399: INFO: namespace statefulset-7223 deletion completed in 6.139056981s

• [SLOW TEST:26.511 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:49:16.399: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-0728f62d-c256-44c8-b639-80bbd77fa2ab
STEP: Creating secret with name s-test-opt-upd-3c6328ca-a434-47ff-b052-12b4a96ec0c9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0728f62d-c256-44c8-b639-80bbd77fa2ab
STEP: Updating secret s-test-opt-upd-3c6328ca-a434-47ff-b052-12b4a96ec0c9
STEP: Creating secret with name s-test-opt-create-4adf6263-ac6e-4425-b9fa-ffa2ba469a1d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:49:24.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9446" for this suite.
Aug 15 17:49:46.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:49:46.677: INFO: namespace secrets-9446 deletion completed in 22.110604479s

• [SLOW TEST:30.277 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:49:46.677: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Aug 15 17:49:46.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 api-versions'
Aug 15 17:49:46.793: INFO: stderr: ""
Aug 15 17:49:46.793: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:49:46.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6811" for this suite.
Aug 15 17:49:52.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:49:52.901: INFO: namespace kubectl-6811 deletion completed in 6.10243744s

• [SLOW TEST:6.224 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:49:52.901: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-7531
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7531
STEP: Deleting pre-stop pod
Aug 15 17:50:07.981: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:50:07.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7531" for this suite.
Aug 15 17:50:46.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:50:46.113: INFO: namespace prestop-7531 deletion completed in 38.119047189s

• [SLOW TEST:53.212 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:50:46.113: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 15 17:50:46.372: INFO: Pod name wrapped-volume-race-b0495d9c-23fe-454b-ae62-2598eccb082c: Found 0 pods out of 5
Aug 15 17:50:51.379: INFO: Pod name wrapped-volume-race-b0495d9c-23fe-454b-ae62-2598eccb082c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b0495d9c-23fe-454b-ae62-2598eccb082c in namespace emptydir-wrapper-9425, will wait for the garbage collector to delete the pods
Aug 15 17:51:01.479: INFO: Deleting ReplicationController wrapped-volume-race-b0495d9c-23fe-454b-ae62-2598eccb082c took: 10.797851ms
Aug 15 17:51:01.880: INFO: Terminating ReplicationController wrapped-volume-race-b0495d9c-23fe-454b-ae62-2598eccb082c pods took: 400.271096ms
STEP: Creating RC which spawns configmap-volume pods
Aug 15 17:51:39.398: INFO: Pod name wrapped-volume-race-33e2cbba-405f-4d4a-bf15-e4d284a88acd: Found 0 pods out of 5
Aug 15 17:51:44.405: INFO: Pod name wrapped-volume-race-33e2cbba-405f-4d4a-bf15-e4d284a88acd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-33e2cbba-405f-4d4a-bf15-e4d284a88acd in namespace emptydir-wrapper-9425, will wait for the garbage collector to delete the pods
Aug 15 17:51:54.494: INFO: Deleting ReplicationController wrapped-volume-race-33e2cbba-405f-4d4a-bf15-e4d284a88acd took: 9.690852ms
Aug 15 17:51:54.894: INFO: Terminating ReplicationController wrapped-volume-race-33e2cbba-405f-4d4a-bf15-e4d284a88acd pods took: 400.252195ms
STEP: Creating RC which spawns configmap-volume pods
Aug 15 17:52:39.823: INFO: Pod name wrapped-volume-race-1896c221-7239-49c7-b903-f2edba90d15a: Found 1 pods out of 5
Aug 15 17:52:44.830: INFO: Pod name wrapped-volume-race-1896c221-7239-49c7-b903-f2edba90d15a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1896c221-7239-49c7-b903-f2edba90d15a in namespace emptydir-wrapper-9425, will wait for the garbage collector to delete the pods
Aug 15 17:52:56.918: INFO: Deleting ReplicationController wrapped-volume-race-1896c221-7239-49c7-b903-f2edba90d15a took: 9.876527ms
Aug 15 17:52:57.318: INFO: Terminating ReplicationController wrapped-volume-race-1896c221-7239-49c7-b903-f2edba90d15a pods took: 400.23933ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:53:39.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9425" for this suite.
Aug 15 17:53:47.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:53:47.902: INFO: namespace emptydir-wrapper-9425 deletion completed in 8.120930528s

• [SLOW TEST:181.789 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:53:47.903: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 15 17:53:47.943: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:53:52.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5399" for this suite.
Aug 15 17:54:14.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:54:14.812: INFO: namespace init-container-5399 deletion completed in 22.117674853s

• [SLOW TEST:26.909 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:54:14.812: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 15 17:54:18.882: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:54:18.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8766" for this suite.
Aug 15 17:54:24.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:54:25.027: INFO: namespace container-runtime-8766 deletion completed in 6.122874902s

• [SLOW TEST:10.214 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:54:25.027: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 17:54:25.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-1143'
Aug 15 17:54:25.613: INFO: stderr: ""
Aug 15 17:54:25.613: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 15 17:54:25.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-1143'
Aug 15 17:54:25.920: INFO: stderr: ""
Aug 15 17:54:25.920: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 15 17:54:26.925: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:54:26.925: INFO: Found 0 / 1
Aug 15 17:54:27.925: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:54:27.925: INFO: Found 0 / 1
Aug 15 17:54:28.925: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:54:28.925: INFO: Found 1 / 1
Aug 15 17:54:28.925: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 15 17:54:28.928: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 17:54:28.928: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 15 17:54:28.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 describe pod redis-master-4rw5n --namespace=kubectl-1143'
Aug 15 17:54:29.024: INFO: stderr: ""
Aug 15 17:54:29.024: INFO: stdout: "Name:           redis-master-4rw5n\nNamespace:      kubectl-1143\nPriority:       0\nNode:           karbon-fifteen-3efeed-k8s-worker-0/10.45.40.137\nStart Time:     Thu, 15 Aug 2019 17:54:25 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             172.20.2.44\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://3c554e45204b2947f69b812267ead73eb105b6ed1e2a1408328506ccc1077b3c\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 15 Aug 2019 17:54:28 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rx8cr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rx8cr:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rx8cr\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                         Message\n  ----    ------     ----  ----                                         -------\n  Normal  Scheduled  4s    default-scheduler                            Successfully assigned kubectl-1143/redis-master-4rw5n to karbon-fifteen-3efeed-k8s-worker-0\n  Normal  Pulled     1s    kubelet, karbon-fifteen-3efeed-k8s-worker-0  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, karbon-fifteen-3efeed-k8s-worker-0  Created container redis-master\n  Normal  Started    1s    kubelet, karbon-fifteen-3efeed-k8s-worker-0  Started container redis-master\n"
Aug 15 17:54:29.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 describe rc redis-master --namespace=kubectl-1143'
Aug 15 17:54:29.142: INFO: stderr: ""
Aug 15 17:54:29.142: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1143\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-4rw5n\n"
Aug 15 17:54:29.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 describe service redis-master --namespace=kubectl-1143'
Aug 15 17:54:29.247: INFO: stderr: ""
Aug 15 17:54:29.247: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1143\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.19.53.210\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.20.2.44:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 15 17:54:29.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 describe node karbon-fifteen-3efeed-k8s-master-0'
Aug 15 17:54:29.377: INFO: stderr: ""
Aug 15 17:54:29.377: INFO: stdout: "Name:               karbon-fifteen-3efeed-k8s-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=karbon-fifteen-3efeed-k8s-master-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"c6:1c:a2:6a:a7:f5\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.45.40.136\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 15 Aug 2019 16:30:34 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 15 Aug 2019 17:53:38 +0000   Thu, 15 Aug 2019 16:30:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 15 Aug 2019 17:53:38 +0000   Thu, 15 Aug 2019 16:30:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 15 Aug 2019 17:53:38 +0000   Thu, 15 Aug 2019 16:30:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 15 Aug 2019 17:53:38 +0000   Thu, 15 Aug 2019 16:30:32 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.45.40.136\n  Hostname:    karbon-fifteen-3efeed-k8s-master-0\nCapacity:\n cpu:                4\n ephemeral-storage:  123723328Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3844196Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  123723328Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3434596Ki\n pods:               110\nSystem Info:\n Machine ID:                 96474ca3c4d44e5aaf49661210c3d9de\n System UUID:                01A0D2DB-CA60-4875-9FE0-196998E32E78\n Boot ID:                    1a04b7f3-f8fd-4982-919f-1c97fbbc57f4\n Kernel Version:             3.10.0-957.10.1.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     172.20.0.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-jh6rk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                kube-apiserver-karbon-fifteen-3efeed-k8s-master-0          300m (7%)     0 (0%)      0 (0%)           0 (0%)         82m\n  kube-system                kube-dns-7754dc67cd-g59s5                                  260m (6%)     200m (5%)   110Mi (3%)       170Mi (5%)     81m\n  kube-system                kube-flannel-ds-fkbj6                                      100m (2%)     500m (12%)  50Mi (1%)        50Mi (1%)      81m\n  kube-system                kube-proxy-ds-lthcv                                        100m (2%)     100m (2%)   70Mi (2%)        70Mi (2%)      81m\n  ntnx-system                fluent-bit-kdkl2                                           100m (2%)     100m (2%)   50Mi (1%)        50Mi (1%)      80m\n  ntnx-system                node-exporter-mckgg                                        112m (2%)     600m (15%)  200Mi (5%)       220Mi (6%)     77m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                972m (24%)   1500m (37%)\n  memory             480Mi (14%)  560Mi (16%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:              <none>\n"
Aug 15 17:54:29.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 describe namespace kubectl-1143'
Aug 15 17:54:29.476: INFO: stderr: ""
Aug 15 17:54:29.476: INFO: stdout: "Name:         kubectl-1143\nLabels:       e2e-framework=kubectl\n              e2e-run=61cea873-2a88-4dee-8a86-f7691a8418b6\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:54:29.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1143" for this suite.
Aug 15 17:54:51.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:54:51.587: INFO: namespace kubectl-1143 deletion completed in 22.107832457s

• [SLOW TEST:26.560 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:54:51.588: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 15 17:54:54.658: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:54:54.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5800" for this suite.
Aug 15 17:55:00.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:55:00.780: INFO: namespace container-runtime-5800 deletion completed in 6.104593742s

• [SLOW TEST:9.192 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:55:00.780: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-14e6498c-47de-41aa-a227-4204f45be7ce
STEP: Creating a pod to test consume secrets
Aug 15 17:55:00.835: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fe3b78d5-7bf9-45e7-9d44-cf301c233b24" in namespace "projected-5712" to be "success or failure"
Aug 15 17:55:00.841: INFO: Pod "pod-projected-secrets-fe3b78d5-7bf9-45e7-9d44-cf301c233b24": Phase="Pending", Reason="", readiness=false. Elapsed: 5.851076ms
Aug 15 17:55:02.845: INFO: Pod "pod-projected-secrets-fe3b78d5-7bf9-45e7-9d44-cf301c233b24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009973656s
Aug 15 17:55:04.849: INFO: Pod "pod-projected-secrets-fe3b78d5-7bf9-45e7-9d44-cf301c233b24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014578778s
STEP: Saw pod success
Aug 15 17:55:04.849: INFO: Pod "pod-projected-secrets-fe3b78d5-7bf9-45e7-9d44-cf301c233b24" satisfied condition "success or failure"
Aug 15 17:55:04.852: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-projected-secrets-fe3b78d5-7bf9-45e7-9d44-cf301c233b24 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 15 17:55:04.877: INFO: Waiting for pod pod-projected-secrets-fe3b78d5-7bf9-45e7-9d44-cf301c233b24 to disappear
Aug 15 17:55:04.881: INFO: Pod pod-projected-secrets-fe3b78d5-7bf9-45e7-9d44-cf301c233b24 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:55:04.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5712" for this suite.
Aug 15 17:55:10.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:55:10.993: INFO: namespace projected-5712 deletion completed in 6.107900325s

• [SLOW TEST:10.213 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:55:10.993: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-23758f32-659c-4baa-9a4f-2677a05366c8
STEP: Creating a pod to test consume secrets
Aug 15 17:55:11.055: INFO: Waiting up to 5m0s for pod "pod-secrets-8b8125a9-ab26-41af-a23c-b26576ea0f4d" in namespace "secrets-6386" to be "success or failure"
Aug 15 17:55:11.060: INFO: Pod "pod-secrets-8b8125a9-ab26-41af-a23c-b26576ea0f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.621846ms
Aug 15 17:55:13.064: INFO: Pod "pod-secrets-8b8125a9-ab26-41af-a23c-b26576ea0f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008794649s
Aug 15 17:55:15.069: INFO: Pod "pod-secrets-8b8125a9-ab26-41af-a23c-b26576ea0f4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013650208s
STEP: Saw pod success
Aug 15 17:55:15.069: INFO: Pod "pod-secrets-8b8125a9-ab26-41af-a23c-b26576ea0f4d" satisfied condition "success or failure"
Aug 15 17:55:15.073: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-secrets-8b8125a9-ab26-41af-a23c-b26576ea0f4d container secret-volume-test: <nil>
STEP: delete the pod
Aug 15 17:55:15.096: INFO: Waiting for pod pod-secrets-8b8125a9-ab26-41af-a23c-b26576ea0f4d to disappear
Aug 15 17:55:15.101: INFO: Pod pod-secrets-8b8125a9-ab26-41af-a23c-b26576ea0f4d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:55:15.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6386" for this suite.
Aug 15 17:55:21.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:55:21.214: INFO: namespace secrets-6386 deletion completed in 6.108852719s

• [SLOW TEST:10.221 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:55:21.215: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 17:55:21.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b618a383-011d-49ca-813f-df65ef832a1f" in namespace "projected-509" to be "success or failure"
Aug 15 17:55:21.267: INFO: Pod "downwardapi-volume-b618a383-011d-49ca-813f-df65ef832a1f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.70124ms
Aug 15 17:55:23.271: INFO: Pod "downwardapi-volume-b618a383-011d-49ca-813f-df65ef832a1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009883442s
Aug 15 17:55:25.275: INFO: Pod "downwardapi-volume-b618a383-011d-49ca-813f-df65ef832a1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01421599s
STEP: Saw pod success
Aug 15 17:55:25.275: INFO: Pod "downwardapi-volume-b618a383-011d-49ca-813f-df65ef832a1f" satisfied condition "success or failure"
Aug 15 17:55:25.278: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod downwardapi-volume-b618a383-011d-49ca-813f-df65ef832a1f container client-container: <nil>
STEP: delete the pod
Aug 15 17:55:25.298: INFO: Waiting for pod downwardapi-volume-b618a383-011d-49ca-813f-df65ef832a1f to disappear
Aug 15 17:55:25.301: INFO: Pod downwardapi-volume-b618a383-011d-49ca-813f-df65ef832a1f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:55:25.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-509" for this suite.
Aug 15 17:55:31.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:55:31.412: INFO: namespace projected-509 deletion completed in 6.107150811s

• [SLOW TEST:10.198 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:55:31.412: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 17:55:31.469: INFO: Create a RollingUpdate DaemonSet
Aug 15 17:55:31.476: INFO: Check that daemon pods launch on every node of the cluster
Aug 15 17:55:31.483: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:31.483: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:31.485: INFO: Number of nodes with available pods: 0
Aug 15 17:55:31.486: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:55:32.490: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:32.490: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:32.494: INFO: Number of nodes with available pods: 0
Aug 15 17:55:32.494: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:55:33.491: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:33.491: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:33.494: INFO: Number of nodes with available pods: 0
Aug 15 17:55:33.494: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 17:55:34.492: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:34.492: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:34.496: INFO: Number of nodes with available pods: 2
Aug 15 17:55:34.496: INFO: Node karbon-fifteen-3efeed-k8s-worker-1 is running more than one daemon pod
Aug 15 17:55:35.493: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:35.493: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:35.497: INFO: Number of nodes with available pods: 3
Aug 15 17:55:35.497: INFO: Number of running nodes: 3, number of available pods: 3
Aug 15 17:55:35.497: INFO: Update the DaemonSet to trigger a rollout
Aug 15 17:55:35.506: INFO: Updating DaemonSet daemon-set
Aug 15 17:55:45.524: INFO: Roll back the DaemonSet before rollout is complete
Aug 15 17:55:45.533: INFO: Updating DaemonSet daemon-set
Aug 15 17:55:45.533: INFO: Make sure DaemonSet rollback is complete
Aug 15 17:55:45.540: INFO: Wrong image for pod: daemon-set-rg24r. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 15 17:55:45.540: INFO: Pod daemon-set-rg24r is not available
Aug 15 17:55:45.545: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:45.545: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:46.550: INFO: Wrong image for pod: daemon-set-rg24r. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 15 17:55:46.550: INFO: Pod daemon-set-rg24r is not available
Aug 15 17:55:46.554: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:46.555: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:47.549: INFO: Pod daemon-set-mmk75 is not available
Aug 15 17:55:47.553: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 17:55:47.553: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4610, will wait for the garbage collector to delete the pods
Aug 15 17:55:47.620: INFO: Deleting DaemonSet.extensions daemon-set took: 8.937415ms
Aug 15 17:55:48.021: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.223325ms
Aug 15 17:55:54.028: INFO: Number of nodes with available pods: 0
Aug 15 17:55:54.028: INFO: Number of running nodes: 0, number of available pods: 0
Aug 15 17:55:54.032: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4610/daemonsets","resourceVersion":"13280"},"items":null}

Aug 15 17:55:54.034: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4610/pods","resourceVersion":"13280"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:55:54.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4610" for this suite.
Aug 15 17:56:00.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:56:00.164: INFO: namespace daemonsets-4610 deletion completed in 6.110441931s

• [SLOW TEST:28.752 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:56:00.164: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 15 17:56:04.744: INFO: Successfully updated pod "labelsupdate32550d85-bfb9-4d2b-b092-d3c33a89ae78"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:56:08.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1620" for this suite.
Aug 15 17:56:30.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:56:30.906: INFO: namespace downward-api-1620 deletion completed in 22.128796513s

• [SLOW TEST:30.742 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:56:30.907: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:56:35.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9270" for this suite.
Aug 15 17:56:41.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:56:41.153: INFO: namespace emptydir-wrapper-9270 deletion completed in 6.125832665s

• [SLOW TEST:10.247 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:56:41.153: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 15 17:56:49.236: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 15 17:56:49.239: INFO: Pod pod-with-prestop-http-hook still exists
Aug 15 17:56:51.239: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 15 17:56:51.243: INFO: Pod pod-with-prestop-http-hook still exists
Aug 15 17:56:53.239: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 15 17:56:53.243: INFO: Pod pod-with-prestop-http-hook still exists
Aug 15 17:56:55.239: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 15 17:56:55.243: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:56:55.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1185" for this suite.
Aug 15 17:57:17.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:57:17.366: INFO: namespace container-lifecycle-hook-1185 deletion completed in 22.11004458s

• [SLOW TEST:36.212 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:57:17.366: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 15 17:57:17.411: INFO: Waiting up to 5m0s for pod "pod-7cdfb1af-8998-4dcc-9d64-936e6e70a964" in namespace "emptydir-636" to be "success or failure"
Aug 15 17:57:17.418: INFO: Pod "pod-7cdfb1af-8998-4dcc-9d64-936e6e70a964": Phase="Pending", Reason="", readiness=false. Elapsed: 6.267526ms
Aug 15 17:57:19.422: INFO: Pod "pod-7cdfb1af-8998-4dcc-9d64-936e6e70a964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010708374s
Aug 15 17:57:21.426: INFO: Pod "pod-7cdfb1af-8998-4dcc-9d64-936e6e70a964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014574034s
STEP: Saw pod success
Aug 15 17:57:21.426: INFO: Pod "pod-7cdfb1af-8998-4dcc-9d64-936e6e70a964" satisfied condition "success or failure"
Aug 15 17:57:21.429: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-7cdfb1af-8998-4dcc-9d64-936e6e70a964 container test-container: <nil>
STEP: delete the pod
Aug 15 17:57:21.452: INFO: Waiting for pod pod-7cdfb1af-8998-4dcc-9d64-936e6e70a964 to disappear
Aug 15 17:57:21.456: INFO: Pod pod-7cdfb1af-8998-4dcc-9d64-936e6e70a964 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:57:21.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-636" for this suite.
Aug 15 17:57:27.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:57:27.575: INFO: namespace emptydir-636 deletion completed in 6.11426538s

• [SLOW TEST:10.209 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:57:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8589.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8589.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8589.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8589.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8589.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8589.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8589.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8589.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8589.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8589.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8589.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8589.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8589.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 122.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.122_udp@PTR;check="$$(dig +tcp +noall +answer +search 122.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.122_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8589.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8589.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8589.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8589.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8589.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8589.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8589.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8589.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8589.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8589.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8589.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8589.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8589.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 122.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.122_udp@PTR;check="$$(dig +tcp +noall +answer +search 122.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.122_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 15 17:57:41.763: INFO: DNS probes using dns-8589/dns-test-911f6cb8-9972-4c94-bcf6-bf431a88de98 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:57:41.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8589" for this suite.
Aug 15 17:57:47.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:57:47.957: INFO: namespace dns-8589 deletion completed in 6.115055004s

• [SLOW TEST:20.382 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:57:47.958: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 15 17:57:47.990: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 15 17:57:47.998: INFO: Waiting for terminating namespaces to be deleted...
Aug 15 17:57:48.002: INFO: 
Logging pods the kubelet thinks is on node karbon-fifteen-3efeed-k8s-worker-0 before test
Aug 15 17:57:48.010: INFO: sonobuoy-e2e-job-ab1ba0141d2d464c from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container e2e ready: true, restart count 0
Aug 15 17:57:48.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 15 17:57:48.010: INFO: kube-proxy-ds-75v2p from kube-system started at 2019-08-15 16:32:55 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 15 17:57:48.010: INFO: csi-node-ntnx-plugin-k97vf from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 15 17:57:48.010: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 15 17:57:48.010: INFO: kube-flannel-ds-wb8nz from kube-system started at 2019-08-15 16:33:19 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 15 17:57:48.010: INFO: node-exporter-m5hlh from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 15 17:57:48.010: INFO: 	Container node-exporter ready: true, restart count 0
Aug 15 17:57:48.010: INFO: prometheus-k8s-0 from ntnx-system started at 2019-08-15 16:37:12 +0000 UTC (3 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container prometheus ready: true, restart count 1
Aug 15 17:57:48.010: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 15 17:57:48.010: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 15 17:57:48.010: INFO: sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-hjpr2 from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 15 17:57:48.010: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 15 17:57:48.010: INFO: kibana-logging-8657c47867-cd27n from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container kibana-logging ready: true, restart count 0
Aug 15 17:57:48.010: INFO: 	Container nginxhttp ready: true, restart count 0
Aug 15 17:57:48.010: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 15 17:57:48.010: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 15 17:57:48.010: INFO: fluent-bit-f6cdg from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.010: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 15 17:57:48.010: INFO: 
Logging pods the kubelet thinks is on node karbon-fifteen-3efeed-k8s-worker-1 before test
Aug 15 17:57:48.020: INFO: csi-attacher-ntnx-plugin-0 from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container csi-attacher ready: true, restart count 0
Aug 15 17:57:48.020: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 15 17:57:48.020: INFO: csi-node-ntnx-plugin-vfwvr from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 15 17:57:48.020: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 15 17:57:48.020: INFO: kubernetes-events-printer-769d866479-jj7bk from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 15 17:57:48.020: INFO: node-exporter-7zm7k from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 15 17:57:48.020: INFO: 	Container node-exporter ready: true, restart count 0
Aug 15 17:57:48.020: INFO: kube-proxy-ds-bh4g7 from kube-system started at 2019-08-15 16:32:55 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 15 17:57:48.020: INFO: kube-flannel-ds-d5nsh from kube-system started at 2019-08-15 16:33:19 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 15 17:57:48.020: INFO: fluent-bit-glnl9 from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 15 17:57:48.020: INFO: prometheus-operator-999c9d4cf-kgcp8 from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 15 17:57:48.020: INFO: sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-7dxs8 from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 15 17:57:48.020: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 15 17:57:48.020: INFO: alertmanager-main-1 from ntnx-system started at 2019-08-15 16:37:14 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container alertmanager ready: true, restart count 0
Aug 15 17:57:48.020: INFO: 	Container config-reloader ready: true, restart count 0
Aug 15 17:57:48.020: INFO: prometheus-k8s-1 from ntnx-system started at 2019-08-15 16:37:52 +0000 UTC (3 container statuses recorded)
Aug 15 17:57:48.020: INFO: 	Container prometheus ready: true, restart count 1
Aug 15 17:57:48.020: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 15 17:57:48.020: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 15 17:57:48.020: INFO: 
Logging pods the kubelet thinks is on node karbon-fifteen-3efeed-k8s-worker-2 before test
Aug 15 17:57:48.029: INFO: kube-state-metrics-7678d97797-ncjkj from ntnx-system started at 2019-08-15 16:37:04 +0000 UTC (4 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container addon-resizer ready: true, restart count 0
Aug 15 17:57:48.029: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 15 17:57:48.029: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 15 17:57:48.029: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 15 17:57:48.029: INFO: sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-hdxn6 from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 15 17:57:48.029: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 15 17:57:48.029: INFO: kube-flannel-ds-gwx8x from kube-system started at 2019-08-15 16:33:19 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 15 17:57:48.029: INFO: csi-node-ntnx-plugin-ljkj9 from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 15 17:57:48.029: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 15 17:57:48.029: INFO: elasticsearch-logging-0 from ntnx-system started at 2019-08-15 16:34:12 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Aug 15 17:57:48.029: INFO: node-exporter-lj4nf from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 15 17:57:48.029: INFO: 	Container node-exporter ready: true, restart count 0
Aug 15 17:57:48.029: INFO: alertmanager-main-0 from ntnx-system started at 2019-08-15 16:37:01 +0000 UTC (2 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container alertmanager ready: true, restart count 0
Aug 15 17:57:48.029: INFO: 	Container config-reloader ready: true, restart count 0
Aug 15 17:57:48.029: INFO: kube-proxy-ds-w9d6p from kube-system started at 2019-08-15 16:32:55 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 15 17:57:48.029: INFO: fluent-bit-jnjtc from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 15 17:57:48.029: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-15 17:34:42 +0000 UTC (1 container statuses recorded)
Aug 15 17:57:48.029: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dd420dfc-c4c8-4061-887c-58abc33245f0 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-dd420dfc-c4c8-4061-887c-58abc33245f0 off the node karbon-fifteen-3efeed-k8s-worker-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dd420dfc-c4c8-4061-887c-58abc33245f0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:57:54.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-549" for this suite.
Aug 15 17:58:02.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:58:02.257: INFO: namespace sched-pred-549 deletion completed in 8.133014814s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:14.299 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:58:02.257: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Aug 15 17:58:02.316: INFO: Waiting up to 5m0s for pod "client-containers-76a9b7d3-aa9c-461a-9687-f2c19e83e239" in namespace "containers-7170" to be "success or failure"
Aug 15 17:58:02.323: INFO: Pod "client-containers-76a9b7d3-aa9c-461a-9687-f2c19e83e239": Phase="Pending", Reason="", readiness=false. Elapsed: 6.1359ms
Aug 15 17:58:04.326: INFO: Pod "client-containers-76a9b7d3-aa9c-461a-9687-f2c19e83e239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009979269s
Aug 15 17:58:06.331: INFO: Pod "client-containers-76a9b7d3-aa9c-461a-9687-f2c19e83e239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014224962s
STEP: Saw pod success
Aug 15 17:58:06.331: INFO: Pod "client-containers-76a9b7d3-aa9c-461a-9687-f2c19e83e239" satisfied condition "success or failure"
Aug 15 17:58:06.334: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod client-containers-76a9b7d3-aa9c-461a-9687-f2c19e83e239 container test-container: <nil>
STEP: delete the pod
Aug 15 17:58:06.357: INFO: Waiting for pod client-containers-76a9b7d3-aa9c-461a-9687-f2c19e83e239 to disappear
Aug 15 17:58:06.360: INFO: Pod client-containers-76a9b7d3-aa9c-461a-9687-f2c19e83e239 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:58:06.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7170" for this suite.
Aug 15 17:58:12.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:58:12.481: INFO: namespace containers-7170 deletion completed in 6.115751885s

• [SLOW TEST:10.223 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:58:12.481: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 15 17:58:12.539: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6768,SelfLink:/api/v1/namespaces/watch-6768/configmaps/e2e-watch-test-label-changed,UID:1e3e0680-5838-4e4e-82b6-606e70ef77a7,ResourceVersion:13812,Generation:0,CreationTimestamp:2019-08-15 17:58:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 15 17:58:12.539: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6768,SelfLink:/api/v1/namespaces/watch-6768/configmaps/e2e-watch-test-label-changed,UID:1e3e0680-5838-4e4e-82b6-606e70ef77a7,ResourceVersion:13813,Generation:0,CreationTimestamp:2019-08-15 17:58:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 15 17:58:12.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6768,SelfLink:/api/v1/namespaces/watch-6768/configmaps/e2e-watch-test-label-changed,UID:1e3e0680-5838-4e4e-82b6-606e70ef77a7,ResourceVersion:13814,Generation:0,CreationTimestamp:2019-08-15 17:58:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 15 17:58:22.634: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6768,SelfLink:/api/v1/namespaces/watch-6768/configmaps/e2e-watch-test-label-changed,UID:1e3e0680-5838-4e4e-82b6-606e70ef77a7,ResourceVersion:13835,Generation:0,CreationTimestamp:2019-08-15 17:58:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 15 17:58:22.635: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6768,SelfLink:/api/v1/namespaces/watch-6768/configmaps/e2e-watch-test-label-changed,UID:1e3e0680-5838-4e4e-82b6-606e70ef77a7,ResourceVersion:13836,Generation:0,CreationTimestamp:2019-08-15 17:58:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 15 17:58:22.635: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6768,SelfLink:/api/v1/namespaces/watch-6768/configmaps/e2e-watch-test-label-changed,UID:1e3e0680-5838-4e4e-82b6-606e70ef77a7,ResourceVersion:13837,Generation:0,CreationTimestamp:2019-08-15 17:58:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:58:22.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6768" for this suite.
Aug 15 17:58:28.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:58:28.750: INFO: namespace watch-6768 deletion completed in 6.110426502s

• [SLOW TEST:16.269 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:58:28.750: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-dca8a428-a332-4e06-9ed7-ab3a5fc62f8c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:58:32.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3693" for this suite.
Aug 15 17:58:54.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:58:54.947: INFO: namespace configmap-3693 deletion completed in 22.1081131s

• [SLOW TEST:26.197 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:58:54.947: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 17:58:54.992: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 15 17:58:57.036: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:58:57.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1406" for this suite.
Aug 15 17:59:03.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:59:03.158: INFO: namespace replication-controller-1406 deletion completed in 6.110356044s

• [SLOW TEST:8.211 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:59:03.158: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 15 17:59:13.279: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:59:13.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0815 17:59:13.279142      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-505" for this suite.
Aug 15 17:59:19.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:59:19.401: INFO: namespace gc-505 deletion completed in 6.117730716s

• [SLOW TEST:16.243 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:59:19.401: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 17:59:19.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e676659c-2d0a-4018-8ab6-04e35408e7b7" in namespace "downward-api-387" to be "success or failure"
Aug 15 17:59:19.446: INFO: Pod "downwardapi-volume-e676659c-2d0a-4018-8ab6-04e35408e7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.681596ms
Aug 15 17:59:21.450: INFO: Pod "downwardapi-volume-e676659c-2d0a-4018-8ab6-04e35408e7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009965878s
Aug 15 17:59:23.454: INFO: Pod "downwardapi-volume-e676659c-2d0a-4018-8ab6-04e35408e7b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01355355s
STEP: Saw pod success
Aug 15 17:59:23.454: INFO: Pod "downwardapi-volume-e676659c-2d0a-4018-8ab6-04e35408e7b7" satisfied condition "success or failure"
Aug 15 17:59:23.457: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-e676659c-2d0a-4018-8ab6-04e35408e7b7 container client-container: <nil>
STEP: delete the pod
Aug 15 17:59:23.484: INFO: Waiting for pod downwardapi-volume-e676659c-2d0a-4018-8ab6-04e35408e7b7 to disappear
Aug 15 17:59:23.487: INFO: Pod downwardapi-volume-e676659c-2d0a-4018-8ab6-04e35408e7b7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:59:23.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-387" for this suite.
Aug 15 17:59:29.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:59:29.599: INFO: namespace downward-api-387 deletion completed in 6.107843969s

• [SLOW TEST:10.199 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:59:29.600: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-00382f5e-88b0-46a5-87c2-9f6a441bcf98
STEP: Creating a pod to test consume configMaps
Aug 15 17:59:29.677: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1160e912-7d71-4702-9e0e-0b76444c83bf" in namespace "projected-1274" to be "success or failure"
Aug 15 17:59:29.682: INFO: Pod "pod-projected-configmaps-1160e912-7d71-4702-9e0e-0b76444c83bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.663153ms
Aug 15 17:59:31.686: INFO: Pod "pod-projected-configmaps-1160e912-7d71-4702-9e0e-0b76444c83bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009022998s
Aug 15 17:59:33.691: INFO: Pod "pod-projected-configmaps-1160e912-7d71-4702-9e0e-0b76444c83bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013916037s
STEP: Saw pod success
Aug 15 17:59:33.691: INFO: Pod "pod-projected-configmaps-1160e912-7d71-4702-9e0e-0b76444c83bf" satisfied condition "success or failure"
Aug 15 17:59:33.694: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-projected-configmaps-1160e912-7d71-4702-9e0e-0b76444c83bf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 17:59:33.717: INFO: Waiting for pod pod-projected-configmaps-1160e912-7d71-4702-9e0e-0b76444c83bf to disappear
Aug 15 17:59:33.720: INFO: Pod pod-projected-configmaps-1160e912-7d71-4702-9e0e-0b76444c83bf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:59:33.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1274" for this suite.
Aug 15 17:59:39.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:59:39.836: INFO: namespace projected-1274 deletion completed in 6.111838832s

• [SLOW TEST:10.237 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:59:39.836: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-c42ddb47-6902-43d2-8eee-bab3f9d7a457
STEP: Creating a pod to test consume configMaps
Aug 15 17:59:39.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe1183b2-3e6d-41dd-8ee6-103a62699199" in namespace "projected-1169" to be "success or failure"
Aug 15 17:59:39.894: INFO: Pod "pod-projected-configmaps-fe1183b2-3e6d-41dd-8ee6-103a62699199": Phase="Pending", Reason="", readiness=false. Elapsed: 9.473009ms
Aug 15 17:59:41.898: INFO: Pod "pod-projected-configmaps-fe1183b2-3e6d-41dd-8ee6-103a62699199": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013208782s
Aug 15 17:59:43.902: INFO: Pod "pod-projected-configmaps-fe1183b2-3e6d-41dd-8ee6-103a62699199": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017479371s
STEP: Saw pod success
Aug 15 17:59:43.902: INFO: Pod "pod-projected-configmaps-fe1183b2-3e6d-41dd-8ee6-103a62699199" satisfied condition "success or failure"
Aug 15 17:59:43.906: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-projected-configmaps-fe1183b2-3e6d-41dd-8ee6-103a62699199 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 17:59:43.927: INFO: Waiting for pod pod-projected-configmaps-fe1183b2-3e6d-41dd-8ee6-103a62699199 to disappear
Aug 15 17:59:43.930: INFO: Pod pod-projected-configmaps-fe1183b2-3e6d-41dd-8ee6-103a62699199 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:59:43.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1169" for this suite.
Aug 15 17:59:49.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 17:59:50.059: INFO: namespace projected-1169 deletion completed in 6.12178142s

• [SLOW TEST:10.222 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 17:59:50.060: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 17:59:55.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7926" for this suite.
Aug 15 18:00:17.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:00:17.254: INFO: namespace replication-controller-7926 deletion completed in 22.112326648s

• [SLOW TEST:27.194 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:00:17.254: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-6gfr
STEP: Creating a pod to test atomic-volume-subpath
Aug 15 18:00:17.338: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6gfr" in namespace "subpath-4726" to be "success or failure"
Aug 15 18:00:17.343: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481238ms
Aug 15 18:00:19.347: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008613101s
Aug 15 18:00:21.351: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 4.013179124s
Aug 15 18:00:23.356: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 6.017978222s
Aug 15 18:00:25.361: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 8.022860356s
Aug 15 18:00:27.364: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 10.026403797s
Aug 15 18:00:29.369: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 12.031162804s
Aug 15 18:00:31.374: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 14.035681276s
Aug 15 18:00:33.379: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 16.041019071s
Aug 15 18:00:35.383: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 18.044853999s
Aug 15 18:00:37.387: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 20.048839766s
Aug 15 18:00:39.391: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Running", Reason="", readiness=true. Elapsed: 22.052846475s
Aug 15 18:00:41.395: INFO: Pod "pod-subpath-test-configmap-6gfr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.056585608s
STEP: Saw pod success
Aug 15 18:00:41.395: INFO: Pod "pod-subpath-test-configmap-6gfr" satisfied condition "success or failure"
Aug 15 18:00:41.397: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-subpath-test-configmap-6gfr container test-container-subpath-configmap-6gfr: <nil>
STEP: delete the pod
Aug 15 18:00:41.419: INFO: Waiting for pod pod-subpath-test-configmap-6gfr to disappear
Aug 15 18:00:41.422: INFO: Pod pod-subpath-test-configmap-6gfr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6gfr
Aug 15 18:00:41.422: INFO: Deleting pod "pod-subpath-test-configmap-6gfr" in namespace "subpath-4726"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:00:41.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4726" for this suite.
Aug 15 18:00:47.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:00:47.538: INFO: namespace subpath-4726 deletion completed in 6.109758927s

• [SLOW TEST:30.284 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:00:47.539: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-8634
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8634 to expose endpoints map[]
Aug 15 18:00:47.587: INFO: Get endpoints failed (3.522545ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Aug 15 18:00:48.591: INFO: successfully validated that service multi-endpoint-test in namespace services-8634 exposes endpoints map[] (1.007415963s elapsed)
STEP: Creating pod pod1 in namespace services-8634
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8634 to expose endpoints map[pod1:[100]]
Aug 15 18:00:51.632: INFO: successfully validated that service multi-endpoint-test in namespace services-8634 exposes endpoints map[pod1:[100]] (3.030753727s elapsed)
STEP: Creating pod pod2 in namespace services-8634
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8634 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 15 18:00:54.682: INFO: successfully validated that service multi-endpoint-test in namespace services-8634 exposes endpoints map[pod1:[100] pod2:[101]] (3.044088436s elapsed)
STEP: Deleting pod pod1 in namespace services-8634
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8634 to expose endpoints map[pod2:[101]]
Aug 15 18:00:55.710: INFO: successfully validated that service multi-endpoint-test in namespace services-8634 exposes endpoints map[pod2:[101]] (1.020091957s elapsed)
STEP: Deleting pod pod2 in namespace services-8634
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8634 to expose endpoints map[]
Aug 15 18:00:56.725: INFO: successfully validated that service multi-endpoint-test in namespace services-8634 exposes endpoints map[] (1.007978158s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:00:56.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8634" for this suite.
Aug 15 18:01:18.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:01:18.870: INFO: namespace services-8634 deletion completed in 22.115697313s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:31.332 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:01:18.871: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:01:18.960: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"59834977-1875-4beb-8f4a-1574943b68bd", Controller:(*bool)(0xc0036ad9ea), BlockOwnerDeletion:(*bool)(0xc0036ad9eb)}}
Aug 15 18:01:18.971: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"79ff4915-42b7-4691-87c7-bfb9da527215", Controller:(*bool)(0xc0026d7626), BlockOwnerDeletion:(*bool)(0xc0026d7627)}}
Aug 15 18:01:18.978: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"58f81005-df76-4778-83fc-8b3eb2d72c40", Controller:(*bool)(0xc0029b1bf6), BlockOwnerDeletion:(*bool)(0xc0029b1bf7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:01:23.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-500" for this suite.
Aug 15 18:01:30.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:01:30.119: INFO: namespace gc-500 deletion completed in 6.121410267s

• [SLOW TEST:11.248 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:01:30.119: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-1d2b79c7-572c-4c75-8e8f-cef769ccd79a in namespace container-probe-2646
Aug 15 18:01:34.167: INFO: Started pod busybox-1d2b79c7-572c-4c75-8e8f-cef769ccd79a in namespace container-probe-2646
STEP: checking the pod's current state and verifying that restartCount is present
Aug 15 18:01:34.170: INFO: Initial restart count of pod busybox-1d2b79c7-572c-4c75-8e8f-cef769ccd79a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:05:34.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2646" for this suite.
Aug 15 18:05:40.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:05:40.835: INFO: namespace container-probe-2646 deletion completed in 6.109386275s

• [SLOW TEST:250.716 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:05:40.835: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 15 18:05:45.412: INFO: Successfully updated pod "annotationupdate9a4621f8-e87b-458e-ac5d-3e1ebc7e0475"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:05:47.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-314" for this suite.
Aug 15 18:06:09.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:06:09.541: INFO: namespace downward-api-314 deletion completed in 22.102903696s

• [SLOW TEST:28.705 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:06:09.541: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6804, will wait for the garbage collector to delete the pods
Aug 15 18:06:13.646: INFO: Deleting Job.batch foo took: 8.471873ms
Aug 15 18:06:14.046: INFO: Terminating Job.batch foo pods took: 400.253135ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:06:54.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6804" for this suite.
Aug 15 18:07:00.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:07:00.781: INFO: namespace job-6804 deletion completed in 6.126686609s

• [SLOW TEST:51.240 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:07:00.781: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:07:00.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 version'
Aug 15 18:07:00.898: INFO: stderr: ""
Aug 15 18:07:00.898: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:07:00.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4602" for this suite.
Aug 15 18:07:06.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:07:07.028: INFO: namespace kubectl-4602 deletion completed in 6.125293812s

• [SLOW TEST:6.247 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:07:07.028: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-49.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-49.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 15 18:07:11.121: INFO: DNS probes using dns-49/dns-test-cb51a70c-d907-4a28-b6aa-371ebd49eac7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:07:11.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-49" for this suite.
Aug 15 18:07:17.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:07:17.258: INFO: namespace dns-49 deletion completed in 6.119457545s

• [SLOW TEST:10.230 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:07:17.259: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 15 18:07:17.338: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:17.338: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:17.341: INFO: Number of nodes with available pods: 0
Aug 15 18:07:17.341: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:07:18.346: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:18.346: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:18.350: INFO: Number of nodes with available pods: 0
Aug 15 18:07:18.350: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:07:19.345: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:19.345: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:19.349: INFO: Number of nodes with available pods: 0
Aug 15 18:07:19.349: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:07:20.346: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:20.346: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:20.350: INFO: Number of nodes with available pods: 2
Aug 15 18:07:20.350: INFO: Node karbon-fifteen-3efeed-k8s-worker-1 is running more than one daemon pod
Aug 15 18:07:21.358: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:21.358: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:21.361: INFO: Number of nodes with available pods: 3
Aug 15 18:07:21.361: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 15 18:07:21.383: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:21.383: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:21.388: INFO: Number of nodes with available pods: 2
Aug 15 18:07:21.388: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:07:22.393: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:22.393: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:22.396: INFO: Number of nodes with available pods: 2
Aug 15 18:07:22.396: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:07:23.393: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:23.393: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:23.396: INFO: Number of nodes with available pods: 2
Aug 15 18:07:23.397: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:07:24.393: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:24.393: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:07:24.396: INFO: Number of nodes with available pods: 3
Aug 15 18:07:24.396: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4412, will wait for the garbage collector to delete the pods
Aug 15 18:07:24.463: INFO: Deleting DaemonSet.extensions daemon-set took: 8.919988ms
Aug 15 18:07:24.864: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.374707ms
Aug 15 18:07:39.367: INFO: Number of nodes with available pods: 0
Aug 15 18:07:39.367: INFO: Number of running nodes: 0, number of available pods: 0
Aug 15 18:07:39.369: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4412/daemonsets","resourceVersion":"15599"},"items":null}

Aug 15 18:07:39.372: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4412/pods","resourceVersion":"15599"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:07:39.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4412" for this suite.
Aug 15 18:07:45.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:07:45.503: INFO: namespace daemonsets-4412 deletion completed in 6.115005338s

• [SLOW TEST:28.244 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:07:45.503: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 18:07:45.545: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6abb2dc3-6fbb-49c7-aa97-0521357e115b" in namespace "downward-api-961" to be "success or failure"
Aug 15 18:07:45.550: INFO: Pod "downwardapi-volume-6abb2dc3-6fbb-49c7-aa97-0521357e115b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.067778ms
Aug 15 18:07:47.555: INFO: Pod "downwardapi-volume-6abb2dc3-6fbb-49c7-aa97-0521357e115b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009144294s
Aug 15 18:07:49.559: INFO: Pod "downwardapi-volume-6abb2dc3-6fbb-49c7-aa97-0521357e115b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013639497s
STEP: Saw pod success
Aug 15 18:07:49.559: INFO: Pod "downwardapi-volume-6abb2dc3-6fbb-49c7-aa97-0521357e115b" satisfied condition "success or failure"
Aug 15 18:07:49.562: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-6abb2dc3-6fbb-49c7-aa97-0521357e115b container client-container: <nil>
STEP: delete the pod
Aug 15 18:07:49.582: INFO: Waiting for pod downwardapi-volume-6abb2dc3-6fbb-49c7-aa97-0521357e115b to disappear
Aug 15 18:07:49.587: INFO: Pod downwardapi-volume-6abb2dc3-6fbb-49c7-aa97-0521357e115b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:07:49.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-961" for this suite.
Aug 15 18:07:55.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:07:55.702: INFO: namespace downward-api-961 deletion completed in 6.111460388s

• [SLOW TEST:10.199 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:07:55.703: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 15 18:07:55.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7661'
Aug 15 18:07:56.147: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 15 18:07:56.148: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Aug 15 18:07:56.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7661'
Aug 15 18:07:56.252: INFO: stderr: ""
Aug 15 18:07:56.252: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:07:56.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7661" for this suite.
Aug 15 18:08:02.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:08:02.390: INFO: namespace kubectl-7661 deletion completed in 6.132833302s

• [SLOW TEST:6.687 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:08:02.391: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:08:02.444: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 15 18:08:07.449: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 15 18:08:07.449: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 15 18:08:13.480: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-6588,SelfLink:/apis/apps/v1/namespaces/deployment-6588/deployments/test-cleanup-deployment,UID:5efd3d19-d7bc-4601-b13d-2cedfb64787d,ResourceVersion:15787,Generation:1,CreationTimestamp:2019-08-15 18:08:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-15 18:08:07 +0000 UTC 2019-08-15 18:08:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-15 18:08:11 +0000 UTC 2019-08-15 18:08:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 15 18:08:13.483: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-6588,SelfLink:/apis/apps/v1/namespaces/deployment-6588/replicasets/test-cleanup-deployment-55bbcbc84c,UID:2aa5d7e4-0d6b-468c-9b5c-f79b1b67466e,ResourceVersion:15776,Generation:1,CreationTimestamp:2019-08-15 18:08:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 5efd3d19-d7bc-4601-b13d-2cedfb64787d 0xc001ffcd97 0xc001ffcd98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 15 18:08:13.487: INFO: Pod "test-cleanup-deployment-55bbcbc84c-7zngk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-7zngk,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-6588,SelfLink:/api/v1/namespaces/deployment-6588/pods/test-cleanup-deployment-55bbcbc84c-7zngk,UID:3c4c4577-f9d8-437d-b4f5-908b27331bd3,ResourceVersion:15775,Generation:0,CreationTimestamp:2019-08-15 18:08:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 2aa5d7e4-0d6b-468c-9b5c-f79b1b67466e 0xc001ffd397 0xc001ffd398}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-75djq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-75djq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-75djq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ffd400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ffd420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:08:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:08:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:08:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:08:07 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.132,PodIP:172.20.4.52,StartTime:2019-08-15 18:08:07 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-15 18:08:11 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c652f7c38f792a3a852045f071b2456f0a9ba03acfee842f1e9c65626a6bdf58}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:08:13.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6588" for this suite.
Aug 15 18:08:19.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:08:19.604: INFO: namespace deployment-6588 deletion completed in 6.112647941s

• [SLOW TEST:17.213 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:08:19.604: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 18:08:19.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c137f2ef-cc57-4ad9-98f8-48d5d8ead94a" in namespace "downward-api-6041" to be "success or failure"
Aug 15 18:08:19.663: INFO: Pod "downwardapi-volume-c137f2ef-cc57-4ad9-98f8-48d5d8ead94a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06167ms
Aug 15 18:08:21.667: INFO: Pod "downwardapi-volume-c137f2ef-cc57-4ad9-98f8-48d5d8ead94a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006888113s
Aug 15 18:08:23.671: INFO: Pod "downwardapi-volume-c137f2ef-cc57-4ad9-98f8-48d5d8ead94a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010552585s
STEP: Saw pod success
Aug 15 18:08:23.671: INFO: Pod "downwardapi-volume-c137f2ef-cc57-4ad9-98f8-48d5d8ead94a" satisfied condition "success or failure"
Aug 15 18:08:23.674: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-c137f2ef-cc57-4ad9-98f8-48d5d8ead94a container client-container: <nil>
STEP: delete the pod
Aug 15 18:08:23.696: INFO: Waiting for pod downwardapi-volume-c137f2ef-cc57-4ad9-98f8-48d5d8ead94a to disappear
Aug 15 18:08:23.699: INFO: Pod downwardapi-volume-c137f2ef-cc57-4ad9-98f8-48d5d8ead94a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:08:23.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6041" for this suite.
Aug 15 18:08:29.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:08:29.816: INFO: namespace downward-api-6041 deletion completed in 6.113273802s

• [SLOW TEST:10.212 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:08:29.816: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 15 18:08:29.857: INFO: Waiting up to 5m0s for pod "downward-api-ff4ec7bb-4153-429a-80a6-2f3991d31509" in namespace "downward-api-920" to be "success or failure"
Aug 15 18:08:29.863: INFO: Pod "downward-api-ff4ec7bb-4153-429a-80a6-2f3991d31509": Phase="Pending", Reason="", readiness=false. Elapsed: 5.743695ms
Aug 15 18:08:31.867: INFO: Pod "downward-api-ff4ec7bb-4153-429a-80a6-2f3991d31509": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009938797s
Aug 15 18:08:33.871: INFO: Pod "downward-api-ff4ec7bb-4153-429a-80a6-2f3991d31509": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014294229s
STEP: Saw pod success
Aug 15 18:08:33.871: INFO: Pod "downward-api-ff4ec7bb-4153-429a-80a6-2f3991d31509" satisfied condition "success or failure"
Aug 15 18:08:33.874: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downward-api-ff4ec7bb-4153-429a-80a6-2f3991d31509 container dapi-container: <nil>
STEP: delete the pod
Aug 15 18:08:33.898: INFO: Waiting for pod downward-api-ff4ec7bb-4153-429a-80a6-2f3991d31509 to disappear
Aug 15 18:08:33.901: INFO: Pod downward-api-ff4ec7bb-4153-429a-80a6-2f3991d31509 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:08:33.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-920" for this suite.
Aug 15 18:08:39.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:08:40.022: INFO: namespace downward-api-920 deletion completed in 6.116469862s

• [SLOW TEST:10.206 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:08:40.023: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:09:06.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3390" for this suite.
Aug 15 18:09:12.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:09:12.278: INFO: namespace namespaces-3390 deletion completed in 6.109415908s
STEP: Destroying namespace "nsdeletetest-2152" for this suite.
Aug 15 18:09:12.281: INFO: Namespace nsdeletetest-2152 was already deleted
STEP: Destroying namespace "nsdeletetest-3938" for this suite.
Aug 15 18:09:18.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:09:18.396: INFO: namespace nsdeletetest-3938 deletion completed in 6.115485516s

• [SLOW TEST:38.374 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:09:18.397: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:09:18.454: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 15 18:09:18.478: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:18.478: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:18.484: INFO: Number of nodes with available pods: 0
Aug 15 18:09:18.484: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:09:19.490: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:19.490: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:19.494: INFO: Number of nodes with available pods: 0
Aug 15 18:09:19.494: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:09:20.489: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:20.489: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:20.493: INFO: Number of nodes with available pods: 0
Aug 15 18:09:20.493: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:09:21.494: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:21.494: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:21.500: INFO: Number of nodes with available pods: 2
Aug 15 18:09:21.500: INFO: Node karbon-fifteen-3efeed-k8s-worker-1 is running more than one daemon pod
Aug 15 18:09:22.489: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:22.489: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:22.493: INFO: Number of nodes with available pods: 3
Aug 15 18:09:22.493: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 15 18:09:22.527: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:22.527: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:22.527: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:22.533: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:22.533: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:23.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:23.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:23.537: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:23.540: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:23.540: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:24.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:24.538: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:24.538: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:24.538: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:24.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:24.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:25.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:25.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:25.537: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:25.537: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:25.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:25.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:26.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:26.538: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:26.538: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:26.538: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:26.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:26.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:27.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:27.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:27.537: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:27.537: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:27.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:27.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:28.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:28.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:28.537: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:28.537: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:28.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:28.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:29.540: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:29.540: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:29.540: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:29.540: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:29.544: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:29.544: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:30.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:30.538: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:30.538: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:30.538: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:30.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:30.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:31.550: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:31.551: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:31.551: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:31.551: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:31.555: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:31.555: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:32.539: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:32.539: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:32.539: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:32.539: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:32.544: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:32.544: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:33.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:33.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:33.537: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:33.537: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:33.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:33.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:34.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:34.538: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:34.538: INFO: Wrong image for pod: daemon-set-c6ck9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:34.538: INFO: Pod daemon-set-c6ck9 is not available
Aug 15 18:09:34.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:34.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:35.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:35.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:35.537: INFO: Pod daemon-set-l8snq is not available
Aug 15 18:09:35.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:35.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:36.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:36.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:36.537: INFO: Pod daemon-set-l8snq is not available
Aug 15 18:09:36.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:36.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:37.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:37.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:37.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:37.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:38.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:38.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:38.537: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:38.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:38.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:39.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:39.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:39.537: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:39.540: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:39.540: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:40.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:40.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:40.537: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:40.540: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:40.540: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:41.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:41.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:41.537: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:41.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:41.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:42.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:42.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:42.537: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:42.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:42.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:43.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:43.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:43.537: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:43.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:43.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:44.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:44.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:44.537: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:44.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:44.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:45.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:45.538: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:45.538: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:45.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:45.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:46.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:46.538: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:46.538: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:46.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:46.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:47.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:47.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:47.537: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:47.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:47.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:48.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:48.537: INFO: Wrong image for pod: daemon-set-9mbqb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:48.537: INFO: Pod daemon-set-9mbqb is not available
Aug 15 18:09:48.544: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:48.544: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:49.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:49.537: INFO: Pod daemon-set-kscz8 is not available
Aug 15 18:09:49.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:49.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:50.536: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:50.536: INFO: Pod daemon-set-kscz8 is not available
Aug 15 18:09:50.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:50.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:51.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:51.538: INFO: Pod daemon-set-kscz8 is not available
Aug 15 18:09:51.543: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:51.543: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:52.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:52.543: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:52.543: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:53.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:53.537: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:09:53.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:53.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:54.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:54.537: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:09:54.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:54.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:55.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:55.538: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:09:55.543: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:55.543: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:56.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:56.537: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:09:56.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:56.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:57.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:57.537: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:09:57.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:57.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:58.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:58.538: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:09:58.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:58.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:59.539: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:09:59.539: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:09:59.544: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:09:59.544: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:00.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:10:00.537: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:10:00.540: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:00.540: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:01.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:10:01.537: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:10:01.541: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:01.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:02.538: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:10:02.538: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:10:02.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:02.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:03.537: INFO: Wrong image for pod: daemon-set-8btrd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 15 18:10:03.537: INFO: Pod daemon-set-8btrd is not available
Aug 15 18:10:03.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:03.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:04.537: INFO: Pod daemon-set-xbq49 is not available
Aug 15 18:10:04.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:04.542: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 15 18:10:04.546: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:04.546: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:04.550: INFO: Number of nodes with available pods: 2
Aug 15 18:10:04.550: INFO: Node karbon-fifteen-3efeed-k8s-worker-1 is running more than one daemon pod
Aug 15 18:10:05.556: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:05.556: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:05.560: INFO: Number of nodes with available pods: 2
Aug 15 18:10:05.560: INFO: Node karbon-fifteen-3efeed-k8s-worker-1 is running more than one daemon pod
Aug 15 18:10:06.555: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:06.555: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:06.559: INFO: Number of nodes with available pods: 2
Aug 15 18:10:06.559: INFO: Node karbon-fifteen-3efeed-k8s-worker-1 is running more than one daemon pod
Aug 15 18:10:07.555: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:07.555: INFO: DaemonSet pods can't tolerate node karbon-fifteen-3efeed-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 15 18:10:07.559: INFO: Number of nodes with available pods: 3
Aug 15 18:10:07.559: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5156, will wait for the garbage collector to delete the pods
Aug 15 18:10:07.636: INFO: Deleting DaemonSet.extensions daemon-set took: 9.15348ms
Aug 15 18:10:08.036: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.235644ms
Aug 15 18:10:14.640: INFO: Number of nodes with available pods: 0
Aug 15 18:10:14.640: INFO: Number of running nodes: 0, number of available pods: 0
Aug 15 18:10:14.643: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5156/daemonsets","resourceVersion":"16230"},"items":null}

Aug 15 18:10:14.646: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5156/pods","resourceVersion":"16230"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:10:14.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5156" for this suite.
Aug 15 18:10:20.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:10:20.790: INFO: namespace daemonsets-5156 deletion completed in 6.128589331s

• [SLOW TEST:62.394 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:10:20.791: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-7ebb78de-e411-448a-8c1e-a5934a9bbec1
STEP: Creating a pod to test consume secrets
Aug 15 18:10:20.841: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc34355d-29e9-4510-b5b0-b337d89537c2" in namespace "projected-3625" to be "success or failure"
Aug 15 18:10:20.845: INFO: Pod "pod-projected-secrets-bc34355d-29e9-4510-b5b0-b337d89537c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.975594ms
Aug 15 18:10:22.848: INFO: Pod "pod-projected-secrets-bc34355d-29e9-4510-b5b0-b337d89537c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007698231s
Aug 15 18:10:24.852: INFO: Pod "pod-projected-secrets-bc34355d-29e9-4510-b5b0-b337d89537c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011216465s
STEP: Saw pod success
Aug 15 18:10:24.852: INFO: Pod "pod-projected-secrets-bc34355d-29e9-4510-b5b0-b337d89537c2" satisfied condition "success or failure"
Aug 15 18:10:24.854: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-projected-secrets-bc34355d-29e9-4510-b5b0-b337d89537c2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 15 18:10:24.887: INFO: Waiting for pod pod-projected-secrets-bc34355d-29e9-4510-b5b0-b337d89537c2 to disappear
Aug 15 18:10:24.891: INFO: Pod pod-projected-secrets-bc34355d-29e9-4510-b5b0-b337d89537c2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:10:24.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3625" for this suite.
Aug 15 18:10:30.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:10:31.010: INFO: namespace projected-3625 deletion completed in 6.115451185s

• [SLOW TEST:10.219 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:10:31.010: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f2930058-19ad-4849-be48-f1c3dcccab9b
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f2930058-19ad-4849-be48-f1c3dcccab9b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:10:37.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2073" for this suite.
Aug 15 18:10:55.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:10:55.262: INFO: namespace projected-2073 deletion completed in 18.120193379s

• [SLOW TEST:24.252 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:10:55.262: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 15 18:10:55.302: INFO: Waiting up to 5m0s for pod "pod-2e2a5562-ced0-4663-a2a2-998943743709" in namespace "emptydir-6187" to be "success or failure"
Aug 15 18:10:55.306: INFO: Pod "pod-2e2a5562-ced0-4663-a2a2-998943743709": Phase="Pending", Reason="", readiness=false. Elapsed: 4.682284ms
Aug 15 18:10:57.311: INFO: Pod "pod-2e2a5562-ced0-4663-a2a2-998943743709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009251968s
Aug 15 18:10:59.315: INFO: Pod "pod-2e2a5562-ced0-4663-a2a2-998943743709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013344711s
STEP: Saw pod success
Aug 15 18:10:59.315: INFO: Pod "pod-2e2a5562-ced0-4663-a2a2-998943743709" satisfied condition "success or failure"
Aug 15 18:10:59.318: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-2e2a5562-ced0-4663-a2a2-998943743709 container test-container: <nil>
STEP: delete the pod
Aug 15 18:10:59.338: INFO: Waiting for pod pod-2e2a5562-ced0-4663-a2a2-998943743709 to disappear
Aug 15 18:10:59.342: INFO: Pod pod-2e2a5562-ced0-4663-a2a2-998943743709 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:10:59.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6187" for this suite.
Aug 15 18:11:05.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:11:05.462: INFO: namespace emptydir-6187 deletion completed in 6.116125479s

• [SLOW TEST:10.200 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:11:05.462: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 15 18:11:05.514: INFO: Waiting up to 5m0s for pod "pod-c5114a87-557d-4bb2-ac10-9acf3deb94df" in namespace "emptydir-7259" to be "success or failure"
Aug 15 18:11:05.521: INFO: Pod "pod-c5114a87-557d-4bb2-ac10-9acf3deb94df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.81246ms
Aug 15 18:11:07.525: INFO: Pod "pod-c5114a87-557d-4bb2-ac10-9acf3deb94df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011153206s
Aug 15 18:11:09.530: INFO: Pod "pod-c5114a87-557d-4bb2-ac10-9acf3deb94df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015693661s
Aug 15 18:11:11.534: INFO: Pod "pod-c5114a87-557d-4bb2-ac10-9acf3deb94df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02032306s
STEP: Saw pod success
Aug 15 18:11:11.534: INFO: Pod "pod-c5114a87-557d-4bb2-ac10-9acf3deb94df" satisfied condition "success or failure"
Aug 15 18:11:11.538: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-c5114a87-557d-4bb2-ac10-9acf3deb94df container test-container: <nil>
STEP: delete the pod
Aug 15 18:11:11.562: INFO: Waiting for pod pod-c5114a87-557d-4bb2-ac10-9acf3deb94df to disappear
Aug 15 18:11:11.566: INFO: Pod pod-c5114a87-557d-4bb2-ac10-9acf3deb94df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:11:11.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7259" for this suite.
Aug 15 18:11:17.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:11:17.694: INFO: namespace emptydir-7259 deletion completed in 6.123765565s

• [SLOW TEST:12.232 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:11:17.695: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:11:17.737: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 15 18:11:22.741: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 15 18:11:22.741: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 15 18:11:24.745: INFO: Creating deployment "test-rollover-deployment"
Aug 15 18:11:24.756: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 15 18:11:26.764: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 15 18:11:26.774: INFO: Ensure that both replica sets have 1 created replica
Aug 15 18:11:26.779: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 15 18:11:26.787: INFO: Updating deployment test-rollover-deployment
Aug 15 18:11:26.787: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 15 18:11:28.797: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 15 18:11:28.803: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 15 18:11:28.808: INFO: all replica sets need to contain the pod-template-hash label
Aug 15 18:11:28.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489486, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:11:30.815: INFO: all replica sets need to contain the pod-template-hash label
Aug 15 18:11:30.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489489, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:11:32.815: INFO: all replica sets need to contain the pod-template-hash label
Aug 15 18:11:32.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489489, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:11:34.816: INFO: all replica sets need to contain the pod-template-hash label
Aug 15 18:11:34.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489489, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:11:36.816: INFO: all replica sets need to contain the pod-template-hash label
Aug 15 18:11:36.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489489, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:11:38.816: INFO: all replica sets need to contain the pod-template-hash label
Aug 15 18:11:38.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489489, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489484, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:11:40.815: INFO: 
Aug 15 18:11:40.815: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 15 18:11:40.823: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-7390,SelfLink:/apis/apps/v1/namespaces/deployment-7390/deployments/test-rollover-deployment,UID:712c1a4c-7be8-4cd9-b8ae-3c41238687e1,ResourceVersion:16603,Generation:2,CreationTimestamp:2019-08-15 18:11:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-15 18:11:24 +0000 UTC 2019-08-15 18:11:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-15 18:11:39 +0000 UTC 2019-08-15 18:11:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 15 18:11:40.826: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-7390,SelfLink:/apis/apps/v1/namespaces/deployment-7390/replicasets/test-rollover-deployment-854595fc44,UID:5625c035-b0ab-4284-ac21-1af51bc98ae5,ResourceVersion:16593,Generation:2,CreationTimestamp:2019-08-15 18:11:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 712c1a4c-7be8-4cd9-b8ae-3c41238687e1 0xc003775537 0xc003775538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 15 18:11:40.826: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 15 18:11:40.827: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-7390,SelfLink:/apis/apps/v1/namespaces/deployment-7390/replicasets/test-rollover-controller,UID:fe1842b4-6cfe-473e-875e-e62fc9be85ac,ResourceVersion:16602,Generation:2,CreationTimestamp:2019-08-15 18:11:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 712c1a4c-7be8-4cd9-b8ae-3c41238687e1 0xc003775467 0xc003775468}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 15 18:11:40.827: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-7390,SelfLink:/apis/apps/v1/namespaces/deployment-7390/replicasets/test-rollover-deployment-9b8b997cf,UID:ded852ef-17a7-4e45-8faa-9161fc6423de,ResourceVersion:16556,Generation:2,CreationTimestamp:2019-08-15 18:11:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 712c1a4c-7be8-4cd9-b8ae-3c41238687e1 0xc003775610 0xc003775611}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 15 18:11:40.830: INFO: Pod "test-rollover-deployment-854595fc44-6d7pg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-6d7pg,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/test-rollover-deployment-854595fc44-6d7pg,UID:69b5bb6e-2f39-4306-a478-0899ba604610,ResourceVersion:16573,Generation:0,CreationTimestamp:2019-08-15 18:11:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 5625c035-b0ab-4284-ac21-1af51bc98ae5 0xc003443177 0xc003443178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lbsjp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lbsjp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lbsjp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0034431e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003443200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:11:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:11:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:11:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:11:26 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.137,PodIP:172.20.2.64,StartTime:2019-08-15 18:11:26 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-15 18:11:28 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ba39777ce1d731168464adcbd53bff110f7d471a6abbd84ca51e39a4aa521e73}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:11:40.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7390" for this suite.
Aug 15 18:11:46.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:11:46.950: INFO: namespace deployment-7390 deletion completed in 6.117238001s

• [SLOW TEST:29.256 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:11:46.950: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-f4a669e3-b55d-4b6c-b524-d631bde43f37
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:11:46.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3233" for this suite.
Aug 15 18:11:52.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:11:53.114: INFO: namespace secrets-3233 deletion completed in 6.127884995s

• [SLOW TEST:6.164 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:11:53.114: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-af5ccc04-686f-41d8-bc13-21201f9958ae
STEP: Creating a pod to test consume configMaps
Aug 15 18:11:53.166: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-06671461-18e7-48e3-8f5d-d9104ded3cc8" in namespace "projected-1132" to be "success or failure"
Aug 15 18:11:53.178: INFO: Pod "pod-projected-configmaps-06671461-18e7-48e3-8f5d-d9104ded3cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.928225ms
Aug 15 18:11:55.182: INFO: Pod "pod-projected-configmaps-06671461-18e7-48e3-8f5d-d9104ded3cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015579312s
Aug 15 18:11:57.187: INFO: Pod "pod-projected-configmaps-06671461-18e7-48e3-8f5d-d9104ded3cc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020532362s
STEP: Saw pod success
Aug 15 18:11:57.187: INFO: Pod "pod-projected-configmaps-06671461-18e7-48e3-8f5d-d9104ded3cc8" satisfied condition "success or failure"
Aug 15 18:11:57.190: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-projected-configmaps-06671461-18e7-48e3-8f5d-d9104ded3cc8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:11:57.220: INFO: Waiting for pod pod-projected-configmaps-06671461-18e7-48e3-8f5d-d9104ded3cc8 to disappear
Aug 15 18:11:57.222: INFO: Pod pod-projected-configmaps-06671461-18e7-48e3-8f5d-d9104ded3cc8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:11:57.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1132" for this suite.
Aug 15 18:12:03.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:12:03.353: INFO: namespace projected-1132 deletion completed in 6.126415852s

• [SLOW TEST:10.239 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:12:03.354: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-vhq5
STEP: Creating a pod to test atomic-volume-subpath
Aug 15 18:12:03.409: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vhq5" in namespace "subpath-9563" to be "success or failure"
Aug 15 18:12:03.413: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026801ms
Aug 15 18:12:05.417: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008386901s
Aug 15 18:12:07.421: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.012096369s
Aug 15 18:12:09.425: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 6.016701801s
Aug 15 18:12:11.430: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 8.021446853s
Aug 15 18:12:13.434: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 10.025879645s
Aug 15 18:12:15.439: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 12.030265039s
Aug 15 18:12:17.443: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 14.034710913s
Aug 15 18:12:19.448: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 16.03928567s
Aug 15 18:12:21.452: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 18.043586467s
Aug 15 18:12:23.456: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 20.047092267s
Aug 15 18:12:25.460: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Running", Reason="", readiness=true. Elapsed: 22.0513102s
Aug 15 18:12:27.464: INFO: Pod "pod-subpath-test-downwardapi-vhq5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.055457288s
STEP: Saw pod success
Aug 15 18:12:27.464: INFO: Pod "pod-subpath-test-downwardapi-vhq5" satisfied condition "success or failure"
Aug 15 18:12:27.467: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-subpath-test-downwardapi-vhq5 container test-container-subpath-downwardapi-vhq5: <nil>
STEP: delete the pod
Aug 15 18:12:27.492: INFO: Waiting for pod pod-subpath-test-downwardapi-vhq5 to disappear
Aug 15 18:12:27.495: INFO: Pod pod-subpath-test-downwardapi-vhq5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vhq5
Aug 15 18:12:27.495: INFO: Deleting pod "pod-subpath-test-downwardapi-vhq5" in namespace "subpath-9563"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:12:27.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9563" for this suite.
Aug 15 18:12:33.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:12:33.616: INFO: namespace subpath-9563 deletion completed in 6.113626702s

• [SLOW TEST:30.263 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:12:33.617: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-9dd445fb-a417-48a5-8f68-17ae44d92568
STEP: Creating configMap with name cm-test-opt-upd-e8b28bd9-131d-4d90-a775-a903dd28f4d8
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9dd445fb-a417-48a5-8f68-17ae44d92568
STEP: Updating configmap cm-test-opt-upd-e8b28bd9-131d-4d90-a775-a903dd28f4d8
STEP: Creating configMap with name cm-test-opt-create-7c0b55d3-ae3b-444e-8b5b-8debd6da8753
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:12:41.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3615" for this suite.
Aug 15 18:13:03.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:13:03.912: INFO: namespace configmap-3615 deletion completed in 22.119243698s

• [SLOW TEST:30.295 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:13:03.912: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 15 18:13:03.951: INFO: PodSpec: initContainers in spec.initContainers
Aug 15 18:13:49.170: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7bcd1134-54ef-4f8b-b756-1287dc4b28aa", GenerateName:"", Namespace:"init-container-7215", SelfLink:"/api/v1/namespaces/init-container-7215/pods/pod-init-7bcd1134-54ef-4f8b-b756-1287dc4b28aa", UID:"4661c9fd-1ad7-4a16-ab62-3bcf0e879dba", ResourceVersion:"17003", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63701489583, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"951603881"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9hpk4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00216a200), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9hpk4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9hpk4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9hpk4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0034f0278), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"karbon-fifteen-3efeed-k8s-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0035fa060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0034f02f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0034f0310)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0034f0318), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0034f031c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489583, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489583, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489583, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701489583, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.45.40.131", PodIP:"172.20.3.53", StartTime:(*v1.Time)(0xc0038cc140), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0038cc180), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00178e1c0)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://74f87d204425ac496eb4838deca0ed1e7ec4c874f8436910f26055c73e1b9678"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038cc1a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038cc160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:13:49.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7215" for this suite.
Aug 15 18:14:11.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:14:11.306: INFO: namespace init-container-7215 deletion completed in 22.12999657s

• [SLOW TEST:67.394 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:14:11.307: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7141
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7141
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7141
Aug 15 18:14:11.362: INFO: Found 0 stateful pods, waiting for 1
Aug 15 18:14:21.367: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 15 18:14:21.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-7141 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:14:21.539: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:14:21.539: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:14:21.539: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 15 18:14:21.543: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 15 18:14:31.547: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 15 18:14:31.547: INFO: Waiting for statefulset status.replicas updated to 0
Aug 15 18:14:31.563: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999687s
Aug 15 18:14:32.567: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994271394s
Aug 15 18:14:33.572: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99007786s
Aug 15 18:14:34.577: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985366295s
Aug 15 18:14:35.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980745022s
Aug 15 18:14:36.585: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976888263s
Aug 15 18:14:37.589: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972598137s
Aug 15 18:14:38.593: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968240596s
Aug 15 18:14:39.597: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.96463583s
Aug 15 18:14:40.602: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.127954ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7141
Aug 15 18:14:41.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-7141 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:14:41.790: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 15 18:14:41.790: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 15 18:14:41.790: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 15 18:14:41.795: INFO: Found 1 stateful pods, waiting for 3
Aug 15 18:14:51.799: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:14:51.799: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:14:51.799: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 15 18:14:51.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-7141 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:14:51.982: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:14:51.982: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:14:51.982: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 15 18:14:51.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-7141 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:14:52.173: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:14:52.173: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:14:52.173: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 15 18:14:52.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-7141 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:14:52.371: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:14:52.371: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:14:52.371: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 15 18:14:52.371: INFO: Waiting for statefulset status.replicas updated to 0
Aug 15 18:14:52.375: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 15 18:15:02.388: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 15 18:15:02.388: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 15 18:15:02.388: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 15 18:15:02.424: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999138s
Aug 15 18:15:03.428: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.977732869s
Aug 15 18:15:04.433: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973039487s
Aug 15 18:15:05.439: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.967981934s
Aug 15 18:15:06.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962226795s
Aug 15 18:15:07.448: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957338427s
Aug 15 18:15:08.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952804848s
Aug 15 18:15:09.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948387145s
Aug 15 18:15:10.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944409227s
Aug 15 18:15:11.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 939.881976ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7141
Aug 15 18:15:12.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-7141 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:15:12.634: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 15 18:15:12.634: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 15 18:15:12.634: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 15 18:15:12.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-7141 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:15:12.790: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 15 18:15:12.791: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 15 18:15:12.791: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 15 18:15:12.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-7141 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:15:12.978: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 15 18:15:12.978: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 15 18:15:12.978: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 15 18:15:12.978: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 15 18:15:32.995: INFO: Deleting all statefulset in ns statefulset-7141
Aug 15 18:15:32.999: INFO: Scaling statefulset ss to 0
Aug 15 18:15:33.011: INFO: Waiting for statefulset status.replicas updated to 0
Aug 15 18:15:33.013: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:15:33.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7141" for this suite.
Aug 15 18:15:39.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:15:39.148: INFO: namespace statefulset-7141 deletion completed in 6.111896261s

• [SLOW TEST:87.841 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:15:39.148: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-104bc6c9-345c-4c70-ae13-1864e4e9cde4
STEP: Creating a pod to test consume configMaps
Aug 15 18:15:39.195: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-21c3af6b-881d-4b97-a325-1dab5f594b55" in namespace "projected-8875" to be "success or failure"
Aug 15 18:15:39.201: INFO: Pod "pod-projected-configmaps-21c3af6b-881d-4b97-a325-1dab5f594b55": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48142ms
Aug 15 18:15:41.205: INFO: Pod "pod-projected-configmaps-21c3af6b-881d-4b97-a325-1dab5f594b55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010344756s
Aug 15 18:15:43.210: INFO: Pod "pod-projected-configmaps-21c3af6b-881d-4b97-a325-1dab5f594b55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015450391s
STEP: Saw pod success
Aug 15 18:15:43.210: INFO: Pod "pod-projected-configmaps-21c3af6b-881d-4b97-a325-1dab5f594b55" satisfied condition "success or failure"
Aug 15 18:15:43.213: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-projected-configmaps-21c3af6b-881d-4b97-a325-1dab5f594b55 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:15:43.240: INFO: Waiting for pod pod-projected-configmaps-21c3af6b-881d-4b97-a325-1dab5f594b55 to disappear
Aug 15 18:15:43.243: INFO: Pod pod-projected-configmaps-21c3af6b-881d-4b97-a325-1dab5f594b55 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:15:43.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8875" for this suite.
Aug 15 18:15:49.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:15:49.358: INFO: namespace projected-8875 deletion completed in 6.111006265s

• [SLOW TEST:10.210 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:15:49.358: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-33350804-b8f8-47cb-8f6b-ec2f9f5ad5b1
STEP: Creating a pod to test consume secrets
Aug 15 18:15:49.407: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4f035e7a-038a-4a2c-9815-93a44f3b95b6" in namespace "projected-3790" to be "success or failure"
Aug 15 18:15:49.412: INFO: Pod "pod-projected-secrets-4f035e7a-038a-4a2c-9815-93a44f3b95b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.57965ms
Aug 15 18:15:51.416: INFO: Pod "pod-projected-secrets-4f035e7a-038a-4a2c-9815-93a44f3b95b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009045767s
Aug 15 18:15:53.421: INFO: Pod "pod-projected-secrets-4f035e7a-038a-4a2c-9815-93a44f3b95b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0133727s
STEP: Saw pod success
Aug 15 18:15:53.421: INFO: Pod "pod-projected-secrets-4f035e7a-038a-4a2c-9815-93a44f3b95b6" satisfied condition "success or failure"
Aug 15 18:15:53.423: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-projected-secrets-4f035e7a-038a-4a2c-9815-93a44f3b95b6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 15 18:15:53.447: INFO: Waiting for pod pod-projected-secrets-4f035e7a-038a-4a2c-9815-93a44f3b95b6 to disappear
Aug 15 18:15:53.450: INFO: Pod pod-projected-secrets-4f035e7a-038a-4a2c-9815-93a44f3b95b6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:15:53.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3790" for this suite.
Aug 15 18:15:59.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:15:59.562: INFO: namespace projected-3790 deletion completed in 6.107666033s

• [SLOW TEST:10.204 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:15:59.562: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Aug 15 18:16:03.624: INFO: Pod pod-hostip-6e384624-9699-4380-a22a-81c913ad18a7 has hostIP: 10.45.40.131
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:16:03.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6601" for this suite.
Aug 15 18:16:25.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:16:25.751: INFO: namespace pods-6601 deletion completed in 22.123332713s

• [SLOW TEST:26.189 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:16:25.751: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-9545/secret-test-ebfeb1b7-abb5-4380-a969-f93c37f5b897
STEP: Creating a pod to test consume secrets
Aug 15 18:16:25.801: INFO: Waiting up to 5m0s for pod "pod-configmaps-c21960a4-faa7-422c-8c87-213c660d6bfd" in namespace "secrets-9545" to be "success or failure"
Aug 15 18:16:25.811: INFO: Pod "pod-configmaps-c21960a4-faa7-422c-8c87-213c660d6bfd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.10346ms
Aug 15 18:16:27.815: INFO: Pod "pod-configmaps-c21960a4-faa7-422c-8c87-213c660d6bfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014367597s
Aug 15 18:16:29.819: INFO: Pod "pod-configmaps-c21960a4-faa7-422c-8c87-213c660d6bfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0186249s
STEP: Saw pod success
Aug 15 18:16:29.819: INFO: Pod "pod-configmaps-c21960a4-faa7-422c-8c87-213c660d6bfd" satisfied condition "success or failure"
Aug 15 18:16:29.822: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-configmaps-c21960a4-faa7-422c-8c87-213c660d6bfd container env-test: <nil>
STEP: delete the pod
Aug 15 18:16:29.843: INFO: Waiting for pod pod-configmaps-c21960a4-faa7-422c-8c87-213c660d6bfd to disappear
Aug 15 18:16:29.847: INFO: Pod pod-configmaps-c21960a4-faa7-422c-8c87-213c660d6bfd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:16:29.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9545" for this suite.
Aug 15 18:16:35.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:16:35.958: INFO: namespace secrets-9545 deletion completed in 6.106268756s

• [SLOW TEST:10.207 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:16:35.959: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 15 18:16:40.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec pod-sharedvolume-c9778247-10b0-4987-8c56-e6fbf6c4d359 -c busybox-main-container --namespace=emptydir-8747 -- cat /usr/share/volumeshare/shareddata.txt'
Aug 15 18:16:40.216: INFO: stderr: ""
Aug 15 18:16:40.216: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:16:40.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8747" for this suite.
Aug 15 18:16:46.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:16:46.325: INFO: namespace emptydir-8747 deletion completed in 6.103455786s

• [SLOW TEST:10.367 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:16:46.326: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-03367d9d-d30d-40e2-b2b6-85259027c112 in namespace container-probe-774
Aug 15 18:16:50.379: INFO: Started pod busybox-03367d9d-d30d-40e2-b2b6-85259027c112 in namespace container-probe-774
STEP: checking the pod's current state and verifying that restartCount is present
Aug 15 18:16:50.382: INFO: Initial restart count of pod busybox-03367d9d-d30d-40e2-b2b6-85259027c112 is 0
Aug 15 18:17:38.492: INFO: Restart count of pod container-probe-774/busybox-03367d9d-d30d-40e2-b2b6-85259027c112 is now 1 (48.110287099s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:17:38.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-774" for this suite.
Aug 15 18:17:44.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:17:44.630: INFO: namespace container-probe-774 deletion completed in 6.119103044s

• [SLOW TEST:58.305 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:17:44.631: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:17:50.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6266" for this suite.
Aug 15 18:17:56.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:17:56.905: INFO: namespace namespaces-6266 deletion completed in 6.118495227s
STEP: Destroying namespace "nsdeletetest-5429" for this suite.
Aug 15 18:17:56.907: INFO: Namespace nsdeletetest-5429 was already deleted
STEP: Destroying namespace "nsdeletetest-7088" for this suite.
Aug 15 18:18:02.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:18:03.052: INFO: namespace nsdeletetest-7088 deletion completed in 6.144566948s

• [SLOW TEST:18.421 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:18:03.052: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-pgx2
STEP: Creating a pod to test atomic-volume-subpath
Aug 15 18:18:03.116: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pgx2" in namespace "subpath-529" to be "success or failure"
Aug 15 18:18:03.122: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.779645ms
Aug 15 18:18:05.127: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010138248s
Aug 15 18:18:07.131: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014961899s
Aug 15 18:18:09.136: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 6.019567446s
Aug 15 18:18:11.140: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 8.0233686s
Aug 15 18:18:13.144: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 10.027511526s
Aug 15 18:18:15.149: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 12.032205932s
Aug 15 18:18:17.153: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 14.036526074s
Aug 15 18:18:19.156: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 16.039781428s
Aug 15 18:18:21.160: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 18.043624402s
Aug 15 18:18:23.163: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 20.047046097s
Aug 15 18:18:25.167: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Running", Reason="", readiness=true. Elapsed: 22.050943556s
Aug 15 18:18:27.171: INFO: Pod "pod-subpath-test-secret-pgx2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.054867877s
STEP: Saw pod success
Aug 15 18:18:27.171: INFO: Pod "pod-subpath-test-secret-pgx2" satisfied condition "success or failure"
Aug 15 18:18:27.175: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-subpath-test-secret-pgx2 container test-container-subpath-secret-pgx2: <nil>
STEP: delete the pod
Aug 15 18:18:27.199: INFO: Waiting for pod pod-subpath-test-secret-pgx2 to disappear
Aug 15 18:18:27.203: INFO: Pod pod-subpath-test-secret-pgx2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-pgx2
Aug 15 18:18:27.204: INFO: Deleting pod "pod-subpath-test-secret-pgx2" in namespace "subpath-529"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:18:27.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-529" for this suite.
Aug 15 18:18:33.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:18:33.324: INFO: namespace subpath-529 deletion completed in 6.111954286s

• [SLOW TEST:30.272 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:18:33.324: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 15 18:18:41.414: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 15 18:18:41.417: INFO: Pod pod-with-poststart-http-hook still exists
Aug 15 18:18:43.417: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 15 18:18:43.421: INFO: Pod pod-with-poststart-http-hook still exists
Aug 15 18:18:45.417: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 15 18:18:45.422: INFO: Pod pod-with-poststart-http-hook still exists
Aug 15 18:18:47.417: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 15 18:18:47.422: INFO: Pod pod-with-poststart-http-hook still exists
Aug 15 18:18:49.417: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 15 18:18:49.422: INFO: Pod pod-with-poststart-http-hook still exists
Aug 15 18:18:51.417: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 15 18:18:51.422: INFO: Pod pod-with-poststart-http-hook still exists
Aug 15 18:18:53.418: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 15 18:18:53.422: INFO: Pod pod-with-poststart-http-hook still exists
Aug 15 18:18:55.417: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 15 18:18:55.421: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:18:55.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7158" for this suite.
Aug 15 18:19:17.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:19:17.528: INFO: namespace container-lifecycle-hook-7158 deletion completed in 22.103285272s

• [SLOW TEST:44.204 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:19:17.529: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Aug 15 18:19:18.140: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0815 18:19:18.140108      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:19:18.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2443" for this suite.
Aug 15 18:19:24.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:19:24.261: INFO: namespace gc-2443 deletion completed in 6.117866883s

• [SLOW TEST:6.732 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:19:24.261: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-007a3d4b-b529-4f5f-8a87-9c080704108b
STEP: Creating a pod to test consume configMaps
Aug 15 18:19:24.308: INFO: Waiting up to 5m0s for pod "pod-configmaps-9eff2f08-9161-4b08-8802-3d1ebdde6eec" in namespace "configmap-3532" to be "success or failure"
Aug 15 18:19:24.313: INFO: Pod "pod-configmaps-9eff2f08-9161-4b08-8802-3d1ebdde6eec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.852048ms
Aug 15 18:19:26.317: INFO: Pod "pod-configmaps-9eff2f08-9161-4b08-8802-3d1ebdde6eec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008620896s
Aug 15 18:19:28.321: INFO: Pod "pod-configmaps-9eff2f08-9161-4b08-8802-3d1ebdde6eec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012581252s
STEP: Saw pod success
Aug 15 18:19:28.321: INFO: Pod "pod-configmaps-9eff2f08-9161-4b08-8802-3d1ebdde6eec" satisfied condition "success or failure"
Aug 15 18:19:28.324: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-configmaps-9eff2f08-9161-4b08-8802-3d1ebdde6eec container configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:19:28.345: INFO: Waiting for pod pod-configmaps-9eff2f08-9161-4b08-8802-3d1ebdde6eec to disappear
Aug 15 18:19:28.348: INFO: Pod pod-configmaps-9eff2f08-9161-4b08-8802-3d1ebdde6eec no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:19:28.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3532" for this suite.
Aug 15 18:19:34.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:19:34.455: INFO: namespace configmap-3532 deletion completed in 6.102772558s

• [SLOW TEST:10.194 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:19:34.455: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 15 18:19:34.487: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:19:38.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1307" for this suite.
Aug 15 18:19:44.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:19:44.273: INFO: namespace init-container-1307 deletion completed in 6.109366256s

• [SLOW TEST:9.818 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:19:44.273: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:19:44.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9491" for this suite.
Aug 15 18:20:06.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:20:06.443: INFO: namespace pods-9491 deletion completed in 22.115839631s

• [SLOW TEST:22.170 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:20:06.443: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-krcx
STEP: Creating a pod to test atomic-volume-subpath
Aug 15 18:20:06.497: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-krcx" in namespace "subpath-858" to be "success or failure"
Aug 15 18:20:06.515: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Pending", Reason="", readiness=false. Elapsed: 18.174729ms
Aug 15 18:20:08.519: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021841531s
Aug 15 18:20:10.524: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 4.026676899s
Aug 15 18:20:12.528: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 6.030752974s
Aug 15 18:20:14.532: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 8.035100732s
Aug 15 18:20:16.537: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 10.039317493s
Aug 15 18:20:18.541: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 12.043230458s
Aug 15 18:20:20.544: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 14.047060004s
Aug 15 18:20:22.548: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 16.051111532s
Aug 15 18:20:24.552: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 18.055202888s
Aug 15 18:20:26.558: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 20.060521757s
Aug 15 18:20:28.562: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Running", Reason="", readiness=true. Elapsed: 22.06463941s
Aug 15 18:20:30.566: INFO: Pod "pod-subpath-test-configmap-krcx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.069031637s
STEP: Saw pod success
Aug 15 18:20:30.566: INFO: Pod "pod-subpath-test-configmap-krcx" satisfied condition "success or failure"
Aug 15 18:20:30.569: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-subpath-test-configmap-krcx container test-container-subpath-configmap-krcx: <nil>
STEP: delete the pod
Aug 15 18:20:30.593: INFO: Waiting for pod pod-subpath-test-configmap-krcx to disappear
Aug 15 18:20:30.596: INFO: Pod pod-subpath-test-configmap-krcx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-krcx
Aug 15 18:20:30.596: INFO: Deleting pod "pod-subpath-test-configmap-krcx" in namespace "subpath-858"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:20:30.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-858" for this suite.
Aug 15 18:20:36.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:20:36.719: INFO: namespace subpath-858 deletion completed in 6.113324414s

• [SLOW TEST:30.276 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:20:36.719: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-bb786ee5-dc3d-4d53-a207-4c7a938e0894
STEP: Creating a pod to test consume secrets
Aug 15 18:20:36.762: INFO: Waiting up to 5m0s for pod "pod-secrets-fe17c141-da61-4da5-a93d-17060bd18585" in namespace "secrets-1808" to be "success or failure"
Aug 15 18:20:36.769: INFO: Pod "pod-secrets-fe17c141-da61-4da5-a93d-17060bd18585": Phase="Pending", Reason="", readiness=false. Elapsed: 6.658209ms
Aug 15 18:20:38.773: INFO: Pod "pod-secrets-fe17c141-da61-4da5-a93d-17060bd18585": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010789154s
Aug 15 18:20:40.777: INFO: Pod "pod-secrets-fe17c141-da61-4da5-a93d-17060bd18585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014407065s
STEP: Saw pod success
Aug 15 18:20:40.777: INFO: Pod "pod-secrets-fe17c141-da61-4da5-a93d-17060bd18585" satisfied condition "success or failure"
Aug 15 18:20:40.780: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-secrets-fe17c141-da61-4da5-a93d-17060bd18585 container secret-volume-test: <nil>
STEP: delete the pod
Aug 15 18:20:40.807: INFO: Waiting for pod pod-secrets-fe17c141-da61-4da5-a93d-17060bd18585 to disappear
Aug 15 18:20:40.811: INFO: Pod pod-secrets-fe17c141-da61-4da5-a93d-17060bd18585 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:20:40.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1808" for this suite.
Aug 15 18:20:46.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:20:46.921: INFO: namespace secrets-1808 deletion completed in 6.105398864s

• [SLOW TEST:10.202 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:20:46.921: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:20:46.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9571" for this suite.
Aug 15 18:20:53.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:20:53.122: INFO: namespace kubelet-test-9571 deletion completed in 6.146557724s

• [SLOW TEST:6.201 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:20:53.123: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Aug 15 18:20:53.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-7966'
Aug 15 18:20:53.574: INFO: stderr: ""
Aug 15 18:20:53.574: INFO: stdout: "pod/pause created\n"
Aug 15 18:20:53.574: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 15 18:20:53.574: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7966" to be "running and ready"
Aug 15 18:20:53.584: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.942595ms
Aug 15 18:20:55.588: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014093884s
Aug 15 18:20:57.594: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.01960877s
Aug 15 18:20:57.594: INFO: Pod "pause" satisfied condition "running and ready"
Aug 15 18:20:57.594: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 15 18:20:57.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 label pods pause testing-label=testing-label-value --namespace=kubectl-7966'
Aug 15 18:20:57.700: INFO: stderr: ""
Aug 15 18:20:57.700: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 15 18:20:57.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pod pause -L testing-label --namespace=kubectl-7966'
Aug 15 18:20:57.782: INFO: stderr: ""
Aug 15 18:20:57.782: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 15 18:20:57.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 label pods pause testing-label- --namespace=kubectl-7966'
Aug 15 18:20:57.862: INFO: stderr: ""
Aug 15 18:20:57.862: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 15 18:20:57.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pod pause -L testing-label --namespace=kubectl-7966'
Aug 15 18:20:57.944: INFO: stderr: ""
Aug 15 18:20:57.944: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Aug 15 18:20:57.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-7966'
Aug 15 18:20:58.039: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 18:20:58.039: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 15 18:20:58.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get rc,svc -l name=pause --no-headers --namespace=kubectl-7966'
Aug 15 18:20:58.144: INFO: stderr: "No resources found.\n"
Aug 15 18:20:58.144: INFO: stdout: ""
Aug 15 18:20:58.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -l name=pause --namespace=kubectl-7966 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 15 18:20:58.249: INFO: stderr: ""
Aug 15 18:20:58.249: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:20:58.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7966" for this suite.
Aug 15 18:21:04.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:21:04.383: INFO: namespace kubectl-7966 deletion completed in 6.126414466s

• [SLOW TEST:11.260 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:21:04.383: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 15 18:21:04.422: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Aug 15 18:21:04.923: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 15 18:21:06.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:21:08.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:21:10.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:21:12.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:21:14.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:21:16.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490064, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:21:20.634: INFO: Waited 1.648172656s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:21:21.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4965" for this suite.
Aug 15 18:21:27.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:21:27.352: INFO: namespace aggregator-4965 deletion completed in 6.199367666s

• [SLOW TEST:22.969 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:21:27.353: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7252.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7252.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7252.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7252.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 15 18:21:41.424: INFO: DNS probes using dns-test-50010597-53db-4300-96be-fc4fda9bc8c7 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7252.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7252.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7252.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7252.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 15 18:21:45.490: INFO: DNS probes using dns-test-07e62894-8cb6-4d1c-9088-23b3151894bb succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7252.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7252.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7252.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7252.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 15 18:21:49.569: INFO: DNS probes using dns-test-e9a20f55-683b-4ecb-ad7a-30970454f1f1 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:21:49.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7252" for this suite.
Aug 15 18:21:55.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:21:55.735: INFO: namespace dns-7252 deletion completed in 6.114184069s

• [SLOW TEST:28.382 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:21:55.735: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-92612ef8-9ce1-400b-a40d-4c81fb129abe
STEP: Creating a pod to test consume secrets
Aug 15 18:21:55.788: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-67cf19e3-71e1-4bdf-b577-5474ac4c03b8" in namespace "projected-6620" to be "success or failure"
Aug 15 18:21:55.796: INFO: Pod "pod-projected-secrets-67cf19e3-71e1-4bdf-b577-5474ac4c03b8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.680038ms
Aug 15 18:21:57.800: INFO: Pod "pod-projected-secrets-67cf19e3-71e1-4bdf-b577-5474ac4c03b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012177608s
Aug 15 18:21:59.805: INFO: Pod "pod-projected-secrets-67cf19e3-71e1-4bdf-b577-5474ac4c03b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016644184s
STEP: Saw pod success
Aug 15 18:21:59.805: INFO: Pod "pod-projected-secrets-67cf19e3-71e1-4bdf-b577-5474ac4c03b8" satisfied condition "success or failure"
Aug 15 18:21:59.809: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-projected-secrets-67cf19e3-71e1-4bdf-b577-5474ac4c03b8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 15 18:21:59.832: INFO: Waiting for pod pod-projected-secrets-67cf19e3-71e1-4bdf-b577-5474ac4c03b8 to disappear
Aug 15 18:21:59.838: INFO: Pod pod-projected-secrets-67cf19e3-71e1-4bdf-b577-5474ac4c03b8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:21:59.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6620" for this suite.
Aug 15 18:22:05.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:22:05.958: INFO: namespace projected-6620 deletion completed in 6.116544801s

• [SLOW TEST:10.223 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:22:05.959: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 15 18:22:16.063: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:16.066: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:18.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:18.070: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:20.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:20.069: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:22.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:22.071: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:24.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:24.070: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:26.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:26.070: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:28.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:28.073: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:30.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:30.071: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:32.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:32.071: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:34.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:34.070: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:36.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:36.070: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:38.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:38.071: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:40.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:40.070: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:42.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:42.070: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 15 18:22:44.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 15 18:22:44.070: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:22:44.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7283" for this suite.
Aug 15 18:23:06.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:23:06.192: INFO: namespace container-lifecycle-hook-7283 deletion completed in 22.117752458s

• [SLOW TEST:60.233 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:23:06.192: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 15 18:23:12.261: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.261: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:12.340: INFO: Exec stderr: ""
Aug 15 18:23:12.340: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.340: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:12.420: INFO: Exec stderr: ""
Aug 15 18:23:12.420: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.420: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:12.499: INFO: Exec stderr: ""
Aug 15 18:23:12.499: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.499: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:12.577: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 15 18:23:12.577: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.577: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:12.659: INFO: Exec stderr: ""
Aug 15 18:23:12.659: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.659: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:12.737: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 15 18:23:12.737: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.737: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:12.814: INFO: Exec stderr: ""
Aug 15 18:23:12.814: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.814: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:12.894: INFO: Exec stderr: ""
Aug 15 18:23:12.894: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.894: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:12.977: INFO: Exec stderr: ""
Aug 15 18:23:12.977: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6507 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:23:12.977: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:23:13.062: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:23:13.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6507" for this suite.
Aug 15 18:23:59.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:23:59.184: INFO: namespace e2e-kubelet-etc-hosts-6507 deletion completed in 46.115024563s

• [SLOW TEST:52.992 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:23:59.185: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:23:59.214: INFO: Creating deployment "nginx-deployment"
Aug 15 18:23:59.222: INFO: Waiting for observed generation 1
Aug 15 18:24:01.237: INFO: Waiting for all required pods to come up
Aug 15 18:24:01.244: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 15 18:24:03.262: INFO: Waiting for deployment "nginx-deployment" to complete
Aug 15 18:24:03.269: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug 15 18:24:03.279: INFO: Updating deployment nginx-deployment
Aug 15 18:24:03.279: INFO: Waiting for observed generation 2
Aug 15 18:24:05.299: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 15 18:24:05.302: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 15 18:24:05.305: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 15 18:24:05.312: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 15 18:24:05.312: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 15 18:24:05.314: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 15 18:24:05.318: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug 15 18:24:05.318: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug 15 18:24:05.326: INFO: Updating deployment nginx-deployment
Aug 15 18:24:05.326: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug 15 18:24:05.342: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 15 18:24:05.353: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 15 18:24:05.404: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-8566,SelfLink:/apis/apps/v1/namespaces/deployment-8566/deployments/nginx-deployment,UID:65a45160-61c8-4142-9205-16759db955fc,ResourceVersion:19297,Generation:3,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-08-15 18:24:03 +0000 UTC 2019-08-15 18:23:59 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-08-15 18:24:05 +0000 UTC 2019-08-15 18:24:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug 15 18:24:05.437: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-8566,SelfLink:/apis/apps/v1/namespaces/deployment-8566/replicasets/nginx-deployment-55fb7cb77f,UID:e751ae34-b04e-4da0-b180-b76faf9f7e7b,ResourceVersion:19293,Generation:3,CreationTimestamp:2019-08-15 18:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 65a45160-61c8-4142-9205-16759db955fc 0xc002c40d47 0xc002c40d48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 15 18:24:05.437: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug 15 18:24:05.437: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-8566,SelfLink:/apis/apps/v1/namespaces/deployment-8566/replicasets/nginx-deployment-7b8c6f4498,UID:7595eefe-6c13-4653-9e1d-1c61c65dbbb2,ResourceVersion:19333,Generation:3,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 65a45160-61c8-4142-9205-16759db955fc 0xc002c40f47 0xc002c40f48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug 15 18:24:05.479: INFO: Pod "nginx-deployment-55fb7cb77f-4m8lm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4m8lm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-4m8lm,UID:d47249ed-3e6b-4a8f-8e15-edbe577f2b3e,ResourceVersion:19348,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3c517 0xc002e3c518}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3c580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3c5a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.479: INFO: Pod "nginx-deployment-55fb7cb77f-68z5x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-68z5x,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-68z5x,UID:e4acfa66-87e2-4056-958d-684369bc67f5,ResourceVersion:19310,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3c620 0xc002e3c621}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3c690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3c6b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.479: INFO: Pod "nginx-deployment-55fb7cb77f-8gvn9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8gvn9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-8gvn9,UID:702d516f-601b-4d3e-ab9a-f4f144a19cbe,ResourceVersion:19342,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3c730 0xc002e3c731}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3c7a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3c7c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.479: INFO: Pod "nginx-deployment-55fb7cb77f-95rjj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-95rjj,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-95rjj,UID:4a85b414-29c6-4505-96cf-42c40c0fedc6,ResourceVersion:19282,Generation:0,CreationTimestamp:2019-08-15 18:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3c840 0xc002e3c841}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3c8b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3c8d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.131,PodIP:,StartTime:2019-08-15 18:24:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.479: INFO: Pod "nginx-deployment-55fb7cb77f-htwh9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-htwh9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-htwh9,UID:fc3a9408-d506-4356-9017-0a8ddb626e1b,ResourceVersion:19340,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3c9a0 0xc002e3c9a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3ca10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3ca30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.480: INFO: Pod "nginx-deployment-55fb7cb77f-jmbx6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jmbx6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-jmbx6,UID:6209a7d2-fbbf-471a-a138-e7fc47eca5de,ResourceVersion:19281,Generation:0,CreationTimestamp:2019-08-15 18:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3cab0 0xc002e3cab1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3cb20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3cb40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.132,PodIP:,StartTime:2019-08-15 18:24:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.480: INFO: Pod "nginx-deployment-55fb7cb77f-jmfk7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jmfk7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-jmfk7,UID:81663928-cf59-43ef-8383-51b750e65bf4,ResourceVersion:19265,Generation:0,CreationTimestamp:2019-08-15 18:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3cc10 0xc002e3cc11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3cc80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3cca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.137,PodIP:,StartTime:2019-08-15 18:24:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.480: INFO: Pod "nginx-deployment-55fb7cb77f-ljqj2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ljqj2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-ljqj2,UID:03c0484e-612f-4bbf-8a7f-c96e8a0c5667,ResourceVersion:19261,Generation:0,CreationTimestamp:2019-08-15 18:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3cd70 0xc002e3cd71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3cde0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3ce00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.131,PodIP:,StartTime:2019-08-15 18:24:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.480: INFO: Pod "nginx-deployment-55fb7cb77f-mfqkc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mfqkc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-mfqkc,UID:39834eae-87f1-4153-b66c-a4dc7aee1e86,ResourceVersion:19316,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3ced0 0xc002e3ced1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3cf40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3cf60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.480: INFO: Pod "nginx-deployment-55fb7cb77f-v9qnt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-v9qnt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-v9qnt,UID:661629cf-6e29-4ff3-b348-4e6eb767f3b7,ResourceVersion:19337,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3cfe0 0xc002e3cfe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3d050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3d070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.481: INFO: Pod "nginx-deployment-55fb7cb77f-wz88w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wz88w,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-wz88w,UID:d1af3e74-9995-4430-8912-186dc150a556,ResourceVersion:19314,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3d0f0 0xc002e3d0f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3d160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3d180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.481: INFO: Pod "nginx-deployment-55fb7cb77f-xhl97" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xhl97,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-xhl97,UID:c3515d0d-105f-416e-873f-d9de64d252e8,ResourceVersion:19339,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3d210 0xc002e3d211}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3d280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3d2a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.481: INFO: Pod "nginx-deployment-55fb7cb77f-zz9mq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zz9mq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-55fb7cb77f-zz9mq,UID:9dd95f3d-cc3d-4a59-8605-5b45cf8af898,ResourceVersion:19255,Generation:0,CreationTimestamp:2019-08-15 18:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f e751ae34-b04e-4da0-b180-b76faf9f7e7b 0xc002e3d320 0xc002e3d321}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3d390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3d3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:03 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.132,PodIP:,StartTime:2019-08-15 18:24:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.481: INFO: Pod "nginx-deployment-7b8c6f4498-28cpj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-28cpj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-28cpj,UID:8adc125b-09e4-42c6-b2d7-def99312da58,ResourceVersion:19213,Generation:0,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc002e3d480 0xc002e3d481}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3d4f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3d510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.131,PodIP:172.20.3.67,StartTime:2019-08-15 18:23:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-15 18:24:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d8b06309c6e56e2f2fe07ccac71d91039da9bfbd8a5080235ecf3785c0f0c032}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.482: INFO: Pod "nginx-deployment-7b8c6f4498-669lv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-669lv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-669lv,UID:9818ef09-c082-4218-b4f4-40dccc5a849b,ResourceVersion:19334,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc002e3d5e0 0xc002e3d5e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3d650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3d670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.137,PodIP:,StartTime:2019-08-15 18:24:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.482: INFO: Pod "nginx-deployment-7b8c6f4498-75dvf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-75dvf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-75dvf,UID:6b7efff4-67a8-4156-9d2f-276cc7b231d6,ResourceVersion:19341,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc002e3d737 0xc002e3d738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3d7b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3d7d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.482: INFO: Pod "nginx-deployment-7b8c6f4498-7jtbd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7jtbd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-7jtbd,UID:e22b47e8-9796-4ca7-8a68-0133a960f6b6,ResourceVersion:19317,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc002e3d860 0xc002e3d861}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3d8c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3d8e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.482: INFO: Pod "nginx-deployment-7b8c6f4498-b7mbf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-b7mbf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-b7mbf,UID:cd75e1ff-cce7-4047-b433-3dbe93bea3ed,ResourceVersion:19227,Generation:0,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc002e3d960 0xc002e3d961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3d9c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3d9e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.137,PodIP:172.20.2.73,StartTime:2019-08-15 18:23:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-15 18:24:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1f874c3a78c2be57a77e4777b8c1d46cd2f7cf0d10aaf45a5de7e3095e89577b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.482: INFO: Pod "nginx-deployment-7b8c6f4498-bm9lr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bm9lr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-bm9lr,UID:dba9a1f1-ae61-4423-b6ad-21d48c9ddad1,ResourceVersion:19219,Generation:0,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc002e3dac0 0xc002e3dac1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3db20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3db40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.132,PodIP:172.20.4.75,StartTime:2019-08-15 18:23:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-15 18:24:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8526a3796346ad07e7c9c03df7e54677211bd83a239bd7229810321a43d24015}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.483: INFO: Pod "nginx-deployment-7b8c6f4498-cgml9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cgml9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-cgml9,UID:bef6e28c-2c92-49c2-8774-d0ba481c499c,ResourceVersion:19208,Generation:0,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc002e3dc20 0xc002e3dc21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3dc80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3dca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.131,PodIP:172.20.3.70,StartTime:2019-08-15 18:23:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-15 18:24:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ccf220ec4dd7144d944fbdee2133b441b26d4bf62a0b630250eb72493b526b5b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.483: INFO: Pod "nginx-deployment-7b8c6f4498-cndkv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cndkv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-cndkv,UID:e1f1775d-0ffb-40b5-a9c0-564ba558d783,ResourceVersion:19222,Generation:0,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc002e3dd70 0xc002e3dd71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3dde0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3de00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.132,PodIP:172.20.4.74,StartTime:2019-08-15 18:23:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-15 18:24:02 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e95fa62ac9ec0d2040d9ab4efbf153202ddc46877532e41972df80bf8a531758}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.483: INFO: Pod "nginx-deployment-7b8c6f4498-cskcx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cskcx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-cskcx,UID:ce44e395-86fb-470d-810a-205db3679c58,ResourceVersion:19192,Generation:0,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc002e3ded0 0xc002e3ded1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e3df40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e3df60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.137,PodIP:172.20.2.72,StartTime:2019-08-15 18:23:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-15 18:24:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8171aa10bc40fa0a5d7e52cdd47c947529b7df25d73da8034dac45a678aa9f3c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.483: INFO: Pod "nginx-deployment-7b8c6f4498-hjcnt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hjcnt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-hjcnt,UID:7ee5a233-15be-4ec1-9192-e21540be1946,ResourceVersion:19315,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031ea0a0 0xc0031ea0a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031ea100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031ea120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.484: INFO: Pod "nginx-deployment-7b8c6f4498-jrhhm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jrhhm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-jrhhm,UID:cebf19f2-d25e-4c55-bdf0-baf86f321943,ResourceVersion:19343,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031ea1a0 0xc0031ea1a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031ea210} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031ea230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.484: INFO: Pod "nginx-deployment-7b8c6f4498-kgjmp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kgjmp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-kgjmp,UID:750255ae-be55-430b-b245-d30cc4ab7f8c,ResourceVersion:19175,Generation:0,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031ea2b0 0xc0031ea2b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031ea330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031ea360}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.131,PodIP:172.20.3.69,StartTime:2019-08-15 18:23:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-15 18:24:00 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4f9e8ee1d59e3330c01b91995a7a0510aba626767073a25fcc8eb712d336c62a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.484: INFO: Pod "nginx-deployment-7b8c6f4498-p5xz4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-p5xz4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-p5xz4,UID:0919343f-9701-405a-b0d0-452b862a93ee,ResourceVersion:19347,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031ea4e0 0xc0031ea4e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031ea670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031ea6b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.132,PodIP:,StartTime:2019-08-15 18:24:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.484: INFO: Pod "nginx-deployment-7b8c6f4498-q5fzj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-q5fzj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-q5fzj,UID:fa833955-9eed-46ec-aa86-1fe1f31d7ee6,ResourceVersion:19350,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031ea877 0xc0031ea878}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031ea8f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031ea910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.131,PodIP:,StartTime:2019-08-15 18:24:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.484: INFO: Pod "nginx-deployment-7b8c6f4498-qh6w9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qh6w9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-qh6w9,UID:c89ac75f-fff8-408c-b441-30a2cdebbaa0,ResourceVersion:19329,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031ea9d7 0xc0031ea9d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031eaac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031eaae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.132,PodIP:,StartTime:2019-08-15 18:24:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.485: INFO: Pod "nginx-deployment-7b8c6f4498-t76hq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t76hq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-t76hq,UID:38efc496-17a6-492b-b0d1-3f4d6226296d,ResourceVersion:19336,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031eaba7 0xc0031eaba8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031eac90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031eacb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.485: INFO: Pod "nginx-deployment-7b8c6f4498-txm2f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-txm2f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-txm2f,UID:66026ab7-606e-47ba-962f-cea5a9bf14b6,ResourceVersion:19338,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031eadd0 0xc0031eadd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031eae30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031eae50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.485: INFO: Pod "nginx-deployment-7b8c6f4498-vr2kt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vr2kt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-vr2kt,UID:8e0d516f-5241-4809-a20d-b91bd2835222,ResourceVersion:19311,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031eaed0 0xc0031eaed1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031eaf30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031eaf50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.485: INFO: Pod "nginx-deployment-7b8c6f4498-vrzw4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vrzw4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-vrzw4,UID:3b79908a-d84f-44c1-b330-17f6a085f538,ResourceVersion:19206,Generation:0,CreationTimestamp:2019-08-15 18:23:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031eafd0 0xc0031eafd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031eb030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031eb050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:23:59 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.131,PodIP:172.20.3.68,StartTime:2019-08-15 18:23:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-15 18:24:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7101b1be5c204a50c2f82c2e3c0288260c68b4ccc9ef8d0db97f1934bbff695a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 15 18:24:05.486: INFO: Pod "nginx-deployment-7b8c6f4498-zflhf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zflhf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8566,SelfLink:/api/v1/namespaces/deployment-8566/pods/nginx-deployment-7b8c6f4498-zflhf,UID:a7bbb0c2-ebdc-4b49-8faa-aa272c79427c,ResourceVersion:19335,Generation:0,CreationTimestamp:2019-08-15 18:24:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7595eefe-6c13-4653-9e1d-1c61c65dbbb2 0xc0031eb120 0xc0031eb121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m265r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m265r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-m265r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031eb180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031eb1a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:24:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:24:05.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8566" for this suite.
Aug 15 18:24:13.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:24:13.623: INFO: namespace deployment-8566 deletion completed in 8.12108032s

• [SLOW TEST:14.438 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:24:13.623: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d5dc33f3-cd74-4146-8e11-f601ea786c50
STEP: Creating a pod to test consume configMaps
Aug 15 18:24:13.671: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b821b326-42c2-440b-8e26-9a0e9927477e" in namespace "projected-4608" to be "success or failure"
Aug 15 18:24:13.675: INFO: Pod "pod-projected-configmaps-b821b326-42c2-440b-8e26-9a0e9927477e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.169815ms
Aug 15 18:24:15.679: INFO: Pod "pod-projected-configmaps-b821b326-42c2-440b-8e26-9a0e9927477e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00807061s
Aug 15 18:24:17.683: INFO: Pod "pod-projected-configmaps-b821b326-42c2-440b-8e26-9a0e9927477e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011520868s
STEP: Saw pod success
Aug 15 18:24:17.683: INFO: Pod "pod-projected-configmaps-b821b326-42c2-440b-8e26-9a0e9927477e" satisfied condition "success or failure"
Aug 15 18:24:17.686: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-projected-configmaps-b821b326-42c2-440b-8e26-9a0e9927477e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:24:17.708: INFO: Waiting for pod pod-projected-configmaps-b821b326-42c2-440b-8e26-9a0e9927477e to disappear
Aug 15 18:24:17.711: INFO: Pod pod-projected-configmaps-b821b326-42c2-440b-8e26-9a0e9927477e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:24:17.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4608" for this suite.
Aug 15 18:24:23.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:24:23.839: INFO: namespace projected-4608 deletion completed in 6.122295479s

• [SLOW TEST:10.216 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:24:23.839: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:24:23.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-885" for this suite.
Aug 15 18:24:29.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:24:30.012: INFO: namespace services-885 deletion completed in 6.11965201s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.173 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:24:30.012: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-69abc676-abb7-441e-a722-59d77cf051c5
STEP: Creating a pod to test consume configMaps
Aug 15 18:24:30.061: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ce6e453a-32a0-46f4-9c32-c72c76a06a4a" in namespace "projected-5978" to be "success or failure"
Aug 15 18:24:30.065: INFO: Pod "pod-projected-configmaps-ce6e453a-32a0-46f4-9c32-c72c76a06a4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13237ms
Aug 15 18:24:32.069: INFO: Pod "pod-projected-configmaps-ce6e453a-32a0-46f4-9c32-c72c76a06a4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007847304s
Aug 15 18:24:34.074: INFO: Pod "pod-projected-configmaps-ce6e453a-32a0-46f4-9c32-c72c76a06a4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012448259s
STEP: Saw pod success
Aug 15 18:24:34.074: INFO: Pod "pod-projected-configmaps-ce6e453a-32a0-46f4-9c32-c72c76a06a4a" satisfied condition "success or failure"
Aug 15 18:24:34.076: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-projected-configmaps-ce6e453a-32a0-46f4-9c32-c72c76a06a4a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:24:34.101: INFO: Waiting for pod pod-projected-configmaps-ce6e453a-32a0-46f4-9c32-c72c76a06a4a to disappear
Aug 15 18:24:34.104: INFO: Pod pod-projected-configmaps-ce6e453a-32a0-46f4-9c32-c72c76a06a4a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:24:34.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5978" for this suite.
Aug 15 18:24:40.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:24:40.228: INFO: namespace projected-5978 deletion completed in 6.117853857s

• [SLOW TEST:10.215 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:24:40.228: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 18:24:40.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09241a5a-60ad-4332-8f88-4b9d8eba5ce7" in namespace "projected-8858" to be "success or failure"
Aug 15 18:24:40.281: INFO: Pod "downwardapi-volume-09241a5a-60ad-4332-8f88-4b9d8eba5ce7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815463ms
Aug 15 18:24:42.285: INFO: Pod "downwardapi-volume-09241a5a-60ad-4332-8f88-4b9d8eba5ce7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008143104s
STEP: Saw pod success
Aug 15 18:24:42.285: INFO: Pod "downwardapi-volume-09241a5a-60ad-4332-8f88-4b9d8eba5ce7" satisfied condition "success or failure"
Aug 15 18:24:42.288: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-09241a5a-60ad-4332-8f88-4b9d8eba5ce7 container client-container: <nil>
STEP: delete the pod
Aug 15 18:24:42.312: INFO: Waiting for pod downwardapi-volume-09241a5a-60ad-4332-8f88-4b9d8eba5ce7 to disappear
Aug 15 18:24:42.316: INFO: Pod downwardapi-volume-09241a5a-60ad-4332-8f88-4b9d8eba5ce7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:24:42.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8858" for this suite.
Aug 15 18:24:48.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:24:48.439: INFO: namespace projected-8858 deletion completed in 6.119982654s

• [SLOW TEST:8.212 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:24:48.440: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 15 18:24:48.480: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-a,UID:7c2e11a3-9077-4b32-bd73-d6427820cdeb,ResourceVersion:19765,Generation:0,CreationTimestamp:2019-08-15 18:24:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 15 18:24:48.480: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-a,UID:7c2e11a3-9077-4b32-bd73-d6427820cdeb,ResourceVersion:19765,Generation:0,CreationTimestamp:2019-08-15 18:24:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 15 18:24:58.493: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-a,UID:7c2e11a3-9077-4b32-bd73-d6427820cdeb,ResourceVersion:19784,Generation:0,CreationTimestamp:2019-08-15 18:24:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 15 18:24:58.493: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-a,UID:7c2e11a3-9077-4b32-bd73-d6427820cdeb,ResourceVersion:19784,Generation:0,CreationTimestamp:2019-08-15 18:24:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 15 18:25:08.503: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-a,UID:7c2e11a3-9077-4b32-bd73-d6427820cdeb,ResourceVersion:19802,Generation:0,CreationTimestamp:2019-08-15 18:24:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 15 18:25:08.503: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-a,UID:7c2e11a3-9077-4b32-bd73-d6427820cdeb,ResourceVersion:19802,Generation:0,CreationTimestamp:2019-08-15 18:24:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 15 18:25:18.512: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-a,UID:7c2e11a3-9077-4b32-bd73-d6427820cdeb,ResourceVersion:19821,Generation:0,CreationTimestamp:2019-08-15 18:24:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 15 18:25:18.513: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-a,UID:7c2e11a3-9077-4b32-bd73-d6427820cdeb,ResourceVersion:19821,Generation:0,CreationTimestamp:2019-08-15 18:24:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 15 18:25:28.524: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-b,UID:afa98e38-7a0a-4171-a178-d5bd4ac7d79f,ResourceVersion:19840,Generation:0,CreationTimestamp:2019-08-15 18:25:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 15 18:25:28.525: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-b,UID:afa98e38-7a0a-4171-a178-d5bd4ac7d79f,ResourceVersion:19840,Generation:0,CreationTimestamp:2019-08-15 18:25:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 15 18:25:38.534: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-b,UID:afa98e38-7a0a-4171-a178-d5bd4ac7d79f,ResourceVersion:19860,Generation:0,CreationTimestamp:2019-08-15 18:25:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 15 18:25:38.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1011,SelfLink:/api/v1/namespaces/watch-1011/configmaps/e2e-watch-test-configmap-b,UID:afa98e38-7a0a-4171-a178-d5bd4ac7d79f,ResourceVersion:19860,Generation:0,CreationTimestamp:2019-08-15 18:25:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:25:48.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1011" for this suite.
Aug 15 18:25:54.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:25:54.651: INFO: namespace watch-1011 deletion completed in 6.111326485s

• [SLOW TEST:66.212 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:25:54.652: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:25:58.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8331" for this suite.
Aug 15 18:26:48.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:26:48.834: INFO: namespace kubelet-test-8331 deletion completed in 50.109338394s

• [SLOW TEST:54.182 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:26:48.834: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Aug 15 18:26:48.874: INFO: Waiting up to 5m0s for pod "client-containers-dfa8c4e1-2926-4671-87fd-e93681a98444" in namespace "containers-5969" to be "success or failure"
Aug 15 18:26:48.880: INFO: Pod "client-containers-dfa8c4e1-2926-4671-87fd-e93681a98444": Phase="Pending", Reason="", readiness=false. Elapsed: 5.659181ms
Aug 15 18:26:50.884: INFO: Pod "client-containers-dfa8c4e1-2926-4671-87fd-e93681a98444": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009708268s
Aug 15 18:26:52.888: INFO: Pod "client-containers-dfa8c4e1-2926-4671-87fd-e93681a98444": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014130504s
STEP: Saw pod success
Aug 15 18:26:52.889: INFO: Pod "client-containers-dfa8c4e1-2926-4671-87fd-e93681a98444" satisfied condition "success or failure"
Aug 15 18:26:52.892: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod client-containers-dfa8c4e1-2926-4671-87fd-e93681a98444 container test-container: <nil>
STEP: delete the pod
Aug 15 18:26:52.917: INFO: Waiting for pod client-containers-dfa8c4e1-2926-4671-87fd-e93681a98444 to disappear
Aug 15 18:26:52.921: INFO: Pod client-containers-dfa8c4e1-2926-4671-87fd-e93681a98444 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:26:52.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5969" for this suite.
Aug 15 18:26:58.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:26:59.047: INFO: namespace containers-5969 deletion completed in 6.121747558s

• [SLOW TEST:10.213 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:26:59.048: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-febdea26-bbd4-492f-9be6-ed49f1545325
STEP: Creating a pod to test consume secrets
Aug 15 18:26:59.099: INFO: Waiting up to 5m0s for pod "pod-secrets-e4107732-2389-4761-a2dd-faa5f53a70e4" in namespace "secrets-2319" to be "success or failure"
Aug 15 18:26:59.103: INFO: Pod "pod-secrets-e4107732-2389-4761-a2dd-faa5f53a70e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.285993ms
Aug 15 18:27:01.107: INFO: Pod "pod-secrets-e4107732-2389-4761-a2dd-faa5f53a70e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007719932s
Aug 15 18:27:03.111: INFO: Pod "pod-secrets-e4107732-2389-4761-a2dd-faa5f53a70e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011659383s
STEP: Saw pod success
Aug 15 18:27:03.111: INFO: Pod "pod-secrets-e4107732-2389-4761-a2dd-faa5f53a70e4" satisfied condition "success or failure"
Aug 15 18:27:03.114: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-secrets-e4107732-2389-4761-a2dd-faa5f53a70e4 container secret-volume-test: <nil>
STEP: delete the pod
Aug 15 18:27:03.140: INFO: Waiting for pod pod-secrets-e4107732-2389-4761-a2dd-faa5f53a70e4 to disappear
Aug 15 18:27:03.144: INFO: Pod pod-secrets-e4107732-2389-4761-a2dd-faa5f53a70e4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:27:03.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2319" for this suite.
Aug 15 18:27:09.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:27:09.270: INFO: namespace secrets-2319 deletion completed in 6.12082666s

• [SLOW TEST:10.223 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:27:09.271: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-20ec8244-707b-4347-aed3-72e3d1eabf6f
STEP: Creating a pod to test consume configMaps
Aug 15 18:27:09.320: INFO: Waiting up to 5m0s for pod "pod-configmaps-4153f5d3-c6bf-4925-98f6-a1d710a03d7f" in namespace "configmap-4111" to be "success or failure"
Aug 15 18:27:09.331: INFO: Pod "pod-configmaps-4153f5d3-c6bf-4925-98f6-a1d710a03d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.455234ms
Aug 15 18:27:11.335: INFO: Pod "pod-configmaps-4153f5d3-c6bf-4925-98f6-a1d710a03d7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015011164s
Aug 15 18:27:13.340: INFO: Pod "pod-configmaps-4153f5d3-c6bf-4925-98f6-a1d710a03d7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019363232s
STEP: Saw pod success
Aug 15 18:27:13.340: INFO: Pod "pod-configmaps-4153f5d3-c6bf-4925-98f6-a1d710a03d7f" satisfied condition "success or failure"
Aug 15 18:27:13.343: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-configmaps-4153f5d3-c6bf-4925-98f6-a1d710a03d7f container configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:27:13.364: INFO: Waiting for pod pod-configmaps-4153f5d3-c6bf-4925-98f6-a1d710a03d7f to disappear
Aug 15 18:27:13.367: INFO: Pod pod-configmaps-4153f5d3-c6bf-4925-98f6-a1d710a03d7f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:27:13.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4111" for this suite.
Aug 15 18:27:19.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:27:19.479: INFO: namespace configmap-4111 deletion completed in 6.108078204s

• [SLOW TEST:10.209 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:27:19.480: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Aug 15 18:27:19.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 --namespace=kubectl-9471 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 15 18:27:22.476: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 15 18:27:22.476: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:27:24.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9471" for this suite.
Aug 15 18:27:36.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:27:36.591: INFO: namespace kubectl-9471 deletion completed in 12.105351241s

• [SLOW TEST:17.112 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:27:36.592: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-15217a88-2f06-41f8-85eb-c78ffcb184aa
STEP: Creating a pod to test consume configMaps
Aug 15 18:27:36.638: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cd0a35a-9977-437a-b7a4-1c7e2684b529" in namespace "configmap-5823" to be "success or failure"
Aug 15 18:27:36.641: INFO: Pod "pod-configmaps-7cd0a35a-9977-437a-b7a4-1c7e2684b529": Phase="Pending", Reason="", readiness=false. Elapsed: 2.582098ms
Aug 15 18:27:38.645: INFO: Pod "pod-configmaps-7cd0a35a-9977-437a-b7a4-1c7e2684b529": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006759714s
Aug 15 18:27:40.649: INFO: Pod "pod-configmaps-7cd0a35a-9977-437a-b7a4-1c7e2684b529": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011000458s
STEP: Saw pod success
Aug 15 18:27:40.649: INFO: Pod "pod-configmaps-7cd0a35a-9977-437a-b7a4-1c7e2684b529" satisfied condition "success or failure"
Aug 15 18:27:40.652: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-configmaps-7cd0a35a-9977-437a-b7a4-1c7e2684b529 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:27:40.675: INFO: Waiting for pod pod-configmaps-7cd0a35a-9977-437a-b7a4-1c7e2684b529 to disappear
Aug 15 18:27:40.678: INFO: Pod pod-configmaps-7cd0a35a-9977-437a-b7a4-1c7e2684b529 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:27:40.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5823" for this suite.
Aug 15 18:27:46.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:27:46.786: INFO: namespace configmap-5823 deletion completed in 6.103053981s

• [SLOW TEST:10.194 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:27:46.786: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:27:46.819: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 15 18:27:46.829: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 15 18:27:51.833: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 15 18:27:51.833: INFO: Creating deployment "test-rolling-update-deployment"
Aug 15 18:27:51.838: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 15 18:27:51.844: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 15 18:27:53.854: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 15 18:27:53.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490471, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490471, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490471, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701490471, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 15 18:27:55.862: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Aug 15 18:27:55.872: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9604,SelfLink:/apis/apps/v1/namespaces/deployment-9604/deployments/test-rolling-update-deployment,UID:eafa41be-e696-4df8-af27-eb78ff15a4f9,ResourceVersion:20309,Generation:1,CreationTimestamp:2019-08-15 18:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-15 18:27:51 +0000 UTC 2019-08-15 18:27:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-15 18:27:54 +0000 UTC 2019-08-15 18:27:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 15 18:27:55.876: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-9604,SelfLink:/apis/apps/v1/namespaces/deployment-9604/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:32ff9edc-b74f-43e8-83af-8abf0f22866b,ResourceVersion:20298,Generation:1,CreationTimestamp:2019-08-15 18:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment eafa41be-e696-4df8-af27-eb78ff15a4f9 0xc00373afd7 0xc00373afd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 15 18:27:55.876: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 15 18:27:55.876: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9604,SelfLink:/apis/apps/v1/namespaces/deployment-9604/replicasets/test-rolling-update-controller,UID:1e324272-6b51-46f4-8ebf-3e82956ce772,ResourceVersion:20307,Generation:2,CreationTimestamp:2019-08-15 18:27:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment eafa41be-e696-4df8-af27-eb78ff15a4f9 0xc00373aee7 0xc00373aee8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 15 18:27:55.879: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-4bnbx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-4bnbx,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-9604,SelfLink:/api/v1/namespaces/deployment-9604/pods/test-rolling-update-deployment-79f6b9d75c-4bnbx,UID:287e1aa7-0889-4478-94bc-057219b617c2,ResourceVersion:20297,Generation:0,CreationTimestamp:2019-08-15 18:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 32ff9edc-b74f-43e8-83af-8abf0f22866b 0xc00373b8c7 0xc00373b8c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r9fxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r9fxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-r9fxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00373b930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00373b950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:27:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:27:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:27:51 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.132,PodIP:172.20.4.98,StartTime:2019-08-15 18:27:51 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-15 18:27:53 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://b67938304e01e30fa39516191989fe0c6e411bbb6f53143297b22b2d9147c977}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:27:55.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9604" for this suite.
Aug 15 18:28:01.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:28:01.995: INFO: namespace deployment-9604 deletion completed in 6.112140204s

• [SLOW TEST:15.209 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:28:01.995: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-242
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-242
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-242
Aug 15 18:28:02.069: INFO: Found 0 stateful pods, waiting for 1
Aug 15 18:28:12.073: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 15 18:28:12.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:28:12.250: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:28:12.250: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:28:12.250: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 15 18:28:12.254: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 15 18:28:22.261: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 15 18:28:22.262: INFO: Waiting for statefulset status.replicas updated to 0
Aug 15 18:28:22.279: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:28:22.279: INFO: ss-0  karbon-fifteen-3efeed-k8s-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  }]
Aug 15 18:28:22.279: INFO: 
Aug 15 18:28:22.279: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 15 18:28:23.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996228702s
Aug 15 18:28:24.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991840611s
Aug 15 18:28:25.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987874692s
Aug 15 18:28:26.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980011385s
Aug 15 18:28:27.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975276161s
Aug 15 18:28:28.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969474192s
Aug 15 18:28:29.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956244332s
Aug 15 18:28:30.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950726938s
Aug 15 18:28:31.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.320437ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-242
Aug 15 18:28:32.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:28:32.503: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 15 18:28:32.503: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 15 18:28:32.503: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 15 18:28:32.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:28:32.680: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 15 18:28:32.680: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 15 18:28:32.680: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 15 18:28:32.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:28:32.846: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 15 18:28:32.846: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 15 18:28:32.846: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 15 18:28:32.851: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 15 18:28:42.856: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:28:42.856: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:28:42.856: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 15 18:28:42.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:28:43.027: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:28:43.027: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:28:43.027: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 15 18:28:43.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:28:43.189: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:28:43.189: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:28:43.189: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 15 18:28:43.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:28:43.356: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:28:43.356: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:28:43.356: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 15 18:28:43.356: INFO: Waiting for statefulset status.replicas updated to 0
Aug 15 18:28:43.359: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 15 18:28:53.368: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 15 18:28:53.368: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 15 18:28:53.368: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 15 18:28:53.380: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:28:53.380: INFO: ss-0  karbon-fifteen-3efeed-k8s-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  }]
Aug 15 18:28:53.380: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:53.380: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:53.380: INFO: 
Aug 15 18:28:53.380: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 15 18:28:54.384: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:28:54.384: INFO: ss-0  karbon-fifteen-3efeed-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  }]
Aug 15 18:28:54.384: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:54.384: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:54.384: INFO: 
Aug 15 18:28:54.384: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 15 18:28:55.388: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:28:55.388: INFO: ss-0  karbon-fifteen-3efeed-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  }]
Aug 15 18:28:55.388: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:55.388: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:55.389: INFO: 
Aug 15 18:28:55.389: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 15 18:28:56.393: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:28:56.393: INFO: ss-0  karbon-fifteen-3efeed-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  }]
Aug 15 18:28:56.393: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:56.393: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:56.393: INFO: 
Aug 15 18:28:56.393: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 15 18:28:57.398: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:28:57.398: INFO: ss-0  karbon-fifteen-3efeed-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  }]
Aug 15 18:28:57.398: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:57.398: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:57.398: INFO: 
Aug 15 18:28:57.398: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 15 18:28:58.402: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:28:58.402: INFO: ss-0  karbon-fifteen-3efeed-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:02 +0000 UTC  }]
Aug 15 18:28:58.402: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:58.403: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:58.403: INFO: 
Aug 15 18:28:58.403: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 15 18:28:59.407: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:28:59.407: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:59.407: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:28:59.407: INFO: 
Aug 15 18:28:59.407: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 15 18:29:00.412: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:29:00.412: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:29:00.412: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:29:00.412: INFO: 
Aug 15 18:29:00.412: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 15 18:29:01.417: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:29:01.417: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:29:01.417: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:29:01.417: INFO: 
Aug 15 18:29:01.417: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 15 18:29:02.423: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Aug 15 18:29:02.423: INFO: ss-1  karbon-fifteen-3efeed-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:29:02.423: INFO: ss-2  karbon-fifteen-3efeed-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:28:22 +0000 UTC  }]
Aug 15 18:29:02.423: INFO: 
Aug 15 18:29:02.423: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-242
Aug 15 18:29:03.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:29:03.580: INFO: rc: 1
Aug 15 18:29:03.581: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00303c9c0 exit status 1 <nil> <nil> true [0xc00244e978 0xc00244e990 0xc00244e9a8] [0xc00244e978 0xc00244e990 0xc00244e9a8] [0xc00244e988 0xc00244e9a0] [0x9d17b0 0x9d17b0] 0xc001e02960 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Aug 15 18:29:13.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:29:13.678: INFO: rc: 1
Aug 15 18:29:13.678: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0024394a0 exit status 1 <nil> <nil> true [0xc0030309e8 0xc003030a00 0xc003030a28] [0xc0030309e8 0xc003030a00 0xc003030a28] [0xc0030309f8 0xc003030a10] [0x9d17b0 0x9d17b0] 0xc001dee0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:29:23.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:29:23.760: INFO: rc: 1
Aug 15 18:29:23.760: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002439800 exit status 1 <nil> <nil> true [0xc003030a30 0xc003030a48 0xc003030a60] [0xc003030a30 0xc003030a48 0xc003030a60] [0xc003030a40 0xc003030a58] [0x9d17b0 0x9d17b0] 0xc001dee4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:29:33.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:29:33.846: INFO: rc: 1
Aug 15 18:29:33.846: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020a65d0 exit status 1 <nil> <nil> true [0xc00348e008 0xc00348e048 0xc00348e070] [0xc00348e008 0xc00348e048 0xc00348e070] [0xc00348e038 0xc00348e068] [0x9d17b0 0x9d17b0] 0xc002976720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:29:43.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:29:43.931: INFO: rc: 1
Aug 15 18:29:43.931: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002cf4570 exit status 1 <nil> <nil> true [0xc00053d4a0 0xc00053d5e0 0xc00053d7d8] [0xc00053d4a0 0xc00053d5e0 0xc00053d7d8] [0xc00053d598 0xc00053d7a8] [0x9d17b0 0x9d17b0] 0xc0022a0540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:29:53.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:29:54.020: INFO: rc: 1
Aug 15 18:29:54.020: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0021d6360 exit status 1 <nil> <nil> true [0xc0013ca048 0xc0013ca208 0xc0013ca3c8] [0xc0013ca048 0xc0013ca208 0xc0013ca3c8] [0xc0013ca1a8 0xc0013ca3a0] [0x9d17b0 0x9d17b0] 0xc0035fa3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:30:04.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:30:04.131: INFO: rc: 1
Aug 15 18:30:04.131: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0021d66c0 exit status 1 <nil> <nil> true [0xc0013ca448 0xc0013ca5e0 0xc0013ca818] [0xc0013ca448 0xc0013ca5e0 0xc0013ca818] [0xc0013ca558 0xc0013ca630] [0x9d17b0 0x9d17b0] 0xc0035fa780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:30:14.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:30:14.213: INFO: rc: 1
Aug 15 18:30:14.213: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020a6b40 exit status 1 <nil> <nil> true [0xc00348e088 0xc00348e0b0 0xc00348e0e8] [0xc00348e088 0xc00348e0b0 0xc00348e0e8] [0xc00348e0a8 0xc00348e0e0] [0x9d17b0 0x9d17b0] 0xc002976e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:30:24.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:30:24.303: INFO: rc: 1
Aug 15 18:30:24.304: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002320390 exit status 1 <nil> <nil> true [0xc000174070 0xc001c52020 0xc001c520b8] [0xc000174070 0xc001c52020 0xc001c520b8] [0xc000174508 0xc001c520a0] [0x9d17b0 0x9d17b0] 0xc0035d4660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:30:34.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:30:34.405: INFO: rc: 1
Aug 15 18:30:34.405: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002cf4900 exit status 1 <nil> <nil> true [0xc00053d848 0xc00053d938 0xc00053db48] [0xc00053d848 0xc00053d938 0xc00053db48] [0xc00053d8b8 0xc00053daa8] [0x9d17b0 0x9d17b0] 0xc0022a09c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:30:44.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:30:44.490: INFO: rc: 1
Aug 15 18:30:44.490: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002cf4c60 exit status 1 <nil> <nil> true [0xc00053dbb8 0xc00053ddb8 0xc00053de38] [0xc00053dbb8 0xc00053ddb8 0xc00053de38] [0xc00053dcd8 0xc00053de18] [0x9d17b0 0x9d17b0] 0xc0022a0f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:30:54.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:30:54.783: INFO: rc: 1
Aug 15 18:30:54.783: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020a6ed0 exit status 1 <nil> <nil> true [0xc00348e100 0xc00348e130 0xc00348e170] [0xc00348e100 0xc00348e130 0xc00348e170] [0xc00348e118 0xc00348e148] [0x9d17b0 0x9d17b0] 0xc002977740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:31:04.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:31:04.874: INFO: rc: 1
Aug 15 18:31:04.875: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020a7260 exit status 1 <nil> <nil> true [0xc00348e188 0xc00348e1d8 0xc00348e200] [0xc00348e188 0xc00348e1d8 0xc00348e200] [0xc00348e1c0 0xc00348e1f8] [0x9d17b0 0x9d17b0] 0xc002977f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:31:14.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:31:14.959: INFO: rc: 1
Aug 15 18:31:14.960: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002cf4ff0 exit status 1 <nil> <nil> true [0xc00053de60 0xc00053df88 0xc000340280] [0xc00053de60 0xc00053df88 0xc000340280] [0xc00053df38 0xc0003401a8] [0x9d17b0 0x9d17b0] 0xc0022a12c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:31:24.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:31:25.042: INFO: rc: 1
Aug 15 18:31:25.042: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020a75f0 exit status 1 <nil> <nil> true [0xc00348e208 0xc00348e258 0xc00348e298] [0xc00348e208 0xc00348e258 0xc00348e298] [0xc00348e240 0xc00348e290] [0x9d17b0 0x9d17b0] 0xc0029f0480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:31:35.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:31:35.131: INFO: rc: 1
Aug 15 18:31:35.131: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0021d6330 exit status 1 <nil> <nil> true [0xc0001742d0 0xc00053d4a8 0xc00053d6c8] [0xc0001742d0 0xc00053d4a8 0xc00053d6c8] [0xc00053d4a0 0xc00053d5e0] [0x9d17b0 0x9d17b0] 0xc002976900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:31:45.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:31:45.215: INFO: rc: 1
Aug 15 18:31:45.215: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0023203c0 exit status 1 <nil> <nil> true [0xc0013ca048 0xc0013ca208 0xc0013ca3c8] [0xc0013ca048 0xc0013ca208 0xc0013ca3c8] [0xc0013ca1a8 0xc0013ca3a0] [0x9d17b0 0x9d17b0] 0xc0035fa2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:31:55.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:31:55.316: INFO: rc: 1
Aug 15 18:31:55.316: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002320720 exit status 1 <nil> <nil> true [0xc0013ca448 0xc0013ca5e0 0xc0013ca818] [0xc0013ca448 0xc0013ca5e0 0xc0013ca818] [0xc0013ca558 0xc0013ca630] [0x9d17b0 0x9d17b0] 0xc0035fa660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:32:05.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:32:05.401: INFO: rc: 1
Aug 15 18:32:05.401: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002320a80 exit status 1 <nil> <nil> true [0xc0013ca8a0 0xc0013caa40 0xc0013caf30] [0xc0013ca8a0 0xc0013caa40 0xc0013caf30] [0xc0013ca9a0 0xc0013cac68] [0x9d17b0 0x9d17b0] 0xc0035fa9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:32:15.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:32:15.482: INFO: rc: 1
Aug 15 18:32:15.483: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020a6630 exit status 1 <nil> <nil> true [0xc001c52020 0xc001c520b8 0xc001c520d8] [0xc001c52020 0xc001c520b8 0xc001c520d8] [0xc001c520a0 0xc001c520c8] [0x9d17b0 0x9d17b0] 0xc0035d4660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:32:25.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:32:25.597: INFO: rc: 1
Aug 15 18:32:25.597: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002cf45a0 exit status 1 <nil> <nil> true [0xc000340150 0xc000340338 0xc000340660] [0xc000340150 0xc000340338 0xc000340660] [0xc000340280 0xc000340608] [0x9d17b0 0x9d17b0] 0xc0022a0540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:32:35.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:32:35.687: INFO: rc: 1
Aug 15 18:32:35.687: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002321020 exit status 1 <nil> <nil> true [0xc0013cb010 0xc0013cb148 0xc0013cb248] [0xc0013cb010 0xc0013cb148 0xc0013cb248] [0xc0013cb0c0 0xc0013cb218] [0x9d17b0 0x9d17b0] 0xc0035fad80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:32:45.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:32:45.773: INFO: rc: 1
Aug 15 18:32:45.773: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0023214a0 exit status 1 <nil> <nil> true [0xc0013cb280 0xc0013cb440 0xc0013cb6d8] [0xc0013cb280 0xc0013cb440 0xc0013cb6d8] [0xc0013cb3d8 0xc0013cb678] [0x9d17b0 0x9d17b0] 0xc0035fb260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:32:55.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:32:55.862: INFO: rc: 1
Aug 15 18:32:55.863: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020a6c00 exit status 1 <nil> <nil> true [0xc001c520f8 0xc001c521c8 0xc001c52260] [0xc001c520f8 0xc001c521c8 0xc001c52260] [0xc001c521a0 0xc001c52250] [0x9d17b0 0x9d17b0] 0xc0035d4e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:33:05.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:33:05.951: INFO: rc: 1
Aug 15 18:33:05.951: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002cf4990 exit status 1 <nil> <nil> true [0xc000340718 0xc000340830 0xc000340a50] [0xc000340718 0xc000340830 0xc000340a50] [0xc0003407c8 0xc000340998] [0x9d17b0 0x9d17b0] 0xc0022a09c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:33:15.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:33:16.043: INFO: rc: 1
Aug 15 18:33:16.043: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020a6f90 exit status 1 <nil> <nil> true [0xc001c52270 0xc001c522e0 0xc001c52328] [0xc001c52270 0xc001c522e0 0xc001c52328] [0xc001c52290 0xc001c52320] [0x9d17b0 0x9d17b0] 0xc0035d52c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:33:26.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:33:26.136: INFO: rc: 1
Aug 15 18:33:26.136: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0020a7350 exit status 1 <nil> <nil> true [0xc001c52338 0xc001c523e0 0xc001c52420] [0xc001c52338 0xc001c523e0 0xc001c52420] [0xc001c523a8 0xc001c52418] [0x9d17b0 0x9d17b0] 0xc0035d5620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:33:36.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:33:36.224: INFO: rc: 1
Aug 15 18:33:36.224: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002cf44e0 exit status 1 <nil> <nil> true [0xc0001742d0 0xc0003401a8 0xc000340540] [0xc0001742d0 0xc0003401a8 0xc000340540] [0xc000340150 0xc000340338] [0x9d17b0 0x9d17b0] 0xc0022a01e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:33:46.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:33:46.309: INFO: rc: 1
Aug 15 18:33:46.309: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002cf48a0 exit status 1 <nil> <nil> true [0xc000340608 0xc000340750 0xc0003408f8] [0xc000340608 0xc000340750 0xc0003408f8] [0xc000340718 0xc000340830] [0x9d17b0 0x9d17b0] 0xc0022a08a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:33:56.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:33:56.398: INFO: rc: 1
Aug 15 18:33:56.398: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002cf4c30 exit status 1 <nil> <nil> true [0xc000340998 0xc000340ab0 0xc000340b38] [0xc000340998 0xc000340ab0 0xc000340b38] [0xc000340a90 0xc000340af0] [0x9d17b0 0x9d17b0] 0xc0022a0d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 15 18:34:06.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-242 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:34:06.480: INFO: rc: 1
Aug 15 18:34:06.480: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Aug 15 18:34:06.480: INFO: Scaling statefulset ss to 0
Aug 15 18:34:06.489: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 15 18:34:06.491: INFO: Deleting all statefulset in ns statefulset-242
Aug 15 18:34:06.494: INFO: Scaling statefulset ss to 0
Aug 15 18:34:06.502: INFO: Waiting for statefulset status.replicas updated to 0
Aug 15 18:34:06.504: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:34:06.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-242" for this suite.
Aug 15 18:34:12.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:34:12.636: INFO: namespace statefulset-242 deletion completed in 6.114450344s

• [SLOW TEST:370.641 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:34:12.636: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Aug 15 18:34:12.669: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 15 18:34:12.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-7524'
Aug 15 18:34:12.946: INFO: stderr: ""
Aug 15 18:34:12.946: INFO: stdout: "service/redis-slave created\n"
Aug 15 18:34:12.947: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 15 18:34:12.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-7524'
Aug 15 18:34:13.217: INFO: stderr: ""
Aug 15 18:34:13.217: INFO: stdout: "service/redis-master created\n"
Aug 15 18:34:13.217: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 15 18:34:13.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-7524'
Aug 15 18:34:13.441: INFO: stderr: ""
Aug 15 18:34:13.441: INFO: stdout: "service/frontend created\n"
Aug 15 18:34:13.442: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 15 18:34:13.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-7524'
Aug 15 18:34:13.622: INFO: stderr: ""
Aug 15 18:34:13.622: INFO: stdout: "deployment.apps/frontend created\n"
Aug 15 18:34:13.622: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 15 18:34:13.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-7524'
Aug 15 18:34:13.837: INFO: stderr: ""
Aug 15 18:34:13.837: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 15 18:34:13.837: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 15 18:34:13.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-7524'
Aug 15 18:34:14.037: INFO: stderr: ""
Aug 15 18:34:14.037: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 15 18:34:14.037: INFO: Waiting for all frontend pods to be Running.
Aug 15 18:34:39.088: INFO: Waiting for frontend to serve content.
Aug 15 18:34:39.104: INFO: Trying to add a new entry to the guestbook.
Aug 15 18:34:39.116: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 15 18:34:39.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-7524'
Aug 15 18:34:39.227: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 18:34:39.227: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 15 18:34:39.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-7524'
Aug 15 18:34:39.326: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 18:34:39.327: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 15 18:34:39.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-7524'
Aug 15 18:34:39.430: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 18:34:39.430: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 15 18:34:39.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-7524'
Aug 15 18:34:39.519: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 18:34:39.520: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 15 18:34:39.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-7524'
Aug 15 18:34:39.613: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 18:34:39.613: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 15 18:34:39.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-7524'
Aug 15 18:34:39.695: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 18:34:39.695: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:34:39.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7524" for this suite.
Aug 15 18:35:17.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:35:17.840: INFO: namespace kubectl-7524 deletion completed in 38.138468757s

• [SLOW TEST:65.204 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:35:17.840: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:35:21.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1178" for this suite.
Aug 15 18:35:27.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:35:28.041: INFO: namespace kubelet-test-1178 deletion completed in 6.117930571s

• [SLOW TEST:10.201 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:35:28.041: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-6546edf1-2951-49a2-8cea-e27813eae95e
STEP: Creating a pod to test consume configMaps
Aug 15 18:35:28.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-32231be5-5a5f-499d-be84-b5f157ebcec5" in namespace "configmap-2038" to be "success or failure"
Aug 15 18:35:28.096: INFO: Pod "pod-configmaps-32231be5-5a5f-499d-be84-b5f157ebcec5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05144ms
Aug 15 18:35:30.101: INFO: Pod "pod-configmaps-32231be5-5a5f-499d-be84-b5f157ebcec5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008484637s
Aug 15 18:35:32.105: INFO: Pod "pod-configmaps-32231be5-5a5f-499d-be84-b5f157ebcec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012369095s
STEP: Saw pod success
Aug 15 18:35:32.105: INFO: Pod "pod-configmaps-32231be5-5a5f-499d-be84-b5f157ebcec5" satisfied condition "success or failure"
Aug 15 18:35:32.108: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-configmaps-32231be5-5a5f-499d-be84-b5f157ebcec5 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:35:32.128: INFO: Waiting for pod pod-configmaps-32231be5-5a5f-499d-be84-b5f157ebcec5 to disappear
Aug 15 18:35:32.130: INFO: Pod pod-configmaps-32231be5-5a5f-499d-be84-b5f157ebcec5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:35:32.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2038" for this suite.
Aug 15 18:35:38.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:35:38.249: INFO: namespace configmap-2038 deletion completed in 6.114927199s

• [SLOW TEST:10.208 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:35:38.249: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 15 18:35:38.293: INFO: Waiting up to 5m0s for pod "pod-173b706d-4d3b-4066-a440-0ae02a4906ec" in namespace "emptydir-6278" to be "success or failure"
Aug 15 18:35:38.296: INFO: Pod "pod-173b706d-4d3b-4066-a440-0ae02a4906ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.865393ms
Aug 15 18:35:40.300: INFO: Pod "pod-173b706d-4d3b-4066-a440-0ae02a4906ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006941681s
Aug 15 18:35:42.304: INFO: Pod "pod-173b706d-4d3b-4066-a440-0ae02a4906ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011125986s
STEP: Saw pod success
Aug 15 18:35:42.304: INFO: Pod "pod-173b706d-4d3b-4066-a440-0ae02a4906ec" satisfied condition "success or failure"
Aug 15 18:35:42.307: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-173b706d-4d3b-4066-a440-0ae02a4906ec container test-container: <nil>
STEP: delete the pod
Aug 15 18:35:42.332: INFO: Waiting for pod pod-173b706d-4d3b-4066-a440-0ae02a4906ec to disappear
Aug 15 18:35:42.336: INFO: Pod pod-173b706d-4d3b-4066-a440-0ae02a4906ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:35:42.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6278" for this suite.
Aug 15 18:35:48.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:35:48.460: INFO: namespace emptydir-6278 deletion completed in 6.120169471s

• [SLOW TEST:10.211 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:35:48.460: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-e11700f6-d91b-4052-9247-3896d8e279f1
STEP: Creating a pod to test consume secrets
Aug 15 18:35:48.506: INFO: Waiting up to 5m0s for pod "pod-secrets-62b6c0ae-d638-414b-853c-862216f3bb9e" in namespace "secrets-3630" to be "success or failure"
Aug 15 18:35:48.511: INFO: Pod "pod-secrets-62b6c0ae-d638-414b-853c-862216f3bb9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809355ms
Aug 15 18:35:50.515: INFO: Pod "pod-secrets-62b6c0ae-d638-414b-853c-862216f3bb9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008880607s
Aug 15 18:35:52.519: INFO: Pod "pod-secrets-62b6c0ae-d638-414b-853c-862216f3bb9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012949995s
STEP: Saw pod success
Aug 15 18:35:52.519: INFO: Pod "pod-secrets-62b6c0ae-d638-414b-853c-862216f3bb9e" satisfied condition "success or failure"
Aug 15 18:35:52.522: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-secrets-62b6c0ae-d638-414b-853c-862216f3bb9e container secret-volume-test: <nil>
STEP: delete the pod
Aug 15 18:35:52.546: INFO: Waiting for pod pod-secrets-62b6c0ae-d638-414b-853c-862216f3bb9e to disappear
Aug 15 18:35:52.549: INFO: Pod pod-secrets-62b6c0ae-d638-414b-853c-862216f3bb9e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:35:52.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3630" for this suite.
Aug 15 18:35:58.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:35:58.680: INFO: namespace secrets-3630 deletion completed in 6.124757116s

• [SLOW TEST:10.220 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:35:58.681: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:36:02.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2699" for this suite.
Aug 15 18:36:44.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:36:44.883: INFO: namespace kubelet-test-2699 deletion completed in 42.125790655s

• [SLOW TEST:46.202 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:36:44.883: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 15 18:36:44.920: INFO: Waiting up to 5m0s for pod "pod-4b689157-4bfe-471f-8485-1879500f0c90" in namespace "emptydir-8303" to be "success or failure"
Aug 15 18:36:44.926: INFO: Pod "pod-4b689157-4bfe-471f-8485-1879500f0c90": Phase="Pending", Reason="", readiness=false. Elapsed: 5.297041ms
Aug 15 18:36:46.930: INFO: Pod "pod-4b689157-4bfe-471f-8485-1879500f0c90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00929016s
Aug 15 18:36:48.934: INFO: Pod "pod-4b689157-4bfe-471f-8485-1879500f0c90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013297416s
STEP: Saw pod success
Aug 15 18:36:48.934: INFO: Pod "pod-4b689157-4bfe-471f-8485-1879500f0c90" satisfied condition "success or failure"
Aug 15 18:36:48.937: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-4b689157-4bfe-471f-8485-1879500f0c90 container test-container: <nil>
STEP: delete the pod
Aug 15 18:36:48.959: INFO: Waiting for pod pod-4b689157-4bfe-471f-8485-1879500f0c90 to disappear
Aug 15 18:36:48.962: INFO: Pod pod-4b689157-4bfe-471f-8485-1879500f0c90 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:36:48.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8303" for this suite.
Aug 15 18:36:54.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:36:55.091: INFO: namespace emptydir-8303 deletion completed in 6.124044704s

• [SLOW TEST:10.208 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:36:55.092: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 15 18:36:58.162: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:36:58.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6135" for this suite.
Aug 15 18:37:04.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:37:04.294: INFO: namespace container-runtime-6135 deletion completed in 6.11298689s

• [SLOW TEST:9.203 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:37:04.295: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-5d31e5be-673d-49aa-bfca-c28e7723627b
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-5d31e5be-673d-49aa-bfca-c28e7723627b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:37:10.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8894" for this suite.
Aug 15 18:37:32.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:37:32.543: INFO: namespace configmap-8894 deletion completed in 22.105734241s

• [SLOW TEST:28.249 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:37:32.544: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug 15 18:38:12.614: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0815 18:38:12.614839      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 15 18:38:12.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8694" for this suite.
Aug 15 18:38:18.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:38:18.728: INFO: namespace gc-8694 deletion completed in 6.10873878s

• [SLOW TEST:46.184 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:38:18.728: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 15 18:38:23.308: INFO: Successfully updated pod "annotationupdate47047a92-c790-492b-a378-38f97d6a51ce"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:38:25.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2333" for this suite.
Aug 15 18:38:47.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:38:47.450: INFO: namespace projected-2333 deletion completed in 22.1169536s

• [SLOW TEST:28.722 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:38:47.450: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:38:47.488: INFO: (0) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.041224ms)
Aug 15 18:38:47.493: INFO: (1) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.964533ms)
Aug 15 18:38:47.497: INFO: (2) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.063474ms)
Aug 15 18:38:47.502: INFO: (3) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.566497ms)
Aug 15 18:38:47.507: INFO: (4) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.705966ms)
Aug 15 18:38:47.511: INFO: (5) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.300025ms)
Aug 15 18:38:47.516: INFO: (6) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.719723ms)
Aug 15 18:38:47.520: INFO: (7) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.296944ms)
Aug 15 18:38:47.524: INFO: (8) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.761173ms)
Aug 15 18:38:47.532: INFO: (9) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 7.574708ms)
Aug 15 18:38:47.537: INFO: (10) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.4493ms)
Aug 15 18:38:47.540: INFO: (11) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.188362ms)
Aug 15 18:38:47.543: INFO: (12) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.054307ms)
Aug 15 18:38:47.546: INFO: (13) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.04967ms)
Aug 15 18:38:47.549: INFO: (14) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 2.915402ms)
Aug 15 18:38:47.552: INFO: (15) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.509002ms)
Aug 15 18:38:47.556: INFO: (16) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.301719ms)
Aug 15 18:38:47.559: INFO: (17) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.089403ms)
Aug 15 18:38:47.562: INFO: (18) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 2.85405ms)
Aug 15 18:38:47.565: INFO: (19) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 2.851252ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:38:47.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6868" for this suite.
Aug 15 18:38:53.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:38:53.675: INFO: namespace proxy-6868 deletion completed in 6.107441437s

• [SLOW TEST:6.225 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:38:53.675: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 15 18:38:53.721: INFO: Waiting up to 5m0s for pod "pod-175c87b4-0e2e-4f8e-b724-4fe183cac7bc" in namespace "emptydir-8031" to be "success or failure"
Aug 15 18:38:53.729: INFO: Pod "pod-175c87b4-0e2e-4f8e-b724-4fe183cac7bc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.535575ms
Aug 15 18:38:55.734: INFO: Pod "pod-175c87b4-0e2e-4f8e-b724-4fe183cac7bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013496292s
Aug 15 18:38:57.738: INFO: Pod "pod-175c87b4-0e2e-4f8e-b724-4fe183cac7bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017193863s
Aug 15 18:38:59.742: INFO: Pod "pod-175c87b4-0e2e-4f8e-b724-4fe183cac7bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021268061s
STEP: Saw pod success
Aug 15 18:38:59.742: INFO: Pod "pod-175c87b4-0e2e-4f8e-b724-4fe183cac7bc" satisfied condition "success or failure"
Aug 15 18:38:59.745: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-175c87b4-0e2e-4f8e-b724-4fe183cac7bc container test-container: <nil>
STEP: delete the pod
Aug 15 18:38:59.765: INFO: Waiting for pod pod-175c87b4-0e2e-4f8e-b724-4fe183cac7bc to disappear
Aug 15 18:38:59.770: INFO: Pod pod-175c87b4-0e2e-4f8e-b724-4fe183cac7bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:38:59.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8031" for this suite.
Aug 15 18:39:05.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:39:05.875: INFO: namespace emptydir-8031 deletion completed in 6.100463192s

• [SLOW TEST:12.200 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:39:05.875: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 15 18:39:05.917: INFO: Waiting up to 5m0s for pod "downward-api-fa50bd69-18b7-4bce-b2ee-ad2cf454dc4a" in namespace "downward-api-4313" to be "success or failure"
Aug 15 18:39:05.920: INFO: Pod "downward-api-fa50bd69-18b7-4bce-b2ee-ad2cf454dc4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758817ms
Aug 15 18:39:07.924: INFO: Pod "downward-api-fa50bd69-18b7-4bce-b2ee-ad2cf454dc4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006598912s
Aug 15 18:39:09.928: INFO: Pod "downward-api-fa50bd69-18b7-4bce-b2ee-ad2cf454dc4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010362918s
STEP: Saw pod success
Aug 15 18:39:09.928: INFO: Pod "downward-api-fa50bd69-18b7-4bce-b2ee-ad2cf454dc4a" satisfied condition "success or failure"
Aug 15 18:39:09.930: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downward-api-fa50bd69-18b7-4bce-b2ee-ad2cf454dc4a container dapi-container: <nil>
STEP: delete the pod
Aug 15 18:39:09.960: INFO: Waiting for pod downward-api-fa50bd69-18b7-4bce-b2ee-ad2cf454dc4a to disappear
Aug 15 18:39:09.963: INFO: Pod downward-api-fa50bd69-18b7-4bce-b2ee-ad2cf454dc4a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:39:09.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4313" for this suite.
Aug 15 18:39:15.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:39:16.080: INFO: namespace downward-api-4313 deletion completed in 6.112308025s

• [SLOW TEST:10.204 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:39:16.081: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3417.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3417.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3417.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3417.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3417.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3417.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 15 18:39:20.177: INFO: DNS probes using dns-3417/dns-test-614c11f5-0299-4974-be4b-e9af42fc579f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:39:20.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3417" for this suite.
Aug 15 18:39:26.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:39:26.325: INFO: namespace dns-3417 deletion completed in 6.129723812s

• [SLOW TEST:10.244 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:39:26.325: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-347
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 15 18:39:26.389: INFO: Found 0 stateful pods, waiting for 3
Aug 15 18:39:36.394: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:39:36.394: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:39:36.394: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 15 18:39:36.424: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 15 18:39:46.460: INFO: Updating stateful set ss2
Aug 15 18:39:46.468: INFO: Waiting for Pod statefulset-347/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Aug 15 18:39:56.547: INFO: Found 2 stateful pods, waiting for 3
Aug 15 18:40:06.553: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:40:06.553: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:40:06.553: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 15 18:40:06.584: INFO: Updating stateful set ss2
Aug 15 18:40:06.594: INFO: Waiting for Pod statefulset-347/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 15 18:40:16.623: INFO: Updating stateful set ss2
Aug 15 18:40:16.632: INFO: Waiting for StatefulSet statefulset-347/ss2 to complete update
Aug 15 18:40:16.632: INFO: Waiting for Pod statefulset-347/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 15 18:40:26.640: INFO: Deleting all statefulset in ns statefulset-347
Aug 15 18:40:26.643: INFO: Scaling statefulset ss2 to 0
Aug 15 18:40:56.662: INFO: Waiting for statefulset status.replicas updated to 0
Aug 15 18:40:56.665: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:40:56.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-347" for this suite.
Aug 15 18:41:02.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:41:02.834: INFO: namespace statefulset-347 deletion completed in 6.150970568s

• [SLOW TEST:96.509 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:41:02.834: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Aug 15 18:41:02.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 cluster-info'
Aug 15 18:41:03.191: INFO: stderr: ""
Aug 15 18:41:03.191: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:41:03.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9286" for this suite.
Aug 15 18:41:09.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:41:09.312: INFO: namespace kubectl-9286 deletion completed in 6.116966303s

• [SLOW TEST:6.478 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:41:09.313: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:41:09.360: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 15 18:41:09.379: INFO: Number of nodes with available pods: 0
Aug 15 18:41:09.379: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 15 18:41:09.401: INFO: Number of nodes with available pods: 0
Aug 15 18:41:09.401: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:41:10.405: INFO: Number of nodes with available pods: 0
Aug 15 18:41:10.405: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:41:11.405: INFO: Number of nodes with available pods: 0
Aug 15 18:41:11.405: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:41:12.407: INFO: Number of nodes with available pods: 1
Aug 15 18:41:12.407: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 15 18:41:12.429: INFO: Number of nodes with available pods: 1
Aug 15 18:41:12.429: INFO: Number of running nodes: 0, number of available pods: 1
Aug 15 18:41:13.434: INFO: Number of nodes with available pods: 0
Aug 15 18:41:13.434: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 15 18:41:13.446: INFO: Number of nodes with available pods: 0
Aug 15 18:41:13.446: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:41:14.450: INFO: Number of nodes with available pods: 0
Aug 15 18:41:14.451: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:41:15.450: INFO: Number of nodes with available pods: 0
Aug 15 18:41:15.450: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:41:16.450: INFO: Number of nodes with available pods: 0
Aug 15 18:41:16.450: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:41:17.451: INFO: Number of nodes with available pods: 0
Aug 15 18:41:17.451: INFO: Node karbon-fifteen-3efeed-k8s-worker-0 is running more than one daemon pod
Aug 15 18:41:18.450: INFO: Number of nodes with available pods: 1
Aug 15 18:41:18.450: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3613, will wait for the garbage collector to delete the pods
Aug 15 18:41:18.518: INFO: Deleting DaemonSet.extensions daemon-set took: 8.811171ms
Aug 15 18:41:18.918: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.259917ms
Aug 15 18:41:29.422: INFO: Number of nodes with available pods: 0
Aug 15 18:41:29.422: INFO: Number of running nodes: 0, number of available pods: 0
Aug 15 18:41:29.424: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3613/daemonsets","resourceVersion":"23048"},"items":null}

Aug 15 18:41:29.427: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3613/pods","resourceVersion":"23048"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:41:29.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3613" for this suite.
Aug 15 18:41:35.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:41:35.563: INFO: namespace daemonsets-3613 deletion completed in 6.111448965s

• [SLOW TEST:26.251 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:41:35.563: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 15 18:41:35.597: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 15 18:41:35.605: INFO: Waiting for terminating namespaces to be deleted...
Aug 15 18:41:35.608: INFO: 
Logging pods the kubelet thinks is on node karbon-fifteen-3efeed-k8s-worker-0 before test
Aug 15 18:41:35.616: INFO: kube-flannel-ds-wb8nz from kube-system started at 2019-08-15 16:33:19 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 15 18:41:35.616: INFO: node-exporter-m5hlh from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 15 18:41:35.616: INFO: 	Container node-exporter ready: true, restart count 0
Aug 15 18:41:35.616: INFO: prometheus-k8s-0 from ntnx-system started at 2019-08-15 16:37:12 +0000 UTC (3 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container prometheus ready: true, restart count 1
Aug 15 18:41:35.616: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 15 18:41:35.616: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 15 18:41:35.616: INFO: sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-hjpr2 from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 15 18:41:35.616: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 15 18:41:35.616: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 15 18:41:35.616: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 15 18:41:35.616: INFO: fluent-bit-f6cdg from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 15 18:41:35.616: INFO: kibana-logging-8657c47867-cd27n from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container kibana-logging ready: true, restart count 0
Aug 15 18:41:35.616: INFO: 	Container nginxhttp ready: true, restart count 0
Aug 15 18:41:35.616: INFO: kube-proxy-ds-75v2p from kube-system started at 2019-08-15 16:32:55 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 15 18:41:35.616: INFO: csi-node-ntnx-plugin-k97vf from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 15 18:41:35.616: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 15 18:41:35.616: INFO: sonobuoy-e2e-job-ab1ba0141d2d464c from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.616: INFO: 	Container e2e ready: true, restart count 0
Aug 15 18:41:35.616: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 15 18:41:35.616: INFO: 
Logging pods the kubelet thinks is on node karbon-fifteen-3efeed-k8s-worker-1 before test
Aug 15 18:41:35.624: INFO: fluent-bit-glnl9 from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 15 18:41:35.624: INFO: prometheus-operator-999c9d4cf-kgcp8 from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 15 18:41:35.624: INFO: sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-7dxs8 from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 15 18:41:35.624: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 15 18:41:35.624: INFO: kube-proxy-ds-bh4g7 from kube-system started at 2019-08-15 16:32:55 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 15 18:41:35.624: INFO: kube-flannel-ds-d5nsh from kube-system started at 2019-08-15 16:33:19 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 15 18:41:35.624: INFO: alertmanager-main-1 from ntnx-system started at 2019-08-15 16:37:14 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container alertmanager ready: true, restart count 0
Aug 15 18:41:35.624: INFO: 	Container config-reloader ready: true, restart count 0
Aug 15 18:41:35.624: INFO: prometheus-k8s-1 from ntnx-system started at 2019-08-15 16:37:52 +0000 UTC (3 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container prometheus ready: true, restart count 1
Aug 15 18:41:35.624: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 15 18:41:35.624: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 15 18:41:35.624: INFO: csi-attacher-ntnx-plugin-0 from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container csi-attacher ready: true, restart count 0
Aug 15 18:41:35.624: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 15 18:41:35.624: INFO: csi-node-ntnx-plugin-vfwvr from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 15 18:41:35.624: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 15 18:41:35.624: INFO: kubernetes-events-printer-769d866479-jj7bk from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 15 18:41:35.624: INFO: node-exporter-7zm7k from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.624: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 15 18:41:35.624: INFO: 	Container node-exporter ready: true, restart count 0
Aug 15 18:41:35.624: INFO: 
Logging pods the kubelet thinks is on node karbon-fifteen-3efeed-k8s-worker-2 before test
Aug 15 18:41:35.632: INFO: fluent-bit-jnjtc from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 15 18:41:35.632: INFO: kube-flannel-ds-gwx8x from kube-system started at 2019-08-15 16:33:19 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 15 18:41:35.632: INFO: csi-node-ntnx-plugin-ljkj9 from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 15 18:41:35.632: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 15 18:41:35.632: INFO: alertmanager-main-0 from ntnx-system started at 2019-08-15 16:37:01 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container alertmanager ready: true, restart count 0
Aug 15 18:41:35.632: INFO: 	Container config-reloader ready: true, restart count 0
Aug 15 18:41:35.632: INFO: kube-proxy-ds-w9d6p from kube-system started at 2019-08-15 16:32:55 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 15 18:41:35.632: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-15 17:34:42 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 15 18:41:35.632: INFO: sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-hdxn6 from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 15 18:41:35.632: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 15 18:41:35.632: INFO: elasticsearch-logging-0 from ntnx-system started at 2019-08-15 16:34:12 +0000 UTC (1 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Aug 15 18:41:35.632: INFO: node-exporter-lj4nf from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (2 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 15 18:41:35.632: INFO: 	Container node-exporter ready: true, restart count 0
Aug 15 18:41:35.632: INFO: kube-state-metrics-7678d97797-ncjkj from ntnx-system started at 2019-08-15 16:37:04 +0000 UTC (4 container statuses recorded)
Aug 15 18:41:35.632: INFO: 	Container addon-resizer ready: true, restart count 0
Aug 15 18:41:35.632: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 15 18:41:35.632: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 15 18:41:35.632: INFO: 	Container kube-state-metrics ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15bb2cbf51710bd4], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:41:36.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5689" for this suite.
Aug 15 18:41:42.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:41:42.779: INFO: namespace sched-pred-5689 deletion completed in 6.112990609s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.216 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:41:42.780: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 15 18:41:42.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-473'
Aug 15 18:41:42.919: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 15 18:41:42.919: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Aug 15 18:41:44.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete deployment e2e-test-nginx-deployment --namespace=kubectl-473'
Aug 15 18:41:45.071: INFO: stderr: ""
Aug 15 18:41:45.071: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:41:45.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-473" for this suite.
Aug 15 18:41:51.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:41:51.192: INFO: namespace kubectl-473 deletion completed in 6.116354603s

• [SLOW TEST:8.413 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:41:51.192: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 15 18:41:51.234: INFO: Waiting up to 5m0s for pod "downward-api-acbe37f0-89a3-4e5a-919f-86286838c7a3" in namespace "downward-api-77" to be "success or failure"
Aug 15 18:41:51.238: INFO: Pod "downward-api-acbe37f0-89a3-4e5a-919f-86286838c7a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054086ms
Aug 15 18:41:53.242: INFO: Pod "downward-api-acbe37f0-89a3-4e5a-919f-86286838c7a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008122386s
Aug 15 18:41:55.246: INFO: Pod "downward-api-acbe37f0-89a3-4e5a-919f-86286838c7a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012221619s
STEP: Saw pod success
Aug 15 18:41:55.246: INFO: Pod "downward-api-acbe37f0-89a3-4e5a-919f-86286838c7a3" satisfied condition "success or failure"
Aug 15 18:41:55.250: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod downward-api-acbe37f0-89a3-4e5a-919f-86286838c7a3 container dapi-container: <nil>
STEP: delete the pod
Aug 15 18:41:55.271: INFO: Waiting for pod downward-api-acbe37f0-89a3-4e5a-919f-86286838c7a3 to disappear
Aug 15 18:41:55.275: INFO: Pod downward-api-acbe37f0-89a3-4e5a-919f-86286838c7a3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:41:55.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-77" for this suite.
Aug 15 18:42:01.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:42:01.397: INFO: namespace downward-api-77 deletion completed in 6.116498273s

• [SLOW TEST:10.204 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:42:01.397: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:42:07.481: INFO: Waiting up to 5m0s for pod "client-envvars-2fd466d0-6047-4d9d-b276-e4f5f9ecac8c" in namespace "pods-4863" to be "success or failure"
Aug 15 18:42:07.490: INFO: Pod "client-envvars-2fd466d0-6047-4d9d-b276-e4f5f9ecac8c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.012832ms
Aug 15 18:42:09.494: INFO: Pod "client-envvars-2fd466d0-6047-4d9d-b276-e4f5f9ecac8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013334834s
Aug 15 18:42:11.499: INFO: Pod "client-envvars-2fd466d0-6047-4d9d-b276-e4f5f9ecac8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017941864s
STEP: Saw pod success
Aug 15 18:42:11.499: INFO: Pod "client-envvars-2fd466d0-6047-4d9d-b276-e4f5f9ecac8c" satisfied condition "success or failure"
Aug 15 18:42:11.502: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod client-envvars-2fd466d0-6047-4d9d-b276-e4f5f9ecac8c container env3cont: <nil>
STEP: delete the pod
Aug 15 18:42:11.523: INFO: Waiting for pod client-envvars-2fd466d0-6047-4d9d-b276-e4f5f9ecac8c to disappear
Aug 15 18:42:11.527: INFO: Pod client-envvars-2fd466d0-6047-4d9d-b276-e4f5f9ecac8c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:42:11.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4863" for this suite.
Aug 15 18:42:57.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:42:57.642: INFO: namespace pods-4863 deletion completed in 46.111079085s

• [SLOW TEST:56.245 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:42:57.642: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Aug 15 18:42:57.680: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-136527297 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:42:57.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-664" for this suite.
Aug 15 18:43:03.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:43:03.892: INFO: namespace kubectl-664 deletion completed in 6.12093731s

• [SLOW TEST:6.250 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:43:03.892: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 15 18:43:03.940: INFO: Waiting up to 5m0s for pod "downward-api-cd8fb806-0d5a-4b7e-8ae6-5106918c4605" in namespace "downward-api-8066" to be "success or failure"
Aug 15 18:43:03.951: INFO: Pod "downward-api-cd8fb806-0d5a-4b7e-8ae6-5106918c4605": Phase="Pending", Reason="", readiness=false. Elapsed: 10.854631ms
Aug 15 18:43:05.956: INFO: Pod "downward-api-cd8fb806-0d5a-4b7e-8ae6-5106918c4605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015626671s
STEP: Saw pod success
Aug 15 18:43:05.956: INFO: Pod "downward-api-cd8fb806-0d5a-4b7e-8ae6-5106918c4605" satisfied condition "success or failure"
Aug 15 18:43:05.959: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downward-api-cd8fb806-0d5a-4b7e-8ae6-5106918c4605 container dapi-container: <nil>
STEP: delete the pod
Aug 15 18:43:05.983: INFO: Waiting for pod downward-api-cd8fb806-0d5a-4b7e-8ae6-5106918c4605 to disappear
Aug 15 18:43:05.988: INFO: Pod downward-api-cd8fb806-0d5a-4b7e-8ae6-5106918c4605 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:43:05.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8066" for this suite.
Aug 15 18:43:12.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:43:12.112: INFO: namespace downward-api-8066 deletion completed in 6.118017628s

• [SLOW TEST:8.220 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:43:12.112: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 15 18:43:16.167: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-57cace2a-b280-4534-95a0-0674891286a6,GenerateName:,Namespace:events-9490,SelfLink:/api/v1/namespaces/events-9490/pods/send-events-57cace2a-b280-4534-95a0-0674891286a6,UID:f1815e44-fc90-41bd-b713-e6d76113d800,ResourceVersion:23419,Generation:0,CreationTimestamp:2019-08-15 18:43:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 142201370,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z45v2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z45v2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-z45v2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-fifteen-3efeed-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020c71e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020c7200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:43:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:43:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:43:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-15 18:43:12 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.132,PodIP:172.20.4.114,StartTime:2019-08-15 18:43:12 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-15 18:43:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://42143fbe76e8df53a29358201c96f5398a6412ad4dc1736980a7ef930c1d3b5a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug 15 18:43:18.173: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 15 18:43:20.177: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:43:20.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9490" for this suite.
Aug 15 18:43:58.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:43:58.296: INFO: namespace events-9490 deletion completed in 38.104957872s

• [SLOW TEST:46.184 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:43:58.296: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 15 18:43:58.341: INFO: Waiting up to 5m0s for pod "pod-41687317-6f7f-4b70-9e95-bd5901511b1b" in namespace "emptydir-1426" to be "success or failure"
Aug 15 18:43:58.344: INFO: Pod "pod-41687317-6f7f-4b70-9e95-bd5901511b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166905ms
Aug 15 18:44:00.349: INFO: Pod "pod-41687317-6f7f-4b70-9e95-bd5901511b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007221561s
Aug 15 18:44:02.353: INFO: Pod "pod-41687317-6f7f-4b70-9e95-bd5901511b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011499923s
STEP: Saw pod success
Aug 15 18:44:02.353: INFO: Pod "pod-41687317-6f7f-4b70-9e95-bd5901511b1b" satisfied condition "success or failure"
Aug 15 18:44:02.355: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-41687317-6f7f-4b70-9e95-bd5901511b1b container test-container: <nil>
STEP: delete the pod
Aug 15 18:44:02.382: INFO: Waiting for pod pod-41687317-6f7f-4b70-9e95-bd5901511b1b to disappear
Aug 15 18:44:02.386: INFO: Pod pod-41687317-6f7f-4b70-9e95-bd5901511b1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:44:02.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1426" for this suite.
Aug 15 18:44:08.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:44:08.501: INFO: namespace emptydir-1426 deletion completed in 6.110438793s

• [SLOW TEST:10.205 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:44:08.502: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 18:44:08.548: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6955f1c-b2f6-4c33-a320-635598253cf6" in namespace "projected-7229" to be "success or failure"
Aug 15 18:44:08.551: INFO: Pod "downwardapi-volume-d6955f1c-b2f6-4c33-a320-635598253cf6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.125236ms
Aug 15 18:44:10.555: INFO: Pod "downwardapi-volume-d6955f1c-b2f6-4c33-a320-635598253cf6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007527795s
Aug 15 18:44:12.560: INFO: Pod "downwardapi-volume-d6955f1c-b2f6-4c33-a320-635598253cf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01178618s
STEP: Saw pod success
Aug 15 18:44:12.560: INFO: Pod "downwardapi-volume-d6955f1c-b2f6-4c33-a320-635598253cf6" satisfied condition "success or failure"
Aug 15 18:44:12.563: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-d6955f1c-b2f6-4c33-a320-635598253cf6 container client-container: <nil>
STEP: delete the pod
Aug 15 18:44:12.587: INFO: Waiting for pod downwardapi-volume-d6955f1c-b2f6-4c33-a320-635598253cf6 to disappear
Aug 15 18:44:12.591: INFO: Pod downwardapi-volume-d6955f1c-b2f6-4c33-a320-635598253cf6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:44:12.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7229" for this suite.
Aug 15 18:44:18.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:44:18.711: INFO: namespace projected-7229 deletion completed in 6.115922846s

• [SLOW TEST:10.210 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:44:18.712: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:44:18.749: INFO: Creating ReplicaSet my-hostname-basic-e2af8c51-0f47-482b-886e-be327cbb8a7c
Aug 15 18:44:18.761: INFO: Pod name my-hostname-basic-e2af8c51-0f47-482b-886e-be327cbb8a7c: Found 0 pods out of 1
Aug 15 18:44:23.766: INFO: Pod name my-hostname-basic-e2af8c51-0f47-482b-886e-be327cbb8a7c: Found 1 pods out of 1
Aug 15 18:44:23.766: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e2af8c51-0f47-482b-886e-be327cbb8a7c" is running
Aug 15 18:44:23.770: INFO: Pod "my-hostname-basic-e2af8c51-0f47-482b-886e-be327cbb8a7c-bgw7v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-15 18:44:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-15 18:44:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-15 18:44:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-15 18:44:18 +0000 UTC Reason: Message:}])
Aug 15 18:44:23.770: INFO: Trying to dial the pod
Aug 15 18:44:28.782: INFO: Controller my-hostname-basic-e2af8c51-0f47-482b-886e-be327cbb8a7c: Got expected result from replica 1 [my-hostname-basic-e2af8c51-0f47-482b-886e-be327cbb8a7c-bgw7v]: "my-hostname-basic-e2af8c51-0f47-482b-886e-be327cbb8a7c-bgw7v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:44:28.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1540" for this suite.
Aug 15 18:44:34.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:44:34.894: INFO: namespace replicaset-1540 deletion completed in 6.105034439s

• [SLOW TEST:16.182 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:44:34.894: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-114c80b7-4584-4fbc-8a69-87c0961287e9
STEP: Creating a pod to test consume configMaps
Aug 15 18:44:34.959: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ff73f76b-c8df-47ff-82e9-3ad6838f1c78" in namespace "projected-5414" to be "success or failure"
Aug 15 18:44:34.962: INFO: Pod "pod-projected-configmaps-ff73f76b-c8df-47ff-82e9-3ad6838f1c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921805ms
Aug 15 18:44:36.966: INFO: Pod "pod-projected-configmaps-ff73f76b-c8df-47ff-82e9-3ad6838f1c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006993938s
Aug 15 18:44:38.971: INFO: Pod "pod-projected-configmaps-ff73f76b-c8df-47ff-82e9-3ad6838f1c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011636753s
STEP: Saw pod success
Aug 15 18:44:38.971: INFO: Pod "pod-projected-configmaps-ff73f76b-c8df-47ff-82e9-3ad6838f1c78" satisfied condition "success or failure"
Aug 15 18:44:38.974: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-projected-configmaps-ff73f76b-c8df-47ff-82e9-3ad6838f1c78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:44:39.007: INFO: Waiting for pod pod-projected-configmaps-ff73f76b-c8df-47ff-82e9-3ad6838f1c78 to disappear
Aug 15 18:44:39.011: INFO: Pod pod-projected-configmaps-ff73f76b-c8df-47ff-82e9-3ad6838f1c78 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:44:39.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5414" for this suite.
Aug 15 18:44:45.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:44:45.120: INFO: namespace projected-5414 deletion completed in 6.104828075s

• [SLOW TEST:10.226 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:44:45.121: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:45:11.181: INFO: Container started at 2019-08-15 18:44:47 +0000 UTC, pod became ready at 2019-08-15 18:45:10 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:45:11.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7418" for this suite.
Aug 15 18:45:33.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:45:33.304: INFO: namespace container-probe-7418 deletion completed in 22.11885038s

• [SLOW TEST:48.184 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:45:33.304: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 15 18:45:33.359: INFO: Waiting up to 5m0s for pod "pod-a18820b6-06af-4422-a947-17f01421d69e" in namespace "emptydir-4327" to be "success or failure"
Aug 15 18:45:33.363: INFO: Pod "pod-a18820b6-06af-4422-a947-17f01421d69e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.463927ms
Aug 15 18:45:35.372: INFO: Pod "pod-a18820b6-06af-4422-a947-17f01421d69e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013369197s
Aug 15 18:45:37.381: INFO: Pod "pod-a18820b6-06af-4422-a947-17f01421d69e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022724166s
STEP: Saw pod success
Aug 15 18:45:37.381: INFO: Pod "pod-a18820b6-06af-4422-a947-17f01421d69e" satisfied condition "success or failure"
Aug 15 18:45:37.385: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-a18820b6-06af-4422-a947-17f01421d69e container test-container: <nil>
STEP: delete the pod
Aug 15 18:45:37.407: INFO: Waiting for pod pod-a18820b6-06af-4422-a947-17f01421d69e to disappear
Aug 15 18:45:37.411: INFO: Pod pod-a18820b6-06af-4422-a947-17f01421d69e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:45:37.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4327" for this suite.
Aug 15 18:45:43.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:45:43.533: INFO: namespace emptydir-4327 deletion completed in 6.11699777s

• [SLOW TEST:10.229 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:45:43.534: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 15 18:45:48.093: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6199 pod-service-account-e0380dbe-ed4e-4054-8230-5b4062f27ac3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 15 18:45:48.268: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6199 pod-service-account-e0380dbe-ed4e-4054-8230-5b4062f27ac3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 15 18:45:48.446: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6199 pod-service-account-e0380dbe-ed4e-4054-8230-5b4062f27ac3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:45:48.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6199" for this suite.
Aug 15 18:45:54.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:45:54.728: INFO: namespace svcaccounts-6199 deletion completed in 6.108492678s

• [SLOW TEST:11.194 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:45:54.728: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 15 18:45:54.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9198'
Aug 15 18:45:54.863: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 15 18:45:54.863: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Aug 15 18:45:54.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete jobs e2e-test-nginx-job --namespace=kubectl-9198'
Aug 15 18:45:54.971: INFO: stderr: ""
Aug 15 18:45:54.971: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:45:54.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9198" for this suite.
Aug 15 18:46:00.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:46:01.102: INFO: namespace kubectl-9198 deletion completed in 6.126814724s

• [SLOW TEST:6.375 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:46:01.102: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-db1012e0-23a8-42ef-a690-a028ab3043d8 in namespace container-probe-1054
Aug 15 18:46:05.155: INFO: Started pod test-webserver-db1012e0-23a8-42ef-a690-a028ab3043d8 in namespace container-probe-1054
STEP: checking the pod's current state and verifying that restartCount is present
Aug 15 18:46:05.160: INFO: Initial restart count of pod test-webserver-db1012e0-23a8-42ef-a690-a028ab3043d8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:50:05.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1054" for this suite.
Aug 15 18:50:11.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:50:11.820: INFO: namespace container-probe-1054 deletion completed in 6.124740903s

• [SLOW TEST:250.718 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:50:11.820: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 15 18:50:11.886: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9421,SelfLink:/api/v1/namespaces/watch-9421/configmaps/e2e-watch-test-resource-version,UID:5a320932-d0f5-4e23-aa14-faa09ef963ac,ResourceVersion:24407,Generation:0,CreationTimestamp:2019-08-15 18:50:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 15 18:50:11.887: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9421,SelfLink:/api/v1/namespaces/watch-9421/configmaps/e2e-watch-test-resource-version,UID:5a320932-d0f5-4e23-aa14-faa09ef963ac,ResourceVersion:24408,Generation:0,CreationTimestamp:2019-08-15 18:50:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:50:11.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9421" for this suite.
Aug 15 18:50:17.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:50:18.012: INFO: namespace watch-9421 deletion completed in 6.118671382s

• [SLOW TEST:6.192 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:50:18.012: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 15 18:50:18.066: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1601,SelfLink:/api/v1/namespaces/watch-1601/configmaps/e2e-watch-test-watch-closed,UID:a05e4080-18bf-4e9d-864d-a399aaaf85bb,ResourceVersion:24427,Generation:0,CreationTimestamp:2019-08-15 18:50:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 15 18:50:18.066: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1601,SelfLink:/api/v1/namespaces/watch-1601/configmaps/e2e-watch-test-watch-closed,UID:a05e4080-18bf-4e9d-864d-a399aaaf85bb,ResourceVersion:24428,Generation:0,CreationTimestamp:2019-08-15 18:50:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 15 18:50:18.080: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1601,SelfLink:/api/v1/namespaces/watch-1601/configmaps/e2e-watch-test-watch-closed,UID:a05e4080-18bf-4e9d-864d-a399aaaf85bb,ResourceVersion:24429,Generation:0,CreationTimestamp:2019-08-15 18:50:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 15 18:50:18.080: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1601,SelfLink:/api/v1/namespaces/watch-1601/configmaps/e2e-watch-test-watch-closed,UID:a05e4080-18bf-4e9d-864d-a399aaaf85bb,ResourceVersion:24430,Generation:0,CreationTimestamp:2019-08-15 18:50:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:50:18.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1601" for this suite.
Aug 15 18:50:24.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:50:24.191: INFO: namespace watch-1601 deletion completed in 6.107088724s

• [SLOW TEST:6.179 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:50:24.192: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 18:50:24.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e6bf904-0754-4a21-896d-e22a3fbd4dac" in namespace "projected-7376" to be "success or failure"
Aug 15 18:50:24.248: INFO: Pod "downwardapi-volume-2e6bf904-0754-4a21-896d-e22a3fbd4dac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.779731ms
Aug 15 18:50:26.251: INFO: Pod "downwardapi-volume-2e6bf904-0754-4a21-896d-e22a3fbd4dac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010544324s
Aug 15 18:50:28.256: INFO: Pod "downwardapi-volume-2e6bf904-0754-4a21-896d-e22a3fbd4dac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014755349s
STEP: Saw pod success
Aug 15 18:50:28.256: INFO: Pod "downwardapi-volume-2e6bf904-0754-4a21-896d-e22a3fbd4dac" satisfied condition "success or failure"
Aug 15 18:50:28.259: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod downwardapi-volume-2e6bf904-0754-4a21-896d-e22a3fbd4dac container client-container: <nil>
STEP: delete the pod
Aug 15 18:50:28.279: INFO: Waiting for pod downwardapi-volume-2e6bf904-0754-4a21-896d-e22a3fbd4dac to disappear
Aug 15 18:50:28.282: INFO: Pod downwardapi-volume-2e6bf904-0754-4a21-896d-e22a3fbd4dac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:50:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7376" for this suite.
Aug 15 18:50:34.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:50:34.403: INFO: namespace projected-7376 deletion completed in 6.116231287s

• [SLOW TEST:10.211 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:50:34.403: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1894
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 15 18:50:34.462: INFO: Found 0 stateful pods, waiting for 3
Aug 15 18:50:44.467: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:50:44.467: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:50:44.467: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 15 18:50:54.466: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:50:54.466: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:50:54.466: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 15 18:50:54.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-1894 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:50:54.640: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:50:54.640: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:50:54.640: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 15 18:51:04.677: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 15 18:51:14.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-1894 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:51:15.063: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 15 18:51:15.063: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 15 18:51:15.063: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 15 18:51:35.084: INFO: Waiting for StatefulSet statefulset-1894/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 15 18:51:45.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-1894 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 15 18:51:45.272: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 15 18:51:45.272: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 15 18:51:45.272: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 15 18:51:55.313: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 15 18:51:55.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 exec --namespace=statefulset-1894 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 15 18:51:55.507: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 15 18:51:55.507: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 15 18:51:55.507: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 15 18:52:05.529: INFO: Waiting for StatefulSet statefulset-1894/ss2 to complete update
Aug 15 18:52:05.529: INFO: Waiting for Pod statefulset-1894/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 15 18:52:05.529: INFO: Waiting for Pod statefulset-1894/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 15 18:52:05.529: INFO: Waiting for Pod statefulset-1894/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 15 18:52:15.536: INFO: Waiting for StatefulSet statefulset-1894/ss2 to complete update
Aug 15 18:52:15.536: INFO: Waiting for Pod statefulset-1894/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 15 18:52:15.536: INFO: Waiting for Pod statefulset-1894/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 15 18:52:25.539: INFO: Waiting for StatefulSet statefulset-1894/ss2 to complete update
Aug 15 18:52:25.539: INFO: Waiting for Pod statefulset-1894/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 15 18:52:35.537: INFO: Waiting for StatefulSet statefulset-1894/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 15 18:52:45.537: INFO: Deleting all statefulset in ns statefulset-1894
Aug 15 18:52:45.540: INFO: Scaling statefulset ss2 to 0
Aug 15 18:53:05.560: INFO: Waiting for statefulset status.replicas updated to 0
Aug 15 18:53:05.563: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:53:05.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1894" for this suite.
Aug 15 18:53:11.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:53:11.715: INFO: namespace statefulset-1894 deletion completed in 6.128326772s

• [SLOW TEST:157.312 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:53:11.715: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:53:15.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1946" for this suite.
Aug 15 18:53:55.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:53:55.896: INFO: namespace kubelet-test-1946 deletion completed in 40.11030585s

• [SLOW TEST:44.181 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:53:55.896: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-3fa9321f-c7d2-4b43-b986-8dd7ce002fe3
STEP: Creating secret with name secret-projected-all-test-volume-a31c9e87-c178-4dba-a919-3f1609a8fe4f
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 15 18:53:55.960: INFO: Waiting up to 5m0s for pod "projected-volume-7d81dc0b-28e5-4c41-8917-2aad5c294b31" in namespace "projected-5279" to be "success or failure"
Aug 15 18:53:55.967: INFO: Pod "projected-volume-7d81dc0b-28e5-4c41-8917-2aad5c294b31": Phase="Pending", Reason="", readiness=false. Elapsed: 7.053818ms
Aug 15 18:53:57.972: INFO: Pod "projected-volume-7d81dc0b-28e5-4c41-8917-2aad5c294b31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011252283s
Aug 15 18:53:59.977: INFO: Pod "projected-volume-7d81dc0b-28e5-4c41-8917-2aad5c294b31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016154711s
STEP: Saw pod success
Aug 15 18:53:59.977: INFO: Pod "projected-volume-7d81dc0b-28e5-4c41-8917-2aad5c294b31" satisfied condition "success or failure"
Aug 15 18:53:59.980: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod projected-volume-7d81dc0b-28e5-4c41-8917-2aad5c294b31 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 15 18:54:00.022: INFO: Waiting for pod projected-volume-7d81dc0b-28e5-4c41-8917-2aad5c294b31 to disappear
Aug 15 18:54:00.027: INFO: Pod projected-volume-7d81dc0b-28e5-4c41-8917-2aad5c294b31 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:54:00.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5279" for this suite.
Aug 15 18:54:06.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:54:06.139: INFO: namespace projected-5279 deletion completed in 6.104792349s

• [SLOW TEST:10.243 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:54:06.140: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-6c99bd4d-bfd5-4541-858f-e0f1a75f0aeb
STEP: Creating a pod to test consume secrets
Aug 15 18:54:06.189: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c562ce71-0d9e-4ec7-b9d5-4d59ee41e41c" in namespace "projected-700" to be "success or failure"
Aug 15 18:54:06.194: INFO: Pod "pod-projected-secrets-c562ce71-0d9e-4ec7-b9d5-4d59ee41e41c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.487023ms
Aug 15 18:54:08.197: INFO: Pod "pod-projected-secrets-c562ce71-0d9e-4ec7-b9d5-4d59ee41e41c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008141544s
Aug 15 18:54:10.202: INFO: Pod "pod-projected-secrets-c562ce71-0d9e-4ec7-b9d5-4d59ee41e41c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012336086s
STEP: Saw pod success
Aug 15 18:54:10.202: INFO: Pod "pod-projected-secrets-c562ce71-0d9e-4ec7-b9d5-4d59ee41e41c" satisfied condition "success or failure"
Aug 15 18:54:10.205: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-projected-secrets-c562ce71-0d9e-4ec7-b9d5-4d59ee41e41c container secret-volume-test: <nil>
STEP: delete the pod
Aug 15 18:54:10.227: INFO: Waiting for pod pod-projected-secrets-c562ce71-0d9e-4ec7-b9d5-4d59ee41e41c to disappear
Aug 15 18:54:10.230: INFO: Pod pod-projected-secrets-c562ce71-0d9e-4ec7-b9d5-4d59ee41e41c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:54:10.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-700" for this suite.
Aug 15 18:54:16.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:54:16.363: INFO: namespace projected-700 deletion completed in 6.12693901s

• [SLOW TEST:10.224 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:54:16.364: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0815 18:54:22.450004      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 15 18:54:22.450: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:54:22.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5209" for this suite.
Aug 15 18:54:28.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:54:28.570: INFO: namespace gc-5209 deletion completed in 6.116421045s

• [SLOW TEST:12.206 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:54:28.570: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Aug 15 18:54:28.607: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-136527297 proxy --unix-socket=/tmp/kubectl-proxy-unix729375776/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:54:28.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3363" for this suite.
Aug 15 18:54:34.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:54:34.796: INFO: namespace kubectl-3363 deletion completed in 6.111278066s

• [SLOW TEST:6.227 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:54:34.797: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-cca9674a-55cb-4f0a-b4cb-3b599df5a844
STEP: Creating a pod to test consume secrets
Aug 15 18:54:34.852: INFO: Waiting up to 5m0s for pod "pod-secrets-c33946ca-139d-4741-babc-59d2bce75b89" in namespace "secrets-6895" to be "success or failure"
Aug 15 18:54:34.859: INFO: Pod "pod-secrets-c33946ca-139d-4741-babc-59d2bce75b89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.976613ms
Aug 15 18:54:36.864: INFO: Pod "pod-secrets-c33946ca-139d-4741-babc-59d2bce75b89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011642797s
Aug 15 18:54:38.868: INFO: Pod "pod-secrets-c33946ca-139d-4741-babc-59d2bce75b89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01573892s
STEP: Saw pod success
Aug 15 18:54:38.868: INFO: Pod "pod-secrets-c33946ca-139d-4741-babc-59d2bce75b89" satisfied condition "success or failure"
Aug 15 18:54:38.871: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-secrets-c33946ca-139d-4741-babc-59d2bce75b89 container secret-env-test: <nil>
STEP: delete the pod
Aug 15 18:54:38.898: INFO: Waiting for pod pod-secrets-c33946ca-139d-4741-babc-59d2bce75b89 to disappear
Aug 15 18:54:38.902: INFO: Pod pod-secrets-c33946ca-139d-4741-babc-59d2bce75b89 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:54:38.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6895" for this suite.
Aug 15 18:54:44.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:54:45.026: INFO: namespace secrets-6895 deletion completed in 6.118583706s

• [SLOW TEST:10.229 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:54:45.026: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 15 18:54:45.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9596'
Aug 15 18:54:45.150: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 15 18:54:45.150: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug 15 18:54:45.162: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-zbrpn]
Aug 15 18:54:45.162: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-zbrpn" in namespace "kubectl-9596" to be "running and ready"
Aug 15 18:54:45.166: INFO: Pod "e2e-test-nginx-rc-zbrpn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.193658ms
Aug 15 18:54:47.170: INFO: Pod "e2e-test-nginx-rc-zbrpn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007518639s
Aug 15 18:54:49.174: INFO: Pod "e2e-test-nginx-rc-zbrpn": Phase="Running", Reason="", readiness=true. Elapsed: 4.011910648s
Aug 15 18:54:49.174: INFO: Pod "e2e-test-nginx-rc-zbrpn" satisfied condition "running and ready"
Aug 15 18:54:49.174: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-zbrpn]
Aug 15 18:54:49.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 logs rc/e2e-test-nginx-rc --namespace=kubectl-9596'
Aug 15 18:54:49.274: INFO: stderr: ""
Aug 15 18:54:49.274: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Aug 15 18:54:49.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete rc e2e-test-nginx-rc --namespace=kubectl-9596'
Aug 15 18:54:49.363: INFO: stderr: ""
Aug 15 18:54:49.363: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:54:49.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9596" for this suite.
Aug 15 18:55:11.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:55:11.480: INFO: namespace kubectl-9596 deletion completed in 22.113540775s

• [SLOW TEST:26.454 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:55:11.480: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 18:55:11.577: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c463f8c-2d4c-48b9-818e-bb8d86751e06" in namespace "downward-api-6175" to be "success or failure"
Aug 15 18:55:11.584: INFO: Pod "downwardapi-volume-0c463f8c-2d4c-48b9-818e-bb8d86751e06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.567912ms
Aug 15 18:55:13.588: INFO: Pod "downwardapi-volume-0c463f8c-2d4c-48b9-818e-bb8d86751e06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011056322s
Aug 15 18:55:15.593: INFO: Pod "downwardapi-volume-0c463f8c-2d4c-48b9-818e-bb8d86751e06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016032455s
STEP: Saw pod success
Aug 15 18:55:15.593: INFO: Pod "downwardapi-volume-0c463f8c-2d4c-48b9-818e-bb8d86751e06" satisfied condition "success or failure"
Aug 15 18:55:15.596: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-0c463f8c-2d4c-48b9-818e-bb8d86751e06 container client-container: <nil>
STEP: delete the pod
Aug 15 18:55:15.619: INFO: Waiting for pod downwardapi-volume-0c463f8c-2d4c-48b9-818e-bb8d86751e06 to disappear
Aug 15 18:55:15.622: INFO: Pod downwardapi-volume-0c463f8c-2d4c-48b9-818e-bb8d86751e06 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:55:15.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6175" for this suite.
Aug 15 18:55:21.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:55:21.783: INFO: namespace downward-api-6175 deletion completed in 6.156824506s

• [SLOW TEST:10.303 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:55:21.783: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Aug 15 18:55:21.823: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3276" to be "success or failure"
Aug 15 18:55:21.827: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.120208ms
Aug 15 18:55:23.832: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009107994s
Aug 15 18:55:25.837: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013549808s
STEP: Saw pod success
Aug 15 18:55:25.837: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 15 18:55:25.839: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 15 18:55:25.859: INFO: Waiting for pod pod-host-path-test to disappear
Aug 15 18:55:25.862: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:55:25.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3276" for this suite.
Aug 15 18:55:31.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:55:31.980: INFO: namespace hostpath-3276 deletion completed in 6.110724034s

• [SLOW TEST:10.197 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:55:31.981: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Aug 15 18:55:32.036: INFO: Waiting up to 5m0s for pod "var-expansion-bb3e8bd1-4808-4fcb-92f3-893663b5a199" in namespace "var-expansion-5368" to be "success or failure"
Aug 15 18:55:32.046: INFO: Pod "var-expansion-bb3e8bd1-4808-4fcb-92f3-893663b5a199": Phase="Pending", Reason="", readiness=false. Elapsed: 9.751332ms
Aug 15 18:55:34.049: INFO: Pod "var-expansion-bb3e8bd1-4808-4fcb-92f3-893663b5a199": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013364654s
Aug 15 18:55:36.053: INFO: Pod "var-expansion-bb3e8bd1-4808-4fcb-92f3-893663b5a199": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017205113s
STEP: Saw pod success
Aug 15 18:55:36.053: INFO: Pod "var-expansion-bb3e8bd1-4808-4fcb-92f3-893663b5a199" satisfied condition "success or failure"
Aug 15 18:55:36.056: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod var-expansion-bb3e8bd1-4808-4fcb-92f3-893663b5a199 container dapi-container: <nil>
STEP: delete the pod
Aug 15 18:55:36.078: INFO: Waiting for pod var-expansion-bb3e8bd1-4808-4fcb-92f3-893663b5a199 to disappear
Aug 15 18:55:36.081: INFO: Pod var-expansion-bb3e8bd1-4808-4fcb-92f3-893663b5a199 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:55:36.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5368" for this suite.
Aug 15 18:55:42.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:55:42.208: INFO: namespace var-expansion-5368 deletion completed in 6.122126651s

• [SLOW TEST:10.227 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:55:42.208: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 15 18:55:47.273: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:55:48.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3951" for this suite.
Aug 15 18:56:10.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:56:10.411: INFO: namespace replicaset-3951 deletion completed in 22.116819134s

• [SLOW TEST:28.203 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:56:10.412: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 15 18:56:14.992: INFO: Successfully updated pod "labelsupdate1b07a8b8-3c0e-47d5-a873-4437e4258ad3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:56:17.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1586" for this suite.
Aug 15 18:56:39.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:56:39.146: INFO: namespace projected-1586 deletion completed in 22.122176901s

• [SLOW TEST:28.734 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:56:39.147: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:57:04.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7125" for this suite.
Aug 15 18:57:10.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:57:10.548: INFO: namespace container-runtime-7125 deletion completed in 6.104427811s

• [SLOW TEST:31.401 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:57:10.548: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 15 18:57:20.629: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:20.634: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:22.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:22.640: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:24.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:24.638: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:26.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:26.638: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:28.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:28.639: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:30.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:30.639: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:32.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:32.639: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:34.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:34.639: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:36.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:36.638: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:38.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:38.638: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:40.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:40.638: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:42.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:42.640: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 15 18:57:44.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 15 18:57:44.638: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:57:44.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3776" for this suite.
Aug 15 18:58:06.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:58:06.755: INFO: namespace container-lifecycle-hook-3776 deletion completed in 22.104468078s

• [SLOW TEST:56.207 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:58:06.756: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8310
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 15 18:58:06.807: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 15 18:58:32.898: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.3.113:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8310 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:58:32.898: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:58:32.986: INFO: Found all expected endpoints: [netserver-0]
Aug 15 18:58:32.990: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.4.135:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8310 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:58:32.990: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:58:33.075: INFO: Found all expected endpoints: [netserver-1]
Aug 15 18:58:33.078: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.2.123:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8310 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 15 18:58:33.079: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
Aug 15 18:58:33.155: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:58:33.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8310" for this suite.
Aug 15 18:58:55.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:58:55.273: INFO: namespace pod-network-test-8310 deletion completed in 22.111498247s

• [SLOW TEST:48.517 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:58:55.273: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-642/configmap-test-18e32774-8d9f-4c68-bb74-ef13a5509a56
STEP: Creating a pod to test consume configMaps
Aug 15 18:58:55.320: INFO: Waiting up to 5m0s for pod "pod-configmaps-1f82f0b3-9b60-42ca-a6e0-64ee96062405" in namespace "configmap-642" to be "success or failure"
Aug 15 18:58:55.323: INFO: Pod "pod-configmaps-1f82f0b3-9b60-42ca-a6e0-64ee96062405": Phase="Pending", Reason="", readiness=false. Elapsed: 3.590109ms
Aug 15 18:58:57.328: INFO: Pod "pod-configmaps-1f82f0b3-9b60-42ca-a6e0-64ee96062405": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008095949s
Aug 15 18:58:59.332: INFO: Pod "pod-configmaps-1f82f0b3-9b60-42ca-a6e0-64ee96062405": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01255325s
STEP: Saw pod success
Aug 15 18:58:59.332: INFO: Pod "pod-configmaps-1f82f0b3-9b60-42ca-a6e0-64ee96062405" satisfied condition "success or failure"
Aug 15 18:58:59.335: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod pod-configmaps-1f82f0b3-9b60-42ca-a6e0-64ee96062405 container env-test: <nil>
STEP: delete the pod
Aug 15 18:58:59.358: INFO: Waiting for pod pod-configmaps-1f82f0b3-9b60-42ca-a6e0-64ee96062405 to disappear
Aug 15 18:58:59.362: INFO: Pod pod-configmaps-1f82f0b3-9b60-42ca-a6e0-64ee96062405 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:58:59.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-642" for this suite.
Aug 15 18:59:05.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:59:05.475: INFO: namespace configmap-642 deletion completed in 6.108661209s

• [SLOW TEST:10.202 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:59:05.475: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:59:05.508: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:59:06.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7416" for this suite.
Aug 15 18:59:12.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:59:12.681: INFO: namespace custom-resource-definition-7416 deletion completed in 6.107923348s

• [SLOW TEST:7.206 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:59:12.681: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:59:18.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9646" for this suite.
Aug 15 18:59:24.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:59:24.393: INFO: namespace watch-9646 deletion completed in 6.203990178s

• [SLOW TEST:11.712 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:59:24.394: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 18:59:24.438: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c22e3861-0f95-483b-845f-5d7c686e0ecf" in namespace "downward-api-2425" to be "success or failure"
Aug 15 18:59:24.450: INFO: Pod "downwardapi-volume-c22e3861-0f95-483b-845f-5d7c686e0ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.974038ms
Aug 15 18:59:26.454: INFO: Pod "downwardapi-volume-c22e3861-0f95-483b-845f-5d7c686e0ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015993569s
Aug 15 18:59:28.458: INFO: Pod "downwardapi-volume-c22e3861-0f95-483b-845f-5d7c686e0ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020073368s
STEP: Saw pod success
Aug 15 18:59:28.458: INFO: Pod "downwardapi-volume-c22e3861-0f95-483b-845f-5d7c686e0ecf" satisfied condition "success or failure"
Aug 15 18:59:28.461: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod downwardapi-volume-c22e3861-0f95-483b-845f-5d7c686e0ecf container client-container: <nil>
STEP: delete the pod
Aug 15 18:59:28.483: INFO: Waiting for pod downwardapi-volume-c22e3861-0f95-483b-845f-5d7c686e0ecf to disappear
Aug 15 18:59:28.486: INFO: Pod downwardapi-volume-c22e3861-0f95-483b-845f-5d7c686e0ecf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:59:28.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2425" for this suite.
Aug 15 18:59:34.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:59:34.621: INFO: namespace downward-api-2425 deletion completed in 6.130878084s

• [SLOW TEST:10.227 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:59:34.622: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-c039f318-1c76-4fad-aa68-b6436ccbe51b
STEP: Creating a pod to test consume configMaps
Aug 15 18:59:34.667: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9177951-9941-4605-bad5-b9dc6a4ebed3" in namespace "configmap-5425" to be "success or failure"
Aug 15 18:59:34.672: INFO: Pod "pod-configmaps-a9177951-9941-4605-bad5-b9dc6a4ebed3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.199166ms
Aug 15 18:59:36.677: INFO: Pod "pod-configmaps-a9177951-9941-4605-bad5-b9dc6a4ebed3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010358294s
Aug 15 18:59:38.682: INFO: Pod "pod-configmaps-a9177951-9941-4605-bad5-b9dc6a4ebed3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015182026s
STEP: Saw pod success
Aug 15 18:59:38.682: INFO: Pod "pod-configmaps-a9177951-9941-4605-bad5-b9dc6a4ebed3" satisfied condition "success or failure"
Aug 15 18:59:38.685: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-configmaps-a9177951-9941-4605-bad5-b9dc6a4ebed3 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 15 18:59:38.713: INFO: Waiting for pod pod-configmaps-a9177951-9941-4605-bad5-b9dc6a4ebed3 to disappear
Aug 15 18:59:38.717: INFO: Pod pod-configmaps-a9177951-9941-4605-bad5-b9dc6a4ebed3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:59:38.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5425" for this suite.
Aug 15 18:59:44.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:59:44.836: INFO: namespace configmap-5425 deletion completed in 6.11410634s

• [SLOW TEST:10.215 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:59:44.837: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 15 18:59:44.867: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 15 18:59:44.875: INFO: Waiting for terminating namespaces to be deleted...
Aug 15 18:59:44.878: INFO: 
Logging pods the kubelet thinks is on node karbon-fifteen-3efeed-k8s-worker-0 before test
Aug 15 18:59:44.885: INFO: sonobuoy-e2e-job-ab1ba0141d2d464c from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container e2e ready: true, restart count 0
Aug 15 18:59:44.885: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 15 18:59:44.885: INFO: kube-proxy-ds-75v2p from kube-system started at 2019-08-15 16:32:55 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 15 18:59:44.885: INFO: csi-node-ntnx-plugin-k97vf from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 15 18:59:44.885: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 15 18:59:44.885: INFO: kube-flannel-ds-wb8nz from kube-system started at 2019-08-15 16:33:19 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 15 18:59:44.885: INFO: node-exporter-m5hlh from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 15 18:59:44.885: INFO: 	Container node-exporter ready: true, restart count 0
Aug 15 18:59:44.885: INFO: prometheus-k8s-0 from ntnx-system started at 2019-08-15 16:37:12 +0000 UTC (3 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container prometheus ready: true, restart count 1
Aug 15 18:59:44.885: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 15 18:59:44.885: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 15 18:59:44.885: INFO: sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-hjpr2 from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 15 18:59:44.885: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 15 18:59:44.885: INFO: kibana-logging-8657c47867-cd27n from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container kibana-logging ready: true, restart count 0
Aug 15 18:59:44.885: INFO: 	Container nginxhttp ready: true, restart count 0
Aug 15 18:59:44.885: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 15 18:59:44.885: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 15 18:59:44.885: INFO: fluent-bit-f6cdg from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.885: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 15 18:59:44.885: INFO: 
Logging pods the kubelet thinks is on node karbon-fifteen-3efeed-k8s-worker-1 before test
Aug 15 18:59:44.893: INFO: kubernetes-events-printer-769d866479-jj7bk from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 15 18:59:44.893: INFO: node-exporter-7zm7k from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 15 18:59:44.893: INFO: 	Container node-exporter ready: true, restart count 0
Aug 15 18:59:44.893: INFO: prometheus-operator-999c9d4cf-kgcp8 from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 15 18:59:44.893: INFO: sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-7dxs8 from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 15 18:59:44.893: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 15 18:59:44.893: INFO: kube-proxy-ds-bh4g7 from kube-system started at 2019-08-15 16:32:55 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 15 18:59:44.893: INFO: kube-flannel-ds-d5nsh from kube-system started at 2019-08-15 16:33:19 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 15 18:59:44.893: INFO: fluent-bit-glnl9 from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 15 18:59:44.893: INFO: alertmanager-main-1 from ntnx-system started at 2019-08-15 16:37:14 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container alertmanager ready: true, restart count 0
Aug 15 18:59:44.893: INFO: 	Container config-reloader ready: true, restart count 0
Aug 15 18:59:44.893: INFO: prometheus-k8s-1 from ntnx-system started at 2019-08-15 16:37:52 +0000 UTC (3 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container prometheus ready: true, restart count 1
Aug 15 18:59:44.893: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 15 18:59:44.893: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 15 18:59:44.893: INFO: csi-attacher-ntnx-plugin-0 from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container csi-attacher ready: true, restart count 0
Aug 15 18:59:44.893: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 15 18:59:44.893: INFO: csi-node-ntnx-plugin-vfwvr from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.893: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 15 18:59:44.893: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 15 18:59:44.893: INFO: 
Logging pods the kubelet thinks is on node karbon-fifteen-3efeed-k8s-worker-2 before test
Aug 15 18:59:44.901: INFO: fluent-bit-jnjtc from ntnx-system started at 2019-08-15 16:34:04 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 15 18:59:44.901: INFO: kube-flannel-ds-gwx8x from kube-system started at 2019-08-15 16:33:19 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 15 18:59:44.901: INFO: csi-node-ntnx-plugin-ljkj9 from ntnx-system started at 2019-08-15 16:33:43 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 15 18:59:44.901: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 15 18:59:44.901: INFO: alertmanager-main-0 from ntnx-system started at 2019-08-15 16:37:01 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container alertmanager ready: true, restart count 0
Aug 15 18:59:44.901: INFO: 	Container config-reloader ready: true, restart count 0
Aug 15 18:59:44.901: INFO: kube-proxy-ds-w9d6p from kube-system started at 2019-08-15 16:32:55 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 15 18:59:44.901: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-15 17:34:42 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 15 18:59:44.901: INFO: elasticsearch-logging-0 from ntnx-system started at 2019-08-15 16:34:12 +0000 UTC (1 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Aug 15 18:59:44.901: INFO: node-exporter-lj4nf from ntnx-system started at 2019-08-15 16:36:51 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 15 18:59:44.901: INFO: 	Container node-exporter ready: true, restart count 0
Aug 15 18:59:44.901: INFO: kube-state-metrics-7678d97797-ncjkj from ntnx-system started at 2019-08-15 16:37:04 +0000 UTC (4 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container addon-resizer ready: true, restart count 0
Aug 15 18:59:44.901: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 15 18:59:44.901: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 15 18:59:44.901: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 15 18:59:44.901: INFO: sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-hdxn6 from heptio-sonobuoy started at 2019-08-15 17:34:49 +0000 UTC (2 container statuses recorded)
Aug 15 18:59:44.901: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 15 18:59:44.901: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node karbon-fifteen-3efeed-k8s-worker-0
STEP: verifying the node has the label node karbon-fifteen-3efeed-k8s-worker-1
STEP: verifying the node has the label node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod sonobuoy requesting resource cpu=0m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod sonobuoy-e2e-job-ab1ba0141d2d464c requesting resource cpu=0m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-7dxs8 requesting resource cpu=0m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-hdxn6 requesting resource cpu=0m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod sonobuoy-systemd-logs-daemon-set-53a641d8907e40a2-hjpr2 requesting resource cpu=0m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod kube-flannel-ds-d5nsh requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod kube-flannel-ds-gwx8x requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod kube-flannel-ds-wb8nz requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod kube-proxy-ds-75v2p requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod kube-proxy-ds-bh4g7 requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod kube-proxy-ds-w9d6p requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod alertmanager-main-0 requesting resource cpu=105m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod alertmanager-main-1 requesting resource cpu=105m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod csi-attacher-ntnx-plugin-0 requesting resource cpu=200m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod csi-node-ntnx-plugin-k97vf requesting resource cpu=200m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod csi-node-ntnx-plugin-ljkj9 requesting resource cpu=200m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod csi-node-ntnx-plugin-vfwvr requesting resource cpu=200m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod csi-provisioner-ntnx-plugin-0 requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod elasticsearch-logging-0 requesting resource cpu=500m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod fluent-bit-f6cdg requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod fluent-bit-glnl9 requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod fluent-bit-jnjtc requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod kibana-logging-8657c47867-cd27n requesting resource cpu=200m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod kube-state-metrics-7678d97797-ncjkj requesting resource cpu=160m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod kubernetes-events-printer-769d866479-jj7bk requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod node-exporter-7zm7k requesting resource cpu=112m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod node-exporter-lj4nf requesting resource cpu=112m on Node karbon-fifteen-3efeed-k8s-worker-2
Aug 15 18:59:44.955: INFO: Pod node-exporter-m5hlh requesting resource cpu=112m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod prometheus-k8s-0 requesting resource cpu=215m on Node karbon-fifteen-3efeed-k8s-worker-0
Aug 15 18:59:44.955: INFO: Pod prometheus-k8s-1 requesting resource cpu=215m on Node karbon-fifteen-3efeed-k8s-worker-1
Aug 15 18:59:44.955: INFO: Pod prometheus-operator-999c9d4cf-kgcp8 requesting resource cpu=100m on Node karbon-fifteen-3efeed-k8s-worker-1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d89d0621-7e44-4bae-afc3-af37135d53d7.15bb2dbcf22b61d9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9984/filler-pod-d89d0621-7e44-4bae-afc3-af37135d53d7 to karbon-fifteen-3efeed-k8s-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d89d0621-7e44-4bae-afc3-af37135d53d7.15bb2dbd7b8329c6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d89d0621-7e44-4bae-afc3-af37135d53d7.15bb2dbd7e6652a3], Reason = [Created], Message = [Created container filler-pod-d89d0621-7e44-4bae-afc3-af37135d53d7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d89d0621-7e44-4bae-afc3-af37135d53d7.15bb2dbd864f8dbc], Reason = [Started], Message = [Started container filler-pod-d89d0621-7e44-4bae-afc3-af37135d53d7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e7bc9206-b721-4e8f-926d-cfacd9ab1e50.15bb2dbcf19e2fac], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9984/filler-pod-e7bc9206-b721-4e8f-926d-cfacd9ab1e50 to karbon-fifteen-3efeed-k8s-worker-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e7bc9206-b721-4e8f-926d-cfacd9ab1e50.15bb2dbd7bb850e5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e7bc9206-b721-4e8f-926d-cfacd9ab1e50.15bb2dbd7deb8233], Reason = [Created], Message = [Created container filler-pod-e7bc9206-b721-4e8f-926d-cfacd9ab1e50]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e7bc9206-b721-4e8f-926d-cfacd9ab1e50.15bb2dbd85bcf461], Reason = [Started], Message = [Started container filler-pod-e7bc9206-b721-4e8f-926d-cfacd9ab1e50]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-efec31b6-9f40-4a4a-b0b3-315d338e88de.15bb2dbcf2e11f64], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9984/filler-pod-efec31b6-9f40-4a4a-b0b3-315d338e88de to karbon-fifteen-3efeed-k8s-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-efec31b6-9f40-4a4a-b0b3-315d338e88de.15bb2dbd7462a81b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-efec31b6-9f40-4a4a-b0b3-315d338e88de.15bb2dbd76d5074e], Reason = [Created], Message = [Created container filler-pod-efec31b6-9f40-4a4a-b0b3-315d338e88de]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-efec31b6-9f40-4a4a-b0b3-315d338e88de.15bb2dbd7ee4102c], Reason = [Started], Message = [Started container filler-pod-efec31b6-9f40-4a4a-b0b3-315d338e88de]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15bb2dbde2e65158], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node karbon-fifteen-3efeed-k8s-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-fifteen-3efeed-k8s-worker-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-fifteen-3efeed-k8s-worker-0
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 18:59:50.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9984" for this suite.
Aug 15 18:59:56.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 18:59:56.207: INFO: namespace sched-pred-9984 deletion completed in 6.12160162s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.371 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 18:59:56.207: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 18:59:56.247: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:00:00.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6894" for this suite.
Aug 15 19:00:38.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:00:38.560: INFO: namespace pods-6894 deletion completed in 38.112629188s

• [SLOW TEST:42.353 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:00:38.561: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 15 19:00:38.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-8154'
Aug 15 19:00:38.869: INFO: stderr: ""
Aug 15 19:00:38.869: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 15 19:00:38.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8154'
Aug 15 19:00:38.972: INFO: stderr: ""
Aug 15 19:00:38.972: INFO: stdout: "update-demo-nautilus-hk8fs update-demo-nautilus-ld8jn "
Aug 15 19:00:38.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-hk8fs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8154'
Aug 15 19:00:39.060: INFO: stderr: ""
Aug 15 19:00:39.060: INFO: stdout: ""
Aug 15 19:00:39.060: INFO: update-demo-nautilus-hk8fs is created but not running
Aug 15 19:00:44.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8154'
Aug 15 19:00:44.155: INFO: stderr: ""
Aug 15 19:00:44.155: INFO: stdout: "update-demo-nautilus-hk8fs update-demo-nautilus-ld8jn "
Aug 15 19:00:44.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-hk8fs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8154'
Aug 15 19:00:44.243: INFO: stderr: ""
Aug 15 19:00:44.243: INFO: stdout: "true"
Aug 15 19:00:44.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-hk8fs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8154'
Aug 15 19:00:44.328: INFO: stderr: ""
Aug 15 19:00:44.328: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:00:44.328: INFO: validating pod update-demo-nautilus-hk8fs
Aug 15 19:00:44.333: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:00:44.333: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:00:44.333: INFO: update-demo-nautilus-hk8fs is verified up and running
Aug 15 19:00:44.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-ld8jn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8154'
Aug 15 19:00:44.412: INFO: stderr: ""
Aug 15 19:00:44.412: INFO: stdout: "true"
Aug 15 19:00:44.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-ld8jn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8154'
Aug 15 19:00:44.486: INFO: stderr: ""
Aug 15 19:00:44.486: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:00:44.486: INFO: validating pod update-demo-nautilus-ld8jn
Aug 15 19:00:44.491: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:00:44.491: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:00:44.491: INFO: update-demo-nautilus-ld8jn is verified up and running
STEP: using delete to clean up resources
Aug 15 19:00:44.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-8154'
Aug 15 19:00:44.579: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 19:00:44.579: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 15 19:00:44.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8154'
Aug 15 19:00:44.677: INFO: stderr: "No resources found.\n"
Aug 15 19:00:44.677: INFO: stdout: ""
Aug 15 19:00:44.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -l name=update-demo --namespace=kubectl-8154 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 15 19:00:44.771: INFO: stderr: ""
Aug 15 19:00:44.771: INFO: stdout: "update-demo-nautilus-hk8fs\nupdate-demo-nautilus-ld8jn\n"
Aug 15 19:00:45.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8154'
Aug 15 19:00:45.366: INFO: stderr: "No resources found.\n"
Aug 15 19:00:45.366: INFO: stdout: ""
Aug 15 19:00:45.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -l name=update-demo --namespace=kubectl-8154 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 15 19:00:45.447: INFO: stderr: ""
Aug 15 19:00:45.447: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:00:45.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8154" for this suite.
Aug 15 19:01:07.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:01:07.565: INFO: namespace kubectl-8154 deletion completed in 22.113095938s

• [SLOW TEST:29.004 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:01:07.565: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-feb091ce-d03c-49a0-bfd3-252756b169ec
STEP: Creating a pod to test consume secrets
Aug 15 19:01:07.662: INFO: Waiting up to 5m0s for pod "pod-secrets-ad0363dc-dc51-473a-88bf-547057043ea3" in namespace "secrets-8165" to be "success or failure"
Aug 15 19:01:07.672: INFO: Pod "pod-secrets-ad0363dc-dc51-473a-88bf-547057043ea3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.436452ms
Aug 15 19:01:09.678: INFO: Pod "pod-secrets-ad0363dc-dc51-473a-88bf-547057043ea3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015711411s
Aug 15 19:01:11.682: INFO: Pod "pod-secrets-ad0363dc-dc51-473a-88bf-547057043ea3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020040016s
STEP: Saw pod success
Aug 15 19:01:11.682: INFO: Pod "pod-secrets-ad0363dc-dc51-473a-88bf-547057043ea3" satisfied condition "success or failure"
Aug 15 19:01:11.686: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-secrets-ad0363dc-dc51-473a-88bf-547057043ea3 container secret-volume-test: <nil>
STEP: delete the pod
Aug 15 19:01:11.708: INFO: Waiting for pod pod-secrets-ad0363dc-dc51-473a-88bf-547057043ea3 to disappear
Aug 15 19:01:11.711: INFO: Pod pod-secrets-ad0363dc-dc51-473a-88bf-547057043ea3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:01:11.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8165" for this suite.
Aug 15 19:01:17.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:01:17.837: INFO: namespace secrets-8165 deletion completed in 6.122020919s
STEP: Destroying namespace "secret-namespace-9813" for this suite.
Aug 15 19:01:23.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:01:23.949: INFO: namespace secret-namespace-9813 deletion completed in 6.11232595s

• [SLOW TEST:16.384 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:01:23.950: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 15 19:01:23.990: INFO: Waiting up to 5m0s for pod "pod-02ebe5ed-54f8-4c92-bf82-13ffe3043339" in namespace "emptydir-1490" to be "success or failure"
Aug 15 19:01:24.003: INFO: Pod "pod-02ebe5ed-54f8-4c92-bf82-13ffe3043339": Phase="Pending", Reason="", readiness=false. Elapsed: 13.336091ms
Aug 15 19:01:26.008: INFO: Pod "pod-02ebe5ed-54f8-4c92-bf82-13ffe3043339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01781563s
Aug 15 19:01:28.012: INFO: Pod "pod-02ebe5ed-54f8-4c92-bf82-13ffe3043339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022018136s
STEP: Saw pod success
Aug 15 19:01:28.012: INFO: Pod "pod-02ebe5ed-54f8-4c92-bf82-13ffe3043339" satisfied condition "success or failure"
Aug 15 19:01:28.018: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-02ebe5ed-54f8-4c92-bf82-13ffe3043339 container test-container: <nil>
STEP: delete the pod
Aug 15 19:01:28.041: INFO: Waiting for pod pod-02ebe5ed-54f8-4c92-bf82-13ffe3043339 to disappear
Aug 15 19:01:28.044: INFO: Pod pod-02ebe5ed-54f8-4c92-bf82-13ffe3043339 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:01:28.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1490" for this suite.
Aug 15 19:01:34.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:01:34.173: INFO: namespace emptydir-1490 deletion completed in 6.124983737s

• [SLOW TEST:10.224 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:01:34.173: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 15 19:01:34.225: INFO: Waiting up to 5m0s for pod "pod-0a840f7e-b3ee-416e-a26a-d27f45e17fd9" in namespace "emptydir-4940" to be "success or failure"
Aug 15 19:01:34.233: INFO: Pod "pod-0a840f7e-b3ee-416e-a26a-d27f45e17fd9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.308896ms
Aug 15 19:01:36.237: INFO: Pod "pod-0a840f7e-b3ee-416e-a26a-d27f45e17fd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012248163s
Aug 15 19:01:38.241: INFO: Pod "pod-0a840f7e-b3ee-416e-a26a-d27f45e17fd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016223463s
STEP: Saw pod success
Aug 15 19:01:38.241: INFO: Pod "pod-0a840f7e-b3ee-416e-a26a-d27f45e17fd9" satisfied condition "success or failure"
Aug 15 19:01:38.244: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-0a840f7e-b3ee-416e-a26a-d27f45e17fd9 container test-container: <nil>
STEP: delete the pod
Aug 15 19:01:38.267: INFO: Waiting for pod pod-0a840f7e-b3ee-416e-a26a-d27f45e17fd9 to disappear
Aug 15 19:01:38.271: INFO: Pod pod-0a840f7e-b3ee-416e-a26a-d27f45e17fd9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:01:38.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4940" for this suite.
Aug 15 19:01:44.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:01:44.384: INFO: namespace emptydir-4940 deletion completed in 6.108306872s

• [SLOW TEST:10.211 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:01:44.385: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 19:01:44.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34e7f47f-5cf5-47c0-a099-f1759e76608f" in namespace "projected-6894" to be "success or failure"
Aug 15 19:01:44.425: INFO: Pod "downwardapi-volume-34e7f47f-5cf5-47c0-a099-f1759e76608f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.79609ms
Aug 15 19:01:46.429: INFO: Pod "downwardapi-volume-34e7f47f-5cf5-47c0-a099-f1759e76608f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006851043s
Aug 15 19:01:48.434: INFO: Pod "downwardapi-volume-34e7f47f-5cf5-47c0-a099-f1759e76608f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011103992s
STEP: Saw pod success
Aug 15 19:01:48.434: INFO: Pod "downwardapi-volume-34e7f47f-5cf5-47c0-a099-f1759e76608f" satisfied condition "success or failure"
Aug 15 19:01:48.437: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-34e7f47f-5cf5-47c0-a099-f1759e76608f container client-container: <nil>
STEP: delete the pod
Aug 15 19:01:48.459: INFO: Waiting for pod downwardapi-volume-34e7f47f-5cf5-47c0-a099-f1759e76608f to disappear
Aug 15 19:01:48.463: INFO: Pod downwardapi-volume-34e7f47f-5cf5-47c0-a099-f1759e76608f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:01:48.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6894" for this suite.
Aug 15 19:01:54.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:01:54.595: INFO: namespace projected-6894 deletion completed in 6.126951383s

• [SLOW TEST:10.210 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:01:54.595: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 15 19:01:54.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5661'
Aug 15 19:01:54.941: INFO: stderr: ""
Aug 15 19:01:54.941: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug 15 19:01:59.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pod e2e-test-nginx-pod --namespace=kubectl-5661 -o json'
Aug 15 19:02:00.086: INFO: stderr: ""
Aug 15 19:02:00.086: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-08-15T19:01:54Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5661\",\n        \"resourceVersion\": \"27197\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5661/pods/e2e-test-nginx-pod\",\n        \"uid\": \"3e42250e-118f-41b2-88d8-24992cd004f8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-hkmx8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"karbon-fifteen-3efeed-k8s-worker-0\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-hkmx8\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-hkmx8\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-15T19:01:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-15T19:01:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-15T19:01:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-15T19:01:54Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://bb6dde57f6b51f4f301aef4ecf4bfb7cfd3e2118d343614322e88da30ebb0d2e\",\n                \"image\": \"docker.io/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-15T19:01:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.45.40.137\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.20.2.129\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-15T19:01:54Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 15 19:02:00.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 replace -f - --namespace=kubectl-5661'
Aug 15 19:02:00.315: INFO: stderr: ""
Aug 15 19:02:00.315: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Aug 15 19:02:00.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete pods e2e-test-nginx-pod --namespace=kubectl-5661'
Aug 15 19:02:01.752: INFO: stderr: ""
Aug 15 19:02:01.752: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:02:01.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5661" for this suite.
Aug 15 19:02:07.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:02:07.876: INFO: namespace kubectl-5661 deletion completed in 6.117359032s

• [SLOW TEST:13.281 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:02:07.877: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7488
I0815 19:02:07.916997      19 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7488, replica count: 1
I0815 19:02:08.967636      19 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0815 19:02:09.967842      19 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0815 19:02:10.968174      19 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 15 19:02:11.081: INFO: Created: latency-svc-5kvmm
Aug 15 19:02:11.087: INFO: Got endpoints: latency-svc-5kvmm [18.919858ms]
Aug 15 19:02:11.106: INFO: Created: latency-svc-qbtrz
Aug 15 19:02:11.112: INFO: Got endpoints: latency-svc-qbtrz [25.085431ms]
Aug 15 19:02:11.119: INFO: Created: latency-svc-vggn8
Aug 15 19:02:11.127: INFO: Got endpoints: latency-svc-vggn8 [40.154394ms]
Aug 15 19:02:11.131: INFO: Created: latency-svc-d8gtk
Aug 15 19:02:11.137: INFO: Got endpoints: latency-svc-d8gtk [50.031851ms]
Aug 15 19:02:11.141: INFO: Created: latency-svc-s9fxk
Aug 15 19:02:11.149: INFO: Got endpoints: latency-svc-s9fxk [61.22341ms]
Aug 15 19:02:11.153: INFO: Created: latency-svc-gtlqw
Aug 15 19:02:11.158: INFO: Got endpoints: latency-svc-gtlqw [70.482964ms]
Aug 15 19:02:11.162: INFO: Created: latency-svc-964sd
Aug 15 19:02:11.168: INFO: Got endpoints: latency-svc-964sd [80.474281ms]
Aug 15 19:02:11.174: INFO: Created: latency-svc-gsh62
Aug 15 19:02:11.191: INFO: Got endpoints: latency-svc-gsh62 [103.875656ms]
Aug 15 19:02:11.209: INFO: Created: latency-svc-sldqz
Aug 15 19:02:11.218: INFO: Got endpoints: latency-svc-sldqz [129.92013ms]
Aug 15 19:02:11.223: INFO: Created: latency-svc-4gcqc
Aug 15 19:02:11.230: INFO: Got endpoints: latency-svc-4gcqc [142.316027ms]
Aug 15 19:02:11.235: INFO: Created: latency-svc-v8p54
Aug 15 19:02:11.241: INFO: Got endpoints: latency-svc-v8p54 [153.632767ms]
Aug 15 19:02:11.245: INFO: Created: latency-svc-4tmx5
Aug 15 19:02:11.251: INFO: Got endpoints: latency-svc-4tmx5 [163.355678ms]
Aug 15 19:02:11.255: INFO: Created: latency-svc-tsccw
Aug 15 19:02:11.262: INFO: Got endpoints: latency-svc-tsccw [174.877065ms]
Aug 15 19:02:11.268: INFO: Created: latency-svc-cksft
Aug 15 19:02:11.274: INFO: Got endpoints: latency-svc-cksft [186.021921ms]
Aug 15 19:02:11.278: INFO: Created: latency-svc-w8jml
Aug 15 19:02:11.284: INFO: Got endpoints: latency-svc-w8jml [196.61597ms]
Aug 15 19:02:11.288: INFO: Created: latency-svc-kgljm
Aug 15 19:02:11.292: INFO: Got endpoints: latency-svc-kgljm [204.31978ms]
Aug 15 19:02:11.300: INFO: Created: latency-svc-cxtxp
Aug 15 19:02:11.307: INFO: Got endpoints: latency-svc-cxtxp [194.435437ms]
Aug 15 19:02:11.310: INFO: Created: latency-svc-f89hw
Aug 15 19:02:11.316: INFO: Got endpoints: latency-svc-f89hw [188.794167ms]
Aug 15 19:02:11.321: INFO: Created: latency-svc-76rm4
Aug 15 19:02:11.328: INFO: Got endpoints: latency-svc-76rm4 [190.75336ms]
Aug 15 19:02:11.329: INFO: Created: latency-svc-lr4mx
Aug 15 19:02:11.336: INFO: Got endpoints: latency-svc-lr4mx [187.564763ms]
Aug 15 19:02:11.340: INFO: Created: latency-svc-pvptl
Aug 15 19:02:11.347: INFO: Got endpoints: latency-svc-pvptl [188.802892ms]
Aug 15 19:02:11.350: INFO: Created: latency-svc-dwbqv
Aug 15 19:02:11.355: INFO: Got endpoints: latency-svc-dwbqv [187.356595ms]
Aug 15 19:02:11.358: INFO: Created: latency-svc-v8b96
Aug 15 19:02:11.364: INFO: Got endpoints: latency-svc-v8b96 [173.082909ms]
Aug 15 19:02:11.370: INFO: Created: latency-svc-tjgqk
Aug 15 19:02:11.376: INFO: Got endpoints: latency-svc-tjgqk [158.45512ms]
Aug 15 19:02:11.380: INFO: Created: latency-svc-lsvff
Aug 15 19:02:11.387: INFO: Got endpoints: latency-svc-lsvff [156.748342ms]
Aug 15 19:02:11.391: INFO: Created: latency-svc-jdk2n
Aug 15 19:02:11.399: INFO: Got endpoints: latency-svc-jdk2n [158.011237ms]
Aug 15 19:02:11.401: INFO: Created: latency-svc-ts52n
Aug 15 19:02:11.408: INFO: Got endpoints: latency-svc-ts52n [157.184116ms]
Aug 15 19:02:11.413: INFO: Created: latency-svc-d2dn5
Aug 15 19:02:11.418: INFO: Got endpoints: latency-svc-d2dn5 [155.317412ms]
Aug 15 19:02:11.422: INFO: Created: latency-svc-khj57
Aug 15 19:02:11.427: INFO: Got endpoints: latency-svc-khj57 [153.024831ms]
Aug 15 19:02:11.431: INFO: Created: latency-svc-jrtrh
Aug 15 19:02:11.436: INFO: Got endpoints: latency-svc-jrtrh [151.928891ms]
Aug 15 19:02:11.441: INFO: Created: latency-svc-8xr9v
Aug 15 19:02:11.445: INFO: Got endpoints: latency-svc-8xr9v [153.485737ms]
Aug 15 19:02:11.449: INFO: Created: latency-svc-2fvpz
Aug 15 19:02:11.455: INFO: Got endpoints: latency-svc-2fvpz [148.002426ms]
Aug 15 19:02:11.458: INFO: Created: latency-svc-727h7
Aug 15 19:02:11.463: INFO: Got endpoints: latency-svc-727h7 [146.517176ms]
Aug 15 19:02:11.470: INFO: Created: latency-svc-v87sd
Aug 15 19:02:11.477: INFO: Got endpoints: latency-svc-v87sd [148.806543ms]
Aug 15 19:02:11.481: INFO: Created: latency-svc-p5v52
Aug 15 19:02:11.486: INFO: Got endpoints: latency-svc-p5v52 [149.69261ms]
Aug 15 19:02:11.489: INFO: Created: latency-svc-ldckp
Aug 15 19:02:11.500: INFO: Got endpoints: latency-svc-ldckp [153.260778ms]
Aug 15 19:02:11.509: INFO: Created: latency-svc-5jjnl
Aug 15 19:02:11.510: INFO: Got endpoints: latency-svc-5jjnl [154.628629ms]
Aug 15 19:02:11.517: INFO: Created: latency-svc-78l2d
Aug 15 19:02:11.523: INFO: Got endpoints: latency-svc-78l2d [158.924125ms]
Aug 15 19:02:11.527: INFO: Created: latency-svc-rlncv
Aug 15 19:02:11.535: INFO: Created: latency-svc-w8vg9
Aug 15 19:02:11.538: INFO: Got endpoints: latency-svc-rlncv [162.215113ms]
Aug 15 19:02:11.546: INFO: Created: latency-svc-fspg2
Aug 15 19:02:11.551: INFO: Created: latency-svc-j47rm
Aug 15 19:02:11.558: INFO: Created: latency-svc-kjpmh
Aug 15 19:02:11.567: INFO: Created: latency-svc-6k28g
Aug 15 19:02:11.579: INFO: Created: latency-svc-9h2bb
Aug 15 19:02:11.587: INFO: Got endpoints: latency-svc-w8vg9 [200.637598ms]
Aug 15 19:02:11.587: INFO: Created: latency-svc-4wxsg
Aug 15 19:02:11.599: INFO: Created: latency-svc-j7mf2
Aug 15 19:02:11.606: INFO: Created: latency-svc-nplrg
Aug 15 19:02:11.614: INFO: Created: latency-svc-vdw9c
Aug 15 19:02:11.621: INFO: Created: latency-svc-rpk5d
Aug 15 19:02:11.630: INFO: Created: latency-svc-nj8q4
Aug 15 19:02:11.636: INFO: Got endpoints: latency-svc-fspg2 [236.862463ms]
Aug 15 19:02:11.639: INFO: Created: latency-svc-85wk5
Aug 15 19:02:11.646: INFO: Created: latency-svc-zhkv7
Aug 15 19:02:11.655: INFO: Created: latency-svc-xhd6k
Aug 15 19:02:11.666: INFO: Created: latency-svc-m9kqs
Aug 15 19:02:11.673: INFO: Created: latency-svc-6ll2g
Aug 15 19:02:11.688: INFO: Got endpoints: latency-svc-j47rm [280.256784ms]
Aug 15 19:02:11.704: INFO: Created: latency-svc-lj6q9
Aug 15 19:02:11.738: INFO: Got endpoints: latency-svc-kjpmh [320.493323ms]
Aug 15 19:02:11.751: INFO: Created: latency-svc-6ht85
Aug 15 19:02:11.787: INFO: Got endpoints: latency-svc-6k28g [360.226744ms]
Aug 15 19:02:11.801: INFO: Created: latency-svc-4f4mc
Aug 15 19:02:11.838: INFO: Got endpoints: latency-svc-9h2bb [401.752499ms]
Aug 15 19:02:11.853: INFO: Created: latency-svc-njhqh
Aug 15 19:02:11.887: INFO: Got endpoints: latency-svc-4wxsg [441.652336ms]
Aug 15 19:02:11.901: INFO: Created: latency-svc-7zjsb
Aug 15 19:02:11.940: INFO: Got endpoints: latency-svc-j7mf2 [485.296667ms]
Aug 15 19:02:11.954: INFO: Created: latency-svc-tj88z
Aug 15 19:02:11.989: INFO: Got endpoints: latency-svc-nplrg [525.633693ms]
Aug 15 19:02:12.004: INFO: Created: latency-svc-chc7d
Aug 15 19:02:12.037: INFO: Got endpoints: latency-svc-vdw9c [559.930718ms]
Aug 15 19:02:12.048: INFO: Created: latency-svc-gbll4
Aug 15 19:02:12.090: INFO: Got endpoints: latency-svc-rpk5d [604.379598ms]
Aug 15 19:02:12.105: INFO: Created: latency-svc-hzfbt
Aug 15 19:02:12.136: INFO: Got endpoints: latency-svc-nj8q4 [636.19757ms]
Aug 15 19:02:12.150: INFO: Created: latency-svc-wngfn
Aug 15 19:02:12.198: INFO: Got endpoints: latency-svc-85wk5 [686.44468ms]
Aug 15 19:02:12.215: INFO: Created: latency-svc-dmv6n
Aug 15 19:02:12.237: INFO: Got endpoints: latency-svc-zhkv7 [713.405752ms]
Aug 15 19:02:12.249: INFO: Created: latency-svc-sbjf4
Aug 15 19:02:12.292: INFO: Got endpoints: latency-svc-xhd6k [753.796144ms]
Aug 15 19:02:12.309: INFO: Created: latency-svc-npdw6
Aug 15 19:02:12.338: INFO: Got endpoints: latency-svc-m9kqs [750.293594ms]
Aug 15 19:02:12.349: INFO: Created: latency-svc-9mntn
Aug 15 19:02:12.387: INFO: Got endpoints: latency-svc-6ll2g [750.6888ms]
Aug 15 19:02:12.405: INFO: Created: latency-svc-4wddx
Aug 15 19:02:12.436: INFO: Got endpoints: latency-svc-lj6q9 [747.746381ms]
Aug 15 19:02:12.450: INFO: Created: latency-svc-vk248
Aug 15 19:02:12.488: INFO: Got endpoints: latency-svc-6ht85 [749.69178ms]
Aug 15 19:02:12.508: INFO: Created: latency-svc-vmmgc
Aug 15 19:02:12.537: INFO: Got endpoints: latency-svc-4f4mc [749.925751ms]
Aug 15 19:02:12.549: INFO: Created: latency-svc-hlp8v
Aug 15 19:02:12.587: INFO: Got endpoints: latency-svc-njhqh [748.938536ms]
Aug 15 19:02:12.603: INFO: Created: latency-svc-8cgbw
Aug 15 19:02:12.636: INFO: Got endpoints: latency-svc-7zjsb [749.334089ms]
Aug 15 19:02:12.650: INFO: Created: latency-svc-ghkvz
Aug 15 19:02:12.688: INFO: Got endpoints: latency-svc-tj88z [747.997202ms]
Aug 15 19:02:12.702: INFO: Created: latency-svc-4d969
Aug 15 19:02:12.736: INFO: Got endpoints: latency-svc-chc7d [747.75735ms]
Aug 15 19:02:12.749: INFO: Created: latency-svc-gc6w8
Aug 15 19:02:12.789: INFO: Got endpoints: latency-svc-gbll4 [752.295078ms]
Aug 15 19:02:12.802: INFO: Created: latency-svc-rq5ht
Aug 15 19:02:12.837: INFO: Got endpoints: latency-svc-hzfbt [746.910637ms]
Aug 15 19:02:12.852: INFO: Created: latency-svc-2dh8n
Aug 15 19:02:12.886: INFO: Got endpoints: latency-svc-wngfn [750.127722ms]
Aug 15 19:02:12.904: INFO: Created: latency-svc-s6qnh
Aug 15 19:02:12.936: INFO: Got endpoints: latency-svc-dmv6n [738.490607ms]
Aug 15 19:02:12.947: INFO: Created: latency-svc-5hrfz
Aug 15 19:02:12.986: INFO: Got endpoints: latency-svc-sbjf4 [749.205201ms]
Aug 15 19:02:13.004: INFO: Created: latency-svc-mbwkr
Aug 15 19:02:13.037: INFO: Got endpoints: latency-svc-npdw6 [744.995115ms]
Aug 15 19:02:13.051: INFO: Created: latency-svc-x4cqm
Aug 15 19:02:13.087: INFO: Got endpoints: latency-svc-9mntn [749.51866ms]
Aug 15 19:02:13.109: INFO: Created: latency-svc-sxrvp
Aug 15 19:02:13.137: INFO: Got endpoints: latency-svc-4wddx [750.280305ms]
Aug 15 19:02:13.149: INFO: Created: latency-svc-jxjl9
Aug 15 19:02:13.194: INFO: Got endpoints: latency-svc-vk248 [757.526249ms]
Aug 15 19:02:13.218: INFO: Created: latency-svc-z6rtw
Aug 15 19:02:13.235: INFO: Got endpoints: latency-svc-vmmgc [747.275234ms]
Aug 15 19:02:13.247: INFO: Created: latency-svc-9rvhc
Aug 15 19:02:13.290: INFO: Got endpoints: latency-svc-hlp8v [752.622018ms]
Aug 15 19:02:13.306: INFO: Created: latency-svc-jm9gm
Aug 15 19:02:13.337: INFO: Got endpoints: latency-svc-8cgbw [750.365121ms]
Aug 15 19:02:13.349: INFO: Created: latency-svc-4t7dg
Aug 15 19:02:13.389: INFO: Got endpoints: latency-svc-ghkvz [752.397425ms]
Aug 15 19:02:13.404: INFO: Created: latency-svc-cgsbj
Aug 15 19:02:13.438: INFO: Got endpoints: latency-svc-4d969 [749.888318ms]
Aug 15 19:02:13.449: INFO: Created: latency-svc-6m5qk
Aug 15 19:02:13.487: INFO: Got endpoints: latency-svc-gc6w8 [750.981803ms]
Aug 15 19:02:13.502: INFO: Created: latency-svc-8cz9g
Aug 15 19:02:13.537: INFO: Got endpoints: latency-svc-rq5ht [747.474919ms]
Aug 15 19:02:13.550: INFO: Created: latency-svc-2zwt2
Aug 15 19:02:13.588: INFO: Got endpoints: latency-svc-2dh8n [750.996196ms]
Aug 15 19:02:13.603: INFO: Created: latency-svc-jhm28
Aug 15 19:02:13.638: INFO: Got endpoints: latency-svc-s6qnh [751.780936ms]
Aug 15 19:02:13.650: INFO: Created: latency-svc-m8xff
Aug 15 19:02:13.687: INFO: Got endpoints: latency-svc-5hrfz [750.22004ms]
Aug 15 19:02:13.699: INFO: Created: latency-svc-x6t6p
Aug 15 19:02:13.742: INFO: Got endpoints: latency-svc-mbwkr [755.902581ms]
Aug 15 19:02:13.756: INFO: Created: latency-svc-9dmng
Aug 15 19:02:13.787: INFO: Got endpoints: latency-svc-x4cqm [750.225305ms]
Aug 15 19:02:13.803: INFO: Created: latency-svc-h8dnq
Aug 15 19:02:13.836: INFO: Got endpoints: latency-svc-sxrvp [747.001134ms]
Aug 15 19:02:13.849: INFO: Created: latency-svc-z2tm7
Aug 15 19:02:13.887: INFO: Got endpoints: latency-svc-jxjl9 [750.106423ms]
Aug 15 19:02:13.901: INFO: Created: latency-svc-qwf2f
Aug 15 19:02:13.938: INFO: Got endpoints: latency-svc-z6rtw [743.789128ms]
Aug 15 19:02:13.951: INFO: Created: latency-svc-wt6ls
Aug 15 19:02:13.987: INFO: Got endpoints: latency-svc-9rvhc [751.811447ms]
Aug 15 19:02:14.003: INFO: Created: latency-svc-rgxld
Aug 15 19:02:14.038: INFO: Got endpoints: latency-svc-jm9gm [748.54539ms]
Aug 15 19:02:14.050: INFO: Created: latency-svc-zw7g4
Aug 15 19:02:14.087: INFO: Got endpoints: latency-svc-4t7dg [749.180505ms]
Aug 15 19:02:14.100: INFO: Created: latency-svc-2s92c
Aug 15 19:02:14.136: INFO: Got endpoints: latency-svc-cgsbj [747.379131ms]
Aug 15 19:02:14.154: INFO: Created: latency-svc-ff22z
Aug 15 19:02:14.213: INFO: Got endpoints: latency-svc-6m5qk [775.207334ms]
Aug 15 19:02:14.226: INFO: Created: latency-svc-jb7vw
Aug 15 19:02:14.238: INFO: Got endpoints: latency-svc-8cz9g [750.473547ms]
Aug 15 19:02:14.253: INFO: Created: latency-svc-rmvvm
Aug 15 19:02:14.287: INFO: Got endpoints: latency-svc-2zwt2 [750.48066ms]
Aug 15 19:02:14.303: INFO: Created: latency-svc-6cm72
Aug 15 19:02:14.337: INFO: Got endpoints: latency-svc-jhm28 [748.419643ms]
Aug 15 19:02:14.350: INFO: Created: latency-svc-gnk92
Aug 15 19:02:14.386: INFO: Got endpoints: latency-svc-m8xff [747.842783ms]
Aug 15 19:02:14.398: INFO: Created: latency-svc-cx2lc
Aug 15 19:02:14.436: INFO: Got endpoints: latency-svc-x6t6p [749.148599ms]
Aug 15 19:02:14.450: INFO: Created: latency-svc-g4tq5
Aug 15 19:02:14.487: INFO: Got endpoints: latency-svc-9dmng [744.631477ms]
Aug 15 19:02:14.502: INFO: Created: latency-svc-ctwpb
Aug 15 19:02:14.537: INFO: Got endpoints: latency-svc-h8dnq [749.893612ms]
Aug 15 19:02:14.550: INFO: Created: latency-svc-pb5xs
Aug 15 19:02:14.588: INFO: Got endpoints: latency-svc-z2tm7 [751.492979ms]
Aug 15 19:02:14.605: INFO: Created: latency-svc-j4zgz
Aug 15 19:02:14.636: INFO: Got endpoints: latency-svc-qwf2f [749.066961ms]
Aug 15 19:02:14.647: INFO: Created: latency-svc-2q72r
Aug 15 19:02:14.687: INFO: Got endpoints: latency-svc-wt6ls [748.767465ms]
Aug 15 19:02:14.701: INFO: Created: latency-svc-xcvm4
Aug 15 19:02:14.737: INFO: Got endpoints: latency-svc-rgxld [750.001488ms]
Aug 15 19:02:14.750: INFO: Created: latency-svc-7h8fp
Aug 15 19:02:14.788: INFO: Got endpoints: latency-svc-zw7g4 [749.34451ms]
Aug 15 19:02:14.808: INFO: Created: latency-svc-qpkkf
Aug 15 19:02:14.840: INFO: Got endpoints: latency-svc-2s92c [753.476845ms]
Aug 15 19:02:14.851: INFO: Created: latency-svc-bmrv4
Aug 15 19:02:14.887: INFO: Got endpoints: latency-svc-ff22z [750.451546ms]
Aug 15 19:02:14.901: INFO: Created: latency-svc-ztxxl
Aug 15 19:02:14.938: INFO: Got endpoints: latency-svc-jb7vw [723.962122ms]
Aug 15 19:02:14.950: INFO: Created: latency-svc-s86qr
Aug 15 19:02:14.987: INFO: Got endpoints: latency-svc-rmvvm [749.195442ms]
Aug 15 19:02:15.002: INFO: Created: latency-svc-8bxmh
Aug 15 19:02:15.037: INFO: Got endpoints: latency-svc-6cm72 [750.043728ms]
Aug 15 19:02:15.053: INFO: Created: latency-svc-lvrks
Aug 15 19:02:15.090: INFO: Got endpoints: latency-svc-gnk92 [752.611605ms]
Aug 15 19:02:15.103: INFO: Created: latency-svc-hpjmt
Aug 15 19:02:15.136: INFO: Got endpoints: latency-svc-cx2lc [750.223965ms]
Aug 15 19:02:15.149: INFO: Created: latency-svc-bd2z9
Aug 15 19:02:15.187: INFO: Got endpoints: latency-svc-g4tq5 [750.653301ms]
Aug 15 19:02:15.200: INFO: Created: latency-svc-njhqt
Aug 15 19:02:15.244: INFO: Got endpoints: latency-svc-ctwpb [757.531023ms]
Aug 15 19:02:15.267: INFO: Created: latency-svc-7wmjh
Aug 15 19:02:15.289: INFO: Got endpoints: latency-svc-pb5xs [751.367456ms]
Aug 15 19:02:15.303: INFO: Created: latency-svc-dzqtl
Aug 15 19:02:15.337: INFO: Got endpoints: latency-svc-j4zgz [748.899717ms]
Aug 15 19:02:15.350: INFO: Created: latency-svc-7f5hl
Aug 15 19:02:15.386: INFO: Got endpoints: latency-svc-2q72r [749.638488ms]
Aug 15 19:02:15.401: INFO: Created: latency-svc-fjvdg
Aug 15 19:02:15.438: INFO: Got endpoints: latency-svc-xcvm4 [751.208571ms]
Aug 15 19:02:15.451: INFO: Created: latency-svc-7szlb
Aug 15 19:02:15.489: INFO: Got endpoints: latency-svc-7h8fp [751.180397ms]
Aug 15 19:02:15.501: INFO: Created: latency-svc-lvgl9
Aug 15 19:02:15.537: INFO: Got endpoints: latency-svc-qpkkf [749.359134ms]
Aug 15 19:02:15.548: INFO: Created: latency-svc-pq54g
Aug 15 19:02:15.588: INFO: Got endpoints: latency-svc-bmrv4 [747.512583ms]
Aug 15 19:02:15.601: INFO: Created: latency-svc-qspxk
Aug 15 19:02:15.636: INFO: Got endpoints: latency-svc-ztxxl [749.276586ms]
Aug 15 19:02:15.652: INFO: Created: latency-svc-nn76h
Aug 15 19:02:15.687: INFO: Got endpoints: latency-svc-s86qr [749.081522ms]
Aug 15 19:02:15.700: INFO: Created: latency-svc-454m7
Aug 15 19:02:15.738: INFO: Got endpoints: latency-svc-8bxmh [750.876853ms]
Aug 15 19:02:15.753: INFO: Created: latency-svc-6zklt
Aug 15 19:02:15.786: INFO: Got endpoints: latency-svc-lvrks [748.560805ms]
Aug 15 19:02:15.798: INFO: Created: latency-svc-s8gn5
Aug 15 19:02:15.837: INFO: Got endpoints: latency-svc-hpjmt [747.220411ms]
Aug 15 19:02:15.849: INFO: Created: latency-svc-zxbtv
Aug 15 19:02:15.887: INFO: Got endpoints: latency-svc-bd2z9 [750.980305ms]
Aug 15 19:02:15.907: INFO: Created: latency-svc-xtn82
Aug 15 19:02:15.937: INFO: Got endpoints: latency-svc-njhqt [750.297605ms]
Aug 15 19:02:15.952: INFO: Created: latency-svc-ssl6b
Aug 15 19:02:15.987: INFO: Got endpoints: latency-svc-7wmjh [742.901201ms]
Aug 15 19:02:16.005: INFO: Created: latency-svc-jd9hv
Aug 15 19:02:16.038: INFO: Got endpoints: latency-svc-dzqtl [748.945706ms]
Aug 15 19:02:16.049: INFO: Created: latency-svc-lfrrj
Aug 15 19:02:16.088: INFO: Got endpoints: latency-svc-7f5hl [751.339015ms]
Aug 15 19:02:16.109: INFO: Created: latency-svc-fjb2k
Aug 15 19:02:16.137: INFO: Got endpoints: latency-svc-fjvdg [750.311402ms]
Aug 15 19:02:16.147: INFO: Created: latency-svc-4jpk9
Aug 15 19:02:16.187: INFO: Got endpoints: latency-svc-7szlb [749.628527ms]
Aug 15 19:02:16.202: INFO: Created: latency-svc-g76c2
Aug 15 19:02:16.236: INFO: Got endpoints: latency-svc-lvgl9 [747.878268ms]
Aug 15 19:02:16.249: INFO: Created: latency-svc-24vtp
Aug 15 19:02:16.287: INFO: Got endpoints: latency-svc-pq54g [749.526056ms]
Aug 15 19:02:16.300: INFO: Created: latency-svc-wvjjn
Aug 15 19:02:16.338: INFO: Got endpoints: latency-svc-qspxk [750.358983ms]
Aug 15 19:02:16.349: INFO: Created: latency-svc-mrdg2
Aug 15 19:02:16.386: INFO: Got endpoints: latency-svc-nn76h [749.683067ms]
Aug 15 19:02:16.401: INFO: Created: latency-svc-vdjfb
Aug 15 19:02:16.436: INFO: Got endpoints: latency-svc-454m7 [749.784352ms]
Aug 15 19:02:16.456: INFO: Created: latency-svc-l89gd
Aug 15 19:02:16.488: INFO: Got endpoints: latency-svc-6zklt [749.97056ms]
Aug 15 19:02:16.505: INFO: Created: latency-svc-5lp2t
Aug 15 19:02:16.536: INFO: Got endpoints: latency-svc-s8gn5 [749.999218ms]
Aug 15 19:02:16.548: INFO: Created: latency-svc-k5fsv
Aug 15 19:02:16.587: INFO: Got endpoints: latency-svc-zxbtv [749.793653ms]
Aug 15 19:02:16.607: INFO: Created: latency-svc-dzf2s
Aug 15 19:02:16.638: INFO: Got endpoints: latency-svc-xtn82 [750.096784ms]
Aug 15 19:02:16.650: INFO: Created: latency-svc-d6psn
Aug 15 19:02:16.687: INFO: Got endpoints: latency-svc-ssl6b [749.942715ms]
Aug 15 19:02:16.709: INFO: Created: latency-svc-gphpt
Aug 15 19:02:16.743: INFO: Got endpoints: latency-svc-jd9hv [755.858763ms]
Aug 15 19:02:16.757: INFO: Created: latency-svc-sqxz7
Aug 15 19:02:16.788: INFO: Got endpoints: latency-svc-lfrrj [749.748914ms]
Aug 15 19:02:16.803: INFO: Created: latency-svc-r5zfb
Aug 15 19:02:16.837: INFO: Got endpoints: latency-svc-fjb2k [748.139921ms]
Aug 15 19:02:16.849: INFO: Created: latency-svc-nsrq8
Aug 15 19:02:16.886: INFO: Got endpoints: latency-svc-4jpk9 [749.732692ms]
Aug 15 19:02:16.906: INFO: Created: latency-svc-kmbc4
Aug 15 19:02:16.938: INFO: Got endpoints: latency-svc-g76c2 [749.983298ms]
Aug 15 19:02:16.950: INFO: Created: latency-svc-95f9f
Aug 15 19:02:16.986: INFO: Got endpoints: latency-svc-24vtp [749.809279ms]
Aug 15 19:02:17.011: INFO: Created: latency-svc-ls5fk
Aug 15 19:02:17.037: INFO: Got endpoints: latency-svc-wvjjn [750.859175ms]
Aug 15 19:02:17.052: INFO: Created: latency-svc-ndksw
Aug 15 19:02:17.087: INFO: Got endpoints: latency-svc-mrdg2 [749.037958ms]
Aug 15 19:02:17.107: INFO: Created: latency-svc-bvqbc
Aug 15 19:02:17.136: INFO: Got endpoints: latency-svc-vdjfb [750.00038ms]
Aug 15 19:02:17.149: INFO: Created: latency-svc-tfwj5
Aug 15 19:02:17.188: INFO: Got endpoints: latency-svc-l89gd [751.092917ms]
Aug 15 19:02:17.203: INFO: Created: latency-svc-vlmqt
Aug 15 19:02:17.245: INFO: Got endpoints: latency-svc-5lp2t [756.845095ms]
Aug 15 19:02:17.263: INFO: Created: latency-svc-7xgkk
Aug 15 19:02:17.288: INFO: Got endpoints: latency-svc-k5fsv [752.225038ms]
Aug 15 19:02:17.309: INFO: Created: latency-svc-cvgvq
Aug 15 19:02:17.337: INFO: Got endpoints: latency-svc-dzf2s [749.984314ms]
Aug 15 19:02:17.348: INFO: Created: latency-svc-vtzqk
Aug 15 19:02:17.386: INFO: Got endpoints: latency-svc-d6psn [748.896465ms]
Aug 15 19:02:17.401: INFO: Created: latency-svc-74zgv
Aug 15 19:02:17.438: INFO: Got endpoints: latency-svc-gphpt [750.597807ms]
Aug 15 19:02:17.452: INFO: Created: latency-svc-scq7d
Aug 15 19:02:17.487: INFO: Got endpoints: latency-svc-sqxz7 [743.301332ms]
Aug 15 19:02:17.500: INFO: Created: latency-svc-t5vlh
Aug 15 19:02:17.536: INFO: Got endpoints: latency-svc-r5zfb [748.658431ms]
Aug 15 19:02:17.548: INFO: Created: latency-svc-m62gr
Aug 15 19:02:17.588: INFO: Got endpoints: latency-svc-nsrq8 [751.021731ms]
Aug 15 19:02:17.605: INFO: Created: latency-svc-5bgnb
Aug 15 19:02:17.637: INFO: Got endpoints: latency-svc-kmbc4 [750.182147ms]
Aug 15 19:02:17.649: INFO: Created: latency-svc-ffl4g
Aug 15 19:02:17.687: INFO: Got endpoints: latency-svc-95f9f [748.267833ms]
Aug 15 19:02:17.704: INFO: Created: latency-svc-9gmpz
Aug 15 19:02:17.736: INFO: Got endpoints: latency-svc-ls5fk [749.614958ms]
Aug 15 19:02:17.747: INFO: Created: latency-svc-rm5fk
Aug 15 19:02:17.786: INFO: Got endpoints: latency-svc-ndksw [748.876714ms]
Aug 15 19:02:17.804: INFO: Created: latency-svc-c2wjk
Aug 15 19:02:17.837: INFO: Got endpoints: latency-svc-bvqbc [750.046165ms]
Aug 15 19:02:17.848: INFO: Created: latency-svc-62s6n
Aug 15 19:02:17.889: INFO: Got endpoints: latency-svc-tfwj5 [753.133524ms]
Aug 15 19:02:17.904: INFO: Created: latency-svc-znrjh
Aug 15 19:02:17.938: INFO: Got endpoints: latency-svc-vlmqt [750.123632ms]
Aug 15 19:02:17.957: INFO: Created: latency-svc-9m6jq
Aug 15 19:02:17.988: INFO: Got endpoints: latency-svc-7xgkk [742.607249ms]
Aug 15 19:02:18.005: INFO: Created: latency-svc-fp768
Aug 15 19:02:18.045: INFO: Got endpoints: latency-svc-cvgvq [757.04129ms]
Aug 15 19:02:18.061: INFO: Created: latency-svc-89l5g
Aug 15 19:02:18.087: INFO: Got endpoints: latency-svc-vtzqk [750.384799ms]
Aug 15 19:02:18.108: INFO: Created: latency-svc-2zxkk
Aug 15 19:02:18.138: INFO: Got endpoints: latency-svc-74zgv [751.261203ms]
Aug 15 19:02:18.151: INFO: Created: latency-svc-g46l5
Aug 15 19:02:18.186: INFO: Got endpoints: latency-svc-scq7d [748.888255ms]
Aug 15 19:02:18.199: INFO: Created: latency-svc-mzdt8
Aug 15 19:02:18.237: INFO: Got endpoints: latency-svc-t5vlh [750.315333ms]
Aug 15 19:02:18.252: INFO: Created: latency-svc-8gw8p
Aug 15 19:02:18.288: INFO: Got endpoints: latency-svc-m62gr [751.386219ms]
Aug 15 19:02:18.313: INFO: Created: latency-svc-s4jn7
Aug 15 19:02:18.337: INFO: Got endpoints: latency-svc-5bgnb [749.096168ms]
Aug 15 19:02:18.349: INFO: Created: latency-svc-bbczs
Aug 15 19:02:18.386: INFO: Got endpoints: latency-svc-ffl4g [749.725542ms]
Aug 15 19:02:18.405: INFO: Created: latency-svc-xssp9
Aug 15 19:02:18.438: INFO: Got endpoints: latency-svc-9gmpz [751.07094ms]
Aug 15 19:02:18.452: INFO: Created: latency-svc-v85fl
Aug 15 19:02:18.487: INFO: Got endpoints: latency-svc-rm5fk [750.611266ms]
Aug 15 19:02:18.501: INFO: Created: latency-svc-m9njs
Aug 15 19:02:18.537: INFO: Got endpoints: latency-svc-c2wjk [750.279832ms]
Aug 15 19:02:18.554: INFO: Created: latency-svc-7fdtf
Aug 15 19:02:18.589: INFO: Got endpoints: latency-svc-62s6n [751.566946ms]
Aug 15 19:02:18.608: INFO: Created: latency-svc-fbm4m
Aug 15 19:02:18.637: INFO: Got endpoints: latency-svc-znrjh [747.473534ms]
Aug 15 19:02:18.651: INFO: Created: latency-svc-j2gx8
Aug 15 19:02:18.687: INFO: Got endpoints: latency-svc-9m6jq [749.08105ms]
Aug 15 19:02:18.702: INFO: Created: latency-svc-7qkgp
Aug 15 19:02:18.737: INFO: Got endpoints: latency-svc-fp768 [748.882912ms]
Aug 15 19:02:18.750: INFO: Created: latency-svc-n69cc
Aug 15 19:02:18.787: INFO: Got endpoints: latency-svc-89l5g [741.267918ms]
Aug 15 19:02:18.801: INFO: Created: latency-svc-7slvn
Aug 15 19:02:18.837: INFO: Got endpoints: latency-svc-2zxkk [749.608561ms]
Aug 15 19:02:18.851: INFO: Created: latency-svc-zwj2x
Aug 15 19:02:18.886: INFO: Got endpoints: latency-svc-g46l5 [748.362469ms]
Aug 15 19:02:18.902: INFO: Created: latency-svc-jq8mb
Aug 15 19:02:18.936: INFO: Got endpoints: latency-svc-mzdt8 [749.909935ms]
Aug 15 19:02:18.988: INFO: Got endpoints: latency-svc-8gw8p [750.581976ms]
Aug 15 19:02:19.036: INFO: Got endpoints: latency-svc-s4jn7 [748.538578ms]
Aug 15 19:02:19.091: INFO: Got endpoints: latency-svc-bbczs [753.694262ms]
Aug 15 19:02:19.136: INFO: Got endpoints: latency-svc-xssp9 [749.436274ms]
Aug 15 19:02:19.188: INFO: Got endpoints: latency-svc-v85fl [749.867695ms]
Aug 15 19:02:19.246: INFO: Got endpoints: latency-svc-m9njs [759.222921ms]
Aug 15 19:02:19.286: INFO: Got endpoints: latency-svc-7fdtf [749.42538ms]
Aug 15 19:02:19.337: INFO: Got endpoints: latency-svc-fbm4m [748.054203ms]
Aug 15 19:02:19.388: INFO: Got endpoints: latency-svc-j2gx8 [750.950134ms]
Aug 15 19:02:19.437: INFO: Got endpoints: latency-svc-7qkgp [750.247987ms]
Aug 15 19:02:19.487: INFO: Got endpoints: latency-svc-n69cc [750.564223ms]
Aug 15 19:02:19.538: INFO: Got endpoints: latency-svc-7slvn [751.475293ms]
Aug 15 19:02:19.587: INFO: Got endpoints: latency-svc-zwj2x [750.590229ms]
Aug 15 19:02:19.636: INFO: Got endpoints: latency-svc-jq8mb [749.841466ms]
Aug 15 19:02:19.636: INFO: Latencies: [25.085431ms 40.154394ms 50.031851ms 61.22341ms 70.482964ms 80.474281ms 103.875656ms 129.92013ms 142.316027ms 146.517176ms 148.002426ms 148.806543ms 149.69261ms 151.928891ms 153.024831ms 153.260778ms 153.485737ms 153.632767ms 154.628629ms 155.317412ms 156.748342ms 157.184116ms 158.011237ms 158.45512ms 158.924125ms 162.215113ms 163.355678ms 173.082909ms 174.877065ms 186.021921ms 187.356595ms 187.564763ms 188.794167ms 188.802892ms 190.75336ms 194.435437ms 196.61597ms 200.637598ms 204.31978ms 236.862463ms 280.256784ms 320.493323ms 360.226744ms 401.752499ms 441.652336ms 485.296667ms 525.633693ms 559.930718ms 604.379598ms 636.19757ms 686.44468ms 713.405752ms 723.962122ms 738.490607ms 741.267918ms 742.607249ms 742.901201ms 743.301332ms 743.789128ms 744.631477ms 744.995115ms 746.910637ms 747.001134ms 747.220411ms 747.275234ms 747.379131ms 747.473534ms 747.474919ms 747.512583ms 747.746381ms 747.75735ms 747.842783ms 747.878268ms 747.997202ms 748.054203ms 748.139921ms 748.267833ms 748.362469ms 748.419643ms 748.538578ms 748.54539ms 748.560805ms 748.658431ms 748.767465ms 748.876714ms 748.882912ms 748.888255ms 748.896465ms 748.899717ms 748.938536ms 748.945706ms 749.037958ms 749.066961ms 749.08105ms 749.081522ms 749.096168ms 749.148599ms 749.180505ms 749.195442ms 749.205201ms 749.276586ms 749.334089ms 749.34451ms 749.359134ms 749.42538ms 749.436274ms 749.51866ms 749.526056ms 749.608561ms 749.614958ms 749.628527ms 749.638488ms 749.683067ms 749.69178ms 749.725542ms 749.732692ms 749.748914ms 749.784352ms 749.793653ms 749.809279ms 749.841466ms 749.867695ms 749.888318ms 749.893612ms 749.909935ms 749.925751ms 749.942715ms 749.97056ms 749.983298ms 749.984314ms 749.999218ms 750.00038ms 750.001488ms 750.043728ms 750.046165ms 750.096784ms 750.106423ms 750.123632ms 750.127722ms 750.182147ms 750.22004ms 750.223965ms 750.225305ms 750.247987ms 750.279832ms 750.280305ms 750.293594ms 750.297605ms 750.311402ms 750.315333ms 750.358983ms 750.365121ms 750.384799ms 750.451546ms 750.473547ms 750.48066ms 750.564223ms 750.581976ms 750.590229ms 750.597807ms 750.611266ms 750.653301ms 750.6888ms 750.859175ms 750.876853ms 750.950134ms 750.980305ms 750.981803ms 750.996196ms 751.021731ms 751.07094ms 751.092917ms 751.180397ms 751.208571ms 751.261203ms 751.339015ms 751.367456ms 751.386219ms 751.475293ms 751.492979ms 751.566946ms 751.780936ms 751.811447ms 752.225038ms 752.295078ms 752.397425ms 752.611605ms 752.622018ms 753.133524ms 753.476845ms 753.694262ms 753.796144ms 755.858763ms 755.902581ms 756.845095ms 757.04129ms 757.526249ms 757.531023ms 759.222921ms 775.207334ms]
Aug 15 19:02:19.636: INFO: 50 %ile: 749.276586ms
Aug 15 19:02:19.636: INFO: 90 %ile: 751.566946ms
Aug 15 19:02:19.636: INFO: 99 %ile: 759.222921ms
Aug 15 19:02:19.636: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:02:19.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7488" for this suite.
Aug 15 19:02:35.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:02:35.762: INFO: namespace svc-latency-7488 deletion completed in 16.117600428s

• [SLOW TEST:27.885 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:02:35.762: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-57be9895-3c48-46a2-b980-5f46dc672c70
STEP: Creating secret with name s-test-opt-upd-1c78edb9-4c97-403c-affd-4fe7e5332e15
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-57be9895-3c48-46a2-b980-5f46dc672c70
STEP: Updating secret s-test-opt-upd-1c78edb9-4c97-403c-affd-4fe7e5332e15
STEP: Creating secret with name s-test-opt-create-796b3f99-5ae1-44b9-a6c3-fd2ee7648e33
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:03:52.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3496" for this suite.
Aug 15 19:04:14.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:04:14.395: INFO: namespace projected-3496 deletion completed in 22.107208156s

• [SLOW TEST:98.633 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:04:14.395: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 15 19:04:14.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-3895'
Aug 15 19:04:14.693: INFO: stderr: ""
Aug 15 19:04:14.693: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 15 19:04:14.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3895'
Aug 15 19:04:14.792: INFO: stderr: ""
Aug 15 19:04:14.792: INFO: stdout: "update-demo-nautilus-j9ws2 update-demo-nautilus-l4ns7 "
Aug 15 19:04:14.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-j9ws2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:14.876: INFO: stderr: ""
Aug 15 19:04:14.876: INFO: stdout: ""
Aug 15 19:04:14.876: INFO: update-demo-nautilus-j9ws2 is created but not running
Aug 15 19:04:19.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3895'
Aug 15 19:04:19.964: INFO: stderr: ""
Aug 15 19:04:19.964: INFO: stdout: "update-demo-nautilus-j9ws2 update-demo-nautilus-l4ns7 "
Aug 15 19:04:19.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-j9ws2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:20.058: INFO: stderr: ""
Aug 15 19:04:20.058: INFO: stdout: "true"
Aug 15 19:04:20.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-j9ws2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:20.136: INFO: stderr: ""
Aug 15 19:04:20.136: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:04:20.136: INFO: validating pod update-demo-nautilus-j9ws2
Aug 15 19:04:20.142: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:04:20.142: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:04:20.142: INFO: update-demo-nautilus-j9ws2 is verified up and running
Aug 15 19:04:20.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-l4ns7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:20.223: INFO: stderr: ""
Aug 15 19:04:20.223: INFO: stdout: "true"
Aug 15 19:04:20.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-l4ns7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:20.303: INFO: stderr: ""
Aug 15 19:04:20.303: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:04:20.303: INFO: validating pod update-demo-nautilus-l4ns7
Aug 15 19:04:20.307: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:04:20.307: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:04:20.307: INFO: update-demo-nautilus-l4ns7 is verified up and running
STEP: scaling down the replication controller
Aug 15 19:04:20.309: INFO: scanned /root for discovery docs: <nil>
Aug 15 19:04:20.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3895'
Aug 15 19:04:21.420: INFO: stderr: ""
Aug 15 19:04:21.420: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 15 19:04:21.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3895'
Aug 15 19:04:21.504: INFO: stderr: ""
Aug 15 19:04:21.504: INFO: stdout: "update-demo-nautilus-j9ws2 update-demo-nautilus-l4ns7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 15 19:04:26.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3895'
Aug 15 19:04:26.589: INFO: stderr: ""
Aug 15 19:04:26.589: INFO: stdout: "update-demo-nautilus-l4ns7 "
Aug 15 19:04:26.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-l4ns7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:26.668: INFO: stderr: ""
Aug 15 19:04:26.668: INFO: stdout: "true"
Aug 15 19:04:26.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-l4ns7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:26.750: INFO: stderr: ""
Aug 15 19:04:26.750: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:04:26.750: INFO: validating pod update-demo-nautilus-l4ns7
Aug 15 19:04:26.755: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:04:26.755: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:04:26.755: INFO: update-demo-nautilus-l4ns7 is verified up and running
STEP: scaling up the replication controller
Aug 15 19:04:26.757: INFO: scanned /root for discovery docs: <nil>
Aug 15 19:04:26.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3895'
Aug 15 19:04:27.883: INFO: stderr: ""
Aug 15 19:04:27.883: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 15 19:04:27.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3895'
Aug 15 19:04:27.977: INFO: stderr: ""
Aug 15 19:04:27.977: INFO: stdout: "update-demo-nautilus-l4ns7 update-demo-nautilus-r6nq2 "
Aug 15 19:04:27.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-l4ns7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:28.066: INFO: stderr: ""
Aug 15 19:04:28.066: INFO: stdout: "true"
Aug 15 19:04:28.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-l4ns7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:28.142: INFO: stderr: ""
Aug 15 19:04:28.142: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:04:28.142: INFO: validating pod update-demo-nautilus-l4ns7
Aug 15 19:04:28.146: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:04:28.146: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:04:28.146: INFO: update-demo-nautilus-l4ns7 is verified up and running
Aug 15 19:04:28.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-r6nq2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:28.229: INFO: stderr: ""
Aug 15 19:04:28.229: INFO: stdout: ""
Aug 15 19:04:28.229: INFO: update-demo-nautilus-r6nq2 is created but not running
Aug 15 19:04:33.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3895'
Aug 15 19:04:33.313: INFO: stderr: ""
Aug 15 19:04:33.313: INFO: stdout: "update-demo-nautilus-l4ns7 update-demo-nautilus-r6nq2 "
Aug 15 19:04:33.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-l4ns7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:33.397: INFO: stderr: ""
Aug 15 19:04:33.397: INFO: stdout: "true"
Aug 15 19:04:33.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-l4ns7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:33.479: INFO: stderr: ""
Aug 15 19:04:33.479: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:04:33.479: INFO: validating pod update-demo-nautilus-l4ns7
Aug 15 19:04:33.483: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:04:33.483: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:04:33.483: INFO: update-demo-nautilus-l4ns7 is verified up and running
Aug 15 19:04:33.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-r6nq2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:33.566: INFO: stderr: ""
Aug 15 19:04:33.566: INFO: stdout: "true"
Aug 15 19:04:33.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-r6nq2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3895'
Aug 15 19:04:33.652: INFO: stderr: ""
Aug 15 19:04:33.652: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:04:33.652: INFO: validating pod update-demo-nautilus-r6nq2
Aug 15 19:04:33.658: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:04:33.658: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:04:33.658: INFO: update-demo-nautilus-r6nq2 is verified up and running
STEP: using delete to clean up resources
Aug 15 19:04:33.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-3895'
Aug 15 19:04:33.745: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 19:04:33.745: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 15 19:04:33.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3895'
Aug 15 19:04:33.837: INFO: stderr: "No resources found.\n"
Aug 15 19:04:33.837: INFO: stdout: ""
Aug 15 19:04:33.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -l name=update-demo --namespace=kubectl-3895 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 15 19:04:33.925: INFO: stderr: ""
Aug 15 19:04:33.925: INFO: stdout: "update-demo-nautilus-l4ns7\nupdate-demo-nautilus-r6nq2\n"
Aug 15 19:04:34.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3895'
Aug 15 19:04:34.522: INFO: stderr: "No resources found.\n"
Aug 15 19:04:34.522: INFO: stdout: ""
Aug 15 19:04:34.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -l name=update-demo --namespace=kubectl-3895 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 15 19:04:34.610: INFO: stderr: ""
Aug 15 19:04:34.610: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:04:34.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3895" for this suite.
Aug 15 19:04:56.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:04:56.729: INFO: namespace kubectl-3895 deletion completed in 22.114654696s

• [SLOW TEST:42.334 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:04:56.729: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-xkblx in namespace proxy-1954
I0815 19:04:56.803614      19 runners.go:180] Created replication controller with name: proxy-service-xkblx, namespace: proxy-1954, replica count: 1
I0815 19:04:57.854162      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0815 19:04:58.854442      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0815 19:04:59.854685      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0815 19:05:00.854906      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0815 19:05:01.855124      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0815 19:05:02.855431      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0815 19:05:03.855646      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0815 19:05:04.855881      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0815 19:05:05.856114      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0815 19:05:06.856348      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0815 19:05:07.856567      19 runners.go:180] proxy-service-xkblx Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 15 19:05:07.861: INFO: setup took 11.096300258s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 15 19:05:07.866: INFO: (0) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 5.127285ms)
Aug 15 19:05:07.868: INFO: (0) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 6.197543ms)
Aug 15 19:05:07.869: INFO: (0) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.830438ms)
Aug 15 19:05:07.870: INFO: (0) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 7.393076ms)
Aug 15 19:05:07.870: INFO: (0) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 7.481796ms)
Aug 15 19:05:07.870: INFO: (0) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 8.628237ms)
Aug 15 19:05:07.871: INFO: (0) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 7.927838ms)
Aug 15 19:05:07.871: INFO: (0) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 9.411938ms)
Aug 15 19:05:07.871: INFO: (0) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 8.517268ms)
Aug 15 19:05:07.871: INFO: (0) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 9.293633ms)
Aug 15 19:05:07.872: INFO: (0) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 9.646166ms)
Aug 15 19:05:07.880: INFO: (0) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 17.699876ms)
Aug 15 19:05:07.880: INFO: (0) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 17.551765ms)
Aug 15 19:05:07.880: INFO: (0) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 18.054388ms)
Aug 15 19:05:07.880: INFO: (0) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 17.608343ms)
Aug 15 19:05:07.880: INFO: (0) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 18.681707ms)
Aug 15 19:05:07.886: INFO: (1) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 5.404521ms)
Aug 15 19:05:07.889: INFO: (1) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 8.081186ms)
Aug 15 19:05:07.889: INFO: (1) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 8.18977ms)
Aug 15 19:05:07.889: INFO: (1) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 8.435084ms)
Aug 15 19:05:07.890: INFO: (1) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 8.752343ms)
Aug 15 19:05:07.891: INFO: (1) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 9.922121ms)
Aug 15 19:05:07.891: INFO: (1) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 9.990802ms)
Aug 15 19:05:07.891: INFO: (1) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 9.919388ms)
Aug 15 19:05:07.891: INFO: (1) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 10.323251ms)
Aug 15 19:05:07.891: INFO: (1) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 10.621133ms)
Aug 15 19:05:07.891: INFO: (1) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 10.539299ms)
Aug 15 19:05:07.891: INFO: (1) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 10.70442ms)
Aug 15 19:05:07.891: INFO: (1) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 10.730295ms)
Aug 15 19:05:07.891: INFO: (1) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 10.580943ms)
Aug 15 19:05:07.892: INFO: (1) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 10.718782ms)
Aug 15 19:05:07.892: INFO: (1) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 10.676558ms)
Aug 15 19:05:07.895: INFO: (2) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 3.413806ms)
Aug 15 19:05:07.896: INFO: (2) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 3.642571ms)
Aug 15 19:05:07.896: INFO: (2) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 4.029434ms)
Aug 15 19:05:07.896: INFO: (2) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 3.935892ms)
Aug 15 19:05:07.897: INFO: (2) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 4.706064ms)
Aug 15 19:05:07.900: INFO: (2) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 7.803645ms)
Aug 15 19:05:07.900: INFO: (2) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 7.680742ms)
Aug 15 19:05:07.900: INFO: (2) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 7.204023ms)
Aug 15 19:05:07.901: INFO: (2) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 7.857251ms)
Aug 15 19:05:07.901: INFO: (2) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 7.712441ms)
Aug 15 19:05:07.901: INFO: (2) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 8.306494ms)
Aug 15 19:05:07.901: INFO: (2) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 8.598849ms)
Aug 15 19:05:07.901: INFO: (2) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 8.692302ms)
Aug 15 19:05:07.902: INFO: (2) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 8.984508ms)
Aug 15 19:05:07.902: INFO: (2) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 8.748187ms)
Aug 15 19:05:07.902: INFO: (2) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 8.952452ms)
Aug 15 19:05:07.907: INFO: (3) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 4.633242ms)
Aug 15 19:05:07.908: INFO: (3) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 5.524241ms)
Aug 15 19:05:07.908: INFO: (3) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 5.511066ms)
Aug 15 19:05:07.908: INFO: (3) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 5.761704ms)
Aug 15 19:05:07.909: INFO: (3) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 6.708412ms)
Aug 15 19:05:07.909: INFO: (3) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 6.58559ms)
Aug 15 19:05:07.910: INFO: (3) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 8.034828ms)
Aug 15 19:05:07.910: INFO: (3) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 8.150297ms)
Aug 15 19:05:07.910: INFO: (3) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 8.437744ms)
Aug 15 19:05:07.910: INFO: (3) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 8.289878ms)
Aug 15 19:05:07.911: INFO: (3) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 8.587603ms)
Aug 15 19:05:07.911: INFO: (3) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 8.944381ms)
Aug 15 19:05:07.911: INFO: (3) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 9.101782ms)
Aug 15 19:05:07.911: INFO: (3) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 9.290727ms)
Aug 15 19:05:07.912: INFO: (3) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 9.262495ms)
Aug 15 19:05:07.912: INFO: (3) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 9.693753ms)
Aug 15 19:05:07.917: INFO: (4) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 4.653593ms)
Aug 15 19:05:07.917: INFO: (4) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 5.299091ms)
Aug 15 19:05:07.918: INFO: (4) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 5.347505ms)
Aug 15 19:05:07.918: INFO: (4) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 4.703336ms)
Aug 15 19:05:07.918: INFO: (4) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 5.480208ms)
Aug 15 19:05:07.918: INFO: (4) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 6.410323ms)
Aug 15 19:05:07.919: INFO: (4) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.153744ms)
Aug 15 19:05:07.919: INFO: (4) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 7.092808ms)
Aug 15 19:05:07.919: INFO: (4) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 7.120487ms)
Aug 15 19:05:07.919: INFO: (4) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.000907ms)
Aug 15 19:05:07.919: INFO: (4) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 6.024927ms)
Aug 15 19:05:07.919: INFO: (4) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 7.822009ms)
Aug 15 19:05:07.920: INFO: (4) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 7.286439ms)
Aug 15 19:05:07.920: INFO: (4) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 6.820671ms)
Aug 15 19:05:07.921: INFO: (4) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 7.997086ms)
Aug 15 19:05:07.921: INFO: (4) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 8.199834ms)
Aug 15 19:05:07.925: INFO: (5) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 3.721876ms)
Aug 15 19:05:07.927: INFO: (5) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 5.052985ms)
Aug 15 19:05:07.927: INFO: (5) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 5.29564ms)
Aug 15 19:05:07.927: INFO: (5) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 5.474531ms)
Aug 15 19:05:07.927: INFO: (5) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 5.645074ms)
Aug 15 19:05:07.927: INFO: (5) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 5.632863ms)
Aug 15 19:05:07.927: INFO: (5) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 5.580337ms)
Aug 15 19:05:07.928: INFO: (5) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 6.119169ms)
Aug 15 19:05:07.928: INFO: (5) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.482559ms)
Aug 15 19:05:07.928: INFO: (5) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 6.86164ms)
Aug 15 19:05:07.929: INFO: (5) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 7.545073ms)
Aug 15 19:05:07.930: INFO: (5) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 8.106676ms)
Aug 15 19:05:07.931: INFO: (5) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 9.282674ms)
Aug 15 19:05:07.931: INFO: (5) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 9.424453ms)
Aug 15 19:05:07.931: INFO: (5) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 9.785575ms)
Aug 15 19:05:07.931: INFO: (5) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 9.747745ms)
Aug 15 19:05:07.936: INFO: (6) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 4.384231ms)
Aug 15 19:05:07.936: INFO: (6) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 4.357571ms)
Aug 15 19:05:07.936: INFO: (6) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 4.542436ms)
Aug 15 19:05:07.936: INFO: (6) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 4.672937ms)
Aug 15 19:05:07.938: INFO: (6) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 6.327185ms)
Aug 15 19:05:07.938: INFO: (6) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 6.560775ms)
Aug 15 19:05:07.938: INFO: (6) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 6.856964ms)
Aug 15 19:05:07.938: INFO: (6) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.863365ms)
Aug 15 19:05:07.938: INFO: (6) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 6.798543ms)
Aug 15 19:05:07.938: INFO: (6) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.859832ms)
Aug 15 19:05:07.938: INFO: (6) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 6.877618ms)
Aug 15 19:05:07.939: INFO: (6) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 7.227213ms)
Aug 15 19:05:07.941: INFO: (6) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 9.317969ms)
Aug 15 19:05:07.941: INFO: (6) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 9.240371ms)
Aug 15 19:05:07.941: INFO: (6) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 9.472226ms)
Aug 15 19:05:07.941: INFO: (6) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 9.240628ms)
Aug 15 19:05:07.948: INFO: (7) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 5.995366ms)
Aug 15 19:05:07.948: INFO: (7) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 5.947871ms)
Aug 15 19:05:07.948: INFO: (7) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 7.040457ms)
Aug 15 19:05:07.949: INFO: (7) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 6.583081ms)
Aug 15 19:05:07.949: INFO: (7) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 7.957982ms)
Aug 15 19:05:07.949: INFO: (7) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 7.762119ms)
Aug 15 19:05:07.949: INFO: (7) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 8.164128ms)
Aug 15 19:05:07.950: INFO: (7) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 7.520669ms)
Aug 15 19:05:07.950: INFO: (7) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 7.876812ms)
Aug 15 19:05:07.950: INFO: (7) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 8.145013ms)
Aug 15 19:05:07.950: INFO: (7) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 8.177578ms)
Aug 15 19:05:07.951: INFO: (7) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 9.12902ms)
Aug 15 19:05:07.951: INFO: (7) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 8.679622ms)
Aug 15 19:05:07.951: INFO: (7) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 8.810722ms)
Aug 15 19:05:07.951: INFO: (7) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 8.674786ms)
Aug 15 19:05:07.952: INFO: (7) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 10.486155ms)
Aug 15 19:05:07.957: INFO: (8) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 5.546405ms)
Aug 15 19:05:07.959: INFO: (8) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 7.203085ms)
Aug 15 19:05:07.959: INFO: (8) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 7.609312ms)
Aug 15 19:05:07.959: INFO: (8) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 7.53732ms)
Aug 15 19:05:07.959: INFO: (8) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 7.680387ms)
Aug 15 19:05:07.960: INFO: (8) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 7.819488ms)
Aug 15 19:05:07.960: INFO: (8) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 7.614304ms)
Aug 15 19:05:07.960: INFO: (8) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 7.704497ms)
Aug 15 19:05:07.960: INFO: (8) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 7.788782ms)
Aug 15 19:05:07.960: INFO: (8) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 7.939225ms)
Aug 15 19:05:07.960: INFO: (8) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 8.569526ms)
Aug 15 19:05:07.961: INFO: (8) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 8.949992ms)
Aug 15 19:05:07.961: INFO: (8) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 8.927141ms)
Aug 15 19:05:07.961: INFO: (8) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 9.644648ms)
Aug 15 19:05:07.961: INFO: (8) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 9.438977ms)
Aug 15 19:05:07.961: INFO: (8) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 9.411237ms)
Aug 15 19:05:07.964: INFO: (9) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 2.851961ms)
Aug 15 19:05:07.965: INFO: (9) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 3.944557ms)
Aug 15 19:05:07.965: INFO: (9) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 3.732982ms)
Aug 15 19:05:07.966: INFO: (9) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 4.396906ms)
Aug 15 19:05:07.966: INFO: (9) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 4.06615ms)
Aug 15 19:05:07.966: INFO: (9) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 4.649217ms)
Aug 15 19:05:07.966: INFO: (9) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 4.830701ms)
Aug 15 19:05:07.966: INFO: (9) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 4.643962ms)
Aug 15 19:05:07.966: INFO: (9) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 5.009077ms)
Aug 15 19:05:07.966: INFO: (9) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 4.639236ms)
Aug 15 19:05:07.968: INFO: (9) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 6.479966ms)
Aug 15 19:05:07.968: INFO: (9) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 6.684034ms)
Aug 15 19:05:07.969: INFO: (9) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 7.304537ms)
Aug 15 19:05:07.969: INFO: (9) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 7.457425ms)
Aug 15 19:05:07.969: INFO: (9) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 7.743638ms)
Aug 15 19:05:07.969: INFO: (9) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 7.838597ms)
Aug 15 19:05:07.973: INFO: (10) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 3.557336ms)
Aug 15 19:05:07.977: INFO: (10) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 7.622735ms)
Aug 15 19:05:07.977: INFO: (10) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 7.464055ms)
Aug 15 19:05:07.978: INFO: (10) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 7.888364ms)
Aug 15 19:05:07.978: INFO: (10) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 7.796733ms)
Aug 15 19:05:07.978: INFO: (10) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 8.123492ms)
Aug 15 19:05:07.978: INFO: (10) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 8.33746ms)
Aug 15 19:05:07.979: INFO: (10) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 8.752632ms)
Aug 15 19:05:07.979: INFO: (10) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 8.920424ms)
Aug 15 19:05:07.979: INFO: (10) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 8.826378ms)
Aug 15 19:05:07.979: INFO: (10) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 8.965578ms)
Aug 15 19:05:07.979: INFO: (10) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 9.33936ms)
Aug 15 19:05:07.979: INFO: (10) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 9.032435ms)
Aug 15 19:05:07.979: INFO: (10) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 9.103116ms)
Aug 15 19:05:07.980: INFO: (10) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 10.049749ms)
Aug 15 19:05:07.981: INFO: (10) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 11.546204ms)
Aug 15 19:05:07.988: INFO: (11) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.312062ms)
Aug 15 19:05:07.989: INFO: (11) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 7.508642ms)
Aug 15 19:05:07.989: INFO: (11) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 7.554191ms)
Aug 15 19:05:07.991: INFO: (11) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 8.79183ms)
Aug 15 19:05:07.991: INFO: (11) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 9.002956ms)
Aug 15 19:05:07.991: INFO: (11) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 9.228879ms)
Aug 15 19:05:07.991: INFO: (11) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 9.032107ms)
Aug 15 19:05:07.991: INFO: (11) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 9.138515ms)
Aug 15 19:05:07.991: INFO: (11) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 9.22116ms)
Aug 15 19:05:07.991: INFO: (11) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 9.56643ms)
Aug 15 19:05:07.992: INFO: (11) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 10.206979ms)
Aug 15 19:05:07.992: INFO: (11) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 10.215502ms)
Aug 15 19:05:07.992: INFO: (11) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 10.467636ms)
Aug 15 19:05:07.992: INFO: (11) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 10.535652ms)
Aug 15 19:05:07.992: INFO: (11) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 10.458781ms)
Aug 15 19:05:07.992: INFO: (11) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 10.630517ms)
Aug 15 19:05:07.997: INFO: (12) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 4.750642ms)
Aug 15 19:05:07.998: INFO: (12) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 5.512444ms)
Aug 15 19:05:07.998: INFO: (12) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 5.97097ms)
Aug 15 19:05:07.999: INFO: (12) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 6.000313ms)
Aug 15 19:05:07.999: INFO: (12) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 5.844751ms)
Aug 15 19:05:07.999: INFO: (12) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 6.390651ms)
Aug 15 19:05:08.000: INFO: (12) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.829618ms)
Aug 15 19:05:08.000: INFO: (12) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 7.228579ms)
Aug 15 19:05:08.000: INFO: (12) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 7.399246ms)
Aug 15 19:05:08.000: INFO: (12) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 7.178129ms)
Aug 15 19:05:08.000: INFO: (12) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 7.998415ms)
Aug 15 19:05:08.000: INFO: (12) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 7.869107ms)
Aug 15 19:05:08.001: INFO: (12) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 8.342201ms)
Aug 15 19:05:08.001: INFO: (12) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 8.348919ms)
Aug 15 19:05:08.001: INFO: (12) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 7.811173ms)
Aug 15 19:05:08.002: INFO: (12) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 9.700313ms)
Aug 15 19:05:08.014: INFO: (13) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 11.836624ms)
Aug 15 19:05:08.014: INFO: (13) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 12.068497ms)
Aug 15 19:05:08.014: INFO: (13) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 11.323756ms)
Aug 15 19:05:08.014: INFO: (13) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 10.974857ms)
Aug 15 19:05:08.014: INFO: (13) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 11.233326ms)
Aug 15 19:05:08.014: INFO: (13) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 11.133899ms)
Aug 15 19:05:08.014: INFO: (13) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 11.461364ms)
Aug 15 19:05:08.014: INFO: (13) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 11.677681ms)
Aug 15 19:05:08.014: INFO: (13) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 10.202411ms)
Aug 15 19:05:08.017: INFO: (13) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 13.155988ms)
Aug 15 19:05:08.017: INFO: (13) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 13.421494ms)
Aug 15 19:05:08.017: INFO: (13) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 12.83405ms)
Aug 15 19:05:08.017: INFO: (13) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 13.121594ms)
Aug 15 19:05:08.020: INFO: (13) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 15.67013ms)
Aug 15 19:05:08.020: INFO: (13) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 16.053522ms)
Aug 15 19:05:08.020: INFO: (13) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 17.13361ms)
Aug 15 19:05:08.025: INFO: (14) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 5.004681ms)
Aug 15 19:05:08.026: INFO: (14) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 5.381323ms)
Aug 15 19:05:08.026: INFO: (14) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 5.44974ms)
Aug 15 19:05:08.026: INFO: (14) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 5.005945ms)
Aug 15 19:05:08.026: INFO: (14) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 6.419305ms)
Aug 15 19:05:08.027: INFO: (14) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 5.407088ms)
Aug 15 19:05:08.027: INFO: (14) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 5.542228ms)
Aug 15 19:05:08.028: INFO: (14) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 6.711235ms)
Aug 15 19:05:08.028: INFO: (14) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.951994ms)
Aug 15 19:05:08.028: INFO: (14) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 6.824897ms)
Aug 15 19:05:08.029: INFO: (14) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 7.654066ms)
Aug 15 19:05:08.029: INFO: (14) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 8.498264ms)
Aug 15 19:05:08.030: INFO: (14) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 9.379007ms)
Aug 15 19:05:08.030: INFO: (14) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 8.871641ms)
Aug 15 19:05:08.030: INFO: (14) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 9.556378ms)
Aug 15 19:05:08.030: INFO: (14) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 9.080023ms)
Aug 15 19:05:08.034: INFO: (15) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 3.48091ms)
Aug 15 19:05:08.036: INFO: (15) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 4.878571ms)
Aug 15 19:05:08.036: INFO: (15) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 5.013873ms)
Aug 15 19:05:08.036: INFO: (15) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 5.434815ms)
Aug 15 19:05:08.036: INFO: (15) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 5.58742ms)
Aug 15 19:05:08.037: INFO: (15) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 5.640817ms)
Aug 15 19:05:08.038: INFO: (15) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.73467ms)
Aug 15 19:05:08.038: INFO: (15) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 7.048702ms)
Aug 15 19:05:08.038: INFO: (15) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 7.819607ms)
Aug 15 19:05:08.038: INFO: (15) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 7.502818ms)
Aug 15 19:05:08.039: INFO: (15) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 7.505076ms)
Aug 15 19:05:08.039: INFO: (15) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 8.2739ms)
Aug 15 19:05:08.041: INFO: (15) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 10.502836ms)
Aug 15 19:05:08.042: INFO: (15) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 10.919016ms)
Aug 15 19:05:08.042: INFO: (15) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 11.106164ms)
Aug 15 19:05:08.042: INFO: (15) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 11.254914ms)
Aug 15 19:05:08.047: INFO: (16) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 4.913915ms)
Aug 15 19:05:08.051: INFO: (16) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 8.694929ms)
Aug 15 19:05:08.051: INFO: (16) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 8.588429ms)
Aug 15 19:05:08.052: INFO: (16) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 9.13437ms)
Aug 15 19:05:08.052: INFO: (16) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 8.91517ms)
Aug 15 19:05:08.052: INFO: (16) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 9.114322ms)
Aug 15 19:05:08.052: INFO: (16) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 9.437071ms)
Aug 15 19:05:08.052: INFO: (16) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 10.012174ms)
Aug 15 19:05:08.052: INFO: (16) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 9.961708ms)
Aug 15 19:05:08.053: INFO: (16) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 9.843089ms)
Aug 15 19:05:08.053: INFO: (16) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 9.762213ms)
Aug 15 19:05:08.053: INFO: (16) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 10.196626ms)
Aug 15 19:05:08.053: INFO: (16) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 10.575262ms)
Aug 15 19:05:08.053: INFO: (16) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 10.718427ms)
Aug 15 19:05:08.054: INFO: (16) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 11.400905ms)
Aug 15 19:05:08.054: INFO: (16) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 11.194072ms)
Aug 15 19:05:08.059: INFO: (17) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 4.189291ms)
Aug 15 19:05:08.059: INFO: (17) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 4.024134ms)
Aug 15 19:05:08.059: INFO: (17) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 4.786272ms)
Aug 15 19:05:08.059: INFO: (17) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 4.545238ms)
Aug 15 19:05:08.060: INFO: (17) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 5.248596ms)
Aug 15 19:05:08.060: INFO: (17) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 5.008676ms)
Aug 15 19:05:08.060: INFO: (17) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 5.69984ms)
Aug 15 19:05:08.061: INFO: (17) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 5.839915ms)
Aug 15 19:05:08.061: INFO: (17) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 5.774122ms)
Aug 15 19:05:08.061: INFO: (17) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 6.146775ms)
Aug 15 19:05:08.061: INFO: (17) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 6.906736ms)
Aug 15 19:05:08.061: INFO: (17) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 7.160305ms)
Aug 15 19:05:08.062: INFO: (17) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 7.791477ms)
Aug 15 19:05:08.062: INFO: (17) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 7.635006ms)
Aug 15 19:05:08.063: INFO: (17) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 7.863071ms)
Aug 15 19:05:08.063: INFO: (17) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 7.976073ms)
Aug 15 19:05:08.067: INFO: (18) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 3.953959ms)
Aug 15 19:05:08.067: INFO: (18) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 4.386935ms)
Aug 15 19:05:08.068: INFO: (18) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 4.28112ms)
Aug 15 19:05:08.068: INFO: (18) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 4.504037ms)
Aug 15 19:05:08.071: INFO: (18) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 6.983162ms)
Aug 15 19:05:08.071: INFO: (18) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 7.309282ms)
Aug 15 19:05:08.071: INFO: (18) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 7.164046ms)
Aug 15 19:05:08.071: INFO: (18) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 7.099155ms)
Aug 15 19:05:08.071: INFO: (18) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 7.104642ms)
Aug 15 19:05:08.071: INFO: (18) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 7.631906ms)
Aug 15 19:05:08.073: INFO: (18) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 8.951856ms)
Aug 15 19:05:08.073: INFO: (18) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 9.129889ms)
Aug 15 19:05:08.073: INFO: (18) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 9.340084ms)
Aug 15 19:05:08.073: INFO: (18) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 10.013845ms)
Aug 15 19:05:08.074: INFO: (18) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 9.948094ms)
Aug 15 19:05:08.074: INFO: (18) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 10.155969ms)
Aug 15 19:05:08.079: INFO: (19) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 4.965374ms)
Aug 15 19:05:08.079: INFO: (19) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 4.462504ms)
Aug 15 19:05:08.081: INFO: (19) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:160/proxy/: foo (200; 6.10608ms)
Aug 15 19:05:08.081: INFO: (19) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:443/proxy/tlsrewritem... (200; 6.039398ms)
Aug 15 19:05:08.082: INFO: (19) /api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/http:proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">... (200; 6.946058ms)
Aug 15 19:05:08.083: INFO: (19) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:462/proxy/: tls qux (200; 7.620837ms)
Aug 15 19:05:08.083: INFO: (19) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:1080/proxy/rewriteme">test<... (200; 7.164628ms)
Aug 15 19:05:08.083: INFO: (19) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname1/proxy/: foo (200; 8.948725ms)
Aug 15 19:05:08.083: INFO: (19) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname1/proxy/: tls baz (200; 8.596834ms)
Aug 15 19:05:08.083: INFO: (19) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/: <a href="/api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx/proxy/rewriteme">test</a> (200; 8.181156ms)
Aug 15 19:05:08.084: INFO: (19) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname1/proxy/: foo (200; 8.905786ms)
Aug 15 19:05:08.084: INFO: (19) /api/v1/namespaces/proxy-1954/services/https:proxy-service-xkblx:tlsportname2/proxy/: tls qux (200; 8.823412ms)
Aug 15 19:05:08.084: INFO: (19) /api/v1/namespaces/proxy-1954/pods/proxy-service-xkblx-lzqmx:162/proxy/: bar (200; 8.619157ms)
Aug 15 19:05:08.084: INFO: (19) /api/v1/namespaces/proxy-1954/pods/https:proxy-service-xkblx-lzqmx:460/proxy/: tls baz (200; 9.163912ms)
Aug 15 19:05:08.084: INFO: (19) /api/v1/namespaces/proxy-1954/services/proxy-service-xkblx:portname2/proxy/: bar (200; 9.855214ms)
Aug 15 19:05:08.085: INFO: (19) /api/v1/namespaces/proxy-1954/services/http:proxy-service-xkblx:portname2/proxy/: bar (200; 10.625671ms)
STEP: deleting ReplicationController proxy-service-xkblx in namespace proxy-1954, will wait for the garbage collector to delete the pods
Aug 15 19:05:08.146: INFO: Deleting ReplicationController proxy-service-xkblx took: 7.505406ms
Aug 15 19:05:10.346: INFO: Terminating ReplicationController proxy-service-xkblx pods took: 2.200304998s
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:05:11.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1954" for this suite.
Aug 15 19:05:17.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:05:17.670: INFO: namespace proxy-1954 deletion completed in 6.118337156s

• [SLOW TEST:20.941 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:05:17.670: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 15 19:05:17.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9901'
Aug 15 19:05:17.802: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 15 19:05:17.802: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Aug 15 19:05:17.811: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Aug 15 19:05:17.820: INFO: scanned /root for discovery docs: <nil>
Aug 15 19:05:17.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9901'
Aug 15 19:05:33.638: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 15 19:05:33.639: INFO: stdout: "Created e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a\nScaling up e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug 15 19:05:33.639: INFO: stdout: "Created e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a\nScaling up e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug 15 19:05:33.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9901'
Aug 15 19:05:33.730: INFO: stderr: ""
Aug 15 19:05:33.730: INFO: stdout: "e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a-xr7j5 "
Aug 15 19:05:33.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a-xr7j5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9901'
Aug 15 19:05:33.811: INFO: stderr: ""
Aug 15 19:05:33.811: INFO: stdout: "true"
Aug 15 19:05:33.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a-xr7j5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9901'
Aug 15 19:05:33.891: INFO: stderr: ""
Aug 15 19:05:33.891: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug 15 19:05:33.891: INFO: e2e-test-nginx-rc-f7126bda69bc19f8c68f80d5fd5ae48a-xr7j5 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Aug 15 19:05:33.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete rc e2e-test-nginx-rc --namespace=kubectl-9901'
Aug 15 19:05:33.978: INFO: stderr: ""
Aug 15 19:05:33.978: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:05:33.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9901" for this suite.
Aug 15 19:05:56.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:05:56.106: INFO: namespace kubectl-9901 deletion completed in 22.122136372s

• [SLOW TEST:38.436 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:05:56.106: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Aug 15 19:05:56.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-4698'
Aug 15 19:05:56.432: INFO: stderr: ""
Aug 15 19:05:56.432: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Aug 15 19:05:57.437: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 19:05:57.437: INFO: Found 0 / 1
Aug 15 19:05:58.437: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 19:05:58.437: INFO: Found 0 / 1
Aug 15 19:05:59.437: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 19:05:59.437: INFO: Found 1 / 1
Aug 15 19:05:59.437: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 15 19:05:59.440: INFO: Selector matched 1 pods for map[app:redis]
Aug 15 19:05:59.440: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug 15 19:05:59.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 logs redis-master-d44hv redis-master --namespace=kubectl-4698'
Aug 15 19:05:59.538: INFO: stderr: ""
Aug 15 19:05:59.538: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 15 Aug 19:05:58.626 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 15 Aug 19:05:58.626 # Server started, Redis version 3.2.12\n1:M 15 Aug 19:05:58.626 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 15 Aug 19:05:58.626 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug 15 19:05:59.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 log redis-master-d44hv redis-master --namespace=kubectl-4698 --tail=1'
Aug 15 19:05:59.633: INFO: stderr: ""
Aug 15 19:05:59.633: INFO: stdout: "1:M 15 Aug 19:05:58.626 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug 15 19:05:59.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 log redis-master-d44hv redis-master --namespace=kubectl-4698 --limit-bytes=1'
Aug 15 19:05:59.724: INFO: stderr: ""
Aug 15 19:05:59.724: INFO: stdout: " "
STEP: exposing timestamps
Aug 15 19:05:59.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 log redis-master-d44hv redis-master --namespace=kubectl-4698 --tail=1 --timestamps'
Aug 15 19:05:59.826: INFO: stderr: ""
Aug 15 19:05:59.826: INFO: stdout: "2019-08-15T19:05:58.626625346Z 1:M 15 Aug 19:05:58.626 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug 15 19:06:02.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 log redis-master-d44hv redis-master --namespace=kubectl-4698 --since=1s'
Aug 15 19:06:02.432: INFO: stderr: ""
Aug 15 19:06:02.432: INFO: stdout: ""
Aug 15 19:06:02.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 log redis-master-d44hv redis-master --namespace=kubectl-4698 --since=24h'
Aug 15 19:06:02.540: INFO: stderr: ""
Aug 15 19:06:02.540: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 15 Aug 19:05:58.626 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 15 Aug 19:05:58.626 # Server started, Redis version 3.2.12\n1:M 15 Aug 19:05:58.626 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 15 Aug 19:05:58.626 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Aug 15 19:06:02.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 delete --grace-period=0 --force -f - --namespace=kubectl-4698'
Aug 15 19:06:02.653: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 15 19:06:02.653: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug 15 19:06:02.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4698'
Aug 15 19:06:02.761: INFO: stderr: "No resources found.\n"
Aug 15 19:06:02.761: INFO: stdout: ""
Aug 15 19:06:02.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -l name=nginx --namespace=kubectl-4698 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 15 19:06:02.853: INFO: stderr: ""
Aug 15 19:06:02.853: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:06:02.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4698" for this suite.
Aug 15 19:06:24.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:06:24.965: INFO: namespace kubectl-4698 deletion completed in 22.10769106s

• [SLOW TEST:28.859 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:06:24.965: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-271cfe19-fe1d-43a6-b531-7170afb6f8a9 in namespace container-probe-6007
Aug 15 19:06:31.022: INFO: Started pod liveness-271cfe19-fe1d-43a6-b531-7170afb6f8a9 in namespace container-probe-6007
STEP: checking the pod's current state and verifying that restartCount is present
Aug 15 19:06:31.026: INFO: Initial restart count of pod liveness-271cfe19-fe1d-43a6-b531-7170afb6f8a9 is 0
Aug 15 19:06:49.068: INFO: Restart count of pod container-probe-6007/liveness-271cfe19-fe1d-43a6-b531-7170afb6f8a9 is now 1 (18.041401164s elapsed)
Aug 15 19:07:09.113: INFO: Restart count of pod container-probe-6007/liveness-271cfe19-fe1d-43a6-b531-7170afb6f8a9 is now 2 (38.086687182s elapsed)
Aug 15 19:07:29.154: INFO: Restart count of pod container-probe-6007/liveness-271cfe19-fe1d-43a6-b531-7170afb6f8a9 is now 3 (58.127837197s elapsed)
Aug 15 19:07:49.204: INFO: Restart count of pod container-probe-6007/liveness-271cfe19-fe1d-43a6-b531-7170afb6f8a9 is now 4 (1m18.177381148s elapsed)
Aug 15 19:08:53.345: INFO: Restart count of pod container-probe-6007/liveness-271cfe19-fe1d-43a6-b531-7170afb6f8a9 is now 5 (2m22.31853602s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:08:53.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6007" for this suite.
Aug 15 19:08:59.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:08:59.492: INFO: namespace container-probe-6007 deletion completed in 6.126750755s

• [SLOW TEST:154.527 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:08:59.492: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-793
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-793 to expose endpoints map[]
Aug 15 19:08:59.542: INFO: Get endpoints failed (5.046779ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Aug 15 19:09:00.547: INFO: successfully validated that service endpoint-test2 in namespace services-793 exposes endpoints map[] (1.00925658s elapsed)
STEP: Creating pod pod1 in namespace services-793
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-793 to expose endpoints map[pod1:[80]]
Aug 15 19:09:03.594: INFO: successfully validated that service endpoint-test2 in namespace services-793 exposes endpoints map[pod1:[80]] (3.034436581s elapsed)
STEP: Creating pod pod2 in namespace services-793
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-793 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 15 19:09:07.676: INFO: successfully validated that service endpoint-test2 in namespace services-793 exposes endpoints map[pod1:[80] pod2:[80]] (4.06502656s elapsed)
STEP: Deleting pod pod1 in namespace services-793
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-793 to expose endpoints map[pod2:[80]]
Aug 15 19:09:08.715: INFO: successfully validated that service endpoint-test2 in namespace services-793 exposes endpoints map[pod2:[80]] (1.032100144s elapsed)
STEP: Deleting pod pod2 in namespace services-793
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-793 to expose endpoints map[]
Aug 15 19:09:08.728: INFO: successfully validated that service endpoint-test2 in namespace services-793 exposes endpoints map[] (4.319201ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:09:08.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-793" for this suite.
Aug 15 19:09:30.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:09:30.892: INFO: namespace services-793 deletion completed in 22.123031856s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:31.400 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:09:30.892: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 15 19:09:30.938: INFO: Waiting up to 5m0s for pod "pod-3b19c940-2b18-4f46-9cfa-095154723595" in namespace "emptydir-4546" to be "success or failure"
Aug 15 19:09:30.943: INFO: Pod "pod-3b19c940-2b18-4f46-9cfa-095154723595": Phase="Pending", Reason="", readiness=false. Elapsed: 4.751179ms
Aug 15 19:09:32.948: INFO: Pod "pod-3b19c940-2b18-4f46-9cfa-095154723595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009134962s
Aug 15 19:09:34.952: INFO: Pod "pod-3b19c940-2b18-4f46-9cfa-095154723595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013824332s
STEP: Saw pod success
Aug 15 19:09:34.952: INFO: Pod "pod-3b19c940-2b18-4f46-9cfa-095154723595" satisfied condition "success or failure"
Aug 15 19:09:34.955: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-1 pod pod-3b19c940-2b18-4f46-9cfa-095154723595 container test-container: <nil>
STEP: delete the pod
Aug 15 19:09:34.978: INFO: Waiting for pod pod-3b19c940-2b18-4f46-9cfa-095154723595 to disappear
Aug 15 19:09:34.981: INFO: Pod pod-3b19c940-2b18-4f46-9cfa-095154723595 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:09:34.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4546" for this suite.
Aug 15 19:09:40.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:09:41.109: INFO: namespace emptydir-4546 deletion completed in 6.124204825s

• [SLOW TEST:10.217 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:09:41.110: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 15 19:09:41.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ec80db3-0674-433b-b33a-4cac504b15f1" in namespace "downward-api-9055" to be "success or failure"
Aug 15 19:09:41.161: INFO: Pod "downwardapi-volume-9ec80db3-0674-433b-b33a-4cac504b15f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509773ms
Aug 15 19:09:43.166: INFO: Pod "downwardapi-volume-9ec80db3-0674-433b-b33a-4cac504b15f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009791601s
Aug 15 19:09:45.170: INFO: Pod "downwardapi-volume-9ec80db3-0674-433b-b33a-4cac504b15f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014040716s
STEP: Saw pod success
Aug 15 19:09:45.170: INFO: Pod "downwardapi-volume-9ec80db3-0674-433b-b33a-4cac504b15f1" satisfied condition "success or failure"
Aug 15 19:09:45.173: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downwardapi-volume-9ec80db3-0674-433b-b33a-4cac504b15f1 container client-container: <nil>
STEP: delete the pod
Aug 15 19:09:45.197: INFO: Waiting for pod downwardapi-volume-9ec80db3-0674-433b-b33a-4cac504b15f1 to disappear
Aug 15 19:09:45.200: INFO: Pod downwardapi-volume-9ec80db3-0674-433b-b33a-4cac504b15f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:09:45.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9055" for this suite.
Aug 15 19:09:51.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:09:51.321: INFO: namespace downward-api-9055 deletion completed in 6.117088814s

• [SLOW TEST:10.211 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:09:51.321: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Aug 15 19:10:01.388: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0815 19:10:01.388336      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 15 19:10:01.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9679" for this suite.
Aug 15 19:10:07.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:10:07.499: INFO: namespace gc-9679 deletion completed in 6.10808217s

• [SLOW TEST:16.178 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:10:07.500: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Aug 15 19:10:07.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 create -f - --namespace=kubectl-4813'
Aug 15 19:10:07.731: INFO: stderr: ""
Aug 15 19:10:07.731: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 15 19:10:07.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4813'
Aug 15 19:10:07.833: INFO: stderr: ""
Aug 15 19:10:07.833: INFO: stdout: "update-demo-nautilus-dqc6p update-demo-nautilus-f4c98 "
Aug 15 19:10:07.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-dqc6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4813'
Aug 15 19:10:07.915: INFO: stderr: ""
Aug 15 19:10:07.915: INFO: stdout: ""
Aug 15 19:10:07.915: INFO: update-demo-nautilus-dqc6p is created but not running
Aug 15 19:10:12.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4813'
Aug 15 19:10:12.997: INFO: stderr: ""
Aug 15 19:10:12.997: INFO: stdout: "update-demo-nautilus-dqc6p update-demo-nautilus-f4c98 "
Aug 15 19:10:12.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-dqc6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4813'
Aug 15 19:10:13.076: INFO: stderr: ""
Aug 15 19:10:13.076: INFO: stdout: "true"
Aug 15 19:10:13.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-dqc6p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4813'
Aug 15 19:10:13.151: INFO: stderr: ""
Aug 15 19:10:13.151: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:10:13.151: INFO: validating pod update-demo-nautilus-dqc6p
Aug 15 19:10:13.155: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:10:13.155: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:10:13.155: INFO: update-demo-nautilus-dqc6p is verified up and running
Aug 15 19:10:13.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-f4c98 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4813'
Aug 15 19:10:13.242: INFO: stderr: ""
Aug 15 19:10:13.242: INFO: stdout: "true"
Aug 15 19:10:13.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-nautilus-f4c98 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4813'
Aug 15 19:10:13.323: INFO: stderr: ""
Aug 15 19:10:13.323: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 15 19:10:13.323: INFO: validating pod update-demo-nautilus-f4c98
Aug 15 19:10:13.328: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 15 19:10:13.328: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 15 19:10:13.328: INFO: update-demo-nautilus-f4c98 is verified up and running
STEP: rolling-update to new replication controller
Aug 15 19:10:13.330: INFO: scanned /root for discovery docs: <nil>
Aug 15 19:10:13.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4813'
Aug 15 19:10:37.786: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 15 19:10:37.786: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 15 19:10:37.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4813'
Aug 15 19:10:37.884: INFO: stderr: ""
Aug 15 19:10:37.884: INFO: stdout: "update-demo-kitten-cqtbq update-demo-kitten-njjhp "
Aug 15 19:10:37.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-kitten-cqtbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4813'
Aug 15 19:10:37.976: INFO: stderr: ""
Aug 15 19:10:37.976: INFO: stdout: "true"
Aug 15 19:10:37.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-kitten-cqtbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4813'
Aug 15 19:10:38.059: INFO: stderr: ""
Aug 15 19:10:38.060: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 15 19:10:38.060: INFO: validating pod update-demo-kitten-cqtbq
Aug 15 19:10:38.064: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 15 19:10:38.064: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 15 19:10:38.064: INFO: update-demo-kitten-cqtbq is verified up and running
Aug 15 19:10:38.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-kitten-njjhp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4813'
Aug 15 19:10:38.147: INFO: stderr: ""
Aug 15 19:10:38.147: INFO: stdout: "true"
Aug 15 19:10:38.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136527297 get pods update-demo-kitten-njjhp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4813'
Aug 15 19:10:38.230: INFO: stderr: ""
Aug 15 19:10:38.230: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 15 19:10:38.230: INFO: validating pod update-demo-kitten-njjhp
Aug 15 19:10:38.235: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 15 19:10:38.235: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 15 19:10:38.235: INFO: update-demo-kitten-njjhp is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:10:38.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4813" for this suite.
Aug 15 19:10:56.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:10:56.370: INFO: namespace kubectl-4813 deletion completed in 18.130713547s

• [SLOW TEST:48.871 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:10:56.370: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-acd2f83e-c029-4166-b850-abb39f639df1
STEP: Creating a pod to test consume secrets
Aug 15 19:10:56.425: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d97d9e2-e499-4085-bcac-bd1addd2cf74" in namespace "projected-1760" to be "success or failure"
Aug 15 19:10:56.440: INFO: Pod "pod-projected-secrets-5d97d9e2-e499-4085-bcac-bd1addd2cf74": Phase="Pending", Reason="", readiness=false. Elapsed: 15.339354ms
Aug 15 19:10:58.446: INFO: Pod "pod-projected-secrets-5d97d9e2-e499-4085-bcac-bd1addd2cf74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021589431s
Aug 15 19:11:00.450: INFO: Pod "pod-projected-secrets-5d97d9e2-e499-4085-bcac-bd1addd2cf74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025664452s
STEP: Saw pod success
Aug 15 19:11:00.451: INFO: Pod "pod-projected-secrets-5d97d9e2-e499-4085-bcac-bd1addd2cf74" satisfied condition "success or failure"
Aug 15 19:11:00.453: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-0 pod pod-projected-secrets-5d97d9e2-e499-4085-bcac-bd1addd2cf74 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 15 19:11:00.474: INFO: Waiting for pod pod-projected-secrets-5d97d9e2-e499-4085-bcac-bd1addd2cf74 to disappear
Aug 15 19:11:00.478: INFO: Pod pod-projected-secrets-5d97d9e2-e499-4085-bcac-bd1addd2cf74 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:11:00.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1760" for this suite.
Aug 15 19:11:06.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:11:06.596: INFO: namespace projected-1760 deletion completed in 6.113288057s

• [SLOW TEST:10.225 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:11:06.596: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 15 19:11:06.644: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:11:08.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6093" for this suite.
Aug 15 19:11:14.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:11:14.784: INFO: namespace replication-controller-6093 deletion completed in 6.106071733s

• [SLOW TEST:8.188 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:11:14.784: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-fe0817ef-44e5-489e-8bb5-e9ed969c715d in namespace container-probe-6372
Aug 15 19:11:18.841: INFO: Started pod liveness-fe0817ef-44e5-489e-8bb5-e9ed969c715d in namespace container-probe-6372
STEP: checking the pod's current state and verifying that restartCount is present
Aug 15 19:11:18.844: INFO: Initial restart count of pod liveness-fe0817ef-44e5-489e-8bb5-e9ed969c715d is 0
Aug 15 19:11:42.895: INFO: Restart count of pod container-probe-6372/liveness-fe0817ef-44e5-489e-8bb5-e9ed969c715d is now 1 (24.05067462s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:11:42.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6372" for this suite.
Aug 15 19:11:48.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:11:49.032: INFO: namespace container-probe-6372 deletion completed in 6.118924459s

• [SLOW TEST:34.248 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:11:49.032: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 15 19:11:49.075: INFO: Waiting up to 5m0s for pod "downward-api-773c190d-1499-46f4-8f95-41a3700ec81a" in namespace "downward-api-5590" to be "success or failure"
Aug 15 19:11:49.082: INFO: Pod "downward-api-773c190d-1499-46f4-8f95-41a3700ec81a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.779262ms
Aug 15 19:11:51.088: INFO: Pod "downward-api-773c190d-1499-46f4-8f95-41a3700ec81a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013120602s
Aug 15 19:11:53.092: INFO: Pod "downward-api-773c190d-1499-46f4-8f95-41a3700ec81a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017084576s
STEP: Saw pod success
Aug 15 19:11:53.092: INFO: Pod "downward-api-773c190d-1499-46f4-8f95-41a3700ec81a" satisfied condition "success or failure"
Aug 15 19:11:53.095: INFO: Trying to get logs from node karbon-fifteen-3efeed-k8s-worker-2 pod downward-api-773c190d-1499-46f4-8f95-41a3700ec81a container dapi-container: <nil>
STEP: delete the pod
Aug 15 19:11:53.117: INFO: Waiting for pod downward-api-773c190d-1499-46f4-8f95-41a3700ec81a to disappear
Aug 15 19:11:53.120: INFO: Pod downward-api-773c190d-1499-46f4-8f95-41a3700ec81a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:11:53.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5590" for this suite.
Aug 15 19:11:59.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:11:59.230: INFO: namespace downward-api-5590 deletion completed in 6.104982411s

• [SLOW TEST:10.197 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 15 19:11:59.230: INFO: >>> kubeConfig: /tmp/kubeconfig-136527297
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 15 19:11:59.272: INFO: (0) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.137351ms)
Aug 15 19:11:59.276: INFO: (1) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.127255ms)
Aug 15 19:11:59.282: INFO: (2) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 5.290888ms)
Aug 15 19:11:59.286: INFO: (3) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.367446ms)
Aug 15 19:11:59.290: INFO: (4) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.574225ms)
Aug 15 19:11:59.293: INFO: (5) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.161419ms)
Aug 15 19:11:59.296: INFO: (6) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.399976ms)
Aug 15 19:11:59.300: INFO: (7) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.612036ms)
Aug 15 19:11:59.304: INFO: (8) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.791396ms)
Aug 15 19:11:59.307: INFO: (9) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.66251ms)
Aug 15 19:11:59.311: INFO: (10) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.448135ms)
Aug 15 19:11:59.315: INFO: (11) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.722638ms)
Aug 15 19:11:59.318: INFO: (12) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.304097ms)
Aug 15 19:11:59.321: INFO: (13) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.35528ms)
Aug 15 19:11:59.325: INFO: (14) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.062875ms)
Aug 15 19:11:59.328: INFO: (15) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.175629ms)
Aug 15 19:11:59.331: INFO: (16) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.333211ms)
Aug 15 19:11:59.335: INFO: (17) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.835606ms)
Aug 15 19:11:59.339: INFO: (18) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.235792ms)
Aug 15 19:11:59.343: INFO: (19) /api/v1/nodes/karbon-fifteen-3efeed-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.844534ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 15 19:11:59.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1603" for this suite.
Aug 15 19:12:05.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 15 19:12:05.466: INFO: namespace proxy-1603 deletion completed in 6.118699533s

• [SLOW TEST:6.236 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSAug 15 19:12:05.466: INFO: Running AfterSuite actions on all nodes
Aug 15 19:12:05.466: INFO: Running AfterSuite actions on node 1
Aug 15 19:12:05.466: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 5815.156 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h36m56.757306272s
Test Suite Passed
