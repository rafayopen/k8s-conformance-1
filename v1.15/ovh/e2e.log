I0713 09:03:47.201354      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-018000938
I0713 09:03:47.201545      15 e2e.go:241] Starting e2e run "b3114902-d1d3-439d-8891-9b6137dadfcb" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1563008626 - Will randomize all specs
Will run 215 of 4411 specs

Jul 13 09:03:47.415: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 09:03:47.416: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 13 09:03:47.474: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 13 09:03:47.539: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 13 09:03:47.539: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jul 13 09:03:47.539: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 13 09:03:47.560: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Jul 13 09:03:47.560: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 13 09:03:47.560: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'wormhole' (0 seconds elapsed)
Jul 13 09:03:47.560: INFO: e2e test version: v1.15.0
Jul 13 09:03:47.565: INFO: kube-apiserver version: v1.15.0
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:03:47.568: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
Jul 13 09:03:47.717: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 13 09:03:47.745: INFO: Waiting up to 5m0s for pod "downward-api-262570b4-d9ba-47fa-b757-412ea89bc52e" in namespace "downward-api-8242" to be "success or failure"
Jul 13 09:03:47.760: INFO: Pod "downward-api-262570b4-d9ba-47fa-b757-412ea89bc52e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.173107ms
Jul 13 09:03:49.771: INFO: Pod "downward-api-262570b4-d9ba-47fa-b757-412ea89bc52e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026139291s
Jul 13 09:03:51.778: INFO: Pod "downward-api-262570b4-d9ba-47fa-b757-412ea89bc52e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032996705s
STEP: Saw pod success
Jul 13 09:03:51.778: INFO: Pod "downward-api-262570b4-d9ba-47fa-b757-412ea89bc52e" satisfied condition "success or failure"
Jul 13 09:03:51.786: INFO: Trying to get logs from node cncf-1-15 pod downward-api-262570b4-d9ba-47fa-b757-412ea89bc52e container dapi-container: <nil>
STEP: delete the pod
Jul 13 09:03:51.946: INFO: Waiting for pod downward-api-262570b4-d9ba-47fa-b757-412ea89bc52e to disappear
Jul 13 09:03:51.954: INFO: Pod downward-api-262570b4-d9ba-47fa-b757-412ea89bc52e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:03:51.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8242" for this suite.
Jul 13 09:03:57.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:03:58.304: INFO: namespace downward-api-8242 deletion completed in 6.341175331s

• [SLOW TEST:10.737 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:03:58.306: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-ca0e14ee-b1a2-4b33-8dc7-d88538c58649
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:03:58.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2224" for this suite.
Jul 13 09:04:04.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:04:04.854: INFO: namespace configmap-2224 deletion completed in 6.380801617s

• [SLOW TEST:6.548 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:04:04.855: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:04:09.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2513" for this suite.
Jul 13 09:05:05.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:05:05.421: INFO: namespace kubelet-test-2513 deletion completed in 56.377942067s

• [SLOW TEST:60.566 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:05:05.422: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:05:11.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2300" for this suite.
Jul 13 09:05:17.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:05:18.211: INFO: namespace namespaces-2300 deletion completed in 6.359675224s
STEP: Destroying namespace "nsdeletetest-8400" for this suite.
Jul 13 09:05:18.220: INFO: Namespace nsdeletetest-8400 was already deleted
STEP: Destroying namespace "nsdeletetest-3120" for this suite.
Jul 13 09:05:24.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:05:24.566: INFO: namespace nsdeletetest-3120 deletion completed in 6.345754985s

• [SLOW TEST:19.144 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:05:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 09:05:24.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-444386e6-a9f1-41b7-b487-66f9570a2759" in namespace "projected-2340" to be "success or failure"
Jul 13 09:05:24.766: INFO: Pod "downwardapi-volume-444386e6-a9f1-41b7-b487-66f9570a2759": Phase="Pending", Reason="", readiness=false. Elapsed: 8.963524ms
Jul 13 09:05:26.774: INFO: Pod "downwardapi-volume-444386e6-a9f1-41b7-b487-66f9570a2759": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017764351s
Jul 13 09:05:28.787: INFO: Pod "downwardapi-volume-444386e6-a9f1-41b7-b487-66f9570a2759": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030378159s
STEP: Saw pod success
Jul 13 09:05:28.787: INFO: Pod "downwardapi-volume-444386e6-a9f1-41b7-b487-66f9570a2759" satisfied condition "success or failure"
Jul 13 09:05:28.798: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-444386e6-a9f1-41b7-b487-66f9570a2759 container client-container: <nil>
STEP: delete the pod
Jul 13 09:05:28.903: INFO: Waiting for pod downwardapi-volume-444386e6-a9f1-41b7-b487-66f9570a2759 to disappear
Jul 13 09:05:28.911: INFO: Pod downwardapi-volume-444386e6-a9f1-41b7-b487-66f9570a2759 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:05:28.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2340" for this suite.
Jul 13 09:05:34.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:05:35.293: INFO: namespace projected-2340 deletion completed in 6.373333231s

• [SLOW TEST:10.726 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:05:35.297: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 13 09:05:35.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9695'
Jul 13 09:05:35.852: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 09:05:35.852: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jul 13 09:05:35.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete jobs e2e-test-nginx-job --namespace=kubectl-9695'
Jul 13 09:05:36.041: INFO: stderr: ""
Jul 13 09:05:36.041: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:05:36.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9695" for this suite.
Jul 13 09:05:42.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:05:42.411: INFO: namespace kubectl-9695 deletion completed in 6.355877966s

• [SLOW TEST:7.114 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:05:42.411: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 13 09:05:42.594: INFO: Waiting up to 5m0s for pod "pod-d95715cc-d70e-4365-80a9-a10594690146" in namespace "emptydir-2694" to be "success or failure"
Jul 13 09:05:42.609: INFO: Pod "pod-d95715cc-d70e-4365-80a9-a10594690146": Phase="Pending", Reason="", readiness=false. Elapsed: 14.835311ms
Jul 13 09:05:44.622: INFO: Pod "pod-d95715cc-d70e-4365-80a9-a10594690146": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027876151s
Jul 13 09:05:46.633: INFO: Pod "pod-d95715cc-d70e-4365-80a9-a10594690146": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038450845s
STEP: Saw pod success
Jul 13 09:05:46.633: INFO: Pod "pod-d95715cc-d70e-4365-80a9-a10594690146" satisfied condition "success or failure"
Jul 13 09:05:46.642: INFO: Trying to get logs from node cncf-1-15 pod pod-d95715cc-d70e-4365-80a9-a10594690146 container test-container: <nil>
STEP: delete the pod
Jul 13 09:05:46.693: INFO: Waiting for pod pod-d95715cc-d70e-4365-80a9-a10594690146 to disappear
Jul 13 09:05:46.712: INFO: Pod pod-d95715cc-d70e-4365-80a9-a10594690146 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:05:46.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2694" for this suite.
Jul 13 09:05:52.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:05:53.051: INFO: namespace emptydir-2694 deletion completed in 6.329517921s

• [SLOW TEST:10.640 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:05:53.051: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5599/configmap-test-2807493c-62f9-478d-88bf-1e68370c8890
STEP: Creating a pod to test consume configMaps
Jul 13 09:05:53.204: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2f1c785-b5f3-4d70-ab49-9a170584475b" in namespace "configmap-5599" to be "success or failure"
Jul 13 09:05:53.212: INFO: Pod "pod-configmaps-b2f1c785-b5f3-4d70-ab49-9a170584475b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.778064ms
Jul 13 09:05:55.223: INFO: Pod "pod-configmaps-b2f1c785-b5f3-4d70-ab49-9a170584475b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01901935s
Jul 13 09:05:57.234: INFO: Pod "pod-configmaps-b2f1c785-b5f3-4d70-ab49-9a170584475b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02963985s
STEP: Saw pod success
Jul 13 09:05:57.234: INFO: Pod "pod-configmaps-b2f1c785-b5f3-4d70-ab49-9a170584475b" satisfied condition "success or failure"
Jul 13 09:05:57.242: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-b2f1c785-b5f3-4d70-ab49-9a170584475b container env-test: <nil>
STEP: delete the pod
Jul 13 09:05:57.330: INFO: Waiting for pod pod-configmaps-b2f1c785-b5f3-4d70-ab49-9a170584475b to disappear
Jul 13 09:05:57.337: INFO: Pod pod-configmaps-b2f1c785-b5f3-4d70-ab49-9a170584475b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:05:57.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5599" for this suite.
Jul 13 09:06:03.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:06:03.722: INFO: namespace configmap-5599 deletion completed in 6.376689359s

• [SLOW TEST:10.672 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:06:03.725: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:06:03.918: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e315a9d1-4197-4bd8-ac12-4dee814c5163", Controller:(*bool)(0xc002f484b2), BlockOwnerDeletion:(*bool)(0xc002f484b3)}}
Jul 13 09:06:03.930: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4b5c18cd-1ed1-473c-b139-53232f61c6fe", Controller:(*bool)(0xc002c6e0ca), BlockOwnerDeletion:(*bool)(0xc002c6e0cb)}}
Jul 13 09:06:03.944: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f565915f-8f70-411c-8fb0-430f1f5748f9", Controller:(*bool)(0xc002f4879a), BlockOwnerDeletion:(*bool)(0xc002f4879b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:06:08.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5980" for this suite.
Jul 13 09:06:15.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:06:15.346: INFO: namespace gc-5980 deletion completed in 6.369217409s

• [SLOW TEST:11.621 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:06:15.349: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 13 09:06:15.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-835,SelfLink:/api/v1/namespaces/watch-835/configmaps/e2e-watch-test-label-changed,UID:4bac1466-8e63-47cc-bdb7-d0a5bae3bf88,ResourceVersion:223476005,Generation:0,CreationTimestamp:2019-07-13 09:06:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 09:06:15.533: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-835,SelfLink:/api/v1/namespaces/watch-835/configmaps/e2e-watch-test-label-changed,UID:4bac1466-8e63-47cc-bdb7-d0a5bae3bf88,ResourceVersion:223476006,Generation:0,CreationTimestamp:2019-07-13 09:06:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 13 09:06:15.534: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-835,SelfLink:/api/v1/namespaces/watch-835/configmaps/e2e-watch-test-label-changed,UID:4bac1466-8e63-47cc-bdb7-d0a5bae3bf88,ResourceVersion:223476007,Generation:0,CreationTimestamp:2019-07-13 09:06:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 13 09:06:25.629: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-835,SelfLink:/api/v1/namespaces/watch-835/configmaps/e2e-watch-test-label-changed,UID:4bac1466-8e63-47cc-bdb7-d0a5bae3bf88,ResourceVersion:223476084,Generation:0,CreationTimestamp:2019-07-13 09:06:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 09:06:25.629: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-835,SelfLink:/api/v1/namespaces/watch-835/configmaps/e2e-watch-test-label-changed,UID:4bac1466-8e63-47cc-bdb7-d0a5bae3bf88,ResourceVersion:223476085,Generation:0,CreationTimestamp:2019-07-13 09:06:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 13 09:06:25.629: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-835,SelfLink:/api/v1/namespaces/watch-835/configmaps/e2e-watch-test-label-changed,UID:4bac1466-8e63-47cc-bdb7-d0a5bae3bf88,ResourceVersion:223476086,Generation:0,CreationTimestamp:2019-07-13 09:06:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:06:25.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-835" for this suite.
Jul 13 09:06:31.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:06:32.029: INFO: namespace watch-835 deletion completed in 6.390627456s

• [SLOW TEST:16.681 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:06:32.032: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 13 09:06:32.176: INFO: Waiting up to 5m0s for pod "pod-0a137094-0635-486a-bc3d-57719db4819c" in namespace "emptydir-3836" to be "success or failure"
Jul 13 09:06:32.184: INFO: Pod "pod-0a137094-0635-486a-bc3d-57719db4819c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.6083ms
Jul 13 09:06:34.194: INFO: Pod "pod-0a137094-0635-486a-bc3d-57719db4819c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017525749s
Jul 13 09:06:36.204: INFO: Pod "pod-0a137094-0635-486a-bc3d-57719db4819c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027483167s
STEP: Saw pod success
Jul 13 09:06:36.204: INFO: Pod "pod-0a137094-0635-486a-bc3d-57719db4819c" satisfied condition "success or failure"
Jul 13 09:06:36.212: INFO: Trying to get logs from node cncf-1-15 pod pod-0a137094-0635-486a-bc3d-57719db4819c container test-container: <nil>
STEP: delete the pod
Jul 13 09:06:36.259: INFO: Waiting for pod pod-0a137094-0635-486a-bc3d-57719db4819c to disappear
Jul 13 09:06:36.265: INFO: Pod pod-0a137094-0635-486a-bc3d-57719db4819c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:06:36.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3836" for this suite.
Jul 13 09:06:42.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:06:42.636: INFO: namespace emptydir-3836 deletion completed in 6.361973618s

• [SLOW TEST:10.604 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:06:42.636: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 13 09:06:46.829: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-e721489c-0642-4cc4-b56d-65116e3d2321,GenerateName:,Namespace:events-978,SelfLink:/api/v1/namespaces/events-978/pods/send-events-e721489c-0642-4cc4-b56d-65116e3d2321,UID:70d47b56-0b0d-41d9-be68-48304b533a35,ResourceVersion:223476282,Generation:0,CreationTimestamp:2019-07-13 09:06:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 767900898,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.11/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rnbdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rnbdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-rnbdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002f2af80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002f2afa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 09:06:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 09:06:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 09:06:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 09:06:42 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:10.2.1.11,StartTime:2019-07-13 09:06:42 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-13 09:06:45 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://e22ecc325831a45458269aab43bf26dcc5834b61cd742b74ef49808a14213797}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul 13 09:06:48.841: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 13 09:06:50.854: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:06:50.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-978" for this suite.
Jul 13 09:07:30.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:07:31.403: INFO: namespace events-978 deletion completed in 40.489556032s

• [SLOW TEST:48.767 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:07:31.405: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul 13 09:07:35.614: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-018000938 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 13 09:07:50.803: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:07:50.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8363" for this suite.
Jul 13 09:07:56.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:07:57.179: INFO: namespace pods-8363 deletion completed in 6.354205887s

• [SLOW TEST:25.774 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:07:57.180: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-723607ab-a85c-45a7-b2fc-9dcd99e31b38
STEP: Creating a pod to test consume secrets
Jul 13 09:07:57.336: INFO: Waiting up to 5m0s for pod "pod-secrets-db8888db-1b52-41f1-b5b3-d523bf6f5910" in namespace "secrets-1926" to be "success or failure"
Jul 13 09:07:57.348: INFO: Pod "pod-secrets-db8888db-1b52-41f1-b5b3-d523bf6f5910": Phase="Pending", Reason="", readiness=false. Elapsed: 11.203999ms
Jul 13 09:07:59.365: INFO: Pod "pod-secrets-db8888db-1b52-41f1-b5b3-d523bf6f5910": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028284811s
Jul 13 09:08:01.375: INFO: Pod "pod-secrets-db8888db-1b52-41f1-b5b3-d523bf6f5910": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038440111s
STEP: Saw pod success
Jul 13 09:08:01.375: INFO: Pod "pod-secrets-db8888db-1b52-41f1-b5b3-d523bf6f5910" satisfied condition "success or failure"
Jul 13 09:08:01.406: INFO: Trying to get logs from node cncf-1-15 pod pod-secrets-db8888db-1b52-41f1-b5b3-d523bf6f5910 container secret-env-test: <nil>
STEP: delete the pod
Jul 13 09:08:01.498: INFO: Waiting for pod pod-secrets-db8888db-1b52-41f1-b5b3-d523bf6f5910 to disappear
Jul 13 09:08:01.506: INFO: Pod pod-secrets-db8888db-1b52-41f1-b5b3-d523bf6f5910 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:08:01.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1926" for this suite.
Jul 13 09:08:07.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:08:07.875: INFO: namespace secrets-1926 deletion completed in 6.359341498s

• [SLOW TEST:10.696 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:08:07.876: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jul 13 09:08:08.019: INFO: Waiting up to 5m0s for pod "client-containers-fbb0729e-746d-442b-94e8-0d945910b600" in namespace "containers-3997" to be "success or failure"
Jul 13 09:08:08.027: INFO: Pod "client-containers-fbb0729e-746d-442b-94e8-0d945910b600": Phase="Pending", Reason="", readiness=false. Elapsed: 7.837737ms
Jul 13 09:08:10.036: INFO: Pod "client-containers-fbb0729e-746d-442b-94e8-0d945910b600": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016745786s
Jul 13 09:08:12.047: INFO: Pod "client-containers-fbb0729e-746d-442b-94e8-0d945910b600": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028209868s
STEP: Saw pod success
Jul 13 09:08:12.047: INFO: Pod "client-containers-fbb0729e-746d-442b-94e8-0d945910b600" satisfied condition "success or failure"
Jul 13 09:08:12.056: INFO: Trying to get logs from node cncf-1-15 pod client-containers-fbb0729e-746d-442b-94e8-0d945910b600 container test-container: <nil>
STEP: delete the pod
Jul 13 09:08:12.108: INFO: Waiting for pod client-containers-fbb0729e-746d-442b-94e8-0d945910b600 to disappear
Jul 13 09:08:12.117: INFO: Pod client-containers-fbb0729e-746d-442b-94e8-0d945910b600 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:08:12.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3997" for this suite.
Jul 13 09:08:18.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:08:18.503: INFO: namespace containers-3997 deletion completed in 6.377370706s

• [SLOW TEST:10.627 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:08:18.505: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-45813bee-80f4-4505-be33-8dc7c0c9e00a
STEP: Creating a pod to test consume configMaps
Jul 13 09:08:18.669: INFO: Waiting up to 5m0s for pod "pod-configmaps-93ecde94-57fb-4f5a-b35f-042d93a1f2a4" in namespace "configmap-8535" to be "success or failure"
Jul 13 09:08:18.682: INFO: Pod "pod-configmaps-93ecde94-57fb-4f5a-b35f-042d93a1f2a4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.893195ms
Jul 13 09:08:20.693: INFO: Pod "pod-configmaps-93ecde94-57fb-4f5a-b35f-042d93a1f2a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023396942s
Jul 13 09:08:22.704: INFO: Pod "pod-configmaps-93ecde94-57fb-4f5a-b35f-042d93a1f2a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034579764s
STEP: Saw pod success
Jul 13 09:08:22.704: INFO: Pod "pod-configmaps-93ecde94-57fb-4f5a-b35f-042d93a1f2a4" satisfied condition "success or failure"
Jul 13 09:08:22.715: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-93ecde94-57fb-4f5a-b35f-042d93a1f2a4 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 09:08:22.773: INFO: Waiting for pod pod-configmaps-93ecde94-57fb-4f5a-b35f-042d93a1f2a4 to disappear
Jul 13 09:08:22.782: INFO: Pod pod-configmaps-93ecde94-57fb-4f5a-b35f-042d93a1f2a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:08:22.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8535" for this suite.
Jul 13 09:08:28.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:08:29.186: INFO: namespace configmap-8535 deletion completed in 6.39507982s

• [SLOW TEST:10.681 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:08:29.187: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 13 09:08:29.299: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 13 09:08:29.322: INFO: Waiting for terminating namespaces to be deleted...
Jul 13 09:08:29.331: INFO: 
Logging pods the kubelet thinks is on node cncf-1-15 before test
Jul 13 09:08:29.350: INFO: canal-r8n6w from kube-system started at 2019-07-13 08:59:39 +0000 UTC (2 container statuses recorded)
Jul 13 09:08:29.351: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 09:08:29.351: INFO: 	Container kube-flannel ready: true, restart count 1
Jul 13 09:08:29.351: INFO: kube-proxy-qsskh from kube-system started at 2019-07-13 08:59:39 +0000 UTC (1 container statuses recorded)
Jul 13 09:08:29.351: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 13 09:08:29.351: INFO: wormhole-svb5x from kube-system started at 2019-07-13 08:59:58 +0000 UTC (1 container statuses recorded)
Jul 13 09:08:29.351: INFO: 	Container wormhole ready: true, restart count 0
Jul 13 09:08:29.351: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-13 09:03:04 +0000 UTC (1 container statuses recorded)
Jul 13 09:08:29.351: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 13 09:08:29.351: INFO: sonobuoy-e2e-job-5b31513983fe4ca4 from heptio-sonobuoy started at 2019-07-13 09:03:10 +0000 UTC (2 container statuses recorded)
Jul 13 09:08:29.351: INFO: 	Container e2e ready: true, restart count 0
Jul 13 09:08:29.351: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 09:08:29.351: INFO: sonobuoy-systemd-logs-daemon-set-9454bcfaa70145b2-zks8j from heptio-sonobuoy started at 2019-07-13 09:03:10 +0000 UTC (2 container statuses recorded)
Jul 13 09:08:29.351: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 09:08:29.351: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 09:08:29.351: INFO: 
Logging pods the kubelet thinks is on node cncf-1-15-2 before test
Jul 13 09:08:29.461: INFO: metrics-server-6dd5674bf7-gjr4b from kube-system started at 2019-07-13 08:58:38 +0000 UTC (1 container statuses recorded)
Jul 13 09:08:29.461: INFO: 	Container metrics-server ready: true, restart count 0
Jul 13 09:08:29.461: INFO: wormhole-62ggl from kube-system started at 2019-07-13 08:58:38 +0000 UTC (1 container statuses recorded)
Jul 13 09:08:29.461: INFO: 	Container wormhole ready: true, restart count 0
Jul 13 09:08:29.461: INFO: kube-dns-autoscaler-67595559f7-vj26r from kube-system started at 2019-07-13 08:58:43 +0000 UTC (1 container statuses recorded)
Jul 13 09:08:29.461: INFO: 	Container autoscaler ready: true, restart count 0
Jul 13 09:08:29.461: INFO: kube-dns-b74d847f-8tlpd from kube-system started at 2019-07-13 08:58:43 +0000 UTC (3 container statuses recorded)
Jul 13 09:08:29.461: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 13 09:08:29.461: INFO: 	Container kubedns ready: true, restart count 0
Jul 13 09:08:29.461: INFO: 	Container sidecar ready: true, restart count 0
Jul 13 09:08:29.461: INFO: kube-dns-b74d847f-hr65v from kube-system started at 2019-07-13 08:59:44 +0000 UTC (3 container statuses recorded)
Jul 13 09:08:29.461: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 13 09:08:29.461: INFO: 	Container kubedns ready: true, restart count 0
Jul 13 09:08:29.461: INFO: 	Container sidecar ready: true, restart count 0
Jul 13 09:08:29.461: INFO: sonobuoy-systemd-logs-daemon-set-9454bcfaa70145b2-gdq8g from heptio-sonobuoy started at 2019-07-13 09:03:10 +0000 UTC (2 container statuses recorded)
Jul 13 09:08:29.461: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 09:08:29.461: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 09:08:29.461: INFO: canal-7hs5h from kube-system started at 2019-07-13 08:58:28 +0000 UTC (2 container statuses recorded)
Jul 13 09:08:29.461: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 09:08:29.461: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 13 09:08:29.461: INFO: kube-proxy-spjhx from kube-system started at 2019-07-13 08:58:28 +0000 UTC (1 container statuses recorded)
Jul 13 09:08:29.461: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node cncf-1-15
STEP: verifying the node has the label node cncf-1-15-2
Jul 13 09:08:29.578: INFO: Pod sonobuoy requesting resource cpu=0m on Node cncf-1-15
Jul 13 09:08:29.578: INFO: Pod sonobuoy-e2e-job-5b31513983fe4ca4 requesting resource cpu=0m on Node cncf-1-15
Jul 13 09:08:29.578: INFO: Pod sonobuoy-systemd-logs-daemon-set-9454bcfaa70145b2-gdq8g requesting resource cpu=0m on Node cncf-1-15-2
Jul 13 09:08:29.578: INFO: Pod sonobuoy-systemd-logs-daemon-set-9454bcfaa70145b2-zks8j requesting resource cpu=0m on Node cncf-1-15
Jul 13 09:08:29.578: INFO: Pod canal-7hs5h requesting resource cpu=250m on Node cncf-1-15-2
Jul 13 09:08:29.578: INFO: Pod canal-r8n6w requesting resource cpu=250m on Node cncf-1-15
Jul 13 09:08:29.578: INFO: Pod kube-dns-autoscaler-67595559f7-vj26r requesting resource cpu=20m on Node cncf-1-15-2
Jul 13 09:08:29.578: INFO: Pod kube-dns-b74d847f-8tlpd requesting resource cpu=260m on Node cncf-1-15-2
Jul 13 09:08:29.578: INFO: Pod kube-dns-b74d847f-hr65v requesting resource cpu=260m on Node cncf-1-15-2
Jul 13 09:08:29.578: INFO: Pod kube-proxy-qsskh requesting resource cpu=100m on Node cncf-1-15
Jul 13 09:08:29.578: INFO: Pod kube-proxy-spjhx requesting resource cpu=100m on Node cncf-1-15-2
Jul 13 09:08:29.578: INFO: Pod metrics-server-6dd5674bf7-gjr4b requesting resource cpu=0m on Node cncf-1-15-2
Jul 13 09:08:29.578: INFO: Pod wormhole-62ggl requesting resource cpu=0m on Node cncf-1-15-2
Jul 13 09:08:29.578: INFO: Pod wormhole-svb5x requesting resource cpu=0m on Node cncf-1-15
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d4e67502-866f-4df5-9b45-485445baaec4.15b0ec52768cdbd1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7322/filler-pod-d4e67502-866f-4df5-9b45-485445baaec4 to cncf-1-15-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d4e67502-866f-4df5-9b45-485445baaec4.15b0ec52a86f6e33], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d4e67502-866f-4df5-9b45-485445baaec4.15b0ec52d0759cfe], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d4e67502-866f-4df5-9b45-485445baaec4.15b0ec52d43f3794], Reason = [Created], Message = [Created container filler-pod-d4e67502-866f-4df5-9b45-485445baaec4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d4e67502-866f-4df5-9b45-485445baaec4.15b0ec52ddecb86f], Reason = [Started], Message = [Started container filler-pod-d4e67502-866f-4df5-9b45-485445baaec4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5b3831c-fb9d-4702-a701-d24c53168e16.15b0ec5275b1ada5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7322/filler-pod-f5b3831c-fb9d-4702-a701-d24c53168e16 to cncf-1-15]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5b3831c-fb9d-4702-a701-d24c53168e16.15b0ec52a3aecaad], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5b3831c-fb9d-4702-a701-d24c53168e16.15b0ec52ca3d56ed], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5b3831c-fb9d-4702-a701-d24c53168e16.15b0ec52cf4f5f86], Reason = [Created], Message = [Created container filler-pod-f5b3831c-fb9d-4702-a701-d24c53168e16]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f5b3831c-fb9d-4702-a701-d24c53168e16.15b0ec52d7099edc], Reason = [Started], Message = [Started container filler-pod-f5b3831c-fb9d-4702-a701-d24c53168e16]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b0ec536a5714d8], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node cncf-1-15
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cncf-1-15-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:08:34.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7322" for this suite.
Jul 13 09:08:40.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:08:41.213: INFO: namespace sched-pred-7322 deletion completed in 6.38229952s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:12.027 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:08:41.214: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 13 09:08:47.403: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:08:47.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0713 09:08:47.403867      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-46" for this suite.
Jul 13 09:08:55.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:08:55.742: INFO: namespace gc-46 deletion completed in 8.330903214s

• [SLOW TEST:14.528 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:08:55.743: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-8a8c4228-e6ce-40ca-bf97-ab043711720f
STEP: Creating secret with name s-test-opt-upd-2adedcd4-f4bc-4718-b7df-24ddba9a974a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8a8c4228-e6ce-40ca-bf97-ab043711720f
STEP: Updating secret s-test-opt-upd-2adedcd4-f4bc-4718-b7df-24ddba9a974a
STEP: Creating secret with name s-test-opt-create-1c9fc494-4071-402a-b5f3-64a1c182cb8c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:10:07.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6489" for this suite.
Jul 13 09:10:31.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:10:31.524: INFO: namespace projected-6489 deletion completed in 24.419718694s

• [SLOW TEST:95.781 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:10:31.528: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 13 09:10:31.676: INFO: Waiting up to 5m0s for pod "downward-api-f69d97d7-6d46-4784-9ef2-b4fb1b0dfdf0" in namespace "downward-api-4976" to be "success or failure"
Jul 13 09:10:31.684: INFO: Pod "downward-api-f69d97d7-6d46-4784-9ef2-b4fb1b0dfdf0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.366246ms
Jul 13 09:10:33.695: INFO: Pod "downward-api-f69d97d7-6d46-4784-9ef2-b4fb1b0dfdf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018162074s
Jul 13 09:10:35.705: INFO: Pod "downward-api-f69d97d7-6d46-4784-9ef2-b4fb1b0dfdf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028872721s
STEP: Saw pod success
Jul 13 09:10:35.705: INFO: Pod "downward-api-f69d97d7-6d46-4784-9ef2-b4fb1b0dfdf0" satisfied condition "success or failure"
Jul 13 09:10:35.713: INFO: Trying to get logs from node cncf-1-15 pod downward-api-f69d97d7-6d46-4784-9ef2-b4fb1b0dfdf0 container dapi-container: <nil>
STEP: delete the pod
Jul 13 09:10:35.762: INFO: Waiting for pod downward-api-f69d97d7-6d46-4784-9ef2-b4fb1b0dfdf0 to disappear
Jul 13 09:10:35.769: INFO: Pod downward-api-f69d97d7-6d46-4784-9ef2-b4fb1b0dfdf0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:10:35.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4976" for this suite.
Jul 13 09:10:41.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:10:42.132: INFO: namespace downward-api-4976 deletion completed in 6.352990686s

• [SLOW TEST:10.603 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:10:42.132: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-6d8235de-2d28-486b-bf69-8197314f67b8
STEP: Creating a pod to test consume configMaps
Jul 13 09:10:42.304: INFO: Waiting up to 5m0s for pod "pod-configmaps-b95e2d72-b87f-4829-8d89-13b2ff6ee11b" in namespace "configmap-538" to be "success or failure"
Jul 13 09:10:42.313: INFO: Pod "pod-configmaps-b95e2d72-b87f-4829-8d89-13b2ff6ee11b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.878189ms
Jul 13 09:10:44.324: INFO: Pod "pod-configmaps-b95e2d72-b87f-4829-8d89-13b2ff6ee11b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019755718s
Jul 13 09:10:46.334: INFO: Pod "pod-configmaps-b95e2d72-b87f-4829-8d89-13b2ff6ee11b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029891354s
STEP: Saw pod success
Jul 13 09:10:46.334: INFO: Pod "pod-configmaps-b95e2d72-b87f-4829-8d89-13b2ff6ee11b" satisfied condition "success or failure"
Jul 13 09:10:46.343: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-b95e2d72-b87f-4829-8d89-13b2ff6ee11b container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 09:10:46.416: INFO: Waiting for pod pod-configmaps-b95e2d72-b87f-4829-8d89-13b2ff6ee11b to disappear
Jul 13 09:10:46.434: INFO: Pod pod-configmaps-b95e2d72-b87f-4829-8d89-13b2ff6ee11b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:10:46.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-538" for this suite.
Jul 13 09:10:52.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:10:52.777: INFO: namespace configmap-538 deletion completed in 6.335449892s

• [SLOW TEST:10.645 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:10:52.779: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 13 09:10:53.003: INFO: Number of nodes with available pods: 0
Jul 13 09:10:53.003: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 09:10:54.026: INFO: Number of nodes with available pods: 0
Jul 13 09:10:54.026: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 09:10:55.025: INFO: Number of nodes with available pods: 0
Jul 13 09:10:55.025: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 09:10:56.027: INFO: Number of nodes with available pods: 1
Jul 13 09:10:56.027: INFO: Node cncf-1-15-2 is running more than one daemon pod
Jul 13 09:10:57.024: INFO: Number of nodes with available pods: 2
Jul 13 09:10:57.024: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 13 09:10:57.082: INFO: Number of nodes with available pods: 1
Jul 13 09:10:57.082: INFO: Node cncf-1-15-2 is running more than one daemon pod
Jul 13 09:10:58.103: INFO: Number of nodes with available pods: 1
Jul 13 09:10:58.103: INFO: Node cncf-1-15-2 is running more than one daemon pod
Jul 13 09:10:59.101: INFO: Number of nodes with available pods: 1
Jul 13 09:10:59.102: INFO: Node cncf-1-15-2 is running more than one daemon pod
Jul 13 09:11:00.100: INFO: Number of nodes with available pods: 1
Jul 13 09:11:00.100: INFO: Node cncf-1-15-2 is running more than one daemon pod
Jul 13 09:11:01.101: INFO: Number of nodes with available pods: 2
Jul 13 09:11:01.101: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5346, will wait for the garbage collector to delete the pods
Jul 13 09:11:01.224: INFO: Deleting DaemonSet.extensions daemon-set took: 50.601588ms
Jul 13 09:11:01.624: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.261783ms
Jul 13 09:11:07.838: INFO: Number of nodes with available pods: 0
Jul 13 09:11:07.838: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 09:11:07.848: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5346/daemonsets","resourceVersion":"223479084"},"items":null}

Jul 13 09:11:07.856: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5346/pods","resourceVersion":"223479084"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:11:07.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5346" for this suite.
Jul 13 09:11:13.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:11:14.298: INFO: namespace daemonsets-5346 deletion completed in 6.406721895s

• [SLOW TEST:21.519 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:11:14.301: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:11:38.491: INFO: Container started at 2019-07-13 09:11:16 +0000 UTC, pod became ready at 2019-07-13 09:11:36 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:11:38.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6250" for this suite.
Jul 13 09:12:00.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:12:00.871: INFO: namespace container-probe-6250 deletion completed in 22.370170132s

• [SLOW TEST:46.570 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:12:00.871: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4774
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 13 09:12:01.040: INFO: Found 0 stateful pods, waiting for 3
Jul 13 09:12:11.052: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:12:11.052: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:12:11.052: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Jul 13 09:12:21.051: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:12:21.051: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:12:21.051: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:12:21.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-4774 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 09:12:21.547: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 09:12:21.547: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 09:12:21.547: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 13 09:12:31.631: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 13 09:12:41.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-4774 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 13 09:12:42.132: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 13 09:12:42.132: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 13 09:12:42.132: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 13 09:12:52.195: INFO: Waiting for StatefulSet statefulset-4774/ss2 to complete update
Jul 13 09:12:52.196: INFO: Waiting for Pod statefulset-4774/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 13 09:12:52.196: INFO: Waiting for Pod statefulset-4774/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 13 09:13:02.216: INFO: Waiting for StatefulSet statefulset-4774/ss2 to complete update
Jul 13 09:13:02.216: INFO: Waiting for Pod statefulset-4774/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Jul 13 09:13:12.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-4774 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 09:13:12.629: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 09:13:12.629: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 09:13:12.629: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 13 09:13:22.708: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 13 09:13:32.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-4774 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 13 09:13:33.178: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 13 09:13:33.178: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 13 09:13:33.178: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 13 09:13:53.245: INFO: Deleting all statefulset in ns statefulset-4774
Jul 13 09:13:53.253: INFO: Scaling statefulset ss2 to 0
Jul 13 09:14:13.299: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 09:14:13.311: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:14:13.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4774" for this suite.
Jul 13 09:14:21.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:14:21.741: INFO: namespace statefulset-4774 deletion completed in 8.378703251s

• [SLOW TEST:140.870 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:14:21.741: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jul 13 09:14:21.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-8007'
Jul 13 09:14:22.237: INFO: stderr: ""
Jul 13 09:14:22.237: INFO: stdout: "pod/pause created\n"
Jul 13 09:14:22.237: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 13 09:14:22.237: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8007" to be "running and ready"
Jul 13 09:14:22.245: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.966211ms
Jul 13 09:14:24.254: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016778881s
Jul 13 09:14:26.263: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.026027181s
Jul 13 09:14:26.263: INFO: Pod "pause" satisfied condition "running and ready"
Jul 13 09:14:26.263: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 13 09:14:26.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 label pods pause testing-label=testing-label-value --namespace=kubectl-8007'
Jul 13 09:14:26.422: INFO: stderr: ""
Jul 13 09:14:26.422: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 13 09:14:26.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pod pause -L testing-label --namespace=kubectl-8007'
Jul 13 09:14:26.542: INFO: stderr: ""
Jul 13 09:14:26.542: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 13 09:14:26.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 label pods pause testing-label- --namespace=kubectl-8007'
Jul 13 09:14:26.701: INFO: stderr: ""
Jul 13 09:14:26.701: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 13 09:14:26.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pod pause -L testing-label --namespace=kubectl-8007'
Jul 13 09:14:26.814: INFO: stderr: ""
Jul 13 09:14:26.814: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jul 13 09:14:26.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-8007'
Jul 13 09:14:27.011: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 09:14:27.011: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 13 09:14:27.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get rc,svc -l name=pause --no-headers --namespace=kubectl-8007'
Jul 13 09:14:27.139: INFO: stderr: "No resources found.\n"
Jul 13 09:14:27.139: INFO: stdout: ""
Jul 13 09:14:27.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -l name=pause --namespace=kubectl-8007 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 09:14:27.254: INFO: stderr: ""
Jul 13 09:14:27.254: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:14:27.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8007" for this suite.
Jul 13 09:14:33.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:14:33.621: INFO: namespace kubectl-8007 deletion completed in 6.358530324s

• [SLOW TEST:11.880 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:14:33.622: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 09:14:33.842: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aef01cdb-921a-40fe-8a5d-8489c6543c36" in namespace "projected-1403" to be "success or failure"
Jul 13 09:14:33.865: INFO: Pod "downwardapi-volume-aef01cdb-921a-40fe-8a5d-8489c6543c36": Phase="Pending", Reason="", readiness=false. Elapsed: 23.299766ms
Jul 13 09:14:35.875: INFO: Pod "downwardapi-volume-aef01cdb-921a-40fe-8a5d-8489c6543c36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033454722s
Jul 13 09:14:37.886: INFO: Pod "downwardapi-volume-aef01cdb-921a-40fe-8a5d-8489c6543c36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044336763s
STEP: Saw pod success
Jul 13 09:14:37.886: INFO: Pod "downwardapi-volume-aef01cdb-921a-40fe-8a5d-8489c6543c36" satisfied condition "success or failure"
Jul 13 09:14:37.895: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-aef01cdb-921a-40fe-8a5d-8489c6543c36 container client-container: <nil>
STEP: delete the pod
Jul 13 09:14:37.965: INFO: Waiting for pod downwardapi-volume-aef01cdb-921a-40fe-8a5d-8489c6543c36 to disappear
Jul 13 09:14:37.972: INFO: Pod downwardapi-volume-aef01cdb-921a-40fe-8a5d-8489c6543c36 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:14:37.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1403" for this suite.
Jul 13 09:14:44.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:14:44.332: INFO: namespace projected-1403 deletion completed in 6.350535779s

• [SLOW TEST:10.711 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:14:44.335: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 13 09:14:44.528: INFO: Waiting up to 5m0s for pod "pod-1ffb119b-a4b2-4eea-9c5c-e7baf2b0d1e5" in namespace "emptydir-1809" to be "success or failure"
Jul 13 09:14:44.546: INFO: Pod "pod-1ffb119b-a4b2-4eea-9c5c-e7baf2b0d1e5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.450974ms
Jul 13 09:14:46.557: INFO: Pod "pod-1ffb119b-a4b2-4eea-9c5c-e7baf2b0d1e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02885341s
Jul 13 09:14:48.569: INFO: Pod "pod-1ffb119b-a4b2-4eea-9c5c-e7baf2b0d1e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040359719s
STEP: Saw pod success
Jul 13 09:14:48.569: INFO: Pod "pod-1ffb119b-a4b2-4eea-9c5c-e7baf2b0d1e5" satisfied condition "success or failure"
Jul 13 09:14:48.579: INFO: Trying to get logs from node cncf-1-15 pod pod-1ffb119b-a4b2-4eea-9c5c-e7baf2b0d1e5 container test-container: <nil>
STEP: delete the pod
Jul 13 09:14:48.660: INFO: Waiting for pod pod-1ffb119b-a4b2-4eea-9c5c-e7baf2b0d1e5 to disappear
Jul 13 09:14:48.667: INFO: Pod pod-1ffb119b-a4b2-4eea-9c5c-e7baf2b0d1e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:14:48.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1809" for this suite.
Jul 13 09:14:54.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:14:55.055: INFO: namespace emptydir-1809 deletion completed in 6.379913485s

• [SLOW TEST:10.720 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:14:55.056: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-7b2c5d73-a424-4031-90b4-df9f8d30fc7c in namespace container-probe-2463
Jul 13 09:14:59.234: INFO: Started pod liveness-7b2c5d73-a424-4031-90b4-df9f8d30fc7c in namespace container-probe-2463
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 09:14:59.243: INFO: Initial restart count of pod liveness-7b2c5d73-a424-4031-90b4-df9f8d30fc7c is 0
Jul 13 09:15:21.369: INFO: Restart count of pod container-probe-2463/liveness-7b2c5d73-a424-4031-90b4-df9f8d30fc7c is now 1 (22.125930767s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:15:21.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2463" for this suite.
Jul 13 09:15:27.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:15:27.771: INFO: namespace container-probe-2463 deletion completed in 6.342823582s

• [SLOW TEST:32.716 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:15:27.773: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-8c1b1c49-b6e4-4648-b2d3-eddc521e13d8
STEP: Creating a pod to test consume configMaps
Jul 13 09:15:27.937: INFO: Waiting up to 5m0s for pod "pod-configmaps-bcefce21-81bf-4798-b4e6-247de5857e51" in namespace "configmap-5404" to be "success or failure"
Jul 13 09:15:27.945: INFO: Pod "pod-configmaps-bcefce21-81bf-4798-b4e6-247de5857e51": Phase="Pending", Reason="", readiness=false. Elapsed: 8.670277ms
Jul 13 09:15:29.956: INFO: Pod "pod-configmaps-bcefce21-81bf-4798-b4e6-247de5857e51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019757206s
Jul 13 09:15:31.969: INFO: Pod "pod-configmaps-bcefce21-81bf-4798-b4e6-247de5857e51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03188662s
STEP: Saw pod success
Jul 13 09:15:31.969: INFO: Pod "pod-configmaps-bcefce21-81bf-4798-b4e6-247de5857e51" satisfied condition "success or failure"
Jul 13 09:15:31.978: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-bcefce21-81bf-4798-b4e6-247de5857e51 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 09:15:32.038: INFO: Waiting for pod pod-configmaps-bcefce21-81bf-4798-b4e6-247de5857e51 to disappear
Jul 13 09:15:32.047: INFO: Pod pod-configmaps-bcefce21-81bf-4798-b4e6-247de5857e51 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:15:32.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5404" for this suite.
Jul 13 09:15:38.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:15:38.406: INFO: namespace configmap-5404 deletion completed in 6.349278755s

• [SLOW TEST:10.634 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:15:38.408: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-pnf9
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 09:15:38.600: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pnf9" in namespace "subpath-8283" to be "success or failure"
Jul 13 09:15:38.612: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.132674ms
Jul 13 09:15:40.623: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.02357052s
Jul 13 09:15:42.634: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 4.03437873s
Jul 13 09:15:44.644: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 6.044558337s
Jul 13 09:15:46.655: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 8.055179344s
Jul 13 09:15:48.666: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 10.06610217s
Jul 13 09:15:50.676: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 12.076749402s
Jul 13 09:15:52.688: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 14.087789706s
Jul 13 09:15:54.697: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 16.097465048s
Jul 13 09:15:56.708: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 18.108395073s
Jul 13 09:15:58.720: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 20.120050783s
Jul 13 09:16:00.731: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Running", Reason="", readiness=true. Elapsed: 22.131487548s
Jul 13 09:16:02.742: INFO: Pod "pod-subpath-test-configmap-pnf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.142364928s
STEP: Saw pod success
Jul 13 09:16:02.742: INFO: Pod "pod-subpath-test-configmap-pnf9" satisfied condition "success or failure"
Jul 13 09:16:02.751: INFO: Trying to get logs from node cncf-1-15 pod pod-subpath-test-configmap-pnf9 container test-container-subpath-configmap-pnf9: <nil>
STEP: delete the pod
Jul 13 09:16:02.811: INFO: Waiting for pod pod-subpath-test-configmap-pnf9 to disappear
Jul 13 09:16:02.820: INFO: Pod pod-subpath-test-configmap-pnf9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pnf9
Jul 13 09:16:02.820: INFO: Deleting pod "pod-subpath-test-configmap-pnf9" in namespace "subpath-8283"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:16:02.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8283" for this suite.
Jul 13 09:16:08.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:16:09.187: INFO: namespace subpath-8283 deletion completed in 6.345588908s

• [SLOW TEST:30.779 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:16:09.189: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-6741870a-72bb-4d2c-918c-f39fd80aa9e0
STEP: Creating a pod to test consume secrets
Jul 13 09:16:09.518: INFO: Waiting up to 5m0s for pod "pod-secrets-b62c96c0-707d-43f5-bd59-2f1db8f622b6" in namespace "secrets-4072" to be "success or failure"
Jul 13 09:16:09.530: INFO: Pod "pod-secrets-b62c96c0-707d-43f5-bd59-2f1db8f622b6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.954586ms
Jul 13 09:16:11.540: INFO: Pod "pod-secrets-b62c96c0-707d-43f5-bd59-2f1db8f622b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02176071s
Jul 13 09:16:13.549: INFO: Pod "pod-secrets-b62c96c0-707d-43f5-bd59-2f1db8f622b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031347799s
STEP: Saw pod success
Jul 13 09:16:13.549: INFO: Pod "pod-secrets-b62c96c0-707d-43f5-bd59-2f1db8f622b6" satisfied condition "success or failure"
Jul 13 09:16:13.589: INFO: Trying to get logs from node cncf-1-15 pod pod-secrets-b62c96c0-707d-43f5-bd59-2f1db8f622b6 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 09:16:13.644: INFO: Waiting for pod pod-secrets-b62c96c0-707d-43f5-bd59-2f1db8f622b6 to disappear
Jul 13 09:16:13.651: INFO: Pod pod-secrets-b62c96c0-707d-43f5-bd59-2f1db8f622b6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:16:13.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4072" for this suite.
Jul 13 09:16:19.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:16:20.025: INFO: namespace secrets-4072 deletion completed in 6.364724071s
STEP: Destroying namespace "secret-namespace-1165" for this suite.
Jul 13 09:16:26.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:16:26.390: INFO: namespace secret-namespace-1165 deletion completed in 6.364646247s

• [SLOW TEST:17.200 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:16:26.390: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 13 09:16:26.500: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 13 09:16:26.519: INFO: Waiting for terminating namespaces to be deleted...
Jul 13 09:16:26.528: INFO: 
Logging pods the kubelet thinks is on node cncf-1-15 before test
Jul 13 09:16:26.555: INFO: kube-proxy-qsskh from kube-system started at 2019-07-13 08:59:39 +0000 UTC (1 container statuses recorded)
Jul 13 09:16:26.555: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 13 09:16:26.555: INFO: wormhole-svb5x from kube-system started at 2019-07-13 08:59:58 +0000 UTC (1 container statuses recorded)
Jul 13 09:16:26.555: INFO: 	Container wormhole ready: true, restart count 0
Jul 13 09:16:26.555: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-13 09:03:04 +0000 UTC (1 container statuses recorded)
Jul 13 09:16:26.555: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 13 09:16:26.555: INFO: sonobuoy-e2e-job-5b31513983fe4ca4 from heptio-sonobuoy started at 2019-07-13 09:03:10 +0000 UTC (2 container statuses recorded)
Jul 13 09:16:26.555: INFO: 	Container e2e ready: true, restart count 0
Jul 13 09:16:26.555: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 09:16:26.555: INFO: sonobuoy-systemd-logs-daemon-set-9454bcfaa70145b2-zks8j from heptio-sonobuoy started at 2019-07-13 09:03:10 +0000 UTC (2 container statuses recorded)
Jul 13 09:16:26.555: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 09:16:26.555: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 09:16:26.555: INFO: canal-r8n6w from kube-system started at 2019-07-13 08:59:39 +0000 UTC (2 container statuses recorded)
Jul 13 09:16:26.555: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 09:16:26.555: INFO: 	Container kube-flannel ready: true, restart count 1
Jul 13 09:16:26.555: INFO: 
Logging pods the kubelet thinks is on node cncf-1-15-2 before test
Jul 13 09:16:26.587: INFO: canal-7hs5h from kube-system started at 2019-07-13 08:58:28 +0000 UTC (2 container statuses recorded)
Jul 13 09:16:26.587: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 09:16:26.587: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 13 09:16:26.587: INFO: kube-proxy-spjhx from kube-system started at 2019-07-13 08:58:28 +0000 UTC (1 container statuses recorded)
Jul 13 09:16:26.587: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 13 09:16:26.587: INFO: wormhole-62ggl from kube-system started at 2019-07-13 08:58:38 +0000 UTC (1 container statuses recorded)
Jul 13 09:16:26.587: INFO: 	Container wormhole ready: true, restart count 0
Jul 13 09:16:26.587: INFO: kube-dns-autoscaler-67595559f7-vj26r from kube-system started at 2019-07-13 08:58:43 +0000 UTC (1 container statuses recorded)
Jul 13 09:16:26.587: INFO: 	Container autoscaler ready: true, restart count 0
Jul 13 09:16:26.587: INFO: kube-dns-b74d847f-hr65v from kube-system started at 2019-07-13 08:59:44 +0000 UTC (3 container statuses recorded)
Jul 13 09:16:26.587: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 13 09:16:26.587: INFO: 	Container kubedns ready: true, restart count 0
Jul 13 09:16:26.587: INFO: 	Container sidecar ready: true, restart count 0
Jul 13 09:16:26.587: INFO: metrics-server-6dd5674bf7-gjr4b from kube-system started at 2019-07-13 08:58:38 +0000 UTC (1 container statuses recorded)
Jul 13 09:16:26.587: INFO: 	Container metrics-server ready: true, restart count 0
Jul 13 09:16:26.587: INFO: kube-dns-b74d847f-8tlpd from kube-system started at 2019-07-13 08:58:43 +0000 UTC (3 container statuses recorded)
Jul 13 09:16:26.587: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 13 09:16:26.587: INFO: 	Container kubedns ready: true, restart count 0
Jul 13 09:16:26.587: INFO: 	Container sidecar ready: true, restart count 0
Jul 13 09:16:26.587: INFO: sonobuoy-systemd-logs-daemon-set-9454bcfaa70145b2-gdq8g from heptio-sonobuoy started at 2019-07-13 09:03:10 +0000 UTC (2 container statuses recorded)
Jul 13 09:16:26.587: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 09:16:26.587: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b0ecc1872ffdd4], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:16:27.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3226" for this suite.
Jul 13 09:16:33.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:16:34.016: INFO: namespace sched-pred-3226 deletion completed in 6.354366265s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.626 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:16:34.019: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 13 09:16:34.179: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-a,UID:57d9e91d-a28d-447f-95a2-ccc97f6422d9,ResourceVersion:223482274,Generation:0,CreationTimestamp:2019-07-13 09:16:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 09:16:34.179: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-a,UID:57d9e91d-a28d-447f-95a2-ccc97f6422d9,ResourceVersion:223482274,Generation:0,CreationTimestamp:2019-07-13 09:16:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 13 09:16:44.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-a,UID:57d9e91d-a28d-447f-95a2-ccc97f6422d9,ResourceVersion:223482345,Generation:0,CreationTimestamp:2019-07-13 09:16:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 13 09:16:44.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-a,UID:57d9e91d-a28d-447f-95a2-ccc97f6422d9,ResourceVersion:223482345,Generation:0,CreationTimestamp:2019-07-13 09:16:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 13 09:16:54.255: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-a,UID:57d9e91d-a28d-447f-95a2-ccc97f6422d9,ResourceVersion:223482423,Generation:0,CreationTimestamp:2019-07-13 09:16:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 09:16:54.256: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-a,UID:57d9e91d-a28d-447f-95a2-ccc97f6422d9,ResourceVersion:223482423,Generation:0,CreationTimestamp:2019-07-13 09:16:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 13 09:17:04.314: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-a,UID:57d9e91d-a28d-447f-95a2-ccc97f6422d9,ResourceVersion:223482504,Generation:0,CreationTimestamp:2019-07-13 09:16:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 09:17:04.314: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-a,UID:57d9e91d-a28d-447f-95a2-ccc97f6422d9,ResourceVersion:223482504,Generation:0,CreationTimestamp:2019-07-13 09:16:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 13 09:17:14.366: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-b,UID:19f5742b-08fe-4a3f-996f-fb523f4e7ea5,ResourceVersion:223482584,Generation:0,CreationTimestamp:2019-07-13 09:17:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 09:17:14.367: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-b,UID:19f5742b-08fe-4a3f-996f-fb523f4e7ea5,ResourceVersion:223482584,Generation:0,CreationTimestamp:2019-07-13 09:17:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 13 09:17:24.419: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-b,UID:19f5742b-08fe-4a3f-996f-fb523f4e7ea5,ResourceVersion:223482663,Generation:0,CreationTimestamp:2019-07-13 09:17:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 09:17:24.419: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2462,SelfLink:/api/v1/namespaces/watch-2462/configmaps/e2e-watch-test-configmap-b,UID:19f5742b-08fe-4a3f-996f-fb523f4e7ea5,ResourceVersion:223482663,Generation:0,CreationTimestamp:2019-07-13 09:17:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:17:34.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2462" for this suite.
Jul 13 09:17:40.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:17:40.805: INFO: namespace watch-2462 deletion completed in 6.373399628s

• [SLOW TEST:66.786 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:17:40.807: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-b9717c0d-7a28-444f-a433-5ce6e9715e6d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:17:47.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9270" for this suite.
Jul 13 09:18:11.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:18:11.429: INFO: namespace configmap-9270 deletion completed in 24.371355879s

• [SLOW TEST:30.622 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:18:11.430: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:18:11.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8589" for this suite.
Jul 13 09:18:17.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:18:18.023: INFO: namespace kubelet-test-8589 deletion completed in 6.345633876s

• [SLOW TEST:6.594 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:18:18.024: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 13 09:18:18.157: INFO: Waiting up to 5m0s for pod "pod-f63480ec-1a89-45d5-a8a8-ae4a0f0a7695" in namespace "emptydir-7321" to be "success or failure"
Jul 13 09:18:18.177: INFO: Pod "pod-f63480ec-1a89-45d5-a8a8-ae4a0f0a7695": Phase="Pending", Reason="", readiness=false. Elapsed: 19.294342ms
Jul 13 09:18:20.188: INFO: Pod "pod-f63480ec-1a89-45d5-a8a8-ae4a0f0a7695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030309026s
Jul 13 09:18:22.200: INFO: Pod "pod-f63480ec-1a89-45d5-a8a8-ae4a0f0a7695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042323374s
STEP: Saw pod success
Jul 13 09:18:22.200: INFO: Pod "pod-f63480ec-1a89-45d5-a8a8-ae4a0f0a7695" satisfied condition "success or failure"
Jul 13 09:18:22.209: INFO: Trying to get logs from node cncf-1-15 pod pod-f63480ec-1a89-45d5-a8a8-ae4a0f0a7695 container test-container: <nil>
STEP: delete the pod
Jul 13 09:18:22.259: INFO: Waiting for pod pod-f63480ec-1a89-45d5-a8a8-ae4a0f0a7695 to disappear
Jul 13 09:18:22.265: INFO: Pod pod-f63480ec-1a89-45d5-a8a8-ae4a0f0a7695 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:18:22.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7321" for this suite.
Jul 13 09:18:28.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:18:28.633: INFO: namespace emptydir-7321 deletion completed in 6.35874172s

• [SLOW TEST:10.609 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:18:28.634: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-ef4e06bc-0077-4c0f-91c3-70e1b2768afd
STEP: Creating a pod to test consume secrets
Jul 13 09:18:28.798: INFO: Waiting up to 5m0s for pod "pod-secrets-7bba3d04-8809-45e9-a8bf-18699cb749f5" in namespace "secrets-7796" to be "success or failure"
Jul 13 09:18:28.805: INFO: Pod "pod-secrets-7bba3d04-8809-45e9-a8bf-18699cb749f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.879225ms
Jul 13 09:18:30.816: INFO: Pod "pod-secrets-7bba3d04-8809-45e9-a8bf-18699cb749f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018206406s
Jul 13 09:18:32.827: INFO: Pod "pod-secrets-7bba3d04-8809-45e9-a8bf-18699cb749f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028671218s
STEP: Saw pod success
Jul 13 09:18:32.827: INFO: Pod "pod-secrets-7bba3d04-8809-45e9-a8bf-18699cb749f5" satisfied condition "success or failure"
Jul 13 09:18:32.838: INFO: Trying to get logs from node cncf-1-15 pod pod-secrets-7bba3d04-8809-45e9-a8bf-18699cb749f5 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 09:18:32.892: INFO: Waiting for pod pod-secrets-7bba3d04-8809-45e9-a8bf-18699cb749f5 to disappear
Jul 13 09:18:32.899: INFO: Pod pod-secrets-7bba3d04-8809-45e9-a8bf-18699cb749f5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:18:32.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7796" for this suite.
Jul 13 09:18:38.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:18:39.269: INFO: namespace secrets-7796 deletion completed in 6.358483791s

• [SLOW TEST:10.635 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:18:39.269: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 13 09:18:42.014: INFO: Successfully updated pod "labelsupdate716ad55e-682c-4755-a9ed-c7124fbc3eec"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:18:44.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1365" for this suite.
Jul 13 09:19:08.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:19:08.443: INFO: namespace downward-api-1365 deletion completed in 24.367687466s

• [SLOW TEST:29.173 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:19:08.443: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4158.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4158.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4158.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4158.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4158.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4158.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 09:19:24.726: INFO: DNS probes using dns-4158/dns-test-6d9fe90b-8de8-4d81-9118-68203c43e90a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:19:24.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4158" for this suite.
Jul 13 09:19:30.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:19:31.120: INFO: namespace dns-4158 deletion completed in 6.346508886s

• [SLOW TEST:22.677 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:19:31.122: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 13 09:19:31.258: INFO: Waiting up to 5m0s for pod "pod-94880e9c-07be-467c-bdb8-0552f6440fe6" in namespace "emptydir-3037" to be "success or failure"
Jul 13 09:19:31.268: INFO: Pod "pod-94880e9c-07be-467c-bdb8-0552f6440fe6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.759194ms
Jul 13 09:19:33.276: INFO: Pod "pod-94880e9c-07be-467c-bdb8-0552f6440fe6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018068359s
Jul 13 09:19:35.436: INFO: Pod "pod-94880e9c-07be-467c-bdb8-0552f6440fe6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.178502884s
STEP: Saw pod success
Jul 13 09:19:35.437: INFO: Pod "pod-94880e9c-07be-467c-bdb8-0552f6440fe6" satisfied condition "success or failure"
Jul 13 09:19:35.459: INFO: Trying to get logs from node cncf-1-15 pod pod-94880e9c-07be-467c-bdb8-0552f6440fe6 container test-container: <nil>
STEP: delete the pod
Jul 13 09:19:35.508: INFO: Waiting for pod pod-94880e9c-07be-467c-bdb8-0552f6440fe6 to disappear
Jul 13 09:19:35.513: INFO: Pod pod-94880e9c-07be-467c-bdb8-0552f6440fe6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:19:35.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3037" for this suite.
Jul 13 09:19:41.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:19:41.882: INFO: namespace emptydir-3037 deletion completed in 6.360792439s

• [SLOW TEST:10.760 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:19:41.886: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 13 09:19:46.685: INFO: Successfully updated pod "annotationupdate9caa4065-27e1-4495-a515-69414df5ba2b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:19:48.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2748" for this suite.
Jul 13 09:20:12.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:20:13.099: INFO: namespace downward-api-2748 deletion completed in 24.35503849s

• [SLOW TEST:31.214 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:20:13.101: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-rpxn
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 09:20:13.294: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rpxn" in namespace "subpath-5340" to be "success or failure"
Jul 13 09:20:13.303: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.636574ms
Jul 13 09:20:15.313: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019319375s
Jul 13 09:20:17.325: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 4.030629576s
Jul 13 09:20:19.336: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 6.042097437s
Jul 13 09:20:21.347: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 8.053136624s
Jul 13 09:20:23.358: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 10.06430246s
Jul 13 09:20:25.368: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 12.074373176s
Jul 13 09:20:27.378: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 14.084219247s
Jul 13 09:20:29.399: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 16.105269831s
Jul 13 09:20:31.410: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 18.115922671s
Jul 13 09:20:33.426: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 20.132300382s
Jul 13 09:20:35.435: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Running", Reason="", readiness=true. Elapsed: 22.141485083s
Jul 13 09:20:37.447: INFO: Pod "pod-subpath-test-downwardapi-rpxn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.152921933s
STEP: Saw pod success
Jul 13 09:20:37.447: INFO: Pod "pod-subpath-test-downwardapi-rpxn" satisfied condition "success or failure"
Jul 13 09:20:37.456: INFO: Trying to get logs from node cncf-1-15 pod pod-subpath-test-downwardapi-rpxn container test-container-subpath-downwardapi-rpxn: <nil>
STEP: delete the pod
Jul 13 09:20:37.512: INFO: Waiting for pod pod-subpath-test-downwardapi-rpxn to disappear
Jul 13 09:20:37.519: INFO: Pod pod-subpath-test-downwardapi-rpxn no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rpxn
Jul 13 09:20:37.519: INFO: Deleting pod "pod-subpath-test-downwardapi-rpxn" in namespace "subpath-5340"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:20:37.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5340" for this suite.
Jul 13 09:20:43.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:20:43.938: INFO: namespace subpath-5340 deletion completed in 6.403417299s

• [SLOW TEST:30.838 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:20:43.940: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jul 13 09:20:44.120: INFO: Waiting up to 5m0s for pod "var-expansion-e0a477af-15de-44ba-8b1c-19b8bf57a165" in namespace "var-expansion-9256" to be "success or failure"
Jul 13 09:20:44.132: INFO: Pod "var-expansion-e0a477af-15de-44ba-8b1c-19b8bf57a165": Phase="Pending", Reason="", readiness=false. Elapsed: 11.678525ms
Jul 13 09:20:46.141: INFO: Pod "var-expansion-e0a477af-15de-44ba-8b1c-19b8bf57a165": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021115315s
Jul 13 09:20:48.151: INFO: Pod "var-expansion-e0a477af-15de-44ba-8b1c-19b8bf57a165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031376942s
STEP: Saw pod success
Jul 13 09:20:48.151: INFO: Pod "var-expansion-e0a477af-15de-44ba-8b1c-19b8bf57a165" satisfied condition "success or failure"
Jul 13 09:20:48.160: INFO: Trying to get logs from node cncf-1-15 pod var-expansion-e0a477af-15de-44ba-8b1c-19b8bf57a165 container dapi-container: <nil>
STEP: delete the pod
Jul 13 09:20:48.250: INFO: Waiting for pod var-expansion-e0a477af-15de-44ba-8b1c-19b8bf57a165 to disappear
Jul 13 09:20:48.257: INFO: Pod var-expansion-e0a477af-15de-44ba-8b1c-19b8bf57a165 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:20:48.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9256" for this suite.
Jul 13 09:20:54.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:20:54.606: INFO: namespace var-expansion-9256 deletion completed in 6.340455388s

• [SLOW TEST:10.666 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:20:54.607: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jul 13 09:20:54.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-4972'
Jul 13 09:20:55.205: INFO: stderr: ""
Jul 13 09:20:55.205: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jul 13 09:20:56.216: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:20:56.216: INFO: Found 0 / 1
Jul 13 09:20:57.215: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:20:57.215: INFO: Found 0 / 1
Jul 13 09:20:58.216: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:20:58.216: INFO: Found 1 / 1
Jul 13 09:20:58.216: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 13 09:20:58.252: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:20:58.252: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 13 09:20:58.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 logs redis-master-psmp6 redis-master --namespace=kubectl-4972'
Jul 13 09:20:58.424: INFO: stderr: ""
Jul 13 09:20:58.424: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Jul 09:20:57.566 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Jul 09:20:57.566 # Server started, Redis version 3.2.12\n1:M 13 Jul 09:20:57.566 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Jul 09:20:57.566 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 13 09:20:58.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 log redis-master-psmp6 redis-master --namespace=kubectl-4972 --tail=1'
Jul 13 09:20:58.599: INFO: stderr: ""
Jul 13 09:20:58.599: INFO: stdout: "1:M 13 Jul 09:20:57.566 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 13 09:20:58.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 log redis-master-psmp6 redis-master --namespace=kubectl-4972 --limit-bytes=1'
Jul 13 09:20:58.766: INFO: stderr: ""
Jul 13 09:20:58.766: INFO: stdout: " "
STEP: exposing timestamps
Jul 13 09:20:58.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 log redis-master-psmp6 redis-master --namespace=kubectl-4972 --tail=1 --timestamps'
Jul 13 09:20:58.919: INFO: stderr: ""
Jul 13 09:20:58.920: INFO: stdout: "2019-07-13T09:20:57.567055479Z 1:M 13 Jul 09:20:57.566 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 13 09:21:01.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 log redis-master-psmp6 redis-master --namespace=kubectl-4972 --since=1s'
Jul 13 09:21:01.606: INFO: stderr: ""
Jul 13 09:21:01.606: INFO: stdout: ""
Jul 13 09:21:01.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 log redis-master-psmp6 redis-master --namespace=kubectl-4972 --since=24h'
Jul 13 09:21:01.771: INFO: stderr: ""
Jul 13 09:21:01.771: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Jul 09:20:57.566 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Jul 09:20:57.566 # Server started, Redis version 3.2.12\n1:M 13 Jul 09:20:57.566 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Jul 09:20:57.566 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jul 13 09:21:01.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-4972'
Jul 13 09:21:01.962: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 09:21:01.962: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul 13 09:21:01.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4972'
Jul 13 09:21:02.114: INFO: stderr: "No resources found.\n"
Jul 13 09:21:02.114: INFO: stdout: ""
Jul 13 09:21:02.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -l name=nginx --namespace=kubectl-4972 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 09:21:02.256: INFO: stderr: ""
Jul 13 09:21:02.256: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:21:02.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4972" for this suite.
Jul 13 09:21:08.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:21:08.652: INFO: namespace kubectl-4972 deletion completed in 6.385765036s

• [SLOW TEST:14.045 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:21:08.653: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8202
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8202
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8202
Jul 13 09:21:08.836: INFO: Found 0 stateful pods, waiting for 1
Jul 13 09:21:18.848: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 13 09:21:18.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-8202 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 09:21:19.281: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 09:21:19.281: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 09:21:19.281: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 13 09:21:19.292: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 13 09:21:29.302: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 09:21:29.302: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 09:21:29.348: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999118s
Jul 13 09:21:30.357: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989449206s
Jul 13 09:21:31.368: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.979957271s
Jul 13 09:21:32.381: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.969364746s
Jul 13 09:21:33.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.955771374s
Jul 13 09:21:34.405: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.943109461s
Jul 13 09:21:35.418: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.931685715s
Jul 13 09:21:36.428: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.919443362s
Jul 13 09:21:37.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.908874747s
Jul 13 09:21:38.450: INFO: Verifying statefulset ss doesn't scale past 1 for another 899.83931ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8202
Jul 13 09:21:39.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-8202 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 13 09:21:39.851: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 13 09:21:39.852: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 13 09:21:39.852: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 13 09:21:39.861: INFO: Found 1 stateful pods, waiting for 3
Jul 13 09:21:49.872: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:21:49.872: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:21:49.872: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 13 09:21:49.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-8202 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 09:21:50.305: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 09:21:50.305: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 09:21:50.305: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 13 09:21:50.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-8202 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 09:21:50.769: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 09:21:50.769: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 09:21:50.769: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 13 09:21:50.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-8202 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 09:21:51.170: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 09:21:51.170: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 09:21:51.170: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 13 09:21:51.170: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 09:21:51.179: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 13 09:22:01.201: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 09:22:01.201: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 09:22:01.202: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 09:22:01.240: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999992s
Jul 13 09:22:02.251: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989035527s
Jul 13 09:22:03.263: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97747301s
Jul 13 09:22:04.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965929312s
Jul 13 09:22:05.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.956664528s
Jul 13 09:22:06.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.945629342s
Jul 13 09:22:07.308: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.931652824s
Jul 13 09:22:08.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.92010213s
Jul 13 09:22:09.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.908184522s
Jul 13 09:22:10.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 894.980495ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8202
Jul 13 09:22:11.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-8202 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 13 09:22:11.745: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 13 09:22:11.745: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 13 09:22:11.745: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 13 09:22:11.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-8202 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 13 09:22:12.143: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 13 09:22:12.143: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 13 09:22:12.143: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 13 09:22:12.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-8202 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 13 09:22:12.526: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 13 09:22:12.526: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 13 09:22:12.526: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 13 09:22:12.526: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 13 09:22:22.583: INFO: Deleting all statefulset in ns statefulset-8202
Jul 13 09:22:22.594: INFO: Scaling statefulset ss to 0
Jul 13 09:22:22.629: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 09:22:22.638: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:22:22.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8202" for this suite.
Jul 13 09:22:28.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:22:29.047: INFO: namespace statefulset-8202 deletion completed in 6.365693364s

• [SLOW TEST:80.394 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:22:29.048: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-8b845232-3908-47c8-84a1-c08e0a4270ef
STEP: Creating a pod to test consume secrets
Jul 13 09:22:29.220: INFO: Waiting up to 5m0s for pod "pod-secrets-e1c84d16-dbe0-4a17-a800-4016c6481c96" in namespace "secrets-7953" to be "success or failure"
Jul 13 09:22:29.234: INFO: Pod "pod-secrets-e1c84d16-dbe0-4a17-a800-4016c6481c96": Phase="Pending", Reason="", readiness=false. Elapsed: 13.217282ms
Jul 13 09:22:31.242: INFO: Pod "pod-secrets-e1c84d16-dbe0-4a17-a800-4016c6481c96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021449595s
Jul 13 09:22:33.252: INFO: Pod "pod-secrets-e1c84d16-dbe0-4a17-a800-4016c6481c96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031567097s
STEP: Saw pod success
Jul 13 09:22:33.252: INFO: Pod "pod-secrets-e1c84d16-dbe0-4a17-a800-4016c6481c96" satisfied condition "success or failure"
Jul 13 09:22:33.261: INFO: Trying to get logs from node cncf-1-15 pod pod-secrets-e1c84d16-dbe0-4a17-a800-4016c6481c96 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 09:22:33.323: INFO: Waiting for pod pod-secrets-e1c84d16-dbe0-4a17-a800-4016c6481c96 to disappear
Jul 13 09:22:33.330: INFO: Pod pod-secrets-e1c84d16-dbe0-4a17-a800-4016c6481c96 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:22:33.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7953" for this suite.
Jul 13 09:22:39.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:22:39.693: INFO: namespace secrets-7953 deletion completed in 6.353073487s

• [SLOW TEST:10.645 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:22:39.694: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jul 13 09:22:39.917: INFO: Waiting up to 5m0s for pod "var-expansion-f93175f9-9a98-4697-9b90-97c876645680" in namespace "var-expansion-6478" to be "success or failure"
Jul 13 09:22:39.927: INFO: Pod "var-expansion-f93175f9-9a98-4697-9b90-97c876645680": Phase="Pending", Reason="", readiness=false. Elapsed: 9.341087ms
Jul 13 09:22:41.940: INFO: Pod "var-expansion-f93175f9-9a98-4697-9b90-97c876645680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02269593s
Jul 13 09:22:43.955: INFO: Pod "var-expansion-f93175f9-9a98-4697-9b90-97c876645680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03707675s
STEP: Saw pod success
Jul 13 09:22:43.955: INFO: Pod "var-expansion-f93175f9-9a98-4697-9b90-97c876645680" satisfied condition "success or failure"
Jul 13 09:22:43.963: INFO: Trying to get logs from node cncf-1-15 pod var-expansion-f93175f9-9a98-4697-9b90-97c876645680 container dapi-container: <nil>
STEP: delete the pod
Jul 13 09:22:44.019: INFO: Waiting for pod var-expansion-f93175f9-9a98-4697-9b90-97c876645680 to disappear
Jul 13 09:22:44.025: INFO: Pod var-expansion-f93175f9-9a98-4697-9b90-97c876645680 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:22:44.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6478" for this suite.
Jul 13 09:22:50.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:22:50.377: INFO: namespace var-expansion-6478 deletion completed in 6.344566966s

• [SLOW TEST:10.683 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:22:50.378: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3213
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 13 09:22:50.492: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 13 09:23:12.681: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.56:8080/dial?request=hostName&protocol=udp&host=10.2.1.55&port=8081&tries=1'] Namespace:pod-network-test-3213 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 09:23:12.681: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 09:23:12.970: INFO: Waiting for endpoints: map[]
Jul 13 09:23:12.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.56:8080/dial?request=hostName&protocol=udp&host=10.2.0.17&port=8081&tries=1'] Namespace:pod-network-test-3213 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 09:23:12.977: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 09:23:13.241: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:23:13.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3213" for this suite.
Jul 13 09:23:37.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:23:37.625: INFO: namespace pod-network-test-3213 deletion completed in 24.375379059s

• [SLOW TEST:47.247 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:23:37.625: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 09:23:37.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-900c5493-38c4-4e24-a8ac-715b0b49904b" in namespace "downward-api-469" to be "success or failure"
Jul 13 09:23:37.783: INFO: Pod "downwardapi-volume-900c5493-38c4-4e24-a8ac-715b0b49904b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.837214ms
Jul 13 09:23:39.793: INFO: Pod "downwardapi-volume-900c5493-38c4-4e24-a8ac-715b0b49904b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017975714s
Jul 13 09:23:41.804: INFO: Pod "downwardapi-volume-900c5493-38c4-4e24-a8ac-715b0b49904b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029014722s
STEP: Saw pod success
Jul 13 09:23:41.804: INFO: Pod "downwardapi-volume-900c5493-38c4-4e24-a8ac-715b0b49904b" satisfied condition "success or failure"
Jul 13 09:23:41.814: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-900c5493-38c4-4e24-a8ac-715b0b49904b container client-container: <nil>
STEP: delete the pod
Jul 13 09:23:41.868: INFO: Waiting for pod downwardapi-volume-900c5493-38c4-4e24-a8ac-715b0b49904b to disappear
Jul 13 09:23:41.878: INFO: Pod downwardapi-volume-900c5493-38c4-4e24-a8ac-715b0b49904b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:23:41.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-469" for this suite.
Jul 13 09:23:47.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:23:48.226: INFO: namespace downward-api-469 deletion completed in 6.337267847s

• [SLOW TEST:10.601 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:23:48.226: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 13 09:23:48.346: INFO: PodSpec: initContainers in spec.initContainers
Jul 13 09:24:39.600: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f9b327f3-3a09-4c82-95a3-f9c520d4390e", GenerateName:"", Namespace:"init-container-7510", SelfLink:"/api/v1/namespaces/init-container-7510/pods/pod-init-f9b327f3-3a09-4c82-95a3-f9c520d4390e", UID:"c6e4f476-bf4c-4776-8fab-dc43d1eff0be", ResourceVersion:"223489336", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63698606628, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"346920736"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.2.1.58/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7s5pv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002c44180), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7s5pv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7s5pv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7s5pv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0016b42c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cncf-1-15", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002dc60c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0016b4350)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0016b4370)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0016b4378), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0016b437c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698606628, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698606628, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698606628, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698606628, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"51.68.114.204", PodIP:"10.2.1.58", StartTime:(*v1.Time)(0xc002734240), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001ac8150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001ac81c0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://43d9298a85081a96b9ffdba6182fd2607607fd63c2841742db90583068d991c7"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002734280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002734260), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:24:39.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7510" for this suite.
Jul 13 09:25:03.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:25:03.978: INFO: namespace init-container-7510 deletion completed in 24.360302067s

• [SLOW TEST:75.752 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:25:03.978: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:25:04.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 version'
Jul 13 09:25:04.226: INFO: stderr: ""
Jul 13 09:25:04.226: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:25:04.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4113" for this suite.
Jul 13 09:25:10.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:25:10.568: INFO: namespace kubectl-4113 deletion completed in 6.332863116s

• [SLOW TEST:6.590 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:25:10.570: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 09:25:10.736: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d749a3a-fce2-4e9b-b4a3-852e3866666f" in namespace "downward-api-4413" to be "success or failure"
Jul 13 09:25:10.753: INFO: Pod "downwardapi-volume-6d749a3a-fce2-4e9b-b4a3-852e3866666f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.788905ms
Jul 13 09:25:12.761: INFO: Pod "downwardapi-volume-6d749a3a-fce2-4e9b-b4a3-852e3866666f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025114074s
Jul 13 09:25:14.772: INFO: Pod "downwardapi-volume-6d749a3a-fce2-4e9b-b4a3-852e3866666f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035982937s
STEP: Saw pod success
Jul 13 09:25:14.772: INFO: Pod "downwardapi-volume-6d749a3a-fce2-4e9b-b4a3-852e3866666f" satisfied condition "success or failure"
Jul 13 09:25:14.796: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-6d749a3a-fce2-4e9b-b4a3-852e3866666f container client-container: <nil>
STEP: delete the pod
Jul 13 09:25:14.858: INFO: Waiting for pod downwardapi-volume-6d749a3a-fce2-4e9b-b4a3-852e3866666f to disappear
Jul 13 09:25:14.866: INFO: Pod downwardapi-volume-6d749a3a-fce2-4e9b-b4a3-852e3866666f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:25:14.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4413" for this suite.
Jul 13 09:25:20.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:25:21.186: INFO: namespace downward-api-4413 deletion completed in 6.310414739s

• [SLOW TEST:10.616 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:25:21.186: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 13 09:25:21.337: INFO: Waiting up to 5m0s for pod "pod-9c577332-fca7-409d-8871-d74fb4dec09a" in namespace "emptydir-43" to be "success or failure"
Jul 13 09:25:21.347: INFO: Pod "pod-9c577332-fca7-409d-8871-d74fb4dec09a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.549272ms
Jul 13 09:25:23.359: INFO: Pod "pod-9c577332-fca7-409d-8871-d74fb4dec09a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02179044s
Jul 13 09:25:25.378: INFO: Pod "pod-9c577332-fca7-409d-8871-d74fb4dec09a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041120639s
STEP: Saw pod success
Jul 13 09:25:25.378: INFO: Pod "pod-9c577332-fca7-409d-8871-d74fb4dec09a" satisfied condition "success or failure"
Jul 13 09:25:25.389: INFO: Trying to get logs from node cncf-1-15 pod pod-9c577332-fca7-409d-8871-d74fb4dec09a container test-container: <nil>
STEP: delete the pod
Jul 13 09:25:25.446: INFO: Waiting for pod pod-9c577332-fca7-409d-8871-d74fb4dec09a to disappear
Jul 13 09:25:25.453: INFO: Pod pod-9c577332-fca7-409d-8871-d74fb4dec09a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:25:25.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-43" for this suite.
Jul 13 09:25:31.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:25:31.837: INFO: namespace emptydir-43 deletion completed in 6.376511402s

• [SLOW TEST:10.651 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:25:31.840: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jul 13 09:25:31.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-7230'
Jul 13 09:25:32.205: INFO: stderr: ""
Jul 13 09:25:32.205: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 09:25:32.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7230'
Jul 13 09:25:32.341: INFO: stderr: ""
Jul 13 09:25:32.341: INFO: stdout: "update-demo-nautilus-9z7lv update-demo-nautilus-jgt6m "
Jul 13 09:25:32.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-9z7lv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7230'
Jul 13 09:25:32.467: INFO: stderr: ""
Jul 13 09:25:32.467: INFO: stdout: ""
Jul 13 09:25:32.467: INFO: update-demo-nautilus-9z7lv is created but not running
Jul 13 09:25:37.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7230'
Jul 13 09:25:37.607: INFO: stderr: ""
Jul 13 09:25:37.607: INFO: stdout: "update-demo-nautilus-9z7lv update-demo-nautilus-jgt6m "
Jul 13 09:25:37.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-9z7lv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7230'
Jul 13 09:25:37.748: INFO: stderr: ""
Jul 13 09:25:37.748: INFO: stdout: "true"
Jul 13 09:25:37.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-9z7lv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7230'
Jul 13 09:25:37.883: INFO: stderr: ""
Jul 13 09:25:37.883: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 09:25:37.883: INFO: validating pod update-demo-nautilus-9z7lv
Jul 13 09:25:37.899: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 09:25:37.900: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 09:25:37.900: INFO: update-demo-nautilus-9z7lv is verified up and running
Jul 13 09:25:37.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-jgt6m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7230'
Jul 13 09:25:38.032: INFO: stderr: ""
Jul 13 09:25:38.032: INFO: stdout: "true"
Jul 13 09:25:38.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-jgt6m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7230'
Jul 13 09:25:38.169: INFO: stderr: ""
Jul 13 09:25:38.169: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 09:25:38.169: INFO: validating pod update-demo-nautilus-jgt6m
Jul 13 09:25:38.183: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 09:25:38.184: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 09:25:38.184: INFO: update-demo-nautilus-jgt6m is verified up and running
STEP: rolling-update to new replication controller
Jul 13 09:25:38.187: INFO: scanned /root for discovery docs: <nil>
Jul 13 09:25:38.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7230'
Jul 13 09:26:00.987: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 13 09:26:00.987: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 09:26:00.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7230'
Jul 13 09:26:01.134: INFO: stderr: ""
Jul 13 09:26:01.134: INFO: stdout: "update-demo-kitten-7rpv9 update-demo-kitten-krcvx "
Jul 13 09:26:01.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-kitten-7rpv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7230'
Jul 13 09:26:01.281: INFO: stderr: ""
Jul 13 09:26:01.281: INFO: stdout: "true"
Jul 13 09:26:01.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-kitten-7rpv9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7230'
Jul 13 09:26:01.427: INFO: stderr: ""
Jul 13 09:26:01.427: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 13 09:26:01.427: INFO: validating pod update-demo-kitten-7rpv9
Jul 13 09:26:01.439: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 13 09:26:01.439: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 13 09:26:01.440: INFO: update-demo-kitten-7rpv9 is verified up and running
Jul 13 09:26:01.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-kitten-krcvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7230'
Jul 13 09:26:01.578: INFO: stderr: ""
Jul 13 09:26:01.578: INFO: stdout: "true"
Jul 13 09:26:01.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-kitten-krcvx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7230'
Jul 13 09:26:01.719: INFO: stderr: ""
Jul 13 09:26:01.719: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 13 09:26:01.719: INFO: validating pod update-demo-kitten-krcvx
Jul 13 09:26:01.737: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 13 09:26:01.737: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 13 09:26:01.738: INFO: update-demo-kitten-krcvx is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:26:01.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7230" for this suite.
Jul 13 09:26:25.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:26:26.160: INFO: namespace kubectl-7230 deletion completed in 24.41168556s

• [SLOW TEST:54.320 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:26:26.161: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:26:26.281: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 13 09:26:28.390: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:26:29.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-987" for this suite.
Jul 13 09:26:35.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:26:35.768: INFO: namespace replication-controller-987 deletion completed in 6.350545266s

• [SLOW TEST:9.608 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:26:35.770: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:26:35.906: INFO: (0) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.283972ms)
Jul 13 09:26:35.922: INFO: (1) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 15.795201ms)
Jul 13 09:26:35.961: INFO: (2) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 38.861351ms)
Jul 13 09:26:35.971: INFO: (3) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.761827ms)
Jul 13 09:26:35.982: INFO: (4) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.008554ms)
Jul 13 09:26:35.991: INFO: (5) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.7891ms)
Jul 13 09:26:36.002: INFO: (6) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.895254ms)
Jul 13 09:26:36.012: INFO: (7) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.682063ms)
Jul 13 09:26:36.025: INFO: (8) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.386376ms)
Jul 13 09:26:36.038: INFO: (9) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.937026ms)
Jul 13 09:26:36.050: INFO: (10) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 11.524965ms)
Jul 13 09:26:36.060: INFO: (11) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.216143ms)
Jul 13 09:26:36.076: INFO: (12) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 15.53569ms)
Jul 13 09:26:36.088: INFO: (13) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.592066ms)
Jul 13 09:26:36.106: INFO: (14) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 17.01908ms)
Jul 13 09:26:36.119: INFO: (15) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.095027ms)
Jul 13 09:26:36.131: INFO: (16) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.142992ms)
Jul 13 09:26:36.141: INFO: (17) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.419161ms)
Jul 13 09:26:36.160: INFO: (18) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 18.011385ms)
Jul 13 09:26:36.175: INFO: (19) /api/v1/nodes/cncf-1-15:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 15.210615ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:26:36.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-423" for this suite.
Jul 13 09:26:42.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:26:42.553: INFO: namespace proxy-423 deletion completed in 6.354467143s

• [SLOW TEST:6.783 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:26:42.556: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jul 13 09:26:42.681: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-018000938 proxy --unix-socket=/tmp/kubectl-proxy-unix333371777/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:26:42.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2497" for this suite.
Jul 13 09:26:48.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:26:49.101: INFO: namespace kubectl-2497 deletion completed in 6.327774927s

• [SLOW TEST:6.545 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:26:49.102: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 13 09:26:49.990: INFO: Pod name wrapped-volume-race-c68b2106-14a5-4e23-8207-302e658a06f3: Found 0 pods out of 5
Jul 13 09:26:55.006: INFO: Pod name wrapped-volume-race-c68b2106-14a5-4e23-8207-302e658a06f3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c68b2106-14a5-4e23-8207-302e658a06f3 in namespace emptydir-wrapper-9623, will wait for the garbage collector to delete the pods
Jul 13 09:27:13.154: INFO: Deleting ReplicationController wrapped-volume-race-c68b2106-14a5-4e23-8207-302e658a06f3 took: 23.338679ms
Jul 13 09:27:13.555: INFO: Terminating ReplicationController wrapped-volume-race-c68b2106-14a5-4e23-8207-302e658a06f3 pods took: 400.395323ms
STEP: Creating RC which spawns configmap-volume pods
Jul 13 09:27:57.808: INFO: Pod name wrapped-volume-race-8e9abfcd-f2e8-45f0-bed0-31fbe27b0389: Found 0 pods out of 5
Jul 13 09:28:02.822: INFO: Pod name wrapped-volume-race-8e9abfcd-f2e8-45f0-bed0-31fbe27b0389: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8e9abfcd-f2e8-45f0-bed0-31fbe27b0389 in namespace emptydir-wrapper-9623, will wait for the garbage collector to delete the pods
Jul 13 09:28:20.996: INFO: Deleting ReplicationController wrapped-volume-race-8e9abfcd-f2e8-45f0-bed0-31fbe27b0389 took: 18.830766ms
Jul 13 09:28:21.397: INFO: Terminating ReplicationController wrapped-volume-race-8e9abfcd-f2e8-45f0-bed0-31fbe27b0389 pods took: 400.51296ms
STEP: Creating RC which spawns configmap-volume pods
Jul 13 09:28:57.947: INFO: Pod name wrapped-volume-race-9db41442-2986-4bb4-8385-29649b6cc3c2: Found 0 pods out of 5
Jul 13 09:29:02.964: INFO: Pod name wrapped-volume-race-9db41442-2986-4bb4-8385-29649b6cc3c2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9db41442-2986-4bb4-8385-29649b6cc3c2 in namespace emptydir-wrapper-9623, will wait for the garbage collector to delete the pods
Jul 13 09:29:21.136: INFO: Deleting ReplicationController wrapped-volume-race-9db41442-2986-4bb4-8385-29649b6cc3c2 took: 21.041405ms
Jul 13 09:29:21.538: INFO: Terminating ReplicationController wrapped-volume-race-9db41442-2986-4bb4-8385-29649b6cc3c2 pods took: 402.67215ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:29:58.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9623" for this suite.
Jul 13 09:30:06.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:30:07.117: INFO: namespace emptydir-wrapper-9623 deletion completed in 8.362643076s

• [SLOW TEST:198.015 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:30:07.119: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:30:11.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4878" for this suite.
Jul 13 09:30:17.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:30:17.664: INFO: namespace kubelet-test-4878 deletion completed in 6.364092447s

• [SLOW TEST:10.545 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:30:17.664: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 13 09:30:25.893: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:25.903: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:27.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:27.914: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:29.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:29.914: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:31.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:31.913: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:33.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:33.914: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:35.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:35.914: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:37.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:37.913: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:39.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:39.912: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:41.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:41.921: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:43.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:43.914: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 09:30:45.903: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 09:30:45.913: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:30:45.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8961" for this suite.
Jul 13 09:31:10.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:31:10.294: INFO: namespace container-lifecycle-hook-8961 deletion completed in 24.335956619s

• [SLOW TEST:52.630 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:31:10.298: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0713 09:31:50.518311      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 13 09:31:50.518: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:31:50.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5009" for this suite.
Jul 13 09:31:58.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:31:58.905: INFO: namespace gc-5009 deletion completed in 8.377205795s

• [SLOW TEST:48.608 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:31:58.906: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 13 09:32:02.146: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:32:02.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6345" for this suite.
Jul 13 09:32:08.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:32:08.509: INFO: namespace container-runtime-6345 deletion completed in 6.326641927s

• [SLOW TEST:9.602 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:32:08.509: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 13 09:32:08.649: INFO: Waiting up to 5m0s for pod "pod-02bd0d15-0d61-474f-af9b-0a738737531e" in namespace "emptydir-2234" to be "success or failure"
Jul 13 09:32:08.661: INFO: Pod "pod-02bd0d15-0d61-474f-af9b-0a738737531e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.492182ms
Jul 13 09:32:10.679: INFO: Pod "pod-02bd0d15-0d61-474f-af9b-0a738737531e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029234206s
Jul 13 09:32:12.689: INFO: Pod "pod-02bd0d15-0d61-474f-af9b-0a738737531e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039283568s
STEP: Saw pod success
Jul 13 09:32:12.689: INFO: Pod "pod-02bd0d15-0d61-474f-af9b-0a738737531e" satisfied condition "success or failure"
Jul 13 09:32:12.700: INFO: Trying to get logs from node cncf-1-15 pod pod-02bd0d15-0d61-474f-af9b-0a738737531e container test-container: <nil>
STEP: delete the pod
Jul 13 09:32:12.743: INFO: Waiting for pod pod-02bd0d15-0d61-474f-af9b-0a738737531e to disappear
Jul 13 09:32:12.750: INFO: Pod pod-02bd0d15-0d61-474f-af9b-0a738737531e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:32:12.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2234" for this suite.
Jul 13 09:32:18.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:32:19.135: INFO: namespace emptydir-2234 deletion completed in 6.376502257s

• [SLOW TEST:10.626 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:32:19.136: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:32:19.356: INFO: Create a RollingUpdate DaemonSet
Jul 13 09:32:19.370: INFO: Check that daemon pods launch on every node of the cluster
Jul 13 09:32:19.390: INFO: Number of nodes with available pods: 0
Jul 13 09:32:19.390: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 09:32:20.410: INFO: Number of nodes with available pods: 0
Jul 13 09:32:20.410: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 09:32:21.424: INFO: Number of nodes with available pods: 0
Jul 13 09:32:21.424: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 09:32:22.410: INFO: Number of nodes with available pods: 1
Jul 13 09:32:22.410: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 09:32:23.425: INFO: Number of nodes with available pods: 2
Jul 13 09:32:23.425: INFO: Number of running nodes: 2, number of available pods: 2
Jul 13 09:32:23.425: INFO: Update the DaemonSet to trigger a rollout
Jul 13 09:32:23.445: INFO: Updating DaemonSet daemon-set
Jul 13 09:32:26.532: INFO: Roll back the DaemonSet before rollout is complete
Jul 13 09:32:26.570: INFO: Updating DaemonSet daemon-set
Jul 13 09:32:26.571: INFO: Make sure DaemonSet rollback is complete
Jul 13 09:32:26.580: INFO: Wrong image for pod: daemon-set-xtqf2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 13 09:32:26.580: INFO: Pod daemon-set-xtqf2 is not available
Jul 13 09:32:27.609: INFO: Wrong image for pod: daemon-set-xtqf2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 13 09:32:27.610: INFO: Pod daemon-set-xtqf2 is not available
Jul 13 09:32:28.608: INFO: Pod daemon-set-mnff7 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-106, will wait for the garbage collector to delete the pods
Jul 13 09:32:28.724: INFO: Deleting DaemonSet.extensions daemon-set took: 21.246168ms
Jul 13 09:32:29.124: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.318905ms
Jul 13 09:32:37.737: INFO: Number of nodes with available pods: 0
Jul 13 09:32:37.737: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 09:32:37.746: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-106/daemonsets","resourceVersion":"223501741"},"items":null}

Jul 13 09:32:37.752: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-106/pods","resourceVersion":"223501741"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:32:37.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-106" for this suite.
Jul 13 09:32:43.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:32:44.157: INFO: namespace daemonsets-106 deletion completed in 6.367552431s

• [SLOW TEST:25.021 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:32:44.158: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:32:48.371: INFO: Waiting up to 5m0s for pod "client-envvars-23d84656-d56e-44e9-9ec8-b5c86771c088" in namespace "pods-6466" to be "success or failure"
Jul 13 09:32:48.381: INFO: Pod "client-envvars-23d84656-d56e-44e9-9ec8-b5c86771c088": Phase="Pending", Reason="", readiness=false. Elapsed: 9.931247ms
Jul 13 09:32:50.391: INFO: Pod "client-envvars-23d84656-d56e-44e9-9ec8-b5c86771c088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020212859s
Jul 13 09:32:52.401: INFO: Pod "client-envvars-23d84656-d56e-44e9-9ec8-b5c86771c088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030452975s
STEP: Saw pod success
Jul 13 09:32:52.402: INFO: Pod "client-envvars-23d84656-d56e-44e9-9ec8-b5c86771c088" satisfied condition "success or failure"
Jul 13 09:32:52.410: INFO: Trying to get logs from node cncf-1-15 pod client-envvars-23d84656-d56e-44e9-9ec8-b5c86771c088 container env3cont: <nil>
STEP: delete the pod
Jul 13 09:32:52.468: INFO: Waiting for pod client-envvars-23d84656-d56e-44e9-9ec8-b5c86771c088 to disappear
Jul 13 09:32:52.477: INFO: Pod client-envvars-23d84656-d56e-44e9-9ec8-b5c86771c088 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:32:52.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6466" for this suite.
Jul 13 09:33:32.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:33:32.832: INFO: namespace pods-6466 deletion completed in 40.347361275s

• [SLOW TEST:48.675 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:33:32.834: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7611
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 13 09:33:32.949: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 13 09:33:53.162: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.0.28 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7611 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 09:33:53.162: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 09:33:54.411: INFO: Found all expected endpoints: [netserver-0]
Jul 13 09:33:54.426: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.1.93 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7611 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 09:33:54.426: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 09:33:55.675: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:33:55.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7611" for this suite.
Jul 13 09:34:11.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:34:12.036: INFO: namespace pod-network-test-7611 deletion completed in 16.344556069s

• [SLOW TEST:39.203 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:34:12.038: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:34:12.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-5489'
Jul 13 09:34:12.590: INFO: stderr: ""
Jul 13 09:34:12.590: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 13 09:34:12.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-5489'
Jul 13 09:34:12.860: INFO: stderr: ""
Jul 13 09:34:12.860: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 13 09:34:13.869: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:34:13.869: INFO: Found 0 / 1
Jul 13 09:34:14.871: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:34:14.871: INFO: Found 0 / 1
Jul 13 09:34:15.870: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:34:15.870: INFO: Found 1 / 1
Jul 13 09:34:15.871: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 13 09:34:15.879: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:34:15.879: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 13 09:34:15.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 describe pod redis-master-cfnlk --namespace=kubectl-5489'
Jul 13 09:34:16.062: INFO: stderr: ""
Jul 13 09:34:16.062: INFO: stdout: "Name:           redis-master-cfnlk\nNamespace:      kubectl-5489\nPriority:       0\nNode:           cncf-1-15/51.68.114.204\nStart Time:     Sat, 13 Jul 2019 09:34:12 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 10.2.1.95/32\nStatus:         Running\nIP:             10.2.1.95\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://584fe19cb1e5ba52f2456ee3278c49410788b19ee7fec6f93d9f68356375307b\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 13 Jul 2019 09:34:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9lzjq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-9lzjq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-9lzjq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                Message\n  ----    ------     ----  ----                -------\n  Normal  Scheduled  4s    default-scheduler   Successfully assigned kubectl-5489/redis-master-cfnlk to cncf-1-15\n  Normal  Pulling    3s    kubelet, cncf-1-15  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     2s    kubelet, cncf-1-15  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    2s    kubelet, cncf-1-15  Created container redis-master\n  Normal  Started    2s    kubelet, cncf-1-15  Started container redis-master\n"
Jul 13 09:34:16.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 describe rc redis-master --namespace=kubectl-5489'
Jul 13 09:34:16.268: INFO: stderr: ""
Jul 13 09:34:16.268: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5489\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-cfnlk\n"
Jul 13 09:34:16.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 describe service redis-master --namespace=kubectl-5489'
Jul 13 09:34:16.427: INFO: stderr: ""
Jul 13 09:34:16.427: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5489\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.3.43.170\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.2.1.95:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 13 09:34:16.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 describe node cncf-1-15'
Jul 13 09:34:16.633: INFO: stderr: ""
Jul 13 09:34:16.633: INFO: stdout: "Name:               cncf-1-15\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=0ff9e048-50af-4b2e-bc61-72611d23fca7\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=GRA5\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cncf-1-15\n                    kubernetes.io/os=linux\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"4e:f2:5f:16:f1:06\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 51.68.114.204\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.2.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 13 Jul 2019 08:59:38 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 13 Jul 2019 09:34:12 +0000   Sat, 13 Jul 2019 08:59:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 13 Jul 2019 09:34:12 +0000   Sat, 13 Jul 2019 08:59:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 13 Jul 2019 09:34:12 +0000   Sat, 13 Jul 2019 08:59:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 13 Jul 2019 09:34:12 +0000   Sat, 13 Jul 2019 08:59:58 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  51.68.114.204\n  Hostname:    cncf-1-15\nCapacity:\n attachable-volumes-cinder:  256\n cpu:                        2\n ephemeral-storage:          48375392Ki\n hugepages-2Mi:              0\n memory:                     6966220Ki\n pods:                       110\nAllocatable:\n attachable-volumes-cinder:  256\n cpu:                        1900m\n ephemeral-storage:          48375392Ki\n hugepages-2Mi:              0\n memory:                     5405644Ki\n pods:                       110\nSystem Info:\n Machine ID:                 b8a24c1d6a1748be8a6bfdbe80aaa103\n System UUID:                b8a24c1d-6a17-48be-8a6b-fdbe80aaa103\n Boot ID:                    51a32041-4372-428c-88f7-3f80171daa3e\n Kernel Version:             4.19.50-coreos-r1\n OS Image:                   Container Linux by CoreOS 2135.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     10.2.1.0/24\nProviderID:                  openstack:///b8a24c1d-6a17-48be-8a6b-fdbe80aaa103\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  heptio-sonobuoy            sonobuoy-e2e-job-5b31513983fe4ca4                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-9454bcfaa70145b2-zks8j    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  kube-system                canal-r8n6w                                                250m (13%)    0 (0%)      0 (0%)           0 (0%)         34m\n  kube-system                kube-proxy-qsskh                                           100m (5%)     0 (0%)      200Mi (3%)       200Mi (3%)     34m\n  kube-system                wormhole-svb5x                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  kubectl-5489               redis-master-cfnlk                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        350m (18%)  0 (0%)\n  memory                     200Mi (3%)  200Mi (3%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-cinder  0           0\nEvents:\n  Type    Reason                   Age                From                   Message\n  ----    ------                   ----               ----                   -------\n  Normal  Starting                 34m                kubelet, cncf-1-15     Starting kubelet.\n  Normal  NodeHasSufficientMemory  34m (x2 over 34m)  kubelet, cncf-1-15     Node cncf-1-15 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    34m (x2 over 34m)  kubelet, cncf-1-15     Node cncf-1-15 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     34m (x2 over 34m)  kubelet, cncf-1-15     Node cncf-1-15 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  34m                kubelet, cncf-1-15     Updated limits on kube reserved cgroup /system.slice\n  Normal  NodeAllocatableEnforced  34m                kubelet, cncf-1-15     Updated Node Allocatable limit across pods\n  Normal  Starting                 34m                kube-proxy, cncf-1-15  Starting kube-proxy.\n  Normal  NodeReady                34m                kubelet, cncf-1-15     Node cncf-1-15 status is now: NodeReady\n"
Jul 13 09:34:16.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 describe namespace kubectl-5489'
Jul 13 09:34:16.829: INFO: stderr: ""
Jul 13 09:34:16.829: INFO: stdout: "Name:         kubectl-5489\nLabels:       e2e-framework=kubectl\n              e2e-run=b3114902-d1d3-439d-8891-9b6137dadfcb\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:34:16.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5489" for this suite.
Jul 13 09:34:40.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:34:41.203: INFO: namespace kubectl-5489 deletion completed in 24.363580489s

• [SLOW TEST:29.165 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:34:41.203: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 13 09:34:44.401: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:34:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4108" for this suite.
Jul 13 09:34:50.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:34:50.779: INFO: namespace container-runtime-4108 deletion completed in 6.34145797s

• [SLOW TEST:9.576 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:34:50.779: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 13 09:34:50.930: INFO: Waiting up to 5m0s for pod "pod-bbc3b417-cc79-42b0-9434-66d284a0fd59" in namespace "emptydir-6723" to be "success or failure"
Jul 13 09:34:50.950: INFO: Pod "pod-bbc3b417-cc79-42b0-9434-66d284a0fd59": Phase="Pending", Reason="", readiness=false. Elapsed: 19.246408ms
Jul 13 09:34:52.961: INFO: Pod "pod-bbc3b417-cc79-42b0-9434-66d284a0fd59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030114735s
Jul 13 09:34:54.970: INFO: Pod "pod-bbc3b417-cc79-42b0-9434-66d284a0fd59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039324693s
STEP: Saw pod success
Jul 13 09:34:54.970: INFO: Pod "pod-bbc3b417-cc79-42b0-9434-66d284a0fd59" satisfied condition "success or failure"
Jul 13 09:34:54.978: INFO: Trying to get logs from node cncf-1-15 pod pod-bbc3b417-cc79-42b0-9434-66d284a0fd59 container test-container: <nil>
STEP: delete the pod
Jul 13 09:34:55.036: INFO: Waiting for pod pod-bbc3b417-cc79-42b0-9434-66d284a0fd59 to disappear
Jul 13 09:34:55.043: INFO: Pod pod-bbc3b417-cc79-42b0-9434-66d284a0fd59 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:34:55.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6723" for this suite.
Jul 13 09:35:01.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:35:01.405: INFO: namespace emptydir-6723 deletion completed in 6.352882619s

• [SLOW TEST:10.625 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:35:01.405: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2bf38919-0a54-431d-9086-5fe7d9640dae
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-2bf38919-0a54-431d-9086-5fe7d9640dae
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:36:22.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6177" for this suite.
Jul 13 09:36:46.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:36:47.185: INFO: namespace projected-6177 deletion completed in 24.361703035s

• [SLOW TEST:105.780 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:36:47.185: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-6fad8475-63a8-47b6-a00f-226eeb64283d
STEP: Creating a pod to test consume configMaps
Jul 13 09:36:47.359: INFO: Waiting up to 5m0s for pod "pod-configmaps-caef8909-8c39-481d-b9f3-ea5f5bcc1bb9" in namespace "configmap-2311" to be "success or failure"
Jul 13 09:36:47.376: INFO: Pod "pod-configmaps-caef8909-8c39-481d-b9f3-ea5f5bcc1bb9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.065104ms
Jul 13 09:36:49.385: INFO: Pod "pod-configmaps-caef8909-8c39-481d-b9f3-ea5f5bcc1bb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025406924s
Jul 13 09:36:51.397: INFO: Pod "pod-configmaps-caef8909-8c39-481d-b9f3-ea5f5bcc1bb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037356336s
STEP: Saw pod success
Jul 13 09:36:51.397: INFO: Pod "pod-configmaps-caef8909-8c39-481d-b9f3-ea5f5bcc1bb9" satisfied condition "success or failure"
Jul 13 09:36:51.406: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-caef8909-8c39-481d-b9f3-ea5f5bcc1bb9 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 09:36:51.465: INFO: Waiting for pod pod-configmaps-caef8909-8c39-481d-b9f3-ea5f5bcc1bb9 to disappear
Jul 13 09:36:51.472: INFO: Pod pod-configmaps-caef8909-8c39-481d-b9f3-ea5f5bcc1bb9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:36:51.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2311" for this suite.
Jul 13 09:36:57.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:36:57.809: INFO: namespace configmap-2311 deletion completed in 6.327051653s

• [SLOW TEST:10.623 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:36:57.809: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:37:03.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3082" for this suite.
Jul 13 09:37:09.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:37:09.851: INFO: namespace watch-3082 deletion completed in 6.438989406s

• [SLOW TEST:12.042 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:37:09.853: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-ae89cf76-4d5b-493e-a11d-e68d26bc5386
STEP: Creating a pod to test consume configMaps
Jul 13 09:37:10.023: INFO: Waiting up to 5m0s for pod "pod-configmaps-99dbb68a-d8b4-4081-ac9e-35804744d88e" in namespace "configmap-799" to be "success or failure"
Jul 13 09:37:10.035: INFO: Pod "pod-configmaps-99dbb68a-d8b4-4081-ac9e-35804744d88e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.628367ms
Jul 13 09:37:12.045: INFO: Pod "pod-configmaps-99dbb68a-d8b4-4081-ac9e-35804744d88e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021861784s
Jul 13 09:37:14.054: INFO: Pod "pod-configmaps-99dbb68a-d8b4-4081-ac9e-35804744d88e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03064747s
STEP: Saw pod success
Jul 13 09:37:14.054: INFO: Pod "pod-configmaps-99dbb68a-d8b4-4081-ac9e-35804744d88e" satisfied condition "success or failure"
Jul 13 09:37:14.061: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-99dbb68a-d8b4-4081-ac9e-35804744d88e container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 09:37:14.109: INFO: Waiting for pod pod-configmaps-99dbb68a-d8b4-4081-ac9e-35804744d88e to disappear
Jul 13 09:37:14.119: INFO: Pod pod-configmaps-99dbb68a-d8b4-4081-ac9e-35804744d88e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:37:14.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-799" for this suite.
Jul 13 09:37:20.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:37:20.509: INFO: namespace configmap-799 deletion completed in 6.380737415s

• [SLOW TEST:10.657 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:37:20.510: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-f74469de-f497-4a85-bae5-3322000c9f85
STEP: Creating a pod to test consume secrets
Jul 13 09:37:20.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-232ffcd6-8355-4873-9fa9-b51a43d48166" in namespace "projected-179" to be "success or failure"
Jul 13 09:37:20.690: INFO: Pod "pod-projected-secrets-232ffcd6-8355-4873-9fa9-b51a43d48166": Phase="Pending", Reason="", readiness=false. Elapsed: 13.980827ms
Jul 13 09:37:22.701: INFO: Pod "pod-projected-secrets-232ffcd6-8355-4873-9fa9-b51a43d48166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024590566s
Jul 13 09:37:24.712: INFO: Pod "pod-projected-secrets-232ffcd6-8355-4873-9fa9-b51a43d48166": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036421146s
STEP: Saw pod success
Jul 13 09:37:24.712: INFO: Pod "pod-projected-secrets-232ffcd6-8355-4873-9fa9-b51a43d48166" satisfied condition "success or failure"
Jul 13 09:37:24.722: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-secrets-232ffcd6-8355-4873-9fa9-b51a43d48166 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 09:37:24.790: INFO: Waiting for pod pod-projected-secrets-232ffcd6-8355-4873-9fa9-b51a43d48166 to disappear
Jul 13 09:37:24.798: INFO: Pod pod-projected-secrets-232ffcd6-8355-4873-9fa9-b51a43d48166 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:37:24.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-179" for this suite.
Jul 13 09:37:30.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:37:31.261: INFO: namespace projected-179 deletion completed in 6.454307463s

• [SLOW TEST:10.751 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:37:31.262: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:37:31.386: INFO: Creating deployment "test-recreate-deployment"
Jul 13 09:37:31.403: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 13 09:37:31.428: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jul 13 09:37:33.450: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 13 09:37:33.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607451, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607451, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607451, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607451, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:37:35.466: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 13 09:37:35.489: INFO: Updating deployment test-recreate-deployment
Jul 13 09:37:35.490: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 13 09:37:35.637: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-8514,SelfLink:/apis/apps/v1/namespaces/deployment-8514/deployments/test-recreate-deployment,UID:188fc518-097f-49dc-abab-4b2d4c506643,ResourceVersion:223508217,Generation:2,CreationTimestamp:2019-07-13 09:37:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-13 09:37:35 +0000 UTC 2019-07-13 09:37:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-13 09:37:35 +0000 UTC 2019-07-13 09:37:31 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul 13 09:37:35.647: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-8514,SelfLink:/apis/apps/v1/namespaces/deployment-8514/replicasets/test-recreate-deployment-5c8c9cc69d,UID:cd9612ac-a32e-4328-89d7-0e227c42bda9,ResourceVersion:223508215,Generation:1,CreationTimestamp:2019-07-13 09:37:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 188fc518-097f-49dc-abab-4b2d4c506643 0xc0032e6ea7 0xc0032e6ea8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 13 09:37:35.647: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 13 09:37:35.647: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-8514,SelfLink:/apis/apps/v1/namespaces/deployment-8514/replicasets/test-recreate-deployment-6df85df6b9,UID:ea36e634-5fa5-4d52-80d7-4ec589bef91d,ResourceVersion:223508204,Generation:2,CreationTimestamp:2019-07-13 09:37:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 188fc518-097f-49dc-abab-4b2d4c506643 0xc0032e6f77 0xc0032e6f78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 13 09:37:35.656: INFO: Pod "test-recreate-deployment-5c8c9cc69d-lq4q5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-lq4q5,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-8514,SelfLink:/api/v1/namespaces/deployment-8514/pods/test-recreate-deployment-5c8c9cc69d-lq4q5,UID:bd558b9b-b04f-4892-9428-78ec84b1eb9b,ResourceVersion:223508212,Generation:0,CreationTimestamp:2019-07-13 09:37:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d cd9612ac-a32e-4328-89d7-0e227c42bda9 0xc0032e7857 0xc0032e7858}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stttq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stttq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stttq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032e78c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032e78e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 09:37:35 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:37:35.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8514" for this suite.
Jul 13 09:37:41.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:37:42.043: INFO: namespace deployment-8514 deletion completed in 6.37642029s

• [SLOW TEST:10.781 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:37:42.044: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 13 09:37:42.156: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jul 13 09:37:42.810: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 13 09:37:44.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:37:46.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:37:48.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:37:50.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:37:52.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:37:54.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698607462, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:37:58.170: INFO: Waited 1.024504127s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:37:58.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1974" for this suite.
Jul 13 09:38:04.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:38:05.103: INFO: namespace aggregator-1974 deletion completed in 6.342452407s

• [SLOW TEST:23.060 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:38:05.105: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 13 09:38:05.220: INFO: namespace kubectl-8752
Jul 13 09:38:05.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-8752'
Jul 13 09:38:05.566: INFO: stderr: ""
Jul 13 09:38:05.566: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 13 09:38:06.574: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:38:06.574: INFO: Found 0 / 1
Jul 13 09:38:07.576: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:38:07.576: INFO: Found 0 / 1
Jul 13 09:38:08.577: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:38:08.577: INFO: Found 1 / 1
Jul 13 09:38:08.577: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 13 09:38:08.587: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:38:08.587: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 13 09:38:08.587: INFO: wait on redis-master startup in kubectl-8752 
Jul 13 09:38:08.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 logs redis-master-xgb9z redis-master --namespace=kubectl-8752'
Jul 13 09:38:08.766: INFO: stderr: ""
Jul 13 09:38:08.766: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Jul 09:38:07.413 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Jul 09:38:07.413 # Server started, Redis version 3.2.12\n1:M 13 Jul 09:38:07.413 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Jul 09:38:07.413 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 13 09:38:08.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8752'
Jul 13 09:38:08.944: INFO: stderr: ""
Jul 13 09:38:08.944: INFO: stdout: "service/rm2 exposed\n"
Jul 13 09:38:08.954: INFO: Service rm2 in namespace kubectl-8752 found.
STEP: exposing service
Jul 13 09:38:10.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8752'
Jul 13 09:38:11.149: INFO: stderr: ""
Jul 13 09:38:11.149: INFO: stdout: "service/rm3 exposed\n"
Jul 13 09:38:11.159: INFO: Service rm3 in namespace kubectl-8752 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:38:13.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8752" for this suite.
Jul 13 09:38:37.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:38:37.555: INFO: namespace kubectl-8752 deletion completed in 24.369346433s

• [SLOW TEST:32.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:38:37.557: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 13 09:38:37.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6720'
Jul 13 09:38:37.865: INFO: stderr: ""
Jul 13 09:38:37.865: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 13 09:38:42.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pod e2e-test-nginx-pod --namespace=kubectl-6720 -o json'
Jul 13 09:38:43.051: INFO: stderr: ""
Jul 13 09:38:43.051: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.2.1.106/32\"\n        },\n        \"creationTimestamp\": \"2019-07-13T09:38:37Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6720\",\n        \"resourceVersion\": \"223509806\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6720/pods/e2e-test-nginx-pod\",\n        \"uid\": \"9cf34a3f-d2cc-4332-9683-3d64e00523ed\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-xmr9n\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cncf-1-15\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-xmr9n\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-xmr9n\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-13T09:38:37Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-13T09:38:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-13T09:38:41Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-13T09:38:37Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2390ccd550c41cb3beac4ba02f4784d8bd1acb658b681e80e07e849fe01f3779\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-13T09:38:40Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"51.68.114.204\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.2.1.106\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-13T09:38:37Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 13 09:38:43.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 replace -f - --namespace=kubectl-6720'
Jul 13 09:38:43.356: INFO: stderr: ""
Jul 13 09:38:43.356: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jul 13 09:38:43.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete pods e2e-test-nginx-pod --namespace=kubectl-6720'
Jul 13 09:38:57.704: INFO: stderr: ""
Jul 13 09:38:57.704: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:38:57.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6720" for this suite.
Jul 13 09:39:03.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:39:04.063: INFO: namespace kubectl-6720 deletion completed in 6.346471779s

• [SLOW TEST:26.506 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:39:04.064: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-643d9ab2-5e53-4035-9ddd-be6b2d4a0eaf in namespace container-probe-9168
Jul 13 09:39:08.234: INFO: Started pod busybox-643d9ab2-5e53-4035-9ddd-be6b2d4a0eaf in namespace container-probe-9168
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 09:39:08.244: INFO: Initial restart count of pod busybox-643d9ab2-5e53-4035-9ddd-be6b2d4a0eaf is 0
Jul 13 09:40:02.548: INFO: Restart count of pod container-probe-9168/busybox-643d9ab2-5e53-4035-9ddd-be6b2d4a0eaf is now 1 (54.30424505s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:40:02.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9168" for this suite.
Jul 13 09:40:08.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:40:08.937: INFO: namespace container-probe-9168 deletion completed in 6.345919259s

• [SLOW TEST:64.872 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:40:08.938: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 13 09:40:17.208: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:17.217: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:19.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:19.225: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:21.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:21.228: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:23.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:23.228: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:25.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:25.228: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:27.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:27.227: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:29.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:29.228: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:31.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:31.228: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:33.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:33.227: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:35.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:35.227: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:37.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:37.228: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 09:40:39.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 09:40:39.228: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:40:39.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2350" for this suite.
Jul 13 09:41:03.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:41:03.571: INFO: namespace container-lifecycle-hook-2350 deletion completed in 24.333714723s

• [SLOW TEST:54.633 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:41:03.573: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 13 09:41:03.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-resource-version,UID:7bce5630-18b5-43f9-a2de-b6a9d7b3b291,ResourceVersion:223512927,Generation:0,CreationTimestamp:2019-07-13 09:41:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 09:41:03.737: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-resource-version,UID:7bce5630-18b5-43f9-a2de-b6a9d7b3b291,ResourceVersion:223512929,Generation:0,CreationTimestamp:2019-07-13 09:41:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:41:03.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-320" for this suite.
Jul 13 09:41:09.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:41:10.114: INFO: namespace watch-320 deletion completed in 6.367341965s

• [SLOW TEST:6.542 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:41:10.116: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jul 13 09:41:10.258: INFO: Waiting up to 5m0s for pod "client-containers-e10bfd88-fb8c-4bfb-8f2f-3691493a11a0" in namespace "containers-8057" to be "success or failure"
Jul 13 09:41:10.266: INFO: Pod "client-containers-e10bfd88-fb8c-4bfb-8f2f-3691493a11a0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.35905ms
Jul 13 09:41:12.274: INFO: Pod "client-containers-e10bfd88-fb8c-4bfb-8f2f-3691493a11a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015865003s
Jul 13 09:41:14.285: INFO: Pod "client-containers-e10bfd88-fb8c-4bfb-8f2f-3691493a11a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026852417s
STEP: Saw pod success
Jul 13 09:41:14.285: INFO: Pod "client-containers-e10bfd88-fb8c-4bfb-8f2f-3691493a11a0" satisfied condition "success or failure"
Jul 13 09:41:14.302: INFO: Trying to get logs from node cncf-1-15 pod client-containers-e10bfd88-fb8c-4bfb-8f2f-3691493a11a0 container test-container: <nil>
STEP: delete the pod
Jul 13 09:41:14.363: INFO: Waiting for pod client-containers-e10bfd88-fb8c-4bfb-8f2f-3691493a11a0 to disappear
Jul 13 09:41:14.370: INFO: Pod client-containers-e10bfd88-fb8c-4bfb-8f2f-3691493a11a0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:41:14.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8057" for this suite.
Jul 13 09:41:20.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:41:20.732: INFO: namespace containers-8057 deletion completed in 6.352337884s

• [SLOW TEST:10.616 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:41:20.735: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:41:23.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2870" for this suite.
Jul 13 09:41:29.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:41:29.399: INFO: namespace emptydir-wrapper-2870 deletion completed in 6.375747877s

• [SLOW TEST:8.665 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:41:29.402: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-edae7fae-2606-43d1-86ca-06eb2da54f49
STEP: Creating a pod to test consume configMaps
Jul 13 09:41:29.558: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09aa2bb5-b707-4262-83f0-7deedd155970" in namespace "projected-5949" to be "success or failure"
Jul 13 09:41:29.568: INFO: Pod "pod-projected-configmaps-09aa2bb5-b707-4262-83f0-7deedd155970": Phase="Pending", Reason="", readiness=false. Elapsed: 9.702242ms
Jul 13 09:41:31.578: INFO: Pod "pod-projected-configmaps-09aa2bb5-b707-4262-83f0-7deedd155970": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020416235s
Jul 13 09:41:33.589: INFO: Pod "pod-projected-configmaps-09aa2bb5-b707-4262-83f0-7deedd155970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031054818s
STEP: Saw pod success
Jul 13 09:41:33.589: INFO: Pod "pod-projected-configmaps-09aa2bb5-b707-4262-83f0-7deedd155970" satisfied condition "success or failure"
Jul 13 09:41:33.599: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-configmaps-09aa2bb5-b707-4262-83f0-7deedd155970 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 09:41:33.655: INFO: Waiting for pod pod-projected-configmaps-09aa2bb5-b707-4262-83f0-7deedd155970 to disappear
Jul 13 09:41:33.665: INFO: Pod pod-projected-configmaps-09aa2bb5-b707-4262-83f0-7deedd155970 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:41:33.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5949" for this suite.
Jul 13 09:41:39.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:41:40.068: INFO: namespace projected-5949 deletion completed in 6.391691783s

• [SLOW TEST:10.666 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:41:40.068: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:42:40.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6875" for this suite.
Jul 13 09:43:04.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:43:04.582: INFO: namespace container-probe-6875 deletion completed in 24.31651684s

• [SLOW TEST:84.514 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:43:04.583: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 13 09:43:04.709: INFO: Waiting up to 5m0s for pod "downward-api-a005767c-22eb-47b1-8ff3-01399b5892a7" in namespace "downward-api-5103" to be "success or failure"
Jul 13 09:43:04.717: INFO: Pod "downward-api-a005767c-22eb-47b1-8ff3-01399b5892a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.183522ms
Jul 13 09:43:06.726: INFO: Pod "downward-api-a005767c-22eb-47b1-8ff3-01399b5892a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017654858s
Jul 13 09:43:08.737: INFO: Pod "downward-api-a005767c-22eb-47b1-8ff3-01399b5892a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027919983s
STEP: Saw pod success
Jul 13 09:43:08.737: INFO: Pod "downward-api-a005767c-22eb-47b1-8ff3-01399b5892a7" satisfied condition "success or failure"
Jul 13 09:43:08.745: INFO: Trying to get logs from node cncf-1-15 pod downward-api-a005767c-22eb-47b1-8ff3-01399b5892a7 container dapi-container: <nil>
STEP: delete the pod
Jul 13 09:43:08.795: INFO: Waiting for pod downward-api-a005767c-22eb-47b1-8ff3-01399b5892a7 to disappear
Jul 13 09:43:08.803: INFO: Pod downward-api-a005767c-22eb-47b1-8ff3-01399b5892a7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:43:08.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5103" for this suite.
Jul 13 09:43:14.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:43:15.187: INFO: namespace downward-api-5103 deletion completed in 6.375253038s

• [SLOW TEST:10.603 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:43:15.187: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:43:19.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3975" for this suite.
Jul 13 09:44:09.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:44:09.768: INFO: namespace kubelet-test-3975 deletion completed in 50.351502611s

• [SLOW TEST:54.581 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:44:09.768: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-3f5caedf-b0d0-4218-a129-fa8c4666b67f
STEP: Creating a pod to test consume configMaps
Jul 13 09:44:09.939: INFO: Waiting up to 5m0s for pod "pod-configmaps-b5c0d3a3-45b0-47d5-ac74-41e9bb66728d" in namespace "configmap-5177" to be "success or failure"
Jul 13 09:44:09.946: INFO: Pod "pod-configmaps-b5c0d3a3-45b0-47d5-ac74-41e9bb66728d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.94845ms
Jul 13 09:44:11.957: INFO: Pod "pod-configmaps-b5c0d3a3-45b0-47d5-ac74-41e9bb66728d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01802606s
Jul 13 09:44:13.968: INFO: Pod "pod-configmaps-b5c0d3a3-45b0-47d5-ac74-41e9bb66728d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028208717s
STEP: Saw pod success
Jul 13 09:44:13.968: INFO: Pod "pod-configmaps-b5c0d3a3-45b0-47d5-ac74-41e9bb66728d" satisfied condition "success or failure"
Jul 13 09:44:13.976: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-b5c0d3a3-45b0-47d5-ac74-41e9bb66728d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 09:44:14.044: INFO: Waiting for pod pod-configmaps-b5c0d3a3-45b0-47d5-ac74-41e9bb66728d to disappear
Jul 13 09:44:14.054: INFO: Pod pod-configmaps-b5c0d3a3-45b0-47d5-ac74-41e9bb66728d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:44:14.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5177" for this suite.
Jul 13 09:44:20.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:44:20.460: INFO: namespace configmap-5177 deletion completed in 6.390323075s

• [SLOW TEST:10.692 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:44:20.462: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7210
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 13 09:44:20.601: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 13 09:44:46.806: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.118:8080/dial?request=hostName&protocol=http&host=10.2.1.117&port=8080&tries=1'] Namespace:pod-network-test-7210 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 09:44:46.806: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 09:44:47.070: INFO: Waiting for endpoints: map[]
Jul 13 09:44:47.079: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.118:8080/dial?request=hostName&protocol=http&host=10.2.0.29&port=8080&tries=1'] Namespace:pod-network-test-7210 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 09:44:47.079: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 09:44:47.332: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:44:47.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7210" for this suite.
Jul 13 09:45:11.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:45:11.689: INFO: namespace pod-network-test-7210 deletion completed in 24.345567584s

• [SLOW TEST:51.227 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:45:11.691: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f722583a-2184-437a-9d64-87643c10e8db
STEP: Creating a pod to test consume secrets
Jul 13 09:45:11.852: INFO: Waiting up to 5m0s for pod "pod-secrets-af7c9150-a9eb-46c8-b039-d7bd7c0fe713" in namespace "secrets-2945" to be "success or failure"
Jul 13 09:45:11.866: INFO: Pod "pod-secrets-af7c9150-a9eb-46c8-b039-d7bd7c0fe713": Phase="Pending", Reason="", readiness=false. Elapsed: 14.123774ms
Jul 13 09:45:13.876: INFO: Pod "pod-secrets-af7c9150-a9eb-46c8-b039-d7bd7c0fe713": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024059376s
Jul 13 09:45:15.886: INFO: Pod "pod-secrets-af7c9150-a9eb-46c8-b039-d7bd7c0fe713": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034322256s
STEP: Saw pod success
Jul 13 09:45:15.886: INFO: Pod "pod-secrets-af7c9150-a9eb-46c8-b039-d7bd7c0fe713" satisfied condition "success or failure"
Jul 13 09:45:15.895: INFO: Trying to get logs from node cncf-1-15 pod pod-secrets-af7c9150-a9eb-46c8-b039-d7bd7c0fe713 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 09:45:15.952: INFO: Waiting for pod pod-secrets-af7c9150-a9eb-46c8-b039-d7bd7c0fe713 to disappear
Jul 13 09:45:15.959: INFO: Pod pod-secrets-af7c9150-a9eb-46c8-b039-d7bd7c0fe713 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:45:15.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2945" for this suite.
Jul 13 09:45:22.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:45:22.334: INFO: namespace secrets-2945 deletion completed in 6.364976516s

• [SLOW TEST:10.643 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:45:22.334: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 13 09:45:22.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-8433'
Jul 13 09:45:22.911: INFO: stderr: ""
Jul 13 09:45:22.911: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 13 09:45:23.921: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:45:23.921: INFO: Found 0 / 1
Jul 13 09:45:24.921: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:45:24.921: INFO: Found 0 / 1
Jul 13 09:45:25.922: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:45:25.922: INFO: Found 1 / 1
Jul 13 09:45:25.922: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 13 09:45:25.940: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:45:25.940: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 13 09:45:25.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 patch pod redis-master-m52kf --namespace=kubectl-8433 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 13 09:45:26.090: INFO: stderr: ""
Jul 13 09:45:26.090: INFO: stdout: "pod/redis-master-m52kf patched\n"
STEP: checking annotations
Jul 13 09:45:26.104: INFO: Selector matched 1 pods for map[app:redis]
Jul 13 09:45:26.104: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:45:26.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8433" for this suite.
Jul 13 09:45:50.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:45:50.452: INFO: namespace kubectl-8433 deletion completed in 24.338234095s

• [SLOW TEST:28.118 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:45:50.453: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 13 09:45:58.708: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 09:45:58.717: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 09:46:00.718: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 09:46:00.729: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 09:46:02.718: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 09:46:02.728: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 09:46:04.718: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 09:46:04.728: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 09:46:06.718: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 09:46:06.751: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 09:46:08.718: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 09:46:08.759: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:46:08.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-375" for this suite.
Jul 13 09:46:32.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:46:33.153: INFO: namespace container-lifecycle-hook-375 deletion completed in 24.35967347s

• [SLOW TEST:42.701 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:46:33.154: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-6316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6316 to expose endpoints map[]
Jul 13 09:46:33.321: INFO: successfully validated that service multi-endpoint-test in namespace services-6316 exposes endpoints map[] (16.242989ms elapsed)
STEP: Creating pod pod1 in namespace services-6316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6316 to expose endpoints map[pod1:[100]]
Jul 13 09:46:36.425: INFO: successfully validated that service multi-endpoint-test in namespace services-6316 exposes endpoints map[pod1:[100]] (3.084762526s elapsed)
STEP: Creating pod pod2 in namespace services-6316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6316 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 13 09:46:39.548: INFO: successfully validated that service multi-endpoint-test in namespace services-6316 exposes endpoints map[pod1:[100] pod2:[101]] (3.10777622s elapsed)
STEP: Deleting pod pod1 in namespace services-6316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6316 to expose endpoints map[pod2:[101]]
Jul 13 09:46:40.627: INFO: successfully validated that service multi-endpoint-test in namespace services-6316 exposes endpoints map[pod2:[101]] (1.065101921s elapsed)
STEP: Deleting pod pod2 in namespace services-6316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6316 to expose endpoints map[]
Jul 13 09:46:41.662: INFO: successfully validated that service multi-endpoint-test in namespace services-6316 exposes endpoints map[] (1.020089324s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:46:41.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6316" for this suite.
Jul 13 09:47:05.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:47:06.103: INFO: namespace services-6316 deletion completed in 24.387208055s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:32.949 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:47:06.104: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1422, will wait for the garbage collector to delete the pods
Jul 13 09:47:12.338: INFO: Deleting Job.batch foo took: 18.620291ms
Jul 13 09:47:12.738: INFO: Terminating Job.batch foo pods took: 400.495037ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:47:47.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1422" for this suite.
Jul 13 09:47:53.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:47:54.109: INFO: namespace job-1422 deletion completed in 6.350129591s

• [SLOW TEST:48.005 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:47:54.111: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 09:47:54.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c19345e3-adc0-47da-a7d2-c9bc039c9b53" in namespace "projected-7577" to be "success or failure"
Jul 13 09:47:54.277: INFO: Pod "downwardapi-volume-c19345e3-adc0-47da-a7d2-c9bc039c9b53": Phase="Pending", Reason="", readiness=false. Elapsed: 6.353853ms
Jul 13 09:47:56.289: INFO: Pod "downwardapi-volume-c19345e3-adc0-47da-a7d2-c9bc039c9b53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018126238s
Jul 13 09:47:58.300: INFO: Pod "downwardapi-volume-c19345e3-adc0-47da-a7d2-c9bc039c9b53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029272464s
STEP: Saw pod success
Jul 13 09:47:58.300: INFO: Pod "downwardapi-volume-c19345e3-adc0-47da-a7d2-c9bc039c9b53" satisfied condition "success or failure"
Jul 13 09:47:58.309: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-c19345e3-adc0-47da-a7d2-c9bc039c9b53 container client-container: <nil>
STEP: delete the pod
Jul 13 09:47:58.363: INFO: Waiting for pod downwardapi-volume-c19345e3-adc0-47da-a7d2-c9bc039c9b53 to disappear
Jul 13 09:47:58.369: INFO: Pod downwardapi-volume-c19345e3-adc0-47da-a7d2-c9bc039c9b53 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:47:58.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7577" for this suite.
Jul 13 09:48:04.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:48:04.714: INFO: namespace projected-7577 deletion completed in 6.3364757s

• [SLOW TEST:10.604 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:48:04.716: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-b97f4987-8e7c-46e2-9cc1-00f30f4afc70
STEP: Creating a pod to test consume configMaps
Jul 13 09:48:04.864: INFO: Waiting up to 5m0s for pod "pod-configmaps-cec31306-4a5d-4188-9476-22f413657a1b" in namespace "configmap-1588" to be "success or failure"
Jul 13 09:48:04.872: INFO: Pod "pod-configmaps-cec31306-4a5d-4188-9476-22f413657a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.96238ms
Jul 13 09:48:06.882: INFO: Pod "pod-configmaps-cec31306-4a5d-4188-9476-22f413657a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018028139s
Jul 13 09:48:08.894: INFO: Pod "pod-configmaps-cec31306-4a5d-4188-9476-22f413657a1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029466071s
STEP: Saw pod success
Jul 13 09:48:08.894: INFO: Pod "pod-configmaps-cec31306-4a5d-4188-9476-22f413657a1b" satisfied condition "success or failure"
Jul 13 09:48:08.902: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-cec31306-4a5d-4188-9476-22f413657a1b container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 09:48:08.966: INFO: Waiting for pod pod-configmaps-cec31306-4a5d-4188-9476-22f413657a1b to disappear
Jul 13 09:48:08.973: INFO: Pod pod-configmaps-cec31306-4a5d-4188-9476-22f413657a1b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:48:08.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1588" for this suite.
Jul 13 09:48:15.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:48:15.351: INFO: namespace configmap-1588 deletion completed in 6.369816523s

• [SLOW TEST:10.635 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:48:15.353: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 13 09:48:21.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec pod-sharedvolume-f804ecbb-d0c5-4fa4-bdb6-40a89599173e -c busybox-main-container --namespace=emptydir-3306 -- cat /usr/share/volumeshare/shareddata.txt'
Jul 13 09:48:21.927: INFO: stderr: ""
Jul 13 09:48:21.927: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:48:21.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3306" for this suite.
Jul 13 09:48:27.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:48:28.284: INFO: namespace emptydir-3306 deletion completed in 6.345246355s

• [SLOW TEST:12.932 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:48:28.286: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-jwdlf in namespace proxy-4825
I0713 09:48:28.463039      15 runners.go:180] Created replication controller with name: proxy-service-jwdlf, namespace: proxy-4825, replica count: 1
I0713 09:48:29.513545      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0713 09:48:30.513781      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0713 09:48:31.514082      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0713 09:48:32.514575      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 09:48:33.515068      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 09:48:34.515441      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 09:48:35.515889      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 09:48:36.516308      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 09:48:37.516725      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 09:48:38.517146      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 09:48:39.517586      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 09:48:40.518174      15 runners.go:180] proxy-service-jwdlf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 13 09:48:40.528: INFO: setup took 12.113929091s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 13 09:48:40.560: INFO: (0) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 31.486929ms)
Jul 13 09:48:40.566: INFO: (0) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 37.176545ms)
Jul 13 09:48:40.568: INFO: (0) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 39.978952ms)
Jul 13 09:48:40.574: INFO: (0) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 45.460717ms)
Jul 13 09:48:40.584: INFO: (0) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 55.586194ms)
Jul 13 09:48:40.584: INFO: (0) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 55.390737ms)
Jul 13 09:48:40.584: INFO: (0) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 55.285335ms)
Jul 13 09:48:40.586: INFO: (0) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 58.083834ms)
Jul 13 09:48:40.586: INFO: (0) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 57.994537ms)
Jul 13 09:48:40.586: INFO: (0) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 57.824626ms)
Jul 13 09:48:40.586: INFO: (0) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 58.265623ms)
Jul 13 09:48:40.592: INFO: (0) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 64.341711ms)
Jul 13 09:48:40.592: INFO: (0) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 63.685333ms)
Jul 13 09:48:40.592: INFO: (0) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 63.896865ms)
Jul 13 09:48:40.594: INFO: (0) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 64.42112ms)
Jul 13 09:48:40.595: INFO: (0) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 66.830944ms)
Jul 13 09:48:40.605: INFO: (1) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 9.606261ms)
Jul 13 09:48:40.605: INFO: (1) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 9.650643ms)
Jul 13 09:48:40.605: INFO: (1) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 9.867526ms)
Jul 13 09:48:40.606: INFO: (1) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 10.549777ms)
Jul 13 09:48:40.606: INFO: (1) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 11.117338ms)
Jul 13 09:48:40.606: INFO: (1) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 11.439702ms)
Jul 13 09:48:40.607: INFO: (1) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 11.606436ms)
Jul 13 09:48:40.607: INFO: (1) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 12.416217ms)
Jul 13 09:48:40.607: INFO: (1) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 12.115251ms)
Jul 13 09:48:40.608: INFO: (1) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 12.313401ms)
Jul 13 09:48:40.608: INFO: (1) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 12.257003ms)
Jul 13 09:48:40.610: INFO: (1) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 14.433243ms)
Jul 13 09:48:40.611: INFO: (1) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 15.942988ms)
Jul 13 09:48:40.611: INFO: (1) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 16.358627ms)
Jul 13 09:48:40.612: INFO: (1) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 16.337522ms)
Jul 13 09:48:40.612: INFO: (1) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 16.547043ms)
Jul 13 09:48:40.623: INFO: (2) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 10.493276ms)
Jul 13 09:48:40.623: INFO: (2) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 10.681938ms)
Jul 13 09:48:40.623: INFO: (2) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 10.632774ms)
Jul 13 09:48:40.625: INFO: (2) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 12.296607ms)
Jul 13 09:48:40.625: INFO: (2) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 11.700929ms)
Jul 13 09:48:40.625: INFO: (2) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 12.66645ms)
Jul 13 09:48:40.626: INFO: (2) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 13.05489ms)
Jul 13 09:48:40.626: INFO: (2) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 12.859646ms)
Jul 13 09:48:40.626: INFO: (2) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 12.60978ms)
Jul 13 09:48:40.626: INFO: (2) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 12.773506ms)
Jul 13 09:48:40.627: INFO: (2) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 14.117913ms)
Jul 13 09:48:40.627: INFO: (2) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 15.0742ms)
Jul 13 09:48:40.627: INFO: (2) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 15.503771ms)
Jul 13 09:48:40.627: INFO: (2) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 14.843743ms)
Jul 13 09:48:40.628: INFO: (2) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 15.145316ms)
Jul 13 09:48:40.629: INFO: (2) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 16.344763ms)
Jul 13 09:48:40.639: INFO: (3) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 8.967935ms)
Jul 13 09:48:40.652: INFO: (3) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 21.224719ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 23.588623ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 23.33099ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 23.530229ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 23.894853ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 22.812512ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 23.87672ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 23.708769ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 23.143506ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 23.237846ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 24.315282ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 22.812876ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 23.024955ms)
Jul 13 09:48:40.654: INFO: (3) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 23.193643ms)
Jul 13 09:48:40.657: INFO: (3) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 26.757812ms)
Jul 13 09:48:40.666: INFO: (4) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 8.120278ms)
Jul 13 09:48:40.668: INFO: (4) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 9.258468ms)
Jul 13 09:48:40.668: INFO: (4) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 9.539052ms)
Jul 13 09:48:40.669: INFO: (4) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 11.365245ms)
Jul 13 09:48:40.669: INFO: (4) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 10.489594ms)
Jul 13 09:48:40.670: INFO: (4) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 11.185468ms)
Jul 13 09:48:40.670: INFO: (4) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 12.298309ms)
Jul 13 09:48:40.670: INFO: (4) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 12.123014ms)
Jul 13 09:48:40.670: INFO: (4) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 12.35803ms)
Jul 13 09:48:40.673: INFO: (4) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 14.631885ms)
Jul 13 09:48:40.673: INFO: (4) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 15.164733ms)
Jul 13 09:48:40.673: INFO: (4) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 14.299532ms)
Jul 13 09:48:40.674: INFO: (4) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 14.695777ms)
Jul 13 09:48:40.674: INFO: (4) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 14.391855ms)
Jul 13 09:48:40.674: INFO: (4) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 14.561339ms)
Jul 13 09:48:40.674: INFO: (4) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 14.986105ms)
Jul 13 09:48:40.687: INFO: (5) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 12.011039ms)
Jul 13 09:48:40.687: INFO: (5) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 12.141809ms)
Jul 13 09:48:40.687: INFO: (5) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 12.714917ms)
Jul 13 09:48:40.687: INFO: (5) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 12.642951ms)
Jul 13 09:48:40.689: INFO: (5) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 15.285379ms)
Jul 13 09:48:40.689: INFO: (5) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 15.049697ms)
Jul 13 09:48:40.689: INFO: (5) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 14.564793ms)
Jul 13 09:48:40.690: INFO: (5) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 14.884198ms)
Jul 13 09:48:40.690: INFO: (5) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 15.018949ms)
Jul 13 09:48:40.690: INFO: (5) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 14.900932ms)
Jul 13 09:48:40.691: INFO: (5) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 16.257749ms)
Jul 13 09:48:40.691: INFO: (5) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 16.324762ms)
Jul 13 09:48:40.691: INFO: (5) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 16.362052ms)
Jul 13 09:48:40.692: INFO: (5) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 17.969615ms)
Jul 13 09:48:40.692: INFO: (5) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 18.094343ms)
Jul 13 09:48:40.693: INFO: (5) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 18.680076ms)
Jul 13 09:48:40.703: INFO: (6) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 9.084197ms)
Jul 13 09:48:40.708: INFO: (6) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 13.541274ms)
Jul 13 09:48:40.708: INFO: (6) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 13.745718ms)
Jul 13 09:48:40.708: INFO: (6) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 14.2984ms)
Jul 13 09:48:40.708: INFO: (6) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 14.287468ms)
Jul 13 09:48:40.708: INFO: (6) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 14.286373ms)
Jul 13 09:48:40.709: INFO: (6) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 14.432308ms)
Jul 13 09:48:40.709: INFO: (6) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 14.686021ms)
Jul 13 09:48:40.711: INFO: (6) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 16.845124ms)
Jul 13 09:48:40.711: INFO: (6) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 17.120684ms)
Jul 13 09:48:40.711: INFO: (6) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 17.329663ms)
Jul 13 09:48:40.711: INFO: (6) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 16.833504ms)
Jul 13 09:48:40.713: INFO: (6) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 19.788078ms)
Jul 13 09:48:40.714: INFO: (6) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 19.236985ms)
Jul 13 09:48:40.714: INFO: (6) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 19.437934ms)
Jul 13 09:48:40.714: INFO: (6) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 19.469412ms)
Jul 13 09:48:40.724: INFO: (7) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 10.415768ms)
Jul 13 09:48:40.728: INFO: (7) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 13.407471ms)
Jul 13 09:48:40.728: INFO: (7) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 12.974876ms)
Jul 13 09:48:40.728: INFO: (7) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 13.504459ms)
Jul 13 09:48:40.728: INFO: (7) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 13.467624ms)
Jul 13 09:48:40.731: INFO: (7) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 15.176217ms)
Jul 13 09:48:40.731: INFO: (7) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 15.473205ms)
Jul 13 09:48:40.731: INFO: (7) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 16.069237ms)
Jul 13 09:48:40.731: INFO: (7) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 15.947425ms)
Jul 13 09:48:40.731: INFO: (7) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 16.319316ms)
Jul 13 09:48:40.731: INFO: (7) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 16.600252ms)
Jul 13 09:48:40.732: INFO: (7) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 17.139281ms)
Jul 13 09:48:40.734: INFO: (7) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 20.236721ms)
Jul 13 09:48:40.736: INFO: (7) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 19.897168ms)
Jul 13 09:48:40.736: INFO: (7) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 20.378046ms)
Jul 13 09:48:40.736: INFO: (7) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 20.565931ms)
Jul 13 09:48:40.745: INFO: (8) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 9.323471ms)
Jul 13 09:48:40.752: INFO: (8) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 15.474509ms)
Jul 13 09:48:40.752: INFO: (8) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 15.203129ms)
Jul 13 09:48:40.753: INFO: (8) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 15.893708ms)
Jul 13 09:48:40.753: INFO: (8) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 16.317859ms)
Jul 13 09:48:40.756: INFO: (8) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 19.396391ms)
Jul 13 09:48:40.756: INFO: (8) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 19.181465ms)
Jul 13 09:48:40.756: INFO: (8) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 19.725469ms)
Jul 13 09:48:40.756: INFO: (8) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 19.539644ms)
Jul 13 09:48:40.756: INFO: (8) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 19.977527ms)
Jul 13 09:48:40.756: INFO: (8) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 19.408666ms)
Jul 13 09:48:40.757: INFO: (8) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 19.983384ms)
Jul 13 09:48:40.757: INFO: (8) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 20.506479ms)
Jul 13 09:48:40.759: INFO: (8) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 22.645124ms)
Jul 13 09:48:40.759: INFO: (8) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 22.813893ms)
Jul 13 09:48:40.760: INFO: (8) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 23.431069ms)
Jul 13 09:48:40.774: INFO: (9) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 13.133107ms)
Jul 13 09:48:40.787: INFO: (9) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 25.728595ms)
Jul 13 09:48:40.789: INFO: (9) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 28.603376ms)
Jul 13 09:48:40.789: INFO: (9) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 29.117904ms)
Jul 13 09:48:40.789: INFO: (9) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 27.687373ms)
Jul 13 09:48:40.789: INFO: (9) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 27.542611ms)
Jul 13 09:48:40.790: INFO: (9) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 27.068402ms)
Jul 13 09:48:40.790: INFO: (9) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 27.970147ms)
Jul 13 09:48:40.790: INFO: (9) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 27.754471ms)
Jul 13 09:48:40.793: INFO: (9) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 31.576505ms)
Jul 13 09:48:40.793: INFO: (9) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 31.851569ms)
Jul 13 09:48:40.796: INFO: (9) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 33.437891ms)
Jul 13 09:48:40.796: INFO: (9) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 33.872389ms)
Jul 13 09:48:40.796: INFO: (9) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 34.489122ms)
Jul 13 09:48:40.796: INFO: (9) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 34.95768ms)
Jul 13 09:48:40.796: INFO: (9) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 35.597512ms)
Jul 13 09:48:40.808: INFO: (10) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 11.585753ms)
Jul 13 09:48:40.811: INFO: (10) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 15.016837ms)
Jul 13 09:48:40.811: INFO: (10) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 14.942087ms)
Jul 13 09:48:40.811: INFO: (10) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 14.77366ms)
Jul 13 09:48:40.812: INFO: (10) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 15.001468ms)
Jul 13 09:48:40.812: INFO: (10) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 15.156518ms)
Jul 13 09:48:40.812: INFO: (10) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 15.650667ms)
Jul 13 09:48:40.813: INFO: (10) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 15.701198ms)
Jul 13 09:48:40.813: INFO: (10) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 15.531352ms)
Jul 13 09:48:40.814: INFO: (10) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 16.664533ms)
Jul 13 09:48:40.815: INFO: (10) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 17.472936ms)
Jul 13 09:48:40.815: INFO: (10) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 18.552427ms)
Jul 13 09:48:40.817: INFO: (10) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 20.411883ms)
Jul 13 09:48:40.817: INFO: (10) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 20.237257ms)
Jul 13 09:48:40.818: INFO: (10) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 20.978021ms)
Jul 13 09:48:40.818: INFO: (10) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 20.685139ms)
Jul 13 09:48:40.833: INFO: (11) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 15.673158ms)
Jul 13 09:48:40.835: INFO: (11) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 16.391584ms)
Jul 13 09:48:40.835: INFO: (11) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 16.003109ms)
Jul 13 09:48:40.836: INFO: (11) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 17.605294ms)
Jul 13 09:48:40.836: INFO: (11) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 17.34982ms)
Jul 13 09:48:40.836: INFO: (11) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 17.219999ms)
Jul 13 09:48:40.836: INFO: (11) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 17.0904ms)
Jul 13 09:48:40.837: INFO: (11) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 17.985925ms)
Jul 13 09:48:40.837: INFO: (11) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 18.147522ms)
Jul 13 09:48:40.837: INFO: (11) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 18.112244ms)
Jul 13 09:48:40.886: INFO: (11) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 67.784115ms)
Jul 13 09:48:40.888: INFO: (11) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 69.608908ms)
Jul 13 09:48:40.888: INFO: (11) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 69.432876ms)
Jul 13 09:48:40.888: INFO: (11) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 69.478622ms)
Jul 13 09:48:40.888: INFO: (11) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 70.403561ms)
Jul 13 09:48:40.888: INFO: (11) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 70.330247ms)
Jul 13 09:48:40.905: INFO: (12) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 16.880222ms)
Jul 13 09:48:40.906: INFO: (12) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 16.620468ms)
Jul 13 09:48:40.906: INFO: (12) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 16.839975ms)
Jul 13 09:48:40.906: INFO: (12) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 17.367345ms)
Jul 13 09:48:40.919: INFO: (12) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 29.504073ms)
Jul 13 09:48:40.919: INFO: (12) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 29.432004ms)
Jul 13 09:48:40.919: INFO: (12) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 30.121894ms)
Jul 13 09:48:40.919: INFO: (12) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 29.666872ms)
Jul 13 09:48:40.919: INFO: (12) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 29.914738ms)
Jul 13 09:48:40.919: INFO: (12) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 29.796494ms)
Jul 13 09:48:40.919: INFO: (12) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 29.672897ms)
Jul 13 09:48:40.919: INFO: (12) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 30.195336ms)
Jul 13 09:48:40.920: INFO: (12) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 31.665365ms)
Jul 13 09:48:40.920: INFO: (12) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 31.339105ms)
Jul 13 09:48:40.920: INFO: (12) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 30.906409ms)
Jul 13 09:48:40.922: INFO: (12) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 32.764794ms)
Jul 13 09:48:40.956: INFO: (13) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 32.223512ms)
Jul 13 09:48:40.956: INFO: (13) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 32.675332ms)
Jul 13 09:48:40.956: INFO: (13) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 33.383932ms)
Jul 13 09:48:40.956: INFO: (13) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 33.116666ms)
Jul 13 09:48:40.956: INFO: (13) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 32.66315ms)
Jul 13 09:48:40.956: INFO: (13) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 33.890908ms)
Jul 13 09:48:40.956: INFO: (13) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 33.966474ms)
Jul 13 09:48:40.957: INFO: (13) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 33.309156ms)
Jul 13 09:48:40.957: INFO: (13) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 33.122856ms)
Jul 13 09:48:40.957: INFO: (13) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 33.636627ms)
Jul 13 09:48:40.957: INFO: (13) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 33.428568ms)
Jul 13 09:48:40.957: INFO: (13) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 33.155697ms)
Jul 13 09:48:40.957: INFO: (13) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 33.485091ms)
Jul 13 09:48:40.957: INFO: (13) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 34.041345ms)
Jul 13 09:48:40.960: INFO: (13) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 36.399971ms)
Jul 13 09:48:40.960: INFO: (13) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 36.879706ms)
Jul 13 09:48:40.969: INFO: (14) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 9.171505ms)
Jul 13 09:48:40.971: INFO: (14) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 10.585617ms)
Jul 13 09:48:40.971: INFO: (14) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 10.91717ms)
Jul 13 09:48:40.974: INFO: (14) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 11.927337ms)
Jul 13 09:48:40.975: INFO: (14) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 12.918352ms)
Jul 13 09:48:40.975: INFO: (14) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 15.477501ms)
Jul 13 09:48:40.976: INFO: (14) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 12.848119ms)
Jul 13 09:48:40.976: INFO: (14) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 13.701868ms)
Jul 13 09:48:40.976: INFO: (14) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 15.410712ms)
Jul 13 09:48:40.976: INFO: (14) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 15.26268ms)
Jul 13 09:48:40.977: INFO: (14) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 13.964526ms)
Jul 13 09:48:40.977: INFO: (14) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 14.79091ms)
Jul 13 09:48:40.979: INFO: (14) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 17.019828ms)
Jul 13 09:48:40.980: INFO: (14) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 17.796107ms)
Jul 13 09:48:40.980: INFO: (14) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 17.309713ms)
Jul 13 09:48:40.981: INFO: (14) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 18.599103ms)
Jul 13 09:48:40.997: INFO: (15) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 15.276232ms)
Jul 13 09:48:40.997: INFO: (15) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 15.061013ms)
Jul 13 09:48:41.000: INFO: (15) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 17.938525ms)
Jul 13 09:48:41.000: INFO: (15) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 18.389803ms)
Jul 13 09:48:41.000: INFO: (15) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 18.149758ms)
Jul 13 09:48:41.000: INFO: (15) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 17.916115ms)
Jul 13 09:48:41.001: INFO: (15) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 20.094968ms)
Jul 13 09:48:41.001: INFO: (15) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 19.4507ms)
Jul 13 09:48:41.001: INFO: (15) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 19.528095ms)
Jul 13 09:48:41.004: INFO: (15) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 22.256373ms)
Jul 13 09:48:41.004: INFO: (15) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 22.272042ms)
Jul 13 09:48:41.004: INFO: (15) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 22.085248ms)
Jul 13 09:48:41.005: INFO: (15) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 23.909792ms)
Jul 13 09:48:41.007: INFO: (15) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 25.834421ms)
Jul 13 09:48:41.007: INFO: (15) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 25.938036ms)
Jul 13 09:48:41.008: INFO: (15) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 25.542791ms)
Jul 13 09:48:41.016: INFO: (16) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 8.080362ms)
Jul 13 09:48:41.017: INFO: (16) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 8.990974ms)
Jul 13 09:48:41.017: INFO: (16) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 9.415341ms)
Jul 13 09:48:41.025: INFO: (16) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 16.608371ms)
Jul 13 09:48:41.030: INFO: (16) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 21.331038ms)
Jul 13 09:48:41.030: INFO: (16) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 21.929839ms)
Jul 13 09:48:41.031: INFO: (16) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 22.306268ms)
Jul 13 09:48:41.031: INFO: (16) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 22.562143ms)
Jul 13 09:48:41.031: INFO: (16) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 22.996571ms)
Jul 13 09:48:41.031: INFO: (16) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 22.488847ms)
Jul 13 09:48:41.031: INFO: (16) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 22.817504ms)
Jul 13 09:48:41.031: INFO: (16) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 22.729031ms)
Jul 13 09:48:41.032: INFO: (16) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 22.716982ms)
Jul 13 09:48:41.032: INFO: (16) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 22.965792ms)
Jul 13 09:48:41.032: INFO: (16) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 23.09366ms)
Jul 13 09:48:41.032: INFO: (16) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 23.899081ms)
Jul 13 09:48:41.047: INFO: (17) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 13.779315ms)
Jul 13 09:48:41.049: INFO: (17) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 15.850957ms)
Jul 13 09:48:41.049: INFO: (17) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 16.053796ms)
Jul 13 09:48:41.050: INFO: (17) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 16.558436ms)
Jul 13 09:48:41.050: INFO: (17) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 17.151101ms)
Jul 13 09:48:41.050: INFO: (17) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 16.883196ms)
Jul 13 09:48:41.050: INFO: (17) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 17.856124ms)
Jul 13 09:48:41.050: INFO: (17) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 17.202998ms)
Jul 13 09:48:41.051: INFO: (17) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 17.454679ms)
Jul 13 09:48:41.051: INFO: (17) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 17.376544ms)
Jul 13 09:48:41.053: INFO: (17) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 20.347781ms)
Jul 13 09:48:41.053: INFO: (17) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 20.915914ms)
Jul 13 09:48:41.054: INFO: (17) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 20.809597ms)
Jul 13 09:48:41.054: INFO: (17) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 21.35206ms)
Jul 13 09:48:41.054: INFO: (17) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 20.932313ms)
Jul 13 09:48:41.054: INFO: (17) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 21.333616ms)
Jul 13 09:48:41.070: INFO: (18) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 15.816229ms)
Jul 13 09:48:41.072: INFO: (18) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 17.190482ms)
Jul 13 09:48:41.072: INFO: (18) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 17.193704ms)
Jul 13 09:48:41.076: INFO: (18) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 20.210593ms)
Jul 13 09:48:41.076: INFO: (18) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 21.150436ms)
Jul 13 09:48:41.076: INFO: (18) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 21.973819ms)
Jul 13 09:48:41.077: INFO: (18) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 21.396608ms)
Jul 13 09:48:41.077: INFO: (18) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 21.315269ms)
Jul 13 09:48:41.078: INFO: (18) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 22.184716ms)
Jul 13 09:48:41.078: INFO: (18) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 22.394583ms)
Jul 13 09:48:41.078: INFO: (18) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 22.877132ms)
Jul 13 09:48:41.078: INFO: (18) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 22.32252ms)
Jul 13 09:48:41.079: INFO: (18) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 23.538502ms)
Jul 13 09:48:41.079: INFO: (18) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 23.587674ms)
Jul 13 09:48:41.079: INFO: (18) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 23.703186ms)
Jul 13 09:48:41.079: INFO: (18) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 24.182005ms)
Jul 13 09:48:41.099: INFO: (19) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:460/proxy/: tls baz (200; 19.169067ms)
Jul 13 09:48:41.099: INFO: (19) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname2/proxy/: tls qux (200; 19.737773ms)
Jul 13 09:48:41.099: INFO: (19) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">test<... (200; 19.450856ms)
Jul 13 09:48:41.099: INFO: (19) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:462/proxy/: tls qux (200; 19.825459ms)
Jul 13 09:48:41.099: INFO: (19) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 19.159077ms)
Jul 13 09:48:41.099: INFO: (19) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:160/proxy/: foo (200; 19.567724ms)
Jul 13 09:48:41.101: INFO: (19) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr/proxy/rewriteme">test</a> (200; 20.503449ms)
Jul 13 09:48:41.101: INFO: (19) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:1080/proxy/rewriteme">... (200; 20.836419ms)
Jul 13 09:48:41.101: INFO: (19) /api/v1/namespaces/proxy-4825/pods/proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 20.781594ms)
Jul 13 09:48:41.101: INFO: (19) /api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/: <a href="/api/v1/namespaces/proxy-4825/pods/https:proxy-service-jwdlf-xwvzr:443/proxy/tlsrewritem... (200; 21.146177ms)
Jul 13 09:48:41.101: INFO: (19) /api/v1/namespaces/proxy-4825/pods/http:proxy-service-jwdlf-xwvzr:162/proxy/: bar (200; 20.803135ms)
Jul 13 09:48:41.103: INFO: (19) /api/v1/namespaces/proxy-4825/services/https:proxy-service-jwdlf:tlsportname1/proxy/: tls baz (200; 22.474633ms)
Jul 13 09:48:41.103: INFO: (19) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname1/proxy/: foo (200; 22.794858ms)
Jul 13 09:48:41.103: INFO: (19) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname1/proxy/: foo (200; 22.887371ms)
Jul 13 09:48:41.103: INFO: (19) /api/v1/namespaces/proxy-4825/services/proxy-service-jwdlf:portname2/proxy/: bar (200; 23.976031ms)
Jul 13 09:48:41.105: INFO: (19) /api/v1/namespaces/proxy-4825/services/http:proxy-service-jwdlf:portname2/proxy/: bar (200; 24.984593ms)
STEP: deleting ReplicationController proxy-service-jwdlf in namespace proxy-4825, will wait for the garbage collector to delete the pods
Jul 13 09:48:41.186: INFO: Deleting ReplicationController proxy-service-jwdlf took: 24.159741ms
Jul 13 09:48:41.587: INFO: Terminating ReplicationController proxy-service-jwdlf pods took: 401.363268ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:48:43.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4825" for this suite.
Jul 13 09:48:50.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:48:50.377: INFO: namespace proxy-4825 deletion completed in 6.377059024s

• [SLOW TEST:22.091 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:48:50.377: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 13 09:48:50.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-9306'
Jul 13 09:48:50.636: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 09:48:50.636: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jul 13 09:48:54.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9306'
Jul 13 09:48:54.809: INFO: stderr: ""
Jul 13 09:48:54.809: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:48:54.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9306" for this suite.
Jul 13 09:49:00.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:49:01.152: INFO: namespace kubectl-9306 deletion completed in 6.333087644s

• [SLOW TEST:10.775 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:49:01.157: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 09:49:01.265: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b69a24a-a894-4ead-9a4b-59c6cfe71baf" in namespace "projected-9723" to be "success or failure"
Jul 13 09:49:01.277: INFO: Pod "downwardapi-volume-9b69a24a-a894-4ead-9a4b-59c6cfe71baf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.000688ms
Jul 13 09:49:03.287: INFO: Pod "downwardapi-volume-9b69a24a-a894-4ead-9a4b-59c6cfe71baf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022541942s
Jul 13 09:49:05.298: INFO: Pod "downwardapi-volume-9b69a24a-a894-4ead-9a4b-59c6cfe71baf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032766553s
STEP: Saw pod success
Jul 13 09:49:05.298: INFO: Pod "downwardapi-volume-9b69a24a-a894-4ead-9a4b-59c6cfe71baf" satisfied condition "success or failure"
Jul 13 09:49:05.306: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-9b69a24a-a894-4ead-9a4b-59c6cfe71baf container client-container: <nil>
STEP: delete the pod
Jul 13 09:49:05.354: INFO: Waiting for pod downwardapi-volume-9b69a24a-a894-4ead-9a4b-59c6cfe71baf to disappear
Jul 13 09:49:05.365: INFO: Pod downwardapi-volume-9b69a24a-a894-4ead-9a4b-59c6cfe71baf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:49:05.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9723" for this suite.
Jul 13 09:49:11.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:49:11.745: INFO: namespace projected-9723 deletion completed in 6.357029513s

• [SLOW TEST:10.588 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:49:11.746: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-9f7c3746-1a7c-438e-a122-e80dec557266
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:49:11.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6120" for this suite.
Jul 13 09:49:17.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:49:18.216: INFO: namespace secrets-6120 deletion completed in 6.359077556s

• [SLOW TEST:6.470 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:49:18.216: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-7af45860-08c6-4d8e-9ff3-5eb1084d2e4b
STEP: Creating secret with name s-test-opt-upd-12962cef-ab85-47fc-8f6b-deea40e43d7c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7af45860-08c6-4d8e-9ff3-5eb1084d2e4b
STEP: Updating secret s-test-opt-upd-12962cef-ab85-47fc-8f6b-deea40e43d7c
STEP: Creating secret with name s-test-opt-create-5a2fe452-351c-420b-975d-9d1c30798b12
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:50:37.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6874" for this suite.
Jul 13 09:51:01.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:51:02.033: INFO: namespace secrets-6874 deletion completed in 24.352476166s

• [SLOW TEST:103.817 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:51:02.035: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 09:51:02.186: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b972aec0-c498-4b94-b7a5-a712f7d9d907" in namespace "downward-api-6885" to be "success or failure"
Jul 13 09:51:02.198: INFO: Pod "downwardapi-volume-b972aec0-c498-4b94-b7a5-a712f7d9d907": Phase="Pending", Reason="", readiness=false. Elapsed: 11.403886ms
Jul 13 09:51:04.206: INFO: Pod "downwardapi-volume-b972aec0-c498-4b94-b7a5-a712f7d9d907": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019848615s
STEP: Saw pod success
Jul 13 09:51:04.206: INFO: Pod "downwardapi-volume-b972aec0-c498-4b94-b7a5-a712f7d9d907" satisfied condition "success or failure"
Jul 13 09:51:04.215: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-b972aec0-c498-4b94-b7a5-a712f7d9d907 container client-container: <nil>
STEP: delete the pod
Jul 13 09:51:04.258: INFO: Waiting for pod downwardapi-volume-b972aec0-c498-4b94-b7a5-a712f7d9d907 to disappear
Jul 13 09:51:04.263: INFO: Pod downwardapi-volume-b972aec0-c498-4b94-b7a5-a712f7d9d907 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:51:04.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6885" for this suite.
Jul 13 09:51:10.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:51:10.616: INFO: namespace downward-api-6885 deletion completed in 6.344789917s

• [SLOW TEST:8.580 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:51:10.616: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 13 09:51:10.738: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:51:15.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4453" for this suite.
Jul 13 09:51:21.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:51:22.024: INFO: namespace init-container-4453 deletion completed in 6.344940275s

• [SLOW TEST:11.408 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:51:22.028: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jul 13 09:51:22.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 api-versions'
Jul 13 09:51:22.291: INFO: stderr: ""
Jul 13 09:51:22.291: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:51:22.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-855" for this suite.
Jul 13 09:51:28.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:51:28.660: INFO: namespace kubectl-855 deletion completed in 6.355729491s

• [SLOW TEST:6.632 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:51:28.660: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7100.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7100.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 09:51:34.927: INFO: DNS probes using dns-7100/dns-test-8dcb9e06-2d55-4ac1-a3c3-272c3f53900b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:51:34.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7100" for this suite.
Jul 13 09:51:41.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:51:41.315: INFO: namespace dns-7100 deletion completed in 6.346569058s

• [SLOW TEST:12.655 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:51:41.316: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2148
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 13 09:51:41.498: INFO: Found 0 stateful pods, waiting for 3
Jul 13 09:51:51.528: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:51:51.528: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:51:51.528: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Jul 13 09:52:01.508: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:52:01.508: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:52:01.508: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 13 09:52:01.576: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 13 09:52:11.651: INFO: Updating stateful set ss2
Jul 13 09:52:11.666: INFO: Waiting for Pod statefulset-2148/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jul 13 09:52:21.776: INFO: Found 2 stateful pods, waiting for 3
Jul 13 09:52:31.789: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:52:31.789: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 09:52:31.789: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 13 09:52:31.857: INFO: Updating stateful set ss2
Jul 13 09:52:31.880: INFO: Waiting for Pod statefulset-2148/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 13 09:52:41.936: INFO: Updating stateful set ss2
Jul 13 09:52:41.960: INFO: Waiting for StatefulSet statefulset-2148/ss2 to complete update
Jul 13 09:52:41.960: INFO: Waiting for Pod statefulset-2148/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 13 09:52:51.978: INFO: Deleting all statefulset in ns statefulset-2148
Jul 13 09:52:51.986: INFO: Scaling statefulset ss2 to 0
Jul 13 09:53:02.031: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 09:53:02.039: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:53:02.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2148" for this suite.
Jul 13 09:53:10.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:53:10.430: INFO: namespace statefulset-2148 deletion completed in 8.346764422s

• [SLOW TEST:89.114 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:53:10.430: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 13 09:53:10.575: INFO: Waiting up to 5m0s for pod "pod-5ec49b95-72c2-42d6-a919-a9241af430ff" in namespace "emptydir-8566" to be "success or failure"
Jul 13 09:53:10.586: INFO: Pod "pod-5ec49b95-72c2-42d6-a919-a9241af430ff": Phase="Pending", Reason="", readiness=false. Elapsed: 10.426633ms
Jul 13 09:53:12.597: INFO: Pod "pod-5ec49b95-72c2-42d6-a919-a9241af430ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021613564s
Jul 13 09:53:14.606: INFO: Pod "pod-5ec49b95-72c2-42d6-a919-a9241af430ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030662528s
STEP: Saw pod success
Jul 13 09:53:14.606: INFO: Pod "pod-5ec49b95-72c2-42d6-a919-a9241af430ff" satisfied condition "success or failure"
Jul 13 09:53:14.616: INFO: Trying to get logs from node cncf-1-15 pod pod-5ec49b95-72c2-42d6-a919-a9241af430ff container test-container: <nil>
STEP: delete the pod
Jul 13 09:53:14.688: INFO: Waiting for pod pod-5ec49b95-72c2-42d6-a919-a9241af430ff to disappear
Jul 13 09:53:14.696: INFO: Pod pod-5ec49b95-72c2-42d6-a919-a9241af430ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:53:14.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8566" for this suite.
Jul 13 09:53:20.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:53:21.047: INFO: namespace emptydir-8566 deletion completed in 6.340056444s

• [SLOW TEST:10.616 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:53:21.047: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 09:53:21.189: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af1a2b5d-0a36-4f95-9b1b-23d606294550" in namespace "projected-93" to be "success or failure"
Jul 13 09:53:21.210: INFO: Pod "downwardapi-volume-af1a2b5d-0a36-4f95-9b1b-23d606294550": Phase="Pending", Reason="", readiness=false. Elapsed: 20.175543ms
Jul 13 09:53:23.220: INFO: Pod "downwardapi-volume-af1a2b5d-0a36-4f95-9b1b-23d606294550": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030281128s
Jul 13 09:53:25.229: INFO: Pod "downwardapi-volume-af1a2b5d-0a36-4f95-9b1b-23d606294550": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038951216s
STEP: Saw pod success
Jul 13 09:53:25.229: INFO: Pod "downwardapi-volume-af1a2b5d-0a36-4f95-9b1b-23d606294550" satisfied condition "success or failure"
Jul 13 09:53:25.237: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-af1a2b5d-0a36-4f95-9b1b-23d606294550 container client-container: <nil>
STEP: delete the pod
Jul 13 09:53:25.297: INFO: Waiting for pod downwardapi-volume-af1a2b5d-0a36-4f95-9b1b-23d606294550 to disappear
Jul 13 09:53:25.305: INFO: Pod downwardapi-volume-af1a2b5d-0a36-4f95-9b1b-23d606294550 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:53:25.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-93" for this suite.
Jul 13 09:53:31.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:53:31.677: INFO: namespace projected-93 deletion completed in 6.361688953s

• [SLOW TEST:10.630 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:53:31.680: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 09:53:31.831: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 13 09:53:36.844: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 13 09:53:36.845: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 13 09:53:38.854: INFO: Creating deployment "test-rollover-deployment"
Jul 13 09:53:38.882: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 13 09:53:40.901: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 13 09:53:40.917: INFO: Ensure that both replica sets have 1 created replica
Jul 13 09:53:40.932: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 13 09:53:40.952: INFO: Updating deployment test-rollover-deployment
Jul 13 09:53:40.952: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 13 09:53:42.974: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 13 09:53:42.993: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 13 09:53:43.012: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 09:53:43.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608421, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:53:45.035: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 09:53:45.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608423, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:53:47.040: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 09:53:47.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608423, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:53:49.031: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 09:53:49.031: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608423, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:53:51.033: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 09:53:51.033: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608423, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:53:53.032: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 09:53:53.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608423, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698608418, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 09:53:55.032: INFO: 
Jul 13 09:53:55.032: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 13 09:53:55.060: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-1615,SelfLink:/apis/apps/v1/namespaces/deployment-1615/deployments/test-rollover-deployment,UID:0a8bdcb5-311d-4461-a12f-7d84c945cd6f,ResourceVersion:223528292,Generation:2,CreationTimestamp:2019-07-13 09:53:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-13 09:53:38 +0000 UTC 2019-07-13 09:53:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-13 09:53:53 +0000 UTC 2019-07-13 09:53:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 13 09:53:55.073: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-1615,SelfLink:/apis/apps/v1/namespaces/deployment-1615/replicasets/test-rollover-deployment-854595fc44,UID:24c006ce-1fe7-4b3d-a2c0-81630c40a94a,ResourceVersion:223528281,Generation:2,CreationTimestamp:2019-07-13 09:53:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0a8bdcb5-311d-4461-a12f-7d84c945cd6f 0xc003366e17 0xc003366e18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 13 09:53:55.073: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 13 09:53:55.074: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-1615,SelfLink:/apis/apps/v1/namespaces/deployment-1615/replicasets/test-rollover-controller,UID:eb465417-96ab-4e41-bfac-90d768d0a88f,ResourceVersion:223528290,Generation:2,CreationTimestamp:2019-07-13 09:53:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0a8bdcb5-311d-4461-a12f-7d84c945cd6f 0xc003366d37 0xc003366d38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 13 09:53:55.074: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-1615,SelfLink:/apis/apps/v1/namespaces/deployment-1615/replicasets/test-rollover-deployment-9b8b997cf,UID:9e16b7f6-468c-4adf-9d3f-f95191273aa5,ResourceVersion:223528164,Generation:2,CreationTimestamp:2019-07-13 09:53:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0a8bdcb5-311d-4461-a12f-7d84c945cd6f 0xc003366ee0 0xc003366ee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 13 09:53:55.084: INFO: Pod "test-rollover-deployment-854595fc44-46g9j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-46g9j,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-1615,SelfLink:/api/v1/namespaces/deployment-1615/pods/test-rollover-deployment-854595fc44-46g9j,UID:fcfd1834-8d0d-4de2-851d-bd92fcc9d2d9,ResourceVersion:223528201,Generation:0,CreationTimestamp:2019-07-13 09:53:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.146/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 24c006ce-1fe7-4b3d-a2c0-81630c40a94a 0xc003367b47 0xc003367b48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jtxtf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jtxtf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-jtxtf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003367bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003367bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 09:53:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 09:53:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 09:53:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 09:53:41 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:10.2.1.146,StartTime:2019-07-13 09:53:41 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-13 09:53:42 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://1d1185f26986a7bdafe783391903cf6c0b43a8c6e4f6da911c0b62ede8a5b05d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:53:55.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1615" for this suite.
Jul 13 09:54:01.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:54:01.499: INFO: namespace deployment-1615 deletion completed in 6.405864466s

• [SLOW TEST:29.819 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:54:01.499: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 09:54:01.649: INFO: Waiting up to 5m0s for pod "downwardapi-volume-305bbc46-c4cd-48a9-9ac6-ed29174b4346" in namespace "downward-api-1989" to be "success or failure"
Jul 13 09:54:01.656: INFO: Pod "downwardapi-volume-305bbc46-c4cd-48a9-9ac6-ed29174b4346": Phase="Pending", Reason="", readiness=false. Elapsed: 6.691338ms
Jul 13 09:54:03.665: INFO: Pod "downwardapi-volume-305bbc46-c4cd-48a9-9ac6-ed29174b4346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016132894s
Jul 13 09:54:05.675: INFO: Pod "downwardapi-volume-305bbc46-c4cd-48a9-9ac6-ed29174b4346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025371462s
STEP: Saw pod success
Jul 13 09:54:05.675: INFO: Pod "downwardapi-volume-305bbc46-c4cd-48a9-9ac6-ed29174b4346" satisfied condition "success or failure"
Jul 13 09:54:05.682: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-305bbc46-c4cd-48a9-9ac6-ed29174b4346 container client-container: <nil>
STEP: delete the pod
Jul 13 09:54:05.727: INFO: Waiting for pod downwardapi-volume-305bbc46-c4cd-48a9-9ac6-ed29174b4346 to disappear
Jul 13 09:54:05.734: INFO: Pod downwardapi-volume-305bbc46-c4cd-48a9-9ac6-ed29174b4346 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:54:05.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1989" for this suite.
Jul 13 09:54:11.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:54:12.093: INFO: namespace downward-api-1989 deletion completed in 6.350313336s

• [SLOW TEST:10.593 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:54:12.094: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jul 13 09:54:12.802: INFO: created pod pod-service-account-defaultsa
Jul 13 09:54:12.802: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 13 09:54:12.838: INFO: created pod pod-service-account-mountsa
Jul 13 09:54:12.838: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 13 09:54:12.858: INFO: created pod pod-service-account-nomountsa
Jul 13 09:54:12.858: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 13 09:54:12.871: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 13 09:54:12.871: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 13 09:54:12.882: INFO: created pod pod-service-account-mountsa-mountspec
Jul 13 09:54:12.882: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 13 09:54:12.893: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 13 09:54:12.894: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 13 09:54:12.911: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 13 09:54:12.911: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 13 09:54:12.925: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 13 09:54:12.925: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 13 09:54:12.939: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 13 09:54:12.939: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:54:12.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3868" for this suite.
Jul 13 09:54:36.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:54:37.318: INFO: namespace svcaccounts-3868 deletion completed in 24.36785728s

• [SLOW TEST:25.224 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:54:37.318: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0713 09:54:38.619102      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 13 09:54:38.619: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:54:38.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7693" for this suite.
Jul 13 09:54:44.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:54:45.020: INFO: namespace gc-7693 deletion completed in 6.390977283s

• [SLOW TEST:7.701 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:54:45.020: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 13 09:54:45.147: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 13 09:54:45.168: INFO: Waiting for terminating namespaces to be deleted...
Jul 13 09:54:45.176: INFO: 
Logging pods the kubelet thinks is on node cncf-1-15 before test
Jul 13 09:54:45.199: INFO: kube-proxy-qsskh from kube-system started at 2019-07-13 08:59:39 +0000 UTC (1 container statuses recorded)
Jul 13 09:54:45.199: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 13 09:54:45.199: INFO: wormhole-svb5x from kube-system started at 2019-07-13 08:59:58 +0000 UTC (1 container statuses recorded)
Jul 13 09:54:45.199: INFO: 	Container wormhole ready: true, restart count 0
Jul 13 09:54:45.199: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-13 09:03:04 +0000 UTC (1 container statuses recorded)
Jul 13 09:54:45.199: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 13 09:54:45.199: INFO: sonobuoy-e2e-job-5b31513983fe4ca4 from heptio-sonobuoy started at 2019-07-13 09:03:10 +0000 UTC (2 container statuses recorded)
Jul 13 09:54:45.199: INFO: 	Container e2e ready: true, restart count 0
Jul 13 09:54:45.199: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 09:54:45.199: INFO: sonobuoy-systemd-logs-daemon-set-9454bcfaa70145b2-zks8j from heptio-sonobuoy started at 2019-07-13 09:03:10 +0000 UTC (2 container statuses recorded)
Jul 13 09:54:45.199: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 09:54:45.199: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 09:54:45.199: INFO: canal-r8n6w from kube-system started at 2019-07-13 08:59:39 +0000 UTC (2 container statuses recorded)
Jul 13 09:54:45.199: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 09:54:45.199: INFO: 	Container kube-flannel ready: true, restart count 1
Jul 13 09:54:45.199: INFO: 
Logging pods the kubelet thinks is on node cncf-1-15-2 before test
Jul 13 09:54:45.233: INFO: sonobuoy-systemd-logs-daemon-set-9454bcfaa70145b2-gdq8g from heptio-sonobuoy started at 2019-07-13 09:03:10 +0000 UTC (2 container statuses recorded)
Jul 13 09:54:45.233: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 09:54:45.234: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 09:54:45.234: INFO: metrics-server-6dd5674bf7-gjr4b from kube-system started at 2019-07-13 08:58:38 +0000 UTC (1 container statuses recorded)
Jul 13 09:54:45.234: INFO: 	Container metrics-server ready: true, restart count 0
Jul 13 09:54:45.234: INFO: kube-dns-b74d847f-8tlpd from kube-system started at 2019-07-13 08:58:43 +0000 UTC (3 container statuses recorded)
Jul 13 09:54:45.234: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 13 09:54:45.234: INFO: 	Container kubedns ready: true, restart count 0
Jul 13 09:54:45.234: INFO: 	Container sidecar ready: true, restart count 0
Jul 13 09:54:45.234: INFO: wormhole-62ggl from kube-system started at 2019-07-13 08:58:38 +0000 UTC (1 container statuses recorded)
Jul 13 09:54:45.234: INFO: 	Container wormhole ready: true, restart count 0
Jul 13 09:54:45.234: INFO: kube-dns-autoscaler-67595559f7-vj26r from kube-system started at 2019-07-13 08:58:43 +0000 UTC (1 container statuses recorded)
Jul 13 09:54:45.234: INFO: 	Container autoscaler ready: true, restart count 0
Jul 13 09:54:45.234: INFO: kube-dns-b74d847f-hr65v from kube-system started at 2019-07-13 08:59:44 +0000 UTC (3 container statuses recorded)
Jul 13 09:54:45.234: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 13 09:54:45.234: INFO: 	Container kubedns ready: true, restart count 0
Jul 13 09:54:45.234: INFO: 	Container sidecar ready: true, restart count 0
Jul 13 09:54:45.234: INFO: canal-7hs5h from kube-system started at 2019-07-13 08:58:28 +0000 UTC (2 container statuses recorded)
Jul 13 09:54:45.234: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 09:54:45.234: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 13 09:54:45.234: INFO: kube-proxy-spjhx from kube-system started at 2019-07-13 08:58:28 +0000 UTC (1 container statuses recorded)
Jul 13 09:54:45.234: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ff2d1bd5-cabe-48b3-aaf0-22ff0532d84b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-ff2d1bd5-cabe-48b3-aaf0-22ff0532d84b off the node cncf-1-15
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ff2d1bd5-cabe-48b3-aaf0-22ff0532d84b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:54:53.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-905" for this suite.
Jul 13 09:55:09.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:55:09.908: INFO: namespace sched-pred-905 deletion completed in 16.3586461s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:24.888 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:55:09.908: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-819f88d4-c37f-44e5-a19d-944c437bd1e6
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-819f88d4-c37f-44e5-a19d-944c437bd1e6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:56:21.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6070" for this suite.
Jul 13 09:56:45.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:56:45.593: INFO: namespace configmap-6070 deletion completed in 24.366957623s

• [SLOW TEST:95.685 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:56:45.593: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9883.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9883.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 97.155.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.155.97_udp@PTR;check="$$(dig +tcp +noall +answer +search 97.155.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.155.97_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9883.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9883.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 97.155.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.155.97_udp@PTR;check="$$(dig +tcp +noall +answer +search 97.155.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.155.97_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 09:56:52.091: INFO: DNS probes using dns-9883/dns-test-95ea7fd5-66b1-4235-bb6f-b6b958b56f12 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:56:52.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9883" for this suite.
Jul 13 09:56:58.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:56:58.622: INFO: namespace dns-9883 deletion completed in 6.351782394s

• [SLOW TEST:13.028 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:56:58.623: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-1aafb3ef-e390-44a6-9170-5e1e31e7ccc7
STEP: Creating configMap with name cm-test-opt-upd-1b6f8b44-63fe-45d6-b6ef-69ab7c702bf0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1aafb3ef-e390-44a6-9170-5e1e31e7ccc7
STEP: Updating configmap cm-test-opt-upd-1b6f8b44-63fe-45d6-b6ef-69ab7c702bf0
STEP: Creating configMap with name cm-test-opt-create-2e08b6aa-cc1a-4c93-a563-c56dc83aa123
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:58:28.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4159" for this suite.
Jul 13 09:58:52.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:58:52.592: INFO: namespace projected-4159 deletion completed in 24.383602442s

• [SLOW TEST:113.970 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:58:52.593: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-e90b48c5-abc0-454f-8ead-93668e32c836
STEP: Creating a pod to test consume secrets
Jul 13 09:58:52.756: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2dc40dc8-7a9e-4376-841b-f8fa5cd0edc0" in namespace "projected-145" to be "success or failure"
Jul 13 09:58:52.768: INFO: Pod "pod-projected-secrets-2dc40dc8-7a9e-4376-841b-f8fa5cd0edc0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.844587ms
Jul 13 09:58:54.778: INFO: Pod "pod-projected-secrets-2dc40dc8-7a9e-4376-841b-f8fa5cd0edc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022591386s
Jul 13 09:58:56.792: INFO: Pod "pod-projected-secrets-2dc40dc8-7a9e-4376-841b-f8fa5cd0edc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036514038s
STEP: Saw pod success
Jul 13 09:58:56.792: INFO: Pod "pod-projected-secrets-2dc40dc8-7a9e-4376-841b-f8fa5cd0edc0" satisfied condition "success or failure"
Jul 13 09:58:56.801: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-secrets-2dc40dc8-7a9e-4376-841b-f8fa5cd0edc0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 09:58:56.857: INFO: Waiting for pod pod-projected-secrets-2dc40dc8-7a9e-4376-841b-f8fa5cd0edc0 to disappear
Jul 13 09:58:56.865: INFO: Pod pod-projected-secrets-2dc40dc8-7a9e-4376-841b-f8fa5cd0edc0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:58:56.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-145" for this suite.
Jul 13 09:59:02.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:59:03.231: INFO: namespace projected-145 deletion completed in 6.355154558s

• [SLOW TEST:10.638 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:59:03.231: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:59:07.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-716" for this suite.
Jul 13 09:59:49.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 09:59:49.831: INFO: namespace kubelet-test-716 deletion completed in 42.380723576s

• [SLOW TEST:46.600 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 09:59:49.832: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 13 09:59:54.099: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 09:59:54.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8109" for this suite.
Jul 13 10:00:00.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:00:00.506: INFO: namespace container-runtime-8109 deletion completed in 6.353856414s

• [SLOW TEST:10.674 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:00:00.508: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 13 10:00:00.709: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 13 10:00:05.717: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:00:06.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9336" for this suite.
Jul 13 10:00:12.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:00:13.183: INFO: namespace replication-controller-9336 deletion completed in 6.396089284s

• [SLOW TEST:12.675 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:00:13.184: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 13 10:00:18.420: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:00:19.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9492" for this suite.
Jul 13 10:00:43.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:00:43.876: INFO: namespace replicaset-9492 deletion completed in 24.354929226s

• [SLOW TEST:30.692 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:00:43.877: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-a01673f9-0ed2-4fbc-8c5a-cb241933b3fd in namespace container-probe-1890
Jul 13 10:00:48.059: INFO: Started pod liveness-a01673f9-0ed2-4fbc-8c5a-cb241933b3fd in namespace container-probe-1890
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 10:00:48.068: INFO: Initial restart count of pod liveness-a01673f9-0ed2-4fbc-8c5a-cb241933b3fd is 0
Jul 13 10:01:00.142: INFO: Restart count of pod container-probe-1890/liveness-a01673f9-0ed2-4fbc-8c5a-cb241933b3fd is now 1 (12.073363816s elapsed)
Jul 13 10:01:18.236: INFO: Restart count of pod container-probe-1890/liveness-a01673f9-0ed2-4fbc-8c5a-cb241933b3fd is now 2 (30.167726647s elapsed)
Jul 13 10:01:38.340: INFO: Restart count of pod container-probe-1890/liveness-a01673f9-0ed2-4fbc-8c5a-cb241933b3fd is now 3 (50.271818731s elapsed)
Jul 13 10:01:58.457: INFO: Restart count of pod container-probe-1890/liveness-a01673f9-0ed2-4fbc-8c5a-cb241933b3fd is now 4 (1m10.388427202s elapsed)
Jul 13 10:02:58.792: INFO: Restart count of pod container-probe-1890/liveness-a01673f9-0ed2-4fbc-8c5a-cb241933b3fd is now 5 (2m10.724054754s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:02:58.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1890" for this suite.
Jul 13 10:03:04.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:03:05.207: INFO: namespace container-probe-1890 deletion completed in 6.372049741s

• [SLOW TEST:141.330 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:03:05.209: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 13 10:03:05.324: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:03:12.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9705" for this suite.
Jul 13 10:03:18.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:03:18.943: INFO: namespace init-container-9705 deletion completed in 6.387084607s

• [SLOW TEST:13.734 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:03:18.944: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-e6cf7943-6390-4fd6-b300-8295d5ee07be
STEP: Creating secret with name secret-projected-all-test-volume-88727a9d-71e8-428a-910a-78dc61aaf71f
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 13 10:03:19.108: INFO: Waiting up to 5m0s for pod "projected-volume-a4c647ca-7dc6-4e0f-947e-0daa64f8015c" in namespace "projected-77" to be "success or failure"
Jul 13 10:03:19.116: INFO: Pod "projected-volume-a4c647ca-7dc6-4e0f-947e-0daa64f8015c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.441729ms
Jul 13 10:03:21.127: INFO: Pod "projected-volume-a4c647ca-7dc6-4e0f-947e-0daa64f8015c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019229274s
Jul 13 10:03:23.138: INFO: Pod "projected-volume-a4c647ca-7dc6-4e0f-947e-0daa64f8015c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029837321s
STEP: Saw pod success
Jul 13 10:03:23.138: INFO: Pod "projected-volume-a4c647ca-7dc6-4e0f-947e-0daa64f8015c" satisfied condition "success or failure"
Jul 13 10:03:23.146: INFO: Trying to get logs from node cncf-1-15 pod projected-volume-a4c647ca-7dc6-4e0f-947e-0daa64f8015c container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 13 10:03:23.219: INFO: Waiting for pod projected-volume-a4c647ca-7dc6-4e0f-947e-0daa64f8015c to disappear
Jul 13 10:03:23.229: INFO: Pod projected-volume-a4c647ca-7dc6-4e0f-947e-0daa64f8015c no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:03:23.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-77" for this suite.
Jul 13 10:03:29.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:03:29.630: INFO: namespace projected-77 deletion completed in 6.391547187s

• [SLOW TEST:10.687 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:03:29.631: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-5c6ab33a-5e14-4b45-a2ad-9e37925aed30
Jul 13 10:03:29.804: INFO: Pod name my-hostname-basic-5c6ab33a-5e14-4b45-a2ad-9e37925aed30: Found 0 pods out of 1
Jul 13 10:03:34.857: INFO: Pod name my-hostname-basic-5c6ab33a-5e14-4b45-a2ad-9e37925aed30: Found 1 pods out of 1
Jul 13 10:03:34.857: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5c6ab33a-5e14-4b45-a2ad-9e37925aed30" are running
Jul 13 10:03:34.867: INFO: Pod "my-hostname-basic-5c6ab33a-5e14-4b45-a2ad-9e37925aed30-pwjfs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-13 10:03:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-13 10:03:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-13 10:03:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-13 10:03:29 +0000 UTC Reason: Message:}])
Jul 13 10:03:34.867: INFO: Trying to dial the pod
Jul 13 10:03:39.902: INFO: Controller my-hostname-basic-5c6ab33a-5e14-4b45-a2ad-9e37925aed30: Got expected result from replica 1 [my-hostname-basic-5c6ab33a-5e14-4b45-a2ad-9e37925aed30-pwjfs]: "my-hostname-basic-5c6ab33a-5e14-4b45-a2ad-9e37925aed30-pwjfs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:03:39.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1623" for this suite.
Jul 13 10:03:45.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:03:46.266: INFO: namespace replication-controller-1623 deletion completed in 6.353069083s

• [SLOW TEST:16.636 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:03:46.267: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 13 10:03:46.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4895'
Jul 13 10:03:46.751: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 10:03:46.751: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jul 13 10:03:48.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4895'
Jul 13 10:03:48.929: INFO: stderr: ""
Jul 13 10:03:48.929: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:03:48.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4895" for this suite.
Jul 13 10:03:54.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:03:55.297: INFO: namespace kubectl-4895 deletion completed in 6.359237226s

• [SLOW TEST:9.030 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:03:55.299: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 13 10:03:55.449: INFO: Waiting up to 5m0s for pod "pod-39286fc0-f972-4e31-b0db-a055777b3564" in namespace "emptydir-6343" to be "success or failure"
Jul 13 10:03:55.462: INFO: Pod "pod-39286fc0-f972-4e31-b0db-a055777b3564": Phase="Pending", Reason="", readiness=false. Elapsed: 12.000941ms
Jul 13 10:03:57.470: INFO: Pod "pod-39286fc0-f972-4e31-b0db-a055777b3564": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020347539s
Jul 13 10:03:59.481: INFO: Pod "pod-39286fc0-f972-4e31-b0db-a055777b3564": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03143268s
STEP: Saw pod success
Jul 13 10:03:59.481: INFO: Pod "pod-39286fc0-f972-4e31-b0db-a055777b3564" satisfied condition "success or failure"
Jul 13 10:03:59.490: INFO: Trying to get logs from node cncf-1-15 pod pod-39286fc0-f972-4e31-b0db-a055777b3564 container test-container: <nil>
STEP: delete the pod
Jul 13 10:03:59.575: INFO: Waiting for pod pod-39286fc0-f972-4e31-b0db-a055777b3564 to disappear
Jul 13 10:03:59.583: INFO: Pod pod-39286fc0-f972-4e31-b0db-a055777b3564 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:03:59.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6343" for this suite.
Jul 13 10:04:05.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:04:05.988: INFO: namespace emptydir-6343 deletion completed in 6.398580934s

• [SLOW TEST:10.689 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:04:05.989: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:04:06.148: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 13 10:04:11.161: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 13 10:04:11.161: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 13 10:04:11.235: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8359,SelfLink:/apis/apps/v1/namespaces/deployment-8359/deployments/test-cleanup-deployment,UID:c5716b5b-6f65-49f8-83d7-663e904ed10f,ResourceVersion:223534317,Generation:1,CreationTimestamp:2019-07-13 10:04:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jul 13 10:04:11.246: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-8359,SelfLink:/apis/apps/v1/namespaces/deployment-8359/replicasets/test-cleanup-deployment-55bbcbc84c,UID:2c348ffb-5ac4-4760-ab4e-2e19d2380119,ResourceVersion:223534319,Generation:1,CreationTimestamp:2019-07-13 10:04:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment c5716b5b-6f65-49f8-83d7-663e904ed10f 0xc00052b3d7 0xc00052b3d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 13 10:04:11.247: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul 13 10:04:11.247: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-8359,SelfLink:/apis/apps/v1/namespaces/deployment-8359/replicasets/test-cleanup-controller,UID:e8f5c075-a0bb-4684-8e4f-0d6c8433f5db,ResourceVersion:223534318,Generation:1,CreationTimestamp:2019-07-13 10:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment c5716b5b-6f65-49f8-83d7-663e904ed10f 0xc00052b227 0xc00052b228}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 13 10:04:11.260: INFO: Pod "test-cleanup-controller-vwnkk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-vwnkk,GenerateName:test-cleanup-controller-,Namespace:deployment-8359,SelfLink:/api/v1/namespaces/deployment-8359/pods/test-cleanup-controller-vwnkk,UID:127436af-4ace-4406-9ec0-628722d88f3c,ResourceVersion:223534299,Generation:0,CreationTimestamp:2019-07-13 10:04:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.173/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller e8f5c075-a0bb-4684-8e4f-0d6c8433f5db 0xc000a16577 0xc000a16578}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-g8pxg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-g8pxg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-g8pxg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a166e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a16700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:04:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:04:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:04:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:04:06 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:10.2.1.173,StartTime:2019-07-13 10:04:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-13 10:04:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://224325a14fecdf0fca3ee360e50bddb6ebb33a16ca59de3b2cecc2ee7b0bda80}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:04:11.260: INFO: Pod "test-cleanup-deployment-55bbcbc84c-bb79h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-bb79h,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-8359,SelfLink:/api/v1/namespaces/deployment-8359/pods/test-cleanup-deployment-55bbcbc84c-bb79h,UID:d2cd3a6a-474b-409d-8bd0-f25333c91af2,ResourceVersion:223534323,Generation:0,CreationTimestamp:2019-07-13 10:04:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 2c348ffb-5ac4-4760-ab4e-2e19d2380119 0xc000a16917 0xc000a16918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-g8pxg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-g8pxg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-g8pxg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a16980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a169a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:04:11 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:04:11.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8359" for this suite.
Jul 13 10:04:17.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:04:17.642: INFO: namespace deployment-8359 deletion completed in 6.374614004s

• [SLOW TEST:11.653 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:04:17.642: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0713 10:04:27.820539      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 13 10:04:27.820: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:04:27.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-934" for this suite.
Jul 13 10:04:33.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:04:34.192: INFO: namespace gc-934 deletion completed in 6.361611961s

• [SLOW TEST:16.549 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:04:34.192: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-935ca772-93ce-4fb5-b848-003c3d1fefe7
STEP: Creating a pod to test consume secrets
Jul 13 10:04:34.362: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cfb69a55-3e46-409b-b814-57c9c8a400d8" in namespace "projected-496" to be "success or failure"
Jul 13 10:04:34.377: INFO: Pod "pod-projected-secrets-cfb69a55-3e46-409b-b814-57c9c8a400d8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.804386ms
Jul 13 10:04:36.393: INFO: Pod "pod-projected-secrets-cfb69a55-3e46-409b-b814-57c9c8a400d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031157183s
Jul 13 10:04:38.407: INFO: Pod "pod-projected-secrets-cfb69a55-3e46-409b-b814-57c9c8a400d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045404558s
STEP: Saw pod success
Jul 13 10:04:38.407: INFO: Pod "pod-projected-secrets-cfb69a55-3e46-409b-b814-57c9c8a400d8" satisfied condition "success or failure"
Jul 13 10:04:38.417: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-secrets-cfb69a55-3e46-409b-b814-57c9c8a400d8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 10:04:38.472: INFO: Waiting for pod pod-projected-secrets-cfb69a55-3e46-409b-b814-57c9c8a400d8 to disappear
Jul 13 10:04:38.485: INFO: Pod pod-projected-secrets-cfb69a55-3e46-409b-b814-57c9c8a400d8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:04:38.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-496" for this suite.
Jul 13 10:04:44.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:04:44.937: INFO: namespace projected-496 deletion completed in 6.442825299s

• [SLOW TEST:10.744 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:04:44.937: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 13 10:04:45.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4729'
Jul 13 10:04:45.253: INFO: stderr: ""
Jul 13 10:04:45.253: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jul 13 10:04:45.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete pods e2e-test-nginx-pod --namespace=kubectl-4729'
Jul 13 10:04:48.481: INFO: stderr: ""
Jul 13 10:04:48.481: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:04:48.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4729" for this suite.
Jul 13 10:04:54.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:04:54.839: INFO: namespace kubectl-4729 deletion completed in 6.344677116s

• [SLOW TEST:9.902 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:04:54.840: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 10:04:54.984: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5bd56f88-560f-49a9-be9f-e518c4dd1402" in namespace "projected-8411" to be "success or failure"
Jul 13 10:04:54.993: INFO: Pod "downwardapi-volume-5bd56f88-560f-49a9-be9f-e518c4dd1402": Phase="Pending", Reason="", readiness=false. Elapsed: 8.934218ms
Jul 13 10:04:57.002: INFO: Pod "downwardapi-volume-5bd56f88-560f-49a9-be9f-e518c4dd1402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018419946s
Jul 13 10:04:59.013: INFO: Pod "downwardapi-volume-5bd56f88-560f-49a9-be9f-e518c4dd1402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029697833s
STEP: Saw pod success
Jul 13 10:04:59.014: INFO: Pod "downwardapi-volume-5bd56f88-560f-49a9-be9f-e518c4dd1402" satisfied condition "success or failure"
Jul 13 10:04:59.023: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-5bd56f88-560f-49a9-be9f-e518c4dd1402 container client-container: <nil>
STEP: delete the pod
Jul 13 10:04:59.079: INFO: Waiting for pod downwardapi-volume-5bd56f88-560f-49a9-be9f-e518c4dd1402 to disappear
Jul 13 10:04:59.087: INFO: Pod downwardapi-volume-5bd56f88-560f-49a9-be9f-e518c4dd1402 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:04:59.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8411" for this suite.
Jul 13 10:05:05.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:05:05.452: INFO: namespace projected-8411 deletion completed in 6.354971066s

• [SLOW TEST:10.611 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:05:05.453: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 13 10:05:05.598: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:05:17.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4885" for this suite.
Jul 13 10:05:23.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:05:24.077: INFO: namespace pods-4885 deletion completed in 6.364154287s

• [SLOW TEST:18.625 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:05:24.078: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-6499
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6499
STEP: Deleting pre-stop pod
Jul 13 10:05:37.294: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:05:37.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6499" for this suite.
Jul 13 10:06:17.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:06:17.685: INFO: namespace prestop-6499 deletion completed in 40.357530396s

• [SLOW TEST:53.606 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:06:17.687: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-aec04fe0-c986-45c0-9120-fee95193806c
STEP: Creating configMap with name cm-test-opt-upd-2fbd8cbb-7d09-44d1-961d-f95c03f768b6
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-aec04fe0-c986-45c0-9120-fee95193806c
STEP: Updating configmap cm-test-opt-upd-2fbd8cbb-7d09-44d1-961d-f95c03f768b6
STEP: Creating configMap with name cm-test-opt-create-a0f08f93-9797-4500-9f62-557f8ca6a6da
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:07:51.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8800" for this suite.
Jul 13 10:08:15.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:08:16.039: INFO: namespace configmap-8800 deletion completed in 24.354895352s

• [SLOW TEST:118.352 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:08:16.039: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 13 10:08:20.272: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:08:20.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4575" for this suite.
Jul 13 10:08:26.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:08:26.671: INFO: namespace container-runtime-4575 deletion completed in 6.350878768s

• [SLOW TEST:10.632 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:08:26.674: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6281
I0713 10:08:26.856384      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6281, replica count: 1
I0713 10:08:27.907018      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0713 10:08:28.907499      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 13 10:08:29.040: INFO: Created: latency-svc-tmd5h
Jul 13 10:08:29.053: INFO: Got endpoints: latency-svc-tmd5h [45.243187ms]
Jul 13 10:08:29.101: INFO: Created: latency-svc-fpdsj
Jul 13 10:08:29.110: INFO: Created: latency-svc-pn5wf
Jul 13 10:08:29.110: INFO: Got endpoints: latency-svc-fpdsj [57.087197ms]
Jul 13 10:08:29.120: INFO: Created: latency-svc-jll24
Jul 13 10:08:29.127: INFO: Got endpoints: latency-svc-pn5wf [73.074473ms]
Jul 13 10:08:29.139: INFO: Created: latency-svc-bcq54
Jul 13 10:08:29.139: INFO: Got endpoints: latency-svc-jll24 [84.835125ms]
Jul 13 10:08:29.146: INFO: Created: latency-svc-jmknr
Jul 13 10:08:29.152: INFO: Got endpoints: latency-svc-bcq54 [97.127215ms]
Jul 13 10:08:29.157: INFO: Created: latency-svc-mf94w
Jul 13 10:08:29.162: INFO: Got endpoints: latency-svc-jmknr [106.221267ms]
Jul 13 10:08:29.166: INFO: Got endpoints: latency-svc-mf94w [39.11491ms]
Jul 13 10:08:29.175: INFO: Created: latency-svc-qbtpq
Jul 13 10:08:29.181: INFO: Got endpoints: latency-svc-qbtpq [126.078244ms]
Jul 13 10:08:29.186: INFO: Created: latency-svc-22ncl
Jul 13 10:08:29.190: INFO: Got endpoints: latency-svc-22ncl [136.81869ms]
Jul 13 10:08:29.193: INFO: Created: latency-svc-l6x5p
Jul 13 10:08:29.201: INFO: Got endpoints: latency-svc-l6x5p [146.412826ms]
Jul 13 10:08:29.207: INFO: Created: latency-svc-7g4wx
Jul 13 10:08:29.213: INFO: Got endpoints: latency-svc-7g4wx [155.50922ms]
Jul 13 10:08:29.214: INFO: Created: latency-svc-rslbj
Jul 13 10:08:29.218: INFO: Created: latency-svc-kgwvm
Jul 13 10:08:29.247: INFO: Created: latency-svc-jkk6t
Jul 13 10:08:29.247: INFO: Created: latency-svc-rrzcq
Jul 13 10:08:29.247: INFO: Got endpoints: latency-svc-rslbj [194.263752ms]
Jul 13 10:08:29.247: INFO: Got endpoints: latency-svc-rrzcq [191.46342ms]
Jul 13 10:08:29.247: INFO: Got endpoints: latency-svc-kgwvm [190.373865ms]
Jul 13 10:08:29.252: INFO: Got endpoints: latency-svc-jkk6t [196.153896ms]
Jul 13 10:08:29.252: INFO: Created: latency-svc-ngcrb
Jul 13 10:08:29.259: INFO: Got endpoints: latency-svc-ngcrb [202.76383ms]
Jul 13 10:08:29.260: INFO: Created: latency-svc-6w5n9
Jul 13 10:08:29.266: INFO: Created: latency-svc-n4788
Jul 13 10:08:29.270: INFO: Got endpoints: latency-svc-6w5n9 [212.738441ms]
Jul 13 10:08:29.276: INFO: Created: latency-svc-zw68j
Jul 13 10:08:29.276: INFO: Got endpoints: latency-svc-n4788 [165.959047ms]
Jul 13 10:08:29.284: INFO: Created: latency-svc-7n9n8
Jul 13 10:08:29.284: INFO: Got endpoints: latency-svc-zw68j [145.320045ms]
Jul 13 10:08:29.300: INFO: Got endpoints: latency-svc-7n9n8 [147.315945ms]
Jul 13 10:08:29.300: INFO: Created: latency-svc-t99sg
Jul 13 10:08:29.312: INFO: Created: latency-svc-mjq7p
Jul 13 10:08:29.317: INFO: Got endpoints: latency-svc-t99sg [155.052065ms]
Jul 13 10:08:29.322: INFO: Created: latency-svc-ndj6h
Jul 13 10:08:29.322: INFO: Got endpoints: latency-svc-mjq7p [155.153025ms]
Jul 13 10:08:29.328: INFO: Created: latency-svc-grmtr
Jul 13 10:08:29.332: INFO: Got endpoints: latency-svc-ndj6h [150.226829ms]
Jul 13 10:08:29.336: INFO: Created: latency-svc-svxgq
Jul 13 10:08:29.339: INFO: Got endpoints: latency-svc-grmtr [148.126919ms]
Jul 13 10:08:29.345: INFO: Created: latency-svc-2v98k
Jul 13 10:08:29.346: INFO: Got endpoints: latency-svc-svxgq [144.17581ms]
Jul 13 10:08:29.352: INFO: Got endpoints: latency-svc-2v98k [139.688326ms]
Jul 13 10:08:29.356: INFO: Created: latency-svc-tt9cr
Jul 13 10:08:29.366: INFO: Created: latency-svc-qllgs
Jul 13 10:08:29.366: INFO: Got endpoints: latency-svc-tt9cr [118.149259ms]
Jul 13 10:08:29.369: INFO: Got endpoints: latency-svc-qllgs [121.390804ms]
Jul 13 10:08:29.371: INFO: Created: latency-svc-6mjl9
Jul 13 10:08:29.375: INFO: Got endpoints: latency-svc-6mjl9 [127.708741ms]
Jul 13 10:08:29.380: INFO: Created: latency-svc-btqpx
Jul 13 10:08:29.386: INFO: Got endpoints: latency-svc-btqpx [133.963253ms]
Jul 13 10:08:29.390: INFO: Created: latency-svc-8xz8j
Jul 13 10:08:29.398: INFO: Created: latency-svc-x4xmn
Jul 13 10:08:29.399: INFO: Got endpoints: latency-svc-8xz8j [139.102963ms]
Jul 13 10:08:29.405: INFO: Got endpoints: latency-svc-x4xmn [135.044641ms]
Jul 13 10:08:29.408: INFO: Created: latency-svc-mcrtj
Jul 13 10:08:29.418: INFO: Got endpoints: latency-svc-mcrtj [142.226906ms]
Jul 13 10:08:29.420: INFO: Created: latency-svc-4z4tq
Jul 13 10:08:29.429: INFO: Got endpoints: latency-svc-4z4tq [144.177035ms]
Jul 13 10:08:29.434: INFO: Created: latency-svc-z6jsl
Jul 13 10:08:29.440: INFO: Got endpoints: latency-svc-z6jsl [140.271422ms]
Jul 13 10:08:29.443: INFO: Created: latency-svc-tx9sn
Jul 13 10:08:29.449: INFO: Got endpoints: latency-svc-tx9sn [131.807942ms]
Jul 13 10:08:29.457: INFO: Created: latency-svc-cgvs6
Jul 13 10:08:29.468: INFO: Created: latency-svc-zsq5j
Jul 13 10:08:29.469: INFO: Got endpoints: latency-svc-cgvs6 [146.461128ms]
Jul 13 10:08:29.474: INFO: Got endpoints: latency-svc-zsq5j [141.703155ms]
Jul 13 10:08:29.478: INFO: Created: latency-svc-6g9zb
Jul 13 10:08:29.502: INFO: Created: latency-svc-hxvxv
Jul 13 10:08:29.509: INFO: Created: latency-svc-99ldn
Jul 13 10:08:29.509: INFO: Got endpoints: latency-svc-6g9zb [169.190518ms]
Jul 13 10:08:29.522: INFO: Created: latency-svc-zp5hk
Jul 13 10:08:29.532: INFO: Created: latency-svc-m755q
Jul 13 10:08:29.553: INFO: Created: latency-svc-9zv7f
Jul 13 10:08:29.564: INFO: Got endpoints: latency-svc-hxvxv [218.050407ms]
Jul 13 10:08:29.565: INFO: Created: latency-svc-b8jdk
Jul 13 10:08:29.574: INFO: Created: latency-svc-vcnwc
Jul 13 10:08:29.579: INFO: Created: latency-svc-z5pnr
Jul 13 10:08:29.593: INFO: Created: latency-svc-s426j
Jul 13 10:08:29.601: INFO: Created: latency-svc-b4h4p
Jul 13 10:08:29.607: INFO: Got endpoints: latency-svc-99ldn [254.220823ms]
Jul 13 10:08:29.619: INFO: Created: latency-svc-tnd6z
Jul 13 10:08:29.631: INFO: Created: latency-svc-bpwjn
Jul 13 10:08:29.638: INFO: Created: latency-svc-qz4ts
Jul 13 10:08:29.660: INFO: Created: latency-svc-dmmzb
Jul 13 10:08:29.661: INFO: Got endpoints: latency-svc-zp5hk [294.965454ms]
Jul 13 10:08:29.665: INFO: Created: latency-svc-6p2hx
Jul 13 10:08:29.667: INFO: Created: latency-svc-lpmmr
Jul 13 10:08:29.684: INFO: Created: latency-svc-x67ll
Jul 13 10:08:29.694: INFO: Created: latency-svc-ktq2j
Jul 13 10:08:29.705: INFO: Got endpoints: latency-svc-m755q [335.643985ms]
Jul 13 10:08:29.734: INFO: Created: latency-svc-g8qj2
Jul 13 10:08:29.761: INFO: Got endpoints: latency-svc-9zv7f [385.722574ms]
Jul 13 10:08:29.806: INFO: Created: latency-svc-2sv2h
Jul 13 10:08:29.815: INFO: Got endpoints: latency-svc-b8jdk [428.571061ms]
Jul 13 10:08:29.843: INFO: Created: latency-svc-wkhcs
Jul 13 10:08:29.856: INFO: Got endpoints: latency-svc-vcnwc [457.3125ms]
Jul 13 10:08:29.885: INFO: Created: latency-svc-2dzst
Jul 13 10:08:29.909: INFO: Got endpoints: latency-svc-z5pnr [503.724406ms]
Jul 13 10:08:29.938: INFO: Created: latency-svc-fnhrl
Jul 13 10:08:29.961: INFO: Got endpoints: latency-svc-s426j [542.786008ms]
Jul 13 10:08:29.997: INFO: Created: latency-svc-7sd4c
Jul 13 10:08:30.007: INFO: Got endpoints: latency-svc-b4h4p [578.24931ms]
Jul 13 10:08:30.037: INFO: Created: latency-svc-hkfxl
Jul 13 10:08:30.062: INFO: Got endpoints: latency-svc-tnd6z [621.04895ms]
Jul 13 10:08:30.090: INFO: Created: latency-svc-zjs4z
Jul 13 10:08:30.109: INFO: Got endpoints: latency-svc-bpwjn [659.481092ms]
Jul 13 10:08:30.143: INFO: Created: latency-svc-sslrt
Jul 13 10:08:30.160: INFO: Got endpoints: latency-svc-qz4ts [691.420544ms]
Jul 13 10:08:30.196: INFO: Created: latency-svc-qgdrb
Jul 13 10:08:30.208: INFO: Got endpoints: latency-svc-dmmzb [733.736085ms]
Jul 13 10:08:30.241: INFO: Created: latency-svc-ppcrc
Jul 13 10:08:30.263: INFO: Got endpoints: latency-svc-6p2hx [753.820641ms]
Jul 13 10:08:30.299: INFO: Created: latency-svc-j4ckq
Jul 13 10:08:30.310: INFO: Got endpoints: latency-svc-lpmmr [746.372268ms]
Jul 13 10:08:30.338: INFO: Created: latency-svc-drkqr
Jul 13 10:08:30.364: INFO: Got endpoints: latency-svc-x67ll [757.20635ms]
Jul 13 10:08:30.398: INFO: Created: latency-svc-lcwd6
Jul 13 10:08:30.413: INFO: Got endpoints: latency-svc-ktq2j [752.018476ms]
Jul 13 10:08:30.437: INFO: Created: latency-svc-hjhh5
Jul 13 10:08:30.455: INFO: Got endpoints: latency-svc-g8qj2 [749.389721ms]
Jul 13 10:08:30.484: INFO: Created: latency-svc-6jcqv
Jul 13 10:08:30.508: INFO: Got endpoints: latency-svc-2sv2h [735.928209ms]
Jul 13 10:08:30.536: INFO: Created: latency-svc-mpg5h
Jul 13 10:08:30.559: INFO: Got endpoints: latency-svc-wkhcs [743.383944ms]
Jul 13 10:08:30.587: INFO: Created: latency-svc-hg59j
Jul 13 10:08:30.610: INFO: Got endpoints: latency-svc-2dzst [754.014245ms]
Jul 13 10:08:30.649: INFO: Created: latency-svc-ps2jw
Jul 13 10:08:30.656: INFO: Got endpoints: latency-svc-fnhrl [746.826609ms]
Jul 13 10:08:30.684: INFO: Created: latency-svc-cxt45
Jul 13 10:08:30.707: INFO: Got endpoints: latency-svc-7sd4c [745.577884ms]
Jul 13 10:08:30.737: INFO: Created: latency-svc-26d6h
Jul 13 10:08:30.759: INFO: Got endpoints: latency-svc-hkfxl [751.741628ms]
Jul 13 10:08:30.786: INFO: Created: latency-svc-6tqnj
Jul 13 10:08:30.804: INFO: Got endpoints: latency-svc-zjs4z [742.15324ms]
Jul 13 10:08:30.834: INFO: Created: latency-svc-gn298
Jul 13 10:08:30.860: INFO: Got endpoints: latency-svc-sslrt [750.582081ms]
Jul 13 10:08:30.885: INFO: Created: latency-svc-cxtv8
Jul 13 10:08:30.906: INFO: Got endpoints: latency-svc-qgdrb [745.618561ms]
Jul 13 10:08:30.939: INFO: Created: latency-svc-9m9pp
Jul 13 10:08:30.959: INFO: Got endpoints: latency-svc-ppcrc [751.377462ms]
Jul 13 10:08:30.988: INFO: Created: latency-svc-qv2rb
Jul 13 10:08:31.010: INFO: Got endpoints: latency-svc-j4ckq [745.997879ms]
Jul 13 10:08:31.053: INFO: Created: latency-svc-jvw9h
Jul 13 10:08:31.062: INFO: Got endpoints: latency-svc-drkqr [751.62685ms]
Jul 13 10:08:31.092: INFO: Created: latency-svc-cmqmj
Jul 13 10:08:31.110: INFO: Got endpoints: latency-svc-lcwd6 [746.358251ms]
Jul 13 10:08:31.142: INFO: Created: latency-svc-z2948
Jul 13 10:08:31.159: INFO: Got endpoints: latency-svc-hjhh5 [746.468707ms]
Jul 13 10:08:31.187: INFO: Created: latency-svc-4xcj5
Jul 13 10:08:31.212: INFO: Got endpoints: latency-svc-6jcqv [757.224025ms]
Jul 13 10:08:31.250: INFO: Created: latency-svc-5cljp
Jul 13 10:08:31.258: INFO: Got endpoints: latency-svc-mpg5h [750.240303ms]
Jul 13 10:08:31.283: INFO: Created: latency-svc-mmn2x
Jul 13 10:08:31.313: INFO: Got endpoints: latency-svc-hg59j [753.469459ms]
Jul 13 10:08:31.346: INFO: Created: latency-svc-n78kp
Jul 13 10:08:31.357: INFO: Got endpoints: latency-svc-ps2jw [747.001125ms]
Jul 13 10:08:31.384: INFO: Created: latency-svc-wrmx5
Jul 13 10:08:31.407: INFO: Got endpoints: latency-svc-cxt45 [751.478291ms]
Jul 13 10:08:31.445: INFO: Created: latency-svc-gqx8d
Jul 13 10:08:31.458: INFO: Got endpoints: latency-svc-26d6h [751.458451ms]
Jul 13 10:08:31.487: INFO: Created: latency-svc-p5whd
Jul 13 10:08:31.511: INFO: Got endpoints: latency-svc-6tqnj [751.689022ms]
Jul 13 10:08:31.544: INFO: Created: latency-svc-m2g9p
Jul 13 10:08:31.558: INFO: Got endpoints: latency-svc-gn298 [753.722675ms]
Jul 13 10:08:31.586: INFO: Created: latency-svc-psbrv
Jul 13 10:08:31.611: INFO: Got endpoints: latency-svc-cxtv8 [751.381899ms]
Jul 13 10:08:31.639: INFO: Created: latency-svc-7x7hb
Jul 13 10:08:31.659: INFO: Got endpoints: latency-svc-9m9pp [752.867952ms]
Jul 13 10:08:31.687: INFO: Created: latency-svc-zrjqt
Jul 13 10:08:31.706: INFO: Got endpoints: latency-svc-qv2rb [745.92217ms]
Jul 13 10:08:31.738: INFO: Created: latency-svc-56vx8
Jul 13 10:08:31.765: INFO: Got endpoints: latency-svc-jvw9h [755.717179ms]
Jul 13 10:08:31.795: INFO: Created: latency-svc-hql5x
Jul 13 10:08:31.811: INFO: Got endpoints: latency-svc-cmqmj [749.074608ms]
Jul 13 10:08:31.852: INFO: Created: latency-svc-nn8r7
Jul 13 10:08:31.857: INFO: Got endpoints: latency-svc-z2948 [745.673125ms]
Jul 13 10:08:31.881: INFO: Created: latency-svc-lzn8r
Jul 13 10:08:31.910: INFO: Got endpoints: latency-svc-4xcj5 [750.48101ms]
Jul 13 10:08:31.942: INFO: Created: latency-svc-dr757
Jul 13 10:08:31.969: INFO: Got endpoints: latency-svc-5cljp [756.57682ms]
Jul 13 10:08:32.003: INFO: Created: latency-svc-4r64s
Jul 13 10:08:32.009: INFO: Got endpoints: latency-svc-mmn2x [750.350325ms]
Jul 13 10:08:32.038: INFO: Created: latency-svc-gwkj2
Jul 13 10:08:32.059: INFO: Got endpoints: latency-svc-n78kp [746.282997ms]
Jul 13 10:08:32.089: INFO: Created: latency-svc-bztd9
Jul 13 10:08:32.112: INFO: Got endpoints: latency-svc-wrmx5 [754.046294ms]
Jul 13 10:08:32.151: INFO: Created: latency-svc-nzrjb
Jul 13 10:08:32.163: INFO: Got endpoints: latency-svc-gqx8d [756.309528ms]
Jul 13 10:08:32.193: INFO: Created: latency-svc-89l4g
Jul 13 10:08:32.211: INFO: Got endpoints: latency-svc-p5whd [753.092939ms]
Jul 13 10:08:32.239: INFO: Created: latency-svc-zz5v7
Jul 13 10:08:32.255: INFO: Got endpoints: latency-svc-m2g9p [740.45179ms]
Jul 13 10:08:32.287: INFO: Created: latency-svc-t54j7
Jul 13 10:08:32.314: INFO: Got endpoints: latency-svc-psbrv [756.641673ms]
Jul 13 10:08:32.343: INFO: Created: latency-svc-klqh9
Jul 13 10:08:32.363: INFO: Got endpoints: latency-svc-7x7hb [751.539677ms]
Jul 13 10:08:32.395: INFO: Created: latency-svc-fnvns
Jul 13 10:08:32.406: INFO: Got endpoints: latency-svc-zrjqt [746.602006ms]
Jul 13 10:08:32.435: INFO: Created: latency-svc-5g42g
Jul 13 10:08:32.462: INFO: Got endpoints: latency-svc-56vx8 [755.682888ms]
Jul 13 10:08:32.483: INFO: Created: latency-svc-7sgxv
Jul 13 10:08:32.505: INFO: Got endpoints: latency-svc-hql5x [739.077157ms]
Jul 13 10:08:32.537: INFO: Created: latency-svc-t8dd2
Jul 13 10:08:32.562: INFO: Got endpoints: latency-svc-nn8r7 [750.27361ms]
Jul 13 10:08:32.592: INFO: Created: latency-svc-qct6z
Jul 13 10:08:32.609: INFO: Got endpoints: latency-svc-lzn8r [752.216145ms]
Jul 13 10:08:32.637: INFO: Created: latency-svc-qhmq5
Jul 13 10:08:32.659: INFO: Got endpoints: latency-svc-dr757 [749.431048ms]
Jul 13 10:08:32.692: INFO: Created: latency-svc-6knhh
Jul 13 10:08:32.711: INFO: Got endpoints: latency-svc-4r64s [742.581485ms]
Jul 13 10:08:32.742: INFO: Created: latency-svc-6lccl
Jul 13 10:08:32.759: INFO: Got endpoints: latency-svc-gwkj2 [749.684374ms]
Jul 13 10:08:32.789: INFO: Created: latency-svc-bqgtf
Jul 13 10:08:32.816: INFO: Got endpoints: latency-svc-bztd9 [757.026967ms]
Jul 13 10:08:32.844: INFO: Created: latency-svc-8wff6
Jul 13 10:08:32.865: INFO: Got endpoints: latency-svc-nzrjb [752.892248ms]
Jul 13 10:08:32.900: INFO: Created: latency-svc-6mz89
Jul 13 10:08:32.911: INFO: Got endpoints: latency-svc-89l4g [747.840326ms]
Jul 13 10:08:32.941: INFO: Created: latency-svc-df24h
Jul 13 10:08:32.960: INFO: Got endpoints: latency-svc-zz5v7 [748.988595ms]
Jul 13 10:08:32.995: INFO: Created: latency-svc-msjqn
Jul 13 10:08:33.010: INFO: Got endpoints: latency-svc-t54j7 [754.553749ms]
Jul 13 10:08:33.040: INFO: Created: latency-svc-gplqk
Jul 13 10:08:33.058: INFO: Got endpoints: latency-svc-klqh9 [743.7567ms]
Jul 13 10:08:33.089: INFO: Created: latency-svc-w5qsm
Jul 13 10:08:33.108: INFO: Got endpoints: latency-svc-fnvns [745.289376ms]
Jul 13 10:08:33.140: INFO: Created: latency-svc-dwstc
Jul 13 10:08:33.161: INFO: Got endpoints: latency-svc-5g42g [755.588902ms]
Jul 13 10:08:33.202: INFO: Created: latency-svc-7ktx5
Jul 13 10:08:33.209: INFO: Got endpoints: latency-svc-7sgxv [746.701556ms]
Jul 13 10:08:33.241: INFO: Created: latency-svc-hv5qn
Jul 13 10:08:33.260: INFO: Got endpoints: latency-svc-t8dd2 [755.514375ms]
Jul 13 10:08:33.295: INFO: Created: latency-svc-cwn4j
Jul 13 10:08:33.307: INFO: Got endpoints: latency-svc-qct6z [745.553926ms]
Jul 13 10:08:33.334: INFO: Created: latency-svc-dwc2q
Jul 13 10:08:33.467: INFO: Got endpoints: latency-svc-6lccl [755.329719ms]
Jul 13 10:08:33.468: INFO: Got endpoints: latency-svc-qhmq5 [858.631087ms]
Jul 13 10:08:33.468: INFO: Got endpoints: latency-svc-6knhh [808.670174ms]
Jul 13 10:08:33.507: INFO: Got endpoints: latency-svc-bqgtf [747.757822ms]
Jul 13 10:08:33.521: INFO: Created: latency-svc-ddhg4
Jul 13 10:08:33.528: INFO: Created: latency-svc-jfnp4
Jul 13 10:08:33.538: INFO: Created: latency-svc-62nx8
Jul 13 10:08:33.550: INFO: Created: latency-svc-lqrsh
Jul 13 10:08:33.552: INFO: Got endpoints: latency-svc-8wff6 [735.837955ms]
Jul 13 10:08:33.574: INFO: Created: latency-svc-lsb27
Jul 13 10:08:33.608: INFO: Got endpoints: latency-svc-6mz89 [743.382718ms]
Jul 13 10:08:33.638: INFO: Created: latency-svc-bz4mh
Jul 13 10:08:33.661: INFO: Got endpoints: latency-svc-df24h [749.730094ms]
Jul 13 10:08:33.697: INFO: Created: latency-svc-6v5rt
Jul 13 10:08:33.708: INFO: Got endpoints: latency-svc-msjqn [747.246423ms]
Jul 13 10:08:33.736: INFO: Created: latency-svc-g4sxw
Jul 13 10:08:33.761: INFO: Got endpoints: latency-svc-gplqk [750.926088ms]
Jul 13 10:08:33.793: INFO: Created: latency-svc-ff2dp
Jul 13 10:08:33.808: INFO: Got endpoints: latency-svc-w5qsm [749.899865ms]
Jul 13 10:08:33.837: INFO: Created: latency-svc-r85cw
Jul 13 10:08:33.861: INFO: Got endpoints: latency-svc-dwstc [752.605953ms]
Jul 13 10:08:33.888: INFO: Created: latency-svc-nfr2l
Jul 13 10:08:33.908: INFO: Got endpoints: latency-svc-7ktx5 [746.622779ms]
Jul 13 10:08:33.938: INFO: Created: latency-svc-82z6w
Jul 13 10:08:33.960: INFO: Got endpoints: latency-svc-hv5qn [751.077317ms]
Jul 13 10:08:33.989: INFO: Created: latency-svc-8d5qm
Jul 13 10:08:34.011: INFO: Got endpoints: latency-svc-cwn4j [750.450136ms]
Jul 13 10:08:34.040: INFO: Created: latency-svc-bwxwr
Jul 13 10:08:34.058: INFO: Got endpoints: latency-svc-dwc2q [750.367245ms]
Jul 13 10:08:34.086: INFO: Created: latency-svc-ff87j
Jul 13 10:08:34.109: INFO: Got endpoints: latency-svc-ddhg4 [641.773927ms]
Jul 13 10:08:34.154: INFO: Created: latency-svc-2wg49
Jul 13 10:08:34.171: INFO: Got endpoints: latency-svc-jfnp4 [703.181483ms]
Jul 13 10:08:34.205: INFO: Created: latency-svc-bl68t
Jul 13 10:08:34.205: INFO: Got endpoints: latency-svc-62nx8 [737.220209ms]
Jul 13 10:08:34.230: INFO: Created: latency-svc-5bgw5
Jul 13 10:08:34.259: INFO: Got endpoints: latency-svc-lqrsh [752.654962ms]
Jul 13 10:08:34.286: INFO: Created: latency-svc-wwjcx
Jul 13 10:08:34.314: INFO: Got endpoints: latency-svc-lsb27 [762.246429ms]
Jul 13 10:08:34.341: INFO: Created: latency-svc-nxf6j
Jul 13 10:08:34.359: INFO: Got endpoints: latency-svc-bz4mh [750.633429ms]
Jul 13 10:08:34.388: INFO: Created: latency-svc-8l7m6
Jul 13 10:08:34.409: INFO: Got endpoints: latency-svc-6v5rt [747.289263ms]
Jul 13 10:08:34.440: INFO: Created: latency-svc-qscpz
Jul 13 10:08:34.460: INFO: Got endpoints: latency-svc-g4sxw [751.820181ms]
Jul 13 10:08:34.488: INFO: Created: latency-svc-68bzb
Jul 13 10:08:34.504: INFO: Got endpoints: latency-svc-ff2dp [742.995549ms]
Jul 13 10:08:34.530: INFO: Created: latency-svc-fslvx
Jul 13 10:08:34.559: INFO: Got endpoints: latency-svc-r85cw [750.850837ms]
Jul 13 10:08:34.590: INFO: Created: latency-svc-hdl8s
Jul 13 10:08:34.610: INFO: Got endpoints: latency-svc-nfr2l [748.642328ms]
Jul 13 10:08:34.639: INFO: Created: latency-svc-rcwtz
Jul 13 10:08:34.661: INFO: Got endpoints: latency-svc-82z6w [753.094272ms]
Jul 13 10:08:34.698: INFO: Created: latency-svc-ccmlz
Jul 13 10:08:34.708: INFO: Got endpoints: latency-svc-8d5qm [748.244434ms]
Jul 13 10:08:34.738: INFO: Created: latency-svc-p5p6d
Jul 13 10:08:34.758: INFO: Got endpoints: latency-svc-bwxwr [747.464038ms]
Jul 13 10:08:34.791: INFO: Created: latency-svc-k4gtb
Jul 13 10:08:34.807: INFO: Got endpoints: latency-svc-ff87j [748.650171ms]
Jul 13 10:08:34.837: INFO: Created: latency-svc-djs4c
Jul 13 10:08:34.860: INFO: Got endpoints: latency-svc-2wg49 [751.025579ms]
Jul 13 10:08:34.891: INFO: Created: latency-svc-5zg5g
Jul 13 10:08:34.908: INFO: Got endpoints: latency-svc-bl68t [736.881831ms]
Jul 13 10:08:34.937: INFO: Created: latency-svc-vnb7d
Jul 13 10:08:34.965: INFO: Got endpoints: latency-svc-5bgw5 [759.6802ms]
Jul 13 10:08:34.995: INFO: Created: latency-svc-nv55d
Jul 13 10:08:35.028: INFO: Got endpoints: latency-svc-wwjcx [768.96219ms]
Jul 13 10:08:35.059: INFO: Got endpoints: latency-svc-nxf6j [744.452222ms]
Jul 13 10:08:35.063: INFO: Created: latency-svc-2lmfc
Jul 13 10:08:35.090: INFO: Created: latency-svc-76zf6
Jul 13 10:08:35.109: INFO: Got endpoints: latency-svc-8l7m6 [750.312948ms]
Jul 13 10:08:35.140: INFO: Created: latency-svc-nqff8
Jul 13 10:08:35.163: INFO: Got endpoints: latency-svc-qscpz [754.12839ms]
Jul 13 10:08:35.195: INFO: Created: latency-svc-vbknn
Jul 13 10:08:35.207: INFO: Got endpoints: latency-svc-68bzb [746.911756ms]
Jul 13 10:08:35.236: INFO: Created: latency-svc-m7mw7
Jul 13 10:08:35.259: INFO: Got endpoints: latency-svc-fslvx [755.128466ms]
Jul 13 10:08:35.292: INFO: Created: latency-svc-l7tbh
Jul 13 10:08:35.307: INFO: Got endpoints: latency-svc-hdl8s [748.191376ms]
Jul 13 10:08:35.337: INFO: Created: latency-svc-547bh
Jul 13 10:08:35.360: INFO: Got endpoints: latency-svc-rcwtz [749.463579ms]
Jul 13 10:08:35.393: INFO: Created: latency-svc-dq8cg
Jul 13 10:08:35.410: INFO: Got endpoints: latency-svc-ccmlz [748.914471ms]
Jul 13 10:08:35.442: INFO: Created: latency-svc-q5sj6
Jul 13 10:08:35.465: INFO: Got endpoints: latency-svc-p5p6d [756.624371ms]
Jul 13 10:08:35.493: INFO: Created: latency-svc-swck4
Jul 13 10:08:35.508: INFO: Got endpoints: latency-svc-k4gtb [749.544451ms]
Jul 13 10:08:35.544: INFO: Created: latency-svc-92v7d
Jul 13 10:08:35.571: INFO: Got endpoints: latency-svc-djs4c [764.242549ms]
Jul 13 10:08:35.609: INFO: Created: latency-svc-kq2s5
Jul 13 10:08:35.611: INFO: Got endpoints: latency-svc-5zg5g [749.822872ms]
Jul 13 10:08:35.646: INFO: Created: latency-svc-7kw2b
Jul 13 10:08:35.657: INFO: Got endpoints: latency-svc-vnb7d [748.307007ms]
Jul 13 10:08:35.685: INFO: Created: latency-svc-gk2xq
Jul 13 10:08:35.709: INFO: Got endpoints: latency-svc-nv55d [744.403593ms]
Jul 13 10:08:35.739: INFO: Created: latency-svc-6vzgw
Jul 13 10:08:35.757: INFO: Got endpoints: latency-svc-2lmfc [728.568302ms]
Jul 13 10:08:35.786: INFO: Created: latency-svc-hvskr
Jul 13 10:08:35.808: INFO: Got endpoints: latency-svc-76zf6 [749.241933ms]
Jul 13 10:08:35.841: INFO: Created: latency-svc-826bk
Jul 13 10:08:35.865: INFO: Got endpoints: latency-svc-nqff8 [754.559772ms]
Jul 13 10:08:35.894: INFO: Created: latency-svc-cgnvc
Jul 13 10:08:35.914: INFO: Got endpoints: latency-svc-vbknn [751.200904ms]
Jul 13 10:08:35.944: INFO: Created: latency-svc-dwljl
Jul 13 10:08:35.958: INFO: Got endpoints: latency-svc-m7mw7 [751.407693ms]
Jul 13 10:08:35.987: INFO: Created: latency-svc-t9ln5
Jul 13 10:08:36.008: INFO: Got endpoints: latency-svc-l7tbh [748.515077ms]
Jul 13 10:08:36.043: INFO: Created: latency-svc-vhrrw
Jul 13 10:08:36.055: INFO: Got endpoints: latency-svc-547bh [747.688712ms]
Jul 13 10:08:36.082: INFO: Created: latency-svc-pxjq9
Jul 13 10:08:36.108: INFO: Got endpoints: latency-svc-dq8cg [748.503674ms]
Jul 13 10:08:36.136: INFO: Created: latency-svc-z8b9w
Jul 13 10:08:36.157: INFO: Got endpoints: latency-svc-q5sj6 [746.92873ms]
Jul 13 10:08:36.188: INFO: Created: latency-svc-ldwfr
Jul 13 10:08:36.208: INFO: Got endpoints: latency-svc-swck4 [742.818023ms]
Jul 13 10:08:36.237: INFO: Created: latency-svc-d9ztj
Jul 13 10:08:36.259: INFO: Got endpoints: latency-svc-92v7d [751.281993ms]
Jul 13 10:08:36.304: INFO: Created: latency-svc-g2r54
Jul 13 10:08:36.316: INFO: Got endpoints: latency-svc-kq2s5 [745.086178ms]
Jul 13 10:08:36.339: INFO: Created: latency-svc-kxnf4
Jul 13 10:08:36.363: INFO: Got endpoints: latency-svc-7kw2b [751.705571ms]
Jul 13 10:08:36.392: INFO: Created: latency-svc-gkbqk
Jul 13 10:08:36.408: INFO: Got endpoints: latency-svc-gk2xq [751.422901ms]
Jul 13 10:08:36.436: INFO: Created: latency-svc-89vjn
Jul 13 10:08:36.458: INFO: Got endpoints: latency-svc-6vzgw [748.376137ms]
Jul 13 10:08:36.493: INFO: Created: latency-svc-2pktq
Jul 13 10:08:36.509: INFO: Got endpoints: latency-svc-hvskr [751.812718ms]
Jul 13 10:08:36.531: INFO: Created: latency-svc-q98x5
Jul 13 10:08:36.555: INFO: Got endpoints: latency-svc-826bk [746.494923ms]
Jul 13 10:08:36.582: INFO: Created: latency-svc-jv4pt
Jul 13 10:08:36.609: INFO: Got endpoints: latency-svc-cgnvc [742.952999ms]
Jul 13 10:08:36.647: INFO: Created: latency-svc-hdlnz
Jul 13 10:08:36.660: INFO: Got endpoints: latency-svc-dwljl [745.635201ms]
Jul 13 10:08:36.689: INFO: Created: latency-svc-hpkkx
Jul 13 10:08:36.708: INFO: Got endpoints: latency-svc-t9ln5 [750.056345ms]
Jul 13 10:08:36.742: INFO: Created: latency-svc-4kpm7
Jul 13 10:08:36.757: INFO: Got endpoints: latency-svc-vhrrw [748.740618ms]
Jul 13 10:08:36.784: INFO: Created: latency-svc-cq475
Jul 13 10:08:36.809: INFO: Got endpoints: latency-svc-pxjq9 [753.14621ms]
Jul 13 10:08:36.838: INFO: Created: latency-svc-5kjj9
Jul 13 10:08:36.858: INFO: Got endpoints: latency-svc-z8b9w [749.950507ms]
Jul 13 10:08:36.887: INFO: Created: latency-svc-vqsf5
Jul 13 10:08:36.908: INFO: Got endpoints: latency-svc-ldwfr [750.265666ms]
Jul 13 10:08:36.960: INFO: Got endpoints: latency-svc-d9ztj [752.685107ms]
Jul 13 10:08:37.009: INFO: Got endpoints: latency-svc-g2r54 [749.188124ms]
Jul 13 10:08:37.059: INFO: Got endpoints: latency-svc-kxnf4 [742.76136ms]
Jul 13 10:08:37.112: INFO: Got endpoints: latency-svc-gkbqk [749.074984ms]
Jul 13 10:08:37.159: INFO: Got endpoints: latency-svc-89vjn [751.005215ms]
Jul 13 10:08:37.213: INFO: Got endpoints: latency-svc-2pktq [754.922551ms]
Jul 13 10:08:37.261: INFO: Got endpoints: latency-svc-q98x5 [752.309847ms]
Jul 13 10:08:37.310: INFO: Got endpoints: latency-svc-jv4pt [754.801184ms]
Jul 13 10:08:37.365: INFO: Got endpoints: latency-svc-hdlnz [755.71767ms]
Jul 13 10:08:37.409: INFO: Got endpoints: latency-svc-hpkkx [748.317359ms]
Jul 13 10:08:37.460: INFO: Got endpoints: latency-svc-4kpm7 [747.015472ms]
Jul 13 10:08:37.512: INFO: Got endpoints: latency-svc-cq475 [754.955211ms]
Jul 13 10:08:37.563: INFO: Got endpoints: latency-svc-5kjj9 [753.324756ms]
Jul 13 10:08:37.610: INFO: Got endpoints: latency-svc-vqsf5 [751.154126ms]
Jul 13 10:08:37.611: INFO: Latencies: [39.11491ms 57.087197ms 73.074473ms 84.835125ms 97.127215ms 106.221267ms 118.149259ms 121.390804ms 126.078244ms 127.708741ms 131.807942ms 133.963253ms 135.044641ms 136.81869ms 139.102963ms 139.688326ms 140.271422ms 141.703155ms 142.226906ms 144.17581ms 144.177035ms 145.320045ms 146.412826ms 146.461128ms 147.315945ms 148.126919ms 150.226829ms 155.052065ms 155.153025ms 155.50922ms 165.959047ms 169.190518ms 190.373865ms 191.46342ms 194.263752ms 196.153896ms 202.76383ms 212.738441ms 218.050407ms 254.220823ms 294.965454ms 335.643985ms 385.722574ms 428.571061ms 457.3125ms 503.724406ms 542.786008ms 578.24931ms 621.04895ms 641.773927ms 659.481092ms 691.420544ms 703.181483ms 728.568302ms 733.736085ms 735.837955ms 735.928209ms 736.881831ms 737.220209ms 739.077157ms 740.45179ms 742.15324ms 742.581485ms 742.76136ms 742.818023ms 742.952999ms 742.995549ms 743.382718ms 743.383944ms 743.7567ms 744.403593ms 744.452222ms 745.086178ms 745.289376ms 745.553926ms 745.577884ms 745.618561ms 745.635201ms 745.673125ms 745.92217ms 745.997879ms 746.282997ms 746.358251ms 746.372268ms 746.468707ms 746.494923ms 746.602006ms 746.622779ms 746.701556ms 746.826609ms 746.911756ms 746.92873ms 747.001125ms 747.015472ms 747.246423ms 747.289263ms 747.464038ms 747.688712ms 747.757822ms 747.840326ms 748.191376ms 748.244434ms 748.307007ms 748.317359ms 748.376137ms 748.503674ms 748.515077ms 748.642328ms 748.650171ms 748.740618ms 748.914471ms 748.988595ms 749.074608ms 749.074984ms 749.188124ms 749.241933ms 749.389721ms 749.431048ms 749.463579ms 749.544451ms 749.684374ms 749.730094ms 749.822872ms 749.899865ms 749.950507ms 750.056345ms 750.240303ms 750.265666ms 750.27361ms 750.312948ms 750.350325ms 750.367245ms 750.450136ms 750.48101ms 750.582081ms 750.633429ms 750.850837ms 750.926088ms 751.005215ms 751.025579ms 751.077317ms 751.154126ms 751.200904ms 751.281993ms 751.377462ms 751.381899ms 751.407693ms 751.422901ms 751.458451ms 751.478291ms 751.539677ms 751.62685ms 751.689022ms 751.705571ms 751.741628ms 751.812718ms 751.820181ms 752.018476ms 752.216145ms 752.309847ms 752.605953ms 752.654962ms 752.685107ms 752.867952ms 752.892248ms 753.092939ms 753.094272ms 753.14621ms 753.324756ms 753.469459ms 753.722675ms 753.820641ms 754.014245ms 754.046294ms 754.12839ms 754.553749ms 754.559772ms 754.801184ms 754.922551ms 754.955211ms 755.128466ms 755.329719ms 755.514375ms 755.588902ms 755.682888ms 755.717179ms 755.71767ms 756.309528ms 756.57682ms 756.624371ms 756.641673ms 757.026967ms 757.20635ms 757.224025ms 759.6802ms 762.246429ms 764.242549ms 768.96219ms 808.670174ms 858.631087ms]
Jul 13 10:08:37.612: INFO: 50 %ile: 748.191376ms
Jul 13 10:08:37.612: INFO: 90 %ile: 755.128466ms
Jul 13 10:08:37.612: INFO: 99 %ile: 808.670174ms
Jul 13 10:08:37.612: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:08:37.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6281" for this suite.
Jul 13 10:08:53.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:08:54.006: INFO: namespace svc-latency-6281 deletion completed in 16.380620747s

• [SLOW TEST:27.333 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:08:54.008: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 13 10:08:54.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-2812'
Jul 13 10:08:54.404: INFO: stderr: ""
Jul 13 10:08:54.404: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 10:08:54.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2812'
Jul 13 10:08:54.526: INFO: stderr: ""
Jul 13 10:08:54.526: INFO: stdout: "update-demo-nautilus-88lpb update-demo-nautilus-xlf87 "
Jul 13 10:08:54.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-88lpb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:08:54.653: INFO: stderr: ""
Jul 13 10:08:54.653: INFO: stdout: ""
Jul 13 10:08:54.653: INFO: update-demo-nautilus-88lpb is created but not running
Jul 13 10:08:59.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2812'
Jul 13 10:08:59.788: INFO: stderr: ""
Jul 13 10:08:59.788: INFO: stdout: "update-demo-nautilus-88lpb update-demo-nautilus-xlf87 "
Jul 13 10:08:59.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-88lpb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:08:59.904: INFO: stderr: ""
Jul 13 10:08:59.904: INFO: stdout: "true"
Jul 13 10:08:59.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-88lpb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:00.031: INFO: stderr: ""
Jul 13 10:09:00.031: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 10:09:00.031: INFO: validating pod update-demo-nautilus-88lpb
Jul 13 10:09:00.047: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 10:09:00.047: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 10:09:00.047: INFO: update-demo-nautilus-88lpb is verified up and running
Jul 13 10:09:00.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-xlf87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:00.196: INFO: stderr: ""
Jul 13 10:09:00.196: INFO: stdout: "true"
Jul 13 10:09:00.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-xlf87 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:00.328: INFO: stderr: ""
Jul 13 10:09:00.328: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 10:09:00.328: INFO: validating pod update-demo-nautilus-xlf87
Jul 13 10:09:00.343: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 10:09:00.343: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 10:09:00.343: INFO: update-demo-nautilus-xlf87 is verified up and running
STEP: scaling down the replication controller
Jul 13 10:09:00.347: INFO: scanned /root for discovery docs: <nil>
Jul 13 10:09:00.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2812'
Jul 13 10:09:01.570: INFO: stderr: ""
Jul 13 10:09:01.570: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 10:09:01.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2812'
Jul 13 10:09:01.700: INFO: stderr: ""
Jul 13 10:09:01.700: INFO: stdout: "update-demo-nautilus-88lpb update-demo-nautilus-xlf87 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 13 10:09:06.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2812'
Jul 13 10:09:06.840: INFO: stderr: ""
Jul 13 10:09:06.840: INFO: stdout: "update-demo-nautilus-88lpb update-demo-nautilus-xlf87 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 13 10:09:11.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2812'
Jul 13 10:09:11.988: INFO: stderr: ""
Jul 13 10:09:11.988: INFO: stdout: "update-demo-nautilus-xlf87 "
Jul 13 10:09:11.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-xlf87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:12.119: INFO: stderr: ""
Jul 13 10:09:12.119: INFO: stdout: "true"
Jul 13 10:09:12.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-xlf87 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:12.244: INFO: stderr: ""
Jul 13 10:09:12.244: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 10:09:12.244: INFO: validating pod update-demo-nautilus-xlf87
Jul 13 10:09:12.256: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 10:09:12.256: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 10:09:12.256: INFO: update-demo-nautilus-xlf87 is verified up and running
STEP: scaling up the replication controller
Jul 13 10:09:12.263: INFO: scanned /root for discovery docs: <nil>
Jul 13 10:09:12.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2812'
Jul 13 10:09:13.477: INFO: stderr: ""
Jul 13 10:09:13.477: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 10:09:13.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2812'
Jul 13 10:09:13.616: INFO: stderr: ""
Jul 13 10:09:13.616: INFO: stdout: "update-demo-nautilus-9vqzk update-demo-nautilus-xlf87 "
Jul 13 10:09:13.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-9vqzk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:13.758: INFO: stderr: ""
Jul 13 10:09:13.758: INFO: stdout: ""
Jul 13 10:09:13.758: INFO: update-demo-nautilus-9vqzk is created but not running
Jul 13 10:09:18.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2812'
Jul 13 10:09:18.896: INFO: stderr: ""
Jul 13 10:09:18.897: INFO: stdout: "update-demo-nautilus-9vqzk update-demo-nautilus-xlf87 "
Jul 13 10:09:18.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-9vqzk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:19.028: INFO: stderr: ""
Jul 13 10:09:19.028: INFO: stdout: "true"
Jul 13 10:09:19.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-9vqzk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:19.175: INFO: stderr: ""
Jul 13 10:09:19.175: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 10:09:19.175: INFO: validating pod update-demo-nautilus-9vqzk
Jul 13 10:09:19.190: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 10:09:19.190: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 10:09:19.190: INFO: update-demo-nautilus-9vqzk is verified up and running
Jul 13 10:09:19.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-xlf87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:19.321: INFO: stderr: ""
Jul 13 10:09:19.321: INFO: stdout: "true"
Jul 13 10:09:19.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-xlf87 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2812'
Jul 13 10:09:19.457: INFO: stderr: ""
Jul 13 10:09:19.457: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 10:09:19.457: INFO: validating pod update-demo-nautilus-xlf87
Jul 13 10:09:19.469: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 10:09:19.469: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 10:09:19.469: INFO: update-demo-nautilus-xlf87 is verified up and running
STEP: using delete to clean up resources
Jul 13 10:09:19.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-2812'
Jul 13 10:09:19.657: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 10:09:19.657: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 13 10:09:19.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2812'
Jul 13 10:09:19.794: INFO: stderr: "No resources found.\n"
Jul 13 10:09:19.794: INFO: stdout: ""
Jul 13 10:09:19.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -l name=update-demo --namespace=kubectl-2812 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 10:09:19.927: INFO: stderr: ""
Jul 13 10:09:19.927: INFO: stdout: "update-demo-nautilus-9vqzk\nupdate-demo-nautilus-xlf87\n"
Jul 13 10:09:20.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2812'
Jul 13 10:09:20.573: INFO: stderr: "No resources found.\n"
Jul 13 10:09:20.573: INFO: stdout: ""
Jul 13 10:09:20.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -l name=update-demo --namespace=kubectl-2812 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 10:09:20.705: INFO: stderr: ""
Jul 13 10:09:20.705: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:09:20.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2812" for this suite.
Jul 13 10:09:26.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:09:27.106: INFO: namespace kubectl-2812 deletion completed in 6.387342095s

• [SLOW TEST:33.098 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:09:27.107: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-343a76ab-50a6-41c4-871e-29f32fc09e0a in namespace container-probe-3496
Jul 13 10:09:31.311: INFO: Started pod busybox-343a76ab-50a6-41c4-871e-29f32fc09e0a in namespace container-probe-3496
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 10:09:31.319: INFO: Initial restart count of pod busybox-343a76ab-50a6-41c4-871e-29f32fc09e0a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:13:32.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3496" for this suite.
Jul 13 10:13:38.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:13:39.096: INFO: namespace container-probe-3496 deletion completed in 6.344099326s

• [SLOW TEST:251.989 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:13:39.096: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jul 13 10:13:43.293: INFO: Pod pod-hostip-99b6efbb-4681-407e-bf27-7c2106903ca9 has hostIP: 51.68.114.204
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:13:43.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6453" for this suite.
Jul 13 10:14:07.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:14:07.650: INFO: namespace pods-6453 deletion completed in 24.347087552s

• [SLOW TEST:28.554 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:14:07.650: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:14:07.759: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 13 10:14:07.789: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 13 10:14:12.802: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 13 10:14:12.802: INFO: Creating deployment "test-rolling-update-deployment"
Jul 13 10:14:12.849: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 13 10:14:12.869: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 13 10:14:14.897: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 13 10:14:14.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698609652, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698609652, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63698609652, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63698609652, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 10:14:16.917: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 13 10:14:16.943: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2329,SelfLink:/apis/apps/v1/namespaces/deployment-2329/deployments/test-rolling-update-deployment,UID:3e1a8c1d-18b8-4893-9277-a0fe0a1b4e1f,ResourceVersion:223540927,Generation:1,CreationTimestamp:2019-07-13 10:14:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-13 10:14:12 +0000 UTC 2019-07-13 10:14:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-13 10:14:16 +0000 UTC 2019-07-13 10:14:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 13 10:14:16.952: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-2329,SelfLink:/apis/apps/v1/namespaces/deployment-2329/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:b691c6da-3a45-443a-996c-132e8793dc0a,ResourceVersion:223540916,Generation:1,CreationTimestamp:2019-07-13 10:14:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 3e1a8c1d-18b8-4893-9277-a0fe0a1b4e1f 0xc001853877 0xc001853878}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 13 10:14:16.952: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 13 10:14:16.952: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2329,SelfLink:/apis/apps/v1/namespaces/deployment-2329/replicasets/test-rolling-update-controller,UID:2462b2db-d647-4559-b971-312fb5745520,ResourceVersion:223540926,Generation:2,CreationTimestamp:2019-07-13 10:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 3e1a8c1d-18b8-4893-9277-a0fe0a1b4e1f 0xc0018537a7 0xc0018537a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 13 10:14:16.962: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-55md4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-55md4,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-2329,SelfLink:/api/v1/namespaces/deployment-2329/pods/test-rolling-update-deployment-79f6b9d75c-55md4,UID:0b3691b4-db6c-4300-a6d4-75aa73ae30d9,ResourceVersion:223540915,Generation:0,CreationTimestamp:2019-07-13 10:14:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.189/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c b691c6da-3a45-443a-996c-132e8793dc0a 0xc001ac01c7 0xc001ac01c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p2s8q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2s8q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-p2s8q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ac0240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ac0260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:14:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:14:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:14:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:14:12 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:10.2.1.189,StartTime:2019-07-13 10:14:12 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-13 10:14:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9a63b0d258374d184a9dfed259cf9c1234d28c55ff16cc76734887597392229a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:14:16.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2329" for this suite.
Jul 13 10:14:23.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:14:23.298: INFO: namespace deployment-2329 deletion completed in 6.327122129s

• [SLOW TEST:15.648 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:14:23.303: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 13 10:14:23.442: INFO: Waiting up to 5m0s for pod "downward-api-e219ecc4-67b0-4758-b174-2a5bf60e4cc8" in namespace "downward-api-4436" to be "success or failure"
Jul 13 10:14:23.456: INFO: Pod "downward-api-e219ecc4-67b0-4758-b174-2a5bf60e4cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.59476ms
Jul 13 10:14:25.467: INFO: Pod "downward-api-e219ecc4-67b0-4758-b174-2a5bf60e4cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024630793s
Jul 13 10:14:27.477: INFO: Pod "downward-api-e219ecc4-67b0-4758-b174-2a5bf60e4cc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03514846s
STEP: Saw pod success
Jul 13 10:14:27.478: INFO: Pod "downward-api-e219ecc4-67b0-4758-b174-2a5bf60e4cc8" satisfied condition "success or failure"
Jul 13 10:14:27.486: INFO: Trying to get logs from node cncf-1-15 pod downward-api-e219ecc4-67b0-4758-b174-2a5bf60e4cc8 container dapi-container: <nil>
STEP: delete the pod
Jul 13 10:14:27.543: INFO: Waiting for pod downward-api-e219ecc4-67b0-4758-b174-2a5bf60e4cc8 to disappear
Jul 13 10:14:27.556: INFO: Pod downward-api-e219ecc4-67b0-4758-b174-2a5bf60e4cc8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:14:27.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4436" for this suite.
Jul 13 10:14:33.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:14:34.014: INFO: namespace downward-api-4436 deletion completed in 6.449157141s

• [SLOW TEST:10.711 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:14:34.014: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jul 13 10:15:04.758: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:15:04.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0713 10:15:04.758928      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8015" for this suite.
Jul 13 10:15:10.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:15:11.153: INFO: namespace gc-8015 deletion completed in 6.383971091s

• [SLOW TEST:37.139 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:15:11.154: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:15:38.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2590" for this suite.
Jul 13 10:15:44.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:15:44.979: INFO: namespace namespaces-2590 deletion completed in 6.383196986s
STEP: Destroying namespace "nsdeletetest-552" for this suite.
Jul 13 10:15:44.988: INFO: Namespace nsdeletetest-552 was already deleted
STEP: Destroying namespace "nsdeletetest-4666" for this suite.
Jul 13 10:15:51.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:15:51.323: INFO: namespace nsdeletetest-4666 deletion completed in 6.334558583s

• [SLOW TEST:40.169 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:15:51.325: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jul 13 10:15:51.457: INFO: Waiting up to 5m0s for pod "var-expansion-6484cf4a-86ef-4480-8376-38342638ff8d" in namespace "var-expansion-9220" to be "success or failure"
Jul 13 10:15:51.468: INFO: Pod "var-expansion-6484cf4a-86ef-4480-8376-38342638ff8d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.850871ms
Jul 13 10:15:53.478: INFO: Pod "var-expansion-6484cf4a-86ef-4480-8376-38342638ff8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020515741s
Jul 13 10:15:55.494: INFO: Pod "var-expansion-6484cf4a-86ef-4480-8376-38342638ff8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036656843s
STEP: Saw pod success
Jul 13 10:15:55.494: INFO: Pod "var-expansion-6484cf4a-86ef-4480-8376-38342638ff8d" satisfied condition "success or failure"
Jul 13 10:15:55.505: INFO: Trying to get logs from node cncf-1-15 pod var-expansion-6484cf4a-86ef-4480-8376-38342638ff8d container dapi-container: <nil>
STEP: delete the pod
Jul 13 10:15:55.575: INFO: Waiting for pod var-expansion-6484cf4a-86ef-4480-8376-38342638ff8d to disappear
Jul 13 10:15:55.583: INFO: Pod var-expansion-6484cf4a-86ef-4480-8376-38342638ff8d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:15:55.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9220" for this suite.
Jul 13 10:16:01.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:16:01.938: INFO: namespace var-expansion-9220 deletion completed in 6.346001207s

• [SLOW TEST:10.614 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:16:01.938: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-86117dbe-9c2a-433e-a1b8-4a7525219f0c
STEP: Creating a pod to test consume configMaps
Jul 13 10:16:02.097: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a90603a-88b6-4d56-9b29-fc443685491a" in namespace "projected-1117" to be "success or failure"
Jul 13 10:16:02.109: INFO: Pod "pod-projected-configmaps-4a90603a-88b6-4d56-9b29-fc443685491a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.686108ms
Jul 13 10:16:04.120: INFO: Pod "pod-projected-configmaps-4a90603a-88b6-4d56-9b29-fc443685491a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022429954s
Jul 13 10:16:06.130: INFO: Pod "pod-projected-configmaps-4a90603a-88b6-4d56-9b29-fc443685491a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032546735s
STEP: Saw pod success
Jul 13 10:16:06.130: INFO: Pod "pod-projected-configmaps-4a90603a-88b6-4d56-9b29-fc443685491a" satisfied condition "success or failure"
Jul 13 10:16:06.138: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-configmaps-4a90603a-88b6-4d56-9b29-fc443685491a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 10:16:06.216: INFO: Waiting for pod pod-projected-configmaps-4a90603a-88b6-4d56-9b29-fc443685491a to disappear
Jul 13 10:16:06.228: INFO: Pod pod-projected-configmaps-4a90603a-88b6-4d56-9b29-fc443685491a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:16:06.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1117" for this suite.
Jul 13 10:16:12.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:16:12.606: INFO: namespace projected-1117 deletion completed in 6.369381147s

• [SLOW TEST:10.668 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:16:12.609: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 10:16:12.756: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1603ddf-c77b-48bd-a9be-8d203eb373b4" in namespace "downward-api-480" to be "success or failure"
Jul 13 10:16:12.769: INFO: Pod "downwardapi-volume-d1603ddf-c77b-48bd-a9be-8d203eb373b4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.633926ms
Jul 13 10:16:14.780: INFO: Pod "downwardapi-volume-d1603ddf-c77b-48bd-a9be-8d203eb373b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023534901s
Jul 13 10:16:16.791: INFO: Pod "downwardapi-volume-d1603ddf-c77b-48bd-a9be-8d203eb373b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034132746s
STEP: Saw pod success
Jul 13 10:16:16.791: INFO: Pod "downwardapi-volume-d1603ddf-c77b-48bd-a9be-8d203eb373b4" satisfied condition "success or failure"
Jul 13 10:16:16.801: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-d1603ddf-c77b-48bd-a9be-8d203eb373b4 container client-container: <nil>
STEP: delete the pod
Jul 13 10:16:16.856: INFO: Waiting for pod downwardapi-volume-d1603ddf-c77b-48bd-a9be-8d203eb373b4 to disappear
Jul 13 10:16:16.864: INFO: Pod downwardapi-volume-d1603ddf-c77b-48bd-a9be-8d203eb373b4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:16:16.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-480" for this suite.
Jul 13 10:16:22.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:16:23.222: INFO: namespace downward-api-480 deletion completed in 6.346860977s

• [SLOW TEST:10.613 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:16:23.223: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 10:16:23.360: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f38309cf-c19b-4dab-b57c-6dd576ce7774" in namespace "downward-api-9029" to be "success or failure"
Jul 13 10:16:23.377: INFO: Pod "downwardapi-volume-f38309cf-c19b-4dab-b57c-6dd576ce7774": Phase="Pending", Reason="", readiness=false. Elapsed: 16.870663ms
Jul 13 10:16:25.388: INFO: Pod "downwardapi-volume-f38309cf-c19b-4dab-b57c-6dd576ce7774": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028347236s
Jul 13 10:16:27.409: INFO: Pod "downwardapi-volume-f38309cf-c19b-4dab-b57c-6dd576ce7774": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049171506s
STEP: Saw pod success
Jul 13 10:16:27.409: INFO: Pod "downwardapi-volume-f38309cf-c19b-4dab-b57c-6dd576ce7774" satisfied condition "success or failure"
Jul 13 10:16:27.419: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-f38309cf-c19b-4dab-b57c-6dd576ce7774 container client-container: <nil>
STEP: delete the pod
Jul 13 10:16:27.513: INFO: Waiting for pod downwardapi-volume-f38309cf-c19b-4dab-b57c-6dd576ce7774 to disappear
Jul 13 10:16:27.521: INFO: Pod downwardapi-volume-f38309cf-c19b-4dab-b57c-6dd576ce7774 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:16:27.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9029" for this suite.
Jul 13 10:16:33.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:16:33.951: INFO: namespace downward-api-9029 deletion completed in 6.420104463s

• [SLOW TEST:10.728 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:16:33.957: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3301.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3301.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3301.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3301.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 10:16:40.178: INFO: DNS probes using dns-test-87d88470-3022-44b8-90a3-5e0b2230f192 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3301.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3301.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3301.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3301.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 10:16:46.371: INFO: DNS probes using dns-test-4516beac-18a5-4963-b03b-8fe8cdb1c137 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3301.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3301.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3301.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3301.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 10:16:52.604: INFO: DNS probes using dns-test-0ec993f6-2896-4e9a-84b2-3011bc5db7a0 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:16:52.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3301" for this suite.
Jul 13 10:17:00.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:17:01.013: INFO: namespace dns-3301 deletion completed in 8.315253768s

• [SLOW TEST:27.056 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:17:01.014: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-82bb9010-01d4-4f9f-b49e-1cc72eb67036
STEP: Creating a pod to test consume secrets
Jul 13 10:17:01.187: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6f5cff57-8833-45d9-9dc3-e0138b5ffd6f" in namespace "projected-1100" to be "success or failure"
Jul 13 10:17:01.198: INFO: Pod "pod-projected-secrets-6f5cff57-8833-45d9-9dc3-e0138b5ffd6f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.157379ms
Jul 13 10:17:03.214: INFO: Pod "pod-projected-secrets-6f5cff57-8833-45d9-9dc3-e0138b5ffd6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026474037s
Jul 13 10:17:05.229: INFO: Pod "pod-projected-secrets-6f5cff57-8833-45d9-9dc3-e0138b5ffd6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041983511s
STEP: Saw pod success
Jul 13 10:17:05.229: INFO: Pod "pod-projected-secrets-6f5cff57-8833-45d9-9dc3-e0138b5ffd6f" satisfied condition "success or failure"
Jul 13 10:17:05.243: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-secrets-6f5cff57-8833-45d9-9dc3-e0138b5ffd6f container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 10:17:05.304: INFO: Waiting for pod pod-projected-secrets-6f5cff57-8833-45d9-9dc3-e0138b5ffd6f to disappear
Jul 13 10:17:05.311: INFO: Pod pod-projected-secrets-6f5cff57-8833-45d9-9dc3-e0138b5ffd6f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:17:05.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1100" for this suite.
Jul 13 10:17:11.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:17:11.689: INFO: namespace projected-1100 deletion completed in 6.368453598s

• [SLOW TEST:10.675 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:17:11.696: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jul 13 10:17:11.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 --namespace=kubectl-5720 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 13 10:17:15.251: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 13 10:17:15.251: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:17:17.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5720" for this suite.
Jul 13 10:17:29.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:17:29.689: INFO: namespace kubectl-5720 deletion completed in 12.411820431s

• [SLOW TEST:17.994 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:17:29.691: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:17:29.852: INFO: (0) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.265478ms)
Jul 13 10:17:29.867: INFO: (1) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 14.583234ms)
Jul 13 10:17:29.881: INFO: (2) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.560175ms)
Jul 13 10:17:29.895: INFO: (3) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 14.118439ms)
Jul 13 10:17:29.908: INFO: (4) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.157603ms)
Jul 13 10:17:29.922: INFO: (5) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 14.040298ms)
Jul 13 10:17:29.937: INFO: (6) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 14.516875ms)
Jul 13 10:17:29.950: INFO: (7) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.782447ms)
Jul 13 10:17:29.964: INFO: (8) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.720524ms)
Jul 13 10:17:29.978: INFO: (9) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 14.458239ms)
Jul 13 10:17:29.992: INFO: (10) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.864589ms)
Jul 13 10:17:30.006: INFO: (11) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.400054ms)
Jul 13 10:17:30.019: INFO: (12) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.47605ms)
Jul 13 10:17:30.031: INFO: (13) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.471664ms)
Jul 13 10:17:30.045: INFO: (14) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.540395ms)
Jul 13 10:17:30.059: INFO: (15) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.712495ms)
Jul 13 10:17:30.072: INFO: (16) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.85749ms)
Jul 13 10:17:30.085: INFO: (17) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.814382ms)
Jul 13 10:17:30.098: INFO: (18) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 12.42759ms)
Jul 13 10:17:30.112: INFO: (19) /api/v1/nodes/cncf-1-15/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.548799ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:17:30.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4760" for this suite.
Jul 13 10:17:36.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:17:36.505: INFO: namespace proxy-4760 deletion completed in 6.383054258s

• [SLOW TEST:6.814 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:17:36.509: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:17:36.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4264" for this suite.
Jul 13 10:17:42.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:17:43.009: INFO: namespace services-4264 deletion completed in 6.328310331s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.500 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:17:43.010: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jul 13 10:17:43.136: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-018000938 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:17:43.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6109" for this suite.
Jul 13 10:17:49.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:17:49.626: INFO: namespace kubectl-6109 deletion completed in 6.370878116s

• [SLOW TEST:6.616 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:17:49.628: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:17:54.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6623" for this suite.
Jul 13 10:18:18.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:18:19.292: INFO: namespace replication-controller-6623 deletion completed in 24.401666661s

• [SLOW TEST:29.665 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:18:19.297: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:18:19.508: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 13 10:18:19.535: INFO: Number of nodes with available pods: 0
Jul 13 10:18:19.535: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 13 10:18:19.589: INFO: Number of nodes with available pods: 0
Jul 13 10:18:19.589: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:20.613: INFO: Number of nodes with available pods: 0
Jul 13 10:18:20.613: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:21.600: INFO: Number of nodes with available pods: 0
Jul 13 10:18:21.600: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:22.599: INFO: Number of nodes with available pods: 0
Jul 13 10:18:22.599: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:23.601: INFO: Number of nodes with available pods: 1
Jul 13 10:18:23.601: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 13 10:18:23.703: INFO: Number of nodes with available pods: 1
Jul 13 10:18:23.703: INFO: Number of running nodes: 0, number of available pods: 1
Jul 13 10:18:24.711: INFO: Number of nodes with available pods: 0
Jul 13 10:18:24.711: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 13 10:18:24.741: INFO: Number of nodes with available pods: 0
Jul 13 10:18:24.741: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:25.760: INFO: Number of nodes with available pods: 0
Jul 13 10:18:25.760: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:26.749: INFO: Number of nodes with available pods: 0
Jul 13 10:18:26.749: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:27.751: INFO: Number of nodes with available pods: 0
Jul 13 10:18:27.751: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:28.748: INFO: Number of nodes with available pods: 0
Jul 13 10:18:28.748: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:29.752: INFO: Number of nodes with available pods: 0
Jul 13 10:18:29.752: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:30.753: INFO: Number of nodes with available pods: 0
Jul 13 10:18:30.754: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:31.751: INFO: Number of nodes with available pods: 0
Jul 13 10:18:31.751: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:32.752: INFO: Number of nodes with available pods: 0
Jul 13 10:18:32.752: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:33.752: INFO: Number of nodes with available pods: 0
Jul 13 10:18:33.752: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:34.781: INFO: Number of nodes with available pods: 0
Jul 13 10:18:34.782: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:35.756: INFO: Number of nodes with available pods: 0
Jul 13 10:18:35.756: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:36.751: INFO: Number of nodes with available pods: 0
Jul 13 10:18:36.751: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:37.750: INFO: Number of nodes with available pods: 0
Jul 13 10:18:37.750: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:38.751: INFO: Number of nodes with available pods: 0
Jul 13 10:18:38.751: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:39.751: INFO: Number of nodes with available pods: 0
Jul 13 10:18:39.751: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:40.753: INFO: Number of nodes with available pods: 0
Jul 13 10:18:40.753: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:18:41.751: INFO: Number of nodes with available pods: 1
Jul 13 10:18:41.751: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6803, will wait for the garbage collector to delete the pods
Jul 13 10:18:41.869: INFO: Deleting DaemonSet.extensions daemon-set took: 43.250167ms
Jul 13 10:18:42.270: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.482963ms
Jul 13 10:18:47.780: INFO: Number of nodes with available pods: 0
Jul 13 10:18:47.780: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 10:18:47.788: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6803/daemonsets","resourceVersion":"223543478"},"items":null}

Jul 13 10:18:47.797: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6803/pods","resourceVersion":"223543478"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:18:47.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6803" for this suite.
Jul 13 10:18:53.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:18:54.224: INFO: namespace daemonsets-6803 deletion completed in 6.366431336s

• [SLOW TEST:34.927 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:18:54.224: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-d5349a9a-7bba-4e37-b0ff-48345c82fc40
STEP: Creating a pod to test consume configMaps
Jul 13 10:18:54.441: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ddabe637-26e8-4d99-8d7b-dfd9152b5442" in namespace "projected-4905" to be "success or failure"
Jul 13 10:18:54.451: INFO: Pod "pod-projected-configmaps-ddabe637-26e8-4d99-8d7b-dfd9152b5442": Phase="Pending", Reason="", readiness=false. Elapsed: 10.267234ms
Jul 13 10:18:56.460: INFO: Pod "pod-projected-configmaps-ddabe637-26e8-4d99-8d7b-dfd9152b5442": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01931962s
Jul 13 10:18:58.471: INFO: Pod "pod-projected-configmaps-ddabe637-26e8-4d99-8d7b-dfd9152b5442": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030246738s
STEP: Saw pod success
Jul 13 10:18:58.471: INFO: Pod "pod-projected-configmaps-ddabe637-26e8-4d99-8d7b-dfd9152b5442" satisfied condition "success or failure"
Jul 13 10:18:58.479: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-configmaps-ddabe637-26e8-4d99-8d7b-dfd9152b5442 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 10:18:58.538: INFO: Waiting for pod pod-projected-configmaps-ddabe637-26e8-4d99-8d7b-dfd9152b5442 to disappear
Jul 13 10:18:58.545: INFO: Pod pod-projected-configmaps-ddabe637-26e8-4d99-8d7b-dfd9152b5442 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:18:58.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4905" for this suite.
Jul 13 10:19:04.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:19:04.903: INFO: namespace projected-4905 deletion completed in 6.347527782s

• [SLOW TEST:10.679 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:19:04.903: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-878
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 13 10:19:05.055: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 13 10:19:29.268: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.1.206:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-878 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:19:29.268: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:19:29.490: INFO: Found all expected endpoints: [netserver-0]
Jul 13 10:19:29.498: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.0.41:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-878 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:19:29.498: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:19:29.717: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:19:29.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-878" for this suite.
Jul 13 10:19:53.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:19:54.132: INFO: namespace pod-network-test-878 deletion completed in 24.403277204s

• [SLOW TEST:49.229 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:19:54.136: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:19:54.302: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:19:58.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6872" for this suite.
Jul 13 10:20:38.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:20:38.996: INFO: namespace pods-6872 deletion completed in 40.380482338s

• [SLOW TEST:44.860 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:20:38.997: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-6dwn
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 10:20:39.177: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6dwn" in namespace "subpath-8441" to be "success or failure"
Jul 13 10:20:39.188: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.670691ms
Jul 13 10:20:41.198: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020722355s
Jul 13 10:20:43.240: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 4.062702372s
Jul 13 10:20:45.250: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 6.073101709s
Jul 13 10:20:47.279: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 8.101900053s
Jul 13 10:20:49.289: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 10.111704654s
Jul 13 10:20:51.300: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 12.122607645s
Jul 13 10:20:53.310: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 14.132808029s
Jul 13 10:20:55.321: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 16.143678018s
Jul 13 10:20:57.331: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 18.153828437s
Jul 13 10:20:59.341: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 20.163876111s
Jul 13 10:21:01.352: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Running", Reason="", readiness=true. Elapsed: 22.174915442s
Jul 13 10:21:03.362: INFO: Pod "pod-subpath-test-secret-6dwn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.185039936s
STEP: Saw pod success
Jul 13 10:21:03.362: INFO: Pod "pod-subpath-test-secret-6dwn" satisfied condition "success or failure"
Jul 13 10:21:03.370: INFO: Trying to get logs from node cncf-1-15 pod pod-subpath-test-secret-6dwn container test-container-subpath-secret-6dwn: <nil>
STEP: delete the pod
Jul 13 10:21:03.450: INFO: Waiting for pod pod-subpath-test-secret-6dwn to disappear
Jul 13 10:21:03.458: INFO: Pod pod-subpath-test-secret-6dwn no longer exists
STEP: Deleting pod pod-subpath-test-secret-6dwn
Jul 13 10:21:03.458: INFO: Deleting pod "pod-subpath-test-secret-6dwn" in namespace "subpath-8441"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:21:03.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8441" for this suite.
Jul 13 10:21:09.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:21:09.852: INFO: namespace subpath-8441 deletion completed in 6.375978057s

• [SLOW TEST:30.855 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:21:09.854: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-db856950-3aa7-4ace-8cde-5797fadb5fb7
STEP: Creating a pod to test consume configMaps
Jul 13 10:21:10.014: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-123e4550-d7d2-4d6a-9da6-752fcb2175a4" in namespace "projected-3901" to be "success or failure"
Jul 13 10:21:10.025: INFO: Pod "pod-projected-configmaps-123e4550-d7d2-4d6a-9da6-752fcb2175a4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.809389ms
Jul 13 10:21:12.035: INFO: Pod "pod-projected-configmaps-123e4550-d7d2-4d6a-9da6-752fcb2175a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021041959s
Jul 13 10:21:14.045: INFO: Pod "pod-projected-configmaps-123e4550-d7d2-4d6a-9da6-752fcb2175a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030798539s
STEP: Saw pod success
Jul 13 10:21:14.045: INFO: Pod "pod-projected-configmaps-123e4550-d7d2-4d6a-9da6-752fcb2175a4" satisfied condition "success or failure"
Jul 13 10:21:14.054: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-configmaps-123e4550-d7d2-4d6a-9da6-752fcb2175a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 10:21:14.104: INFO: Waiting for pod pod-projected-configmaps-123e4550-d7d2-4d6a-9da6-752fcb2175a4 to disappear
Jul 13 10:21:14.118: INFO: Pod pod-projected-configmaps-123e4550-d7d2-4d6a-9da6-752fcb2175a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:21:14.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3901" for this suite.
Jul 13 10:21:20.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:21:20.487: INFO: namespace projected-3901 deletion completed in 6.359171673s

• [SLOW TEST:10.634 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:21:20.488: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 13 10:21:25.223: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8304 pod-service-account-b3467e67-866b-496f-a072-77e00823a3dd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 13 10:21:25.680: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8304 pod-service-account-b3467e67-866b-496f-a072-77e00823a3dd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 13 10:21:26.045: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8304 pod-service-account-b3467e67-866b-496f-a072-77e00823a3dd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:21:26.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8304" for this suite.
Jul 13 10:21:32.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:21:32.803: INFO: namespace svcaccounts-8304 deletion completed in 6.368008358s

• [SLOW TEST:12.315 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:21:32.804: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jul 13 10:21:32.964: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 13 10:21:32.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-3750'
Jul 13 10:21:33.246: INFO: stderr: ""
Jul 13 10:21:33.246: INFO: stdout: "service/redis-slave created\n"
Jul 13 10:21:33.246: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 13 10:21:33.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-3750'
Jul 13 10:21:33.476: INFO: stderr: ""
Jul 13 10:21:33.476: INFO: stdout: "service/redis-master created\n"
Jul 13 10:21:33.477: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 13 10:21:33.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-3750'
Jul 13 10:21:33.724: INFO: stderr: ""
Jul 13 10:21:33.724: INFO: stdout: "service/frontend created\n"
Jul 13 10:21:33.724: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 13 10:21:33.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-3750'
Jul 13 10:21:33.974: INFO: stderr: ""
Jul 13 10:21:33.974: INFO: stdout: "deployment.apps/frontend created\n"
Jul 13 10:21:33.975: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 13 10:21:33.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-3750'
Jul 13 10:21:34.180: INFO: stderr: ""
Jul 13 10:21:34.180: INFO: stdout: "deployment.apps/redis-master created\n"
Jul 13 10:21:34.180: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 13 10:21:34.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-3750'
Jul 13 10:21:34.396: INFO: stderr: ""
Jul 13 10:21:34.396: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul 13 10:21:34.396: INFO: Waiting for all frontend pods to be Running.
Jul 13 10:21:54.447: INFO: Waiting for frontend to serve content.
Jul 13 10:21:59.482: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul 13 10:22:04.516: INFO: Trying to add a new entry to the guestbook.
Jul 13 10:22:04.554: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 13 10:22:04.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-3750'
Jul 13 10:22:04.763: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 10:22:04.763: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 10:22:04.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-3750'
Jul 13 10:22:04.932: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 10:22:04.932: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 10:22:04.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-3750'
Jul 13 10:22:05.104: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 10:22:05.104: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 10:22:05.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-3750'
Jul 13 10:22:05.257: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 10:22:05.257: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 10:22:05.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-3750'
Jul 13 10:22:05.422: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 10:22:05.422: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 10:22:05.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-3750'
Jul 13 10:22:05.587: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 10:22:05.587: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:22:05.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3750" for this suite.
Jul 13 10:22:49.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:22:49.979: INFO: namespace kubectl-3750 deletion completed in 44.382455262s

• [SLOW TEST:77.175 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:22:49.981: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-8acdb354-e8c7-400d-aac0-4976e97d5537
STEP: Creating a pod to test consume secrets
Jul 13 10:22:50.141: INFO: Waiting up to 5m0s for pod "pod-secrets-edeec336-22ff-44be-845e-2a3b18575813" in namespace "secrets-5961" to be "success or failure"
Jul 13 10:22:50.164: INFO: Pod "pod-secrets-edeec336-22ff-44be-845e-2a3b18575813": Phase="Pending", Reason="", readiness=false. Elapsed: 23.007268ms
Jul 13 10:22:52.176: INFO: Pod "pod-secrets-edeec336-22ff-44be-845e-2a3b18575813": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034716512s
Jul 13 10:22:54.186: INFO: Pod "pod-secrets-edeec336-22ff-44be-845e-2a3b18575813": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044681341s
STEP: Saw pod success
Jul 13 10:22:54.186: INFO: Pod "pod-secrets-edeec336-22ff-44be-845e-2a3b18575813" satisfied condition "success or failure"
Jul 13 10:22:54.193: INFO: Trying to get logs from node cncf-1-15 pod pod-secrets-edeec336-22ff-44be-845e-2a3b18575813 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 10:22:54.287: INFO: Waiting for pod pod-secrets-edeec336-22ff-44be-845e-2a3b18575813 to disappear
Jul 13 10:22:54.297: INFO: Pod pod-secrets-edeec336-22ff-44be-845e-2a3b18575813 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:22:54.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5961" for this suite.
Jul 13 10:23:00.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:23:00.653: INFO: namespace secrets-5961 deletion completed in 6.347337601s

• [SLOW TEST:10.672 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:23:00.654: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-64b44814-b672-4426-8867-1b40c43f3b44
STEP: Creating a pod to test consume secrets
Jul 13 10:23:00.843: INFO: Waiting up to 5m0s for pod "pod-secrets-29e02afc-3d80-4b37-ada4-a97fdae4b073" in namespace "secrets-1732" to be "success or failure"
Jul 13 10:23:00.850: INFO: Pod "pod-secrets-29e02afc-3d80-4b37-ada4-a97fdae4b073": Phase="Pending", Reason="", readiness=false. Elapsed: 6.986887ms
Jul 13 10:23:02.860: INFO: Pod "pod-secrets-29e02afc-3d80-4b37-ada4-a97fdae4b073": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017350076s
Jul 13 10:23:04.871: INFO: Pod "pod-secrets-29e02afc-3d80-4b37-ada4-a97fdae4b073": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028082447s
STEP: Saw pod success
Jul 13 10:23:04.871: INFO: Pod "pod-secrets-29e02afc-3d80-4b37-ada4-a97fdae4b073" satisfied condition "success or failure"
Jul 13 10:23:04.879: INFO: Trying to get logs from node cncf-1-15 pod pod-secrets-29e02afc-3d80-4b37-ada4-a97fdae4b073 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 10:23:04.936: INFO: Waiting for pod pod-secrets-29e02afc-3d80-4b37-ada4-a97fdae4b073 to disappear
Jul 13 10:23:04.943: INFO: Pod pod-secrets-29e02afc-3d80-4b37-ada4-a97fdae4b073 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:23:04.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1732" for this suite.
Jul 13 10:23:10.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:23:11.311: INFO: namespace secrets-1732 deletion completed in 6.358452101s

• [SLOW TEST:10.657 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:23:11.313: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:23:44.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9773" for this suite.
Jul 13 10:23:50.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:23:50.655: INFO: namespace container-runtime-9773 deletion completed in 6.37256161s

• [SLOW TEST:39.343 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:23:50.657: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 13 10:23:55.440: INFO: Successfully updated pod "pod-update-e96638bf-fe96-442e-854b-63acc4fa7f95"
STEP: verifying the updated pod is in kubernetes
Jul 13 10:23:55.464: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:23:55.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-106" for this suite.
Jul 13 10:24:11.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:24:11.827: INFO: namespace pods-106 deletion completed in 16.353385405s

• [SLOW TEST:21.170 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:24:11.828: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-57f1906f-653c-4758-9cd1-b2c7ae3f8e7e
STEP: Creating a pod to test consume configMaps
Jul 13 10:24:12.012: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e033482a-805b-431f-8f5f-145fb00487dd" in namespace "projected-8502" to be "success or failure"
Jul 13 10:24:12.035: INFO: Pod "pod-projected-configmaps-e033482a-805b-431f-8f5f-145fb00487dd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.858592ms
Jul 13 10:24:14.045: INFO: Pod "pod-projected-configmaps-e033482a-805b-431f-8f5f-145fb00487dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032622828s
Jul 13 10:24:16.056: INFO: Pod "pod-projected-configmaps-e033482a-805b-431f-8f5f-145fb00487dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043627246s
STEP: Saw pod success
Jul 13 10:24:16.056: INFO: Pod "pod-projected-configmaps-e033482a-805b-431f-8f5f-145fb00487dd" satisfied condition "success or failure"
Jul 13 10:24:16.066: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-configmaps-e033482a-805b-431f-8f5f-145fb00487dd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 10:24:16.158: INFO: Waiting for pod pod-projected-configmaps-e033482a-805b-431f-8f5f-145fb00487dd to disappear
Jul 13 10:24:16.166: INFO: Pod pod-projected-configmaps-e033482a-805b-431f-8f5f-145fb00487dd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:24:16.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8502" for this suite.
Jul 13 10:24:22.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:24:22.576: INFO: namespace projected-8502 deletion completed in 6.400993707s

• [SLOW TEST:10.747 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:24:22.576: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 13 10:24:27.343: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b8d0fb40-5f6c-49ab-8158-7216d422d346"
Jul 13 10:24:27.343: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b8d0fb40-5f6c-49ab-8158-7216d422d346" in namespace "pods-1513" to be "terminated due to deadline exceeded"
Jul 13 10:24:27.355: INFO: Pod "pod-update-activedeadlineseconds-b8d0fb40-5f6c-49ab-8158-7216d422d346": Phase="Running", Reason="", readiness=true. Elapsed: 11.151858ms
Jul 13 10:24:29.367: INFO: Pod "pod-update-activedeadlineseconds-b8d0fb40-5f6c-49ab-8158-7216d422d346": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.023939128s
Jul 13 10:24:29.368: INFO: Pod "pod-update-activedeadlineseconds-b8d0fb40-5f6c-49ab-8158-7216d422d346" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:24:29.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1513" for this suite.
Jul 13 10:24:35.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:24:35.830: INFO: namespace pods-1513 deletion completed in 6.451409627s

• [SLOW TEST:13.254 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:24:35.831: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 13 10:24:35.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6492'
Jul 13 10:24:36.088: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 10:24:36.088: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 13 10:24:36.117: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-7prwp]
Jul 13 10:24:36.117: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-7prwp" in namespace "kubectl-6492" to be "running and ready"
Jul 13 10:24:36.125: INFO: Pod "e2e-test-nginx-rc-7prwp": Phase="Pending", Reason="", readiness=false. Elapsed: 7.465697ms
Jul 13 10:24:38.138: INFO: Pod "e2e-test-nginx-rc-7prwp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021092651s
Jul 13 10:24:40.149: INFO: Pod "e2e-test-nginx-rc-7prwp": Phase="Running", Reason="", readiness=true. Elapsed: 4.031201572s
Jul 13 10:24:40.149: INFO: Pod "e2e-test-nginx-rc-7prwp" satisfied condition "running and ready"
Jul 13 10:24:40.149: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-7prwp]
Jul 13 10:24:40.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 logs rc/e2e-test-nginx-rc --namespace=kubectl-6492'
Jul 13 10:24:40.343: INFO: stderr: ""
Jul 13 10:24:40.343: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jul 13 10:24:40.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete rc e2e-test-nginx-rc --namespace=kubectl-6492'
Jul 13 10:24:40.528: INFO: stderr: ""
Jul 13 10:24:40.528: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:24:40.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6492" for this suite.
Jul 13 10:25:04.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:25:06.913: INFO: namespace kubectl-6492 deletion completed in 26.373840149s

• [SLOW TEST:31.082 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:25:06.915: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 13 10:25:07.068: INFO: Waiting up to 5m0s for pod "downward-api-7b56ef43-ca5f-4391-8bb6-003d48499d24" in namespace "downward-api-2690" to be "success or failure"
Jul 13 10:25:07.076: INFO: Pod "downward-api-7b56ef43-ca5f-4391-8bb6-003d48499d24": Phase="Pending", Reason="", readiness=false. Elapsed: 8.347872ms
Jul 13 10:25:09.086: INFO: Pod "downward-api-7b56ef43-ca5f-4391-8bb6-003d48499d24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018381579s
Jul 13 10:25:11.097: INFO: Pod "downward-api-7b56ef43-ca5f-4391-8bb6-003d48499d24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029597068s
STEP: Saw pod success
Jul 13 10:25:11.097: INFO: Pod "downward-api-7b56ef43-ca5f-4391-8bb6-003d48499d24" satisfied condition "success or failure"
Jul 13 10:25:11.107: INFO: Trying to get logs from node cncf-1-15 pod downward-api-7b56ef43-ca5f-4391-8bb6-003d48499d24 container dapi-container: <nil>
STEP: delete the pod
Jul 13 10:25:11.168: INFO: Waiting for pod downward-api-7b56ef43-ca5f-4391-8bb6-003d48499d24 to disappear
Jul 13 10:25:11.176: INFO: Pod downward-api-7b56ef43-ca5f-4391-8bb6-003d48499d24 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:25:11.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2690" for this suite.
Jul 13 10:25:17.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:25:17.537: INFO: namespace downward-api-2690 deletion completed in 6.350926678s

• [SLOW TEST:10.622 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:25:17.540: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 13 10:25:17.702: INFO: Waiting up to 5m0s for pod "pod-81895142-3e29-47b6-ad35-3f910fdb4901" in namespace "emptydir-3919" to be "success or failure"
Jul 13 10:25:17.712: INFO: Pod "pod-81895142-3e29-47b6-ad35-3f910fdb4901": Phase="Pending", Reason="", readiness=false. Elapsed: 10.306821ms
Jul 13 10:25:19.724: INFO: Pod "pod-81895142-3e29-47b6-ad35-3f910fdb4901": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022368186s
Jul 13 10:25:21.734: INFO: Pod "pod-81895142-3e29-47b6-ad35-3f910fdb4901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031903171s
STEP: Saw pod success
Jul 13 10:25:21.734: INFO: Pod "pod-81895142-3e29-47b6-ad35-3f910fdb4901" satisfied condition "success or failure"
Jul 13 10:25:21.743: INFO: Trying to get logs from node cncf-1-15 pod pod-81895142-3e29-47b6-ad35-3f910fdb4901 container test-container: <nil>
STEP: delete the pod
Jul 13 10:25:21.834: INFO: Waiting for pod pod-81895142-3e29-47b6-ad35-3f910fdb4901 to disappear
Jul 13 10:25:21.841: INFO: Pod pod-81895142-3e29-47b6-ad35-3f910fdb4901 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:25:21.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3919" for this suite.
Jul 13 10:25:27.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:25:28.217: INFO: namespace emptydir-3919 deletion completed in 6.366641798s

• [SLOW TEST:10.677 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:25:28.217: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 10:25:28.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3eb99319-7ba5-4a4a-a0d7-012e6774e330" in namespace "downward-api-6642" to be "success or failure"
Jul 13 10:25:28.372: INFO: Pod "downwardapi-volume-3eb99319-7ba5-4a4a-a0d7-012e6774e330": Phase="Pending", Reason="", readiness=false. Elapsed: 10.229467ms
Jul 13 10:25:30.383: INFO: Pod "downwardapi-volume-3eb99319-7ba5-4a4a-a0d7-012e6774e330": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020881373s
Jul 13 10:25:32.394: INFO: Pod "downwardapi-volume-3eb99319-7ba5-4a4a-a0d7-012e6774e330": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032319739s
STEP: Saw pod success
Jul 13 10:25:32.394: INFO: Pod "downwardapi-volume-3eb99319-7ba5-4a4a-a0d7-012e6774e330" satisfied condition "success or failure"
Jul 13 10:25:32.405: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-3eb99319-7ba5-4a4a-a0d7-012e6774e330 container client-container: <nil>
STEP: delete the pod
Jul 13 10:25:32.487: INFO: Waiting for pod downwardapi-volume-3eb99319-7ba5-4a4a-a0d7-012e6774e330 to disappear
Jul 13 10:25:32.495: INFO: Pod downwardapi-volume-3eb99319-7ba5-4a4a-a0d7-012e6774e330 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:25:32.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6642" for this suite.
Jul 13 10:25:38.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:25:38.919: INFO: namespace downward-api-6642 deletion completed in 6.412876734s

• [SLOW TEST:10.702 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:25:38.920: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-89jh
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 10:25:39.105: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-89jh" in namespace "subpath-4495" to be "success or failure"
Jul 13 10:25:39.122: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Pending", Reason="", readiness=false. Elapsed: 17.330566ms
Jul 13 10:25:41.132: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027299259s
Jul 13 10:25:43.143: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 4.037653082s
Jul 13 10:25:45.154: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 6.048674551s
Jul 13 10:25:47.165: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 8.060079895s
Jul 13 10:25:49.178: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 10.073115958s
Jul 13 10:25:51.189: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 12.083493925s
Jul 13 10:25:53.199: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 14.094341985s
Jul 13 10:25:55.211: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 16.105499515s
Jul 13 10:25:57.220: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 18.115358979s
Jul 13 10:25:59.237: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 20.131729352s
Jul 13 10:26:01.248: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Running", Reason="", readiness=true. Elapsed: 22.142418963s
Jul 13 10:26:03.257: INFO: Pod "pod-subpath-test-projected-89jh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.151765779s
STEP: Saw pod success
Jul 13 10:26:03.257: INFO: Pod "pod-subpath-test-projected-89jh" satisfied condition "success or failure"
Jul 13 10:26:03.265: INFO: Trying to get logs from node cncf-1-15 pod pod-subpath-test-projected-89jh container test-container-subpath-projected-89jh: <nil>
STEP: delete the pod
Jul 13 10:26:03.313: INFO: Waiting for pod pod-subpath-test-projected-89jh to disappear
Jul 13 10:26:03.320: INFO: Pod pod-subpath-test-projected-89jh no longer exists
STEP: Deleting pod pod-subpath-test-projected-89jh
Jul 13 10:26:03.320: INFO: Deleting pod "pod-subpath-test-projected-89jh" in namespace "subpath-4495"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:26:03.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4495" for this suite.
Jul 13 10:26:09.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:26:09.702: INFO: namespace subpath-4495 deletion completed in 6.364746513s

• [SLOW TEST:30.782 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:26:09.704: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 10:26:09.857: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2419e71e-4f7e-4e43-9979-6bbfecf58d68" in namespace "projected-202" to be "success or failure"
Jul 13 10:26:09.865: INFO: Pod "downwardapi-volume-2419e71e-4f7e-4e43-9979-6bbfecf58d68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.960951ms
Jul 13 10:26:11.886: INFO: Pod "downwardapi-volume-2419e71e-4f7e-4e43-9979-6bbfecf58d68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028566814s
Jul 13 10:26:13.898: INFO: Pod "downwardapi-volume-2419e71e-4f7e-4e43-9979-6bbfecf58d68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040726261s
STEP: Saw pod success
Jul 13 10:26:13.898: INFO: Pod "downwardapi-volume-2419e71e-4f7e-4e43-9979-6bbfecf58d68" satisfied condition "success or failure"
Jul 13 10:26:13.911: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-2419e71e-4f7e-4e43-9979-6bbfecf58d68 container client-container: <nil>
STEP: delete the pod
Jul 13 10:26:13.976: INFO: Waiting for pod downwardapi-volume-2419e71e-4f7e-4e43-9979-6bbfecf58d68 to disappear
Jul 13 10:26:13.984: INFO: Pod downwardapi-volume-2419e71e-4f7e-4e43-9979-6bbfecf58d68 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:26:13.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-202" for this suite.
Jul 13 10:26:20.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:26:20.370: INFO: namespace projected-202 deletion completed in 6.375010372s

• [SLOW TEST:10.666 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:26:20.372: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jul 13 10:26:20.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 cluster-info'
Jul 13 10:26:20.671: INFO: stderr: ""
Jul 13 10:26:20.671: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:26:20.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1822" for this suite.
Jul 13 10:26:26.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:26:27.063: INFO: namespace kubectl-1822 deletion completed in 6.382093677s

• [SLOW TEST:6.691 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:26:27.063: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:26:27.186: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:26:31.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6303" for this suite.
Jul 13 10:27:21.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:27:21.704: INFO: namespace pods-6303 deletion completed in 50.389547308s

• [SLOW TEST:54.641 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:27:21.705: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-4417/configmap-test-1ab16a19-ced5-4ab3-a500-84fd5e6868be
STEP: Creating a pod to test consume configMaps
Jul 13 10:27:21.929: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e04626a-be4a-4656-9183-78ce0f545f8e" in namespace "configmap-4417" to be "success or failure"
Jul 13 10:27:21.941: INFO: Pod "pod-configmaps-9e04626a-be4a-4656-9183-78ce0f545f8e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.820538ms
Jul 13 10:27:23.977: INFO: Pod "pod-configmaps-9e04626a-be4a-4656-9183-78ce0f545f8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048315233s
Jul 13 10:27:25.988: INFO: Pod "pod-configmaps-9e04626a-be4a-4656-9183-78ce0f545f8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059132914s
STEP: Saw pod success
Jul 13 10:27:25.988: INFO: Pod "pod-configmaps-9e04626a-be4a-4656-9183-78ce0f545f8e" satisfied condition "success or failure"
Jul 13 10:27:25.995: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-9e04626a-be4a-4656-9183-78ce0f545f8e container env-test: <nil>
STEP: delete the pod
Jul 13 10:27:26.043: INFO: Waiting for pod pod-configmaps-9e04626a-be4a-4656-9183-78ce0f545f8e to disappear
Jul 13 10:27:26.051: INFO: Pod pod-configmaps-9e04626a-be4a-4656-9183-78ce0f545f8e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:27:26.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4417" for this suite.
Jul 13 10:27:32.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:27:32.487: INFO: namespace configmap-4417 deletion completed in 6.426896439s

• [SLOW TEST:10.782 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:27:32.487: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 10:27:32.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7483d5c-9dd9-4400-b047-a2fb8141cd29" in namespace "projected-737" to be "success or failure"
Jul 13 10:27:32.648: INFO: Pod "downwardapi-volume-d7483d5c-9dd9-4400-b047-a2fb8141cd29": Phase="Pending", Reason="", readiness=false. Elapsed: 17.67312ms
Jul 13 10:27:34.657: INFO: Pod "downwardapi-volume-d7483d5c-9dd9-4400-b047-a2fb8141cd29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026786169s
Jul 13 10:27:36.667: INFO: Pod "downwardapi-volume-d7483d5c-9dd9-4400-b047-a2fb8141cd29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036892805s
STEP: Saw pod success
Jul 13 10:27:36.668: INFO: Pod "downwardapi-volume-d7483d5c-9dd9-4400-b047-a2fb8141cd29" satisfied condition "success or failure"
Jul 13 10:27:36.676: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-d7483d5c-9dd9-4400-b047-a2fb8141cd29 container client-container: <nil>
STEP: delete the pod
Jul 13 10:27:36.737: INFO: Waiting for pod downwardapi-volume-d7483d5c-9dd9-4400-b047-a2fb8141cd29 to disappear
Jul 13 10:27:36.745: INFO: Pod downwardapi-volume-d7483d5c-9dd9-4400-b047-a2fb8141cd29 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:27:36.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-737" for this suite.
Jul 13 10:27:42.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:27:43.097: INFO: namespace projected-737 deletion completed in 6.341322314s

• [SLOW TEST:10.610 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:27:43.098: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-661/secret-test-12992555-e909-4acb-878a-db08a2dd0bec
STEP: Creating a pod to test consume secrets
Jul 13 10:27:43.267: INFO: Waiting up to 5m0s for pod "pod-configmaps-f31dbecc-e95b-4011-b209-d2a10816daaf" in namespace "secrets-661" to be "success or failure"
Jul 13 10:27:43.279: INFO: Pod "pod-configmaps-f31dbecc-e95b-4011-b209-d2a10816daaf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.559726ms
Jul 13 10:27:45.289: INFO: Pod "pod-configmaps-f31dbecc-e95b-4011-b209-d2a10816daaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022011877s
Jul 13 10:27:47.299: INFO: Pod "pod-configmaps-f31dbecc-e95b-4011-b209-d2a10816daaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031443952s
STEP: Saw pod success
Jul 13 10:27:47.299: INFO: Pod "pod-configmaps-f31dbecc-e95b-4011-b209-d2a10816daaf" satisfied condition "success or failure"
Jul 13 10:27:47.308: INFO: Trying to get logs from node cncf-1-15 pod pod-configmaps-f31dbecc-e95b-4011-b209-d2a10816daaf container env-test: <nil>
STEP: delete the pod
Jul 13 10:27:47.387: INFO: Waiting for pod pod-configmaps-f31dbecc-e95b-4011-b209-d2a10816daaf to disappear
Jul 13 10:27:47.395: INFO: Pod pod-configmaps-f31dbecc-e95b-4011-b209-d2a10816daaf no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:27:47.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-661" for this suite.
Jul 13 10:27:53.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:27:53.769: INFO: namespace secrets-661 deletion completed in 6.364990489s

• [SLOW TEST:10.672 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:27:53.770: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8639
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8639 to expose endpoints map[]
Jul 13 10:27:53.975: INFO: successfully validated that service endpoint-test2 in namespace services-8639 exposes endpoints map[] (13.734579ms elapsed)
STEP: Creating pod pod1 in namespace services-8639
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8639 to expose endpoints map[pod1:[80]]
Jul 13 10:27:57.075: INFO: successfully validated that service endpoint-test2 in namespace services-8639 exposes endpoints map[pod1:[80]] (3.077948691s elapsed)
STEP: Creating pod pod2 in namespace services-8639
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8639 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 13 10:28:00.222: INFO: successfully validated that service endpoint-test2 in namespace services-8639 exposes endpoints map[pod1:[80] pod2:[80]] (3.127706483s elapsed)
STEP: Deleting pod pod1 in namespace services-8639
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8639 to expose endpoints map[pod2:[80]]
Jul 13 10:28:01.280: INFO: successfully validated that service endpoint-test2 in namespace services-8639 exposes endpoints map[pod2:[80]] (1.040643644s elapsed)
STEP: Deleting pod pod2 in namespace services-8639
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8639 to expose endpoints map[]
Jul 13 10:28:02.359: INFO: successfully validated that service endpoint-test2 in namespace services-8639 exposes endpoints map[] (1.017045113s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:28:02.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8639" for this suite.
Jul 13 10:28:08.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:28:08.790: INFO: namespace services-8639 deletion completed in 6.377980657s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:15.020 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:28:08.791: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 13 10:28:08.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8280,SelfLink:/api/v1/namespaces/watch-8280/configmaps/e2e-watch-test-watch-closed,UID:d145b8fa-97aa-41ee-9595-22f397e8e7db,ResourceVersion:223547815,Generation:0,CreationTimestamp:2019-07-13 10:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 10:28:08.957: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8280,SelfLink:/api/v1/namespaces/watch-8280/configmaps/e2e-watch-test-watch-closed,UID:d145b8fa-97aa-41ee-9595-22f397e8e7db,ResourceVersion:223547816,Generation:0,CreationTimestamp:2019-07-13 10:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 13 10:28:08.997: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8280,SelfLink:/api/v1/namespaces/watch-8280/configmaps/e2e-watch-test-watch-closed,UID:d145b8fa-97aa-41ee-9595-22f397e8e7db,ResourceVersion:223547817,Generation:0,CreationTimestamp:2019-07-13 10:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 10:28:08.998: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8280,SelfLink:/api/v1/namespaces/watch-8280/configmaps/e2e-watch-test-watch-closed,UID:d145b8fa-97aa-41ee-9595-22f397e8e7db,ResourceVersion:223547818,Generation:0,CreationTimestamp:2019-07-13 10:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:28:08.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8280" for this suite.
Jul 13 10:28:15.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:28:15.341: INFO: namespace watch-8280 deletion completed in 6.332607661s

• [SLOW TEST:6.550 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:28:15.341: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-45cn
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 10:28:15.509: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-45cn" in namespace "subpath-7906" to be "success or failure"
Jul 13 10:28:15.520: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.681808ms
Jul 13 10:28:17.530: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020474736s
Jul 13 10:28:19.540: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 4.031000937s
Jul 13 10:28:21.551: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 6.041821779s
Jul 13 10:28:23.562: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 8.053026855s
Jul 13 10:28:25.573: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 10.064072919s
Jul 13 10:28:27.582: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 12.072843144s
Jul 13 10:28:29.593: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 14.083630489s
Jul 13 10:28:31.603: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 16.0936587s
Jul 13 10:28:33.632: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 18.12267461s
Jul 13 10:28:35.642: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 20.133153773s
Jul 13 10:28:37.652: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Running", Reason="", readiness=true. Elapsed: 22.14281766s
Jul 13 10:28:39.663: INFO: Pod "pod-subpath-test-configmap-45cn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.153254295s
STEP: Saw pod success
Jul 13 10:28:39.663: INFO: Pod "pod-subpath-test-configmap-45cn" satisfied condition "success or failure"
Jul 13 10:28:39.670: INFO: Trying to get logs from node cncf-1-15 pod pod-subpath-test-configmap-45cn container test-container-subpath-configmap-45cn: <nil>
STEP: delete the pod
Jul 13 10:28:39.745: INFO: Waiting for pod pod-subpath-test-configmap-45cn to disappear
Jul 13 10:28:39.754: INFO: Pod pod-subpath-test-configmap-45cn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-45cn
Jul 13 10:28:39.754: INFO: Deleting pod "pod-subpath-test-configmap-45cn" in namespace "subpath-7906"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:28:39.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7906" for this suite.
Jul 13 10:28:45.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:28:46.146: INFO: namespace subpath-7906 deletion completed in 6.375223916s

• [SLOW TEST:30.805 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:28:46.147: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 13 10:28:56.441: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:56.441: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:56.704: INFO: Exec stderr: ""
Jul 13 10:28:56.704: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:56.704: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:56.949: INFO: Exec stderr: ""
Jul 13 10:28:56.949: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:56.949: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:57.177: INFO: Exec stderr: ""
Jul 13 10:28:57.178: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:57.178: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:57.403: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 13 10:28:57.403: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:57.403: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:57.605: INFO: Exec stderr: ""
Jul 13 10:28:57.605: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:57.605: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:57.822: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 13 10:28:57.822: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:58.085: INFO: Exec stderr: ""
Jul 13 10:28:58.085: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:58.085: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:58.336: INFO: Exec stderr: ""
Jul 13 10:28:58.336: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:58.336: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:58.600: INFO: Exec stderr: ""
Jul 13 10:28:58.600: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5823 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 10:28:58.600: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
Jul 13 10:28:58.840: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:28:58.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5823" for this suite.
Jul 13 10:29:48.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:29:49.232: INFO: namespace e2e-kubelet-etc-hosts-5823 deletion completed in 50.381421042s

• [SLOW TEST:63.084 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:29:49.233: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-05a361b0-3adf-4cba-9188-c830e3ebb996 in namespace container-probe-5172
Jul 13 10:29:53.451: INFO: Started pod test-webserver-05a361b0-3adf-4cba-9188-c830e3ebb996 in namespace container-probe-5172
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 10:29:53.460: INFO: Initial restart count of pod test-webserver-05a361b0-3adf-4cba-9188-c830e3ebb996 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:33:54.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5172" for this suite.
Jul 13 10:34:00.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:34:01.256: INFO: namespace container-probe-5172 deletion completed in 6.376570071s

• [SLOW TEST:252.024 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:34:01.258: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 10:34:01.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fd29b0b-d5a6-4c79-992f-2d37e536997b" in namespace "downward-api-7471" to be "success or failure"
Jul 13 10:34:01.414: INFO: Pod "downwardapi-volume-2fd29b0b-d5a6-4c79-992f-2d37e536997b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.402674ms
Jul 13 10:34:03.423: INFO: Pod "downwardapi-volume-2fd29b0b-d5a6-4c79-992f-2d37e536997b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019807853s
Jul 13 10:34:05.434: INFO: Pod "downwardapi-volume-2fd29b0b-d5a6-4c79-992f-2d37e536997b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030265904s
STEP: Saw pod success
Jul 13 10:34:05.434: INFO: Pod "downwardapi-volume-2fd29b0b-d5a6-4c79-992f-2d37e536997b" satisfied condition "success or failure"
Jul 13 10:34:05.442: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-2fd29b0b-d5a6-4c79-992f-2d37e536997b container client-container: <nil>
STEP: delete the pod
Jul 13 10:34:05.521: INFO: Waiting for pod downwardapi-volume-2fd29b0b-d5a6-4c79-992f-2d37e536997b to disappear
Jul 13 10:34:05.529: INFO: Pod downwardapi-volume-2fd29b0b-d5a6-4c79-992f-2d37e536997b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:34:05.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7471" for this suite.
Jul 13 10:34:11.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:34:11.889: INFO: namespace downward-api-7471 deletion completed in 6.351193132s

• [SLOW TEST:10.632 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:34:11.890: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4930
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4930
STEP: Creating statefulset with conflicting port in namespace statefulset-4930
STEP: Waiting until pod test-pod will start running in namespace statefulset-4930
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4930
Jul 13 10:34:18.113: INFO: Observed stateful pod in namespace: statefulset-4930, name: ss-0, uid: 80652dfa-474b-4298-b88c-b6e67693481d, status phase: Failed. Waiting for statefulset controller to delete.
Jul 13 10:34:18.113: INFO: Observed stateful pod in namespace: statefulset-4930, name: ss-0, uid: 80652dfa-474b-4298-b88c-b6e67693481d, status phase: Failed. Waiting for statefulset controller to delete.
Jul 13 10:34:18.124: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4930
STEP: Removing pod with conflicting port in namespace statefulset-4930
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4930 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 13 10:34:24.205: INFO: Deleting all statefulset in ns statefulset-4930
Jul 13 10:34:24.213: INFO: Scaling statefulset ss to 0
Jul 13 10:34:44.259: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 10:34:44.268: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:34:44.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4930" for this suite.
Jul 13 10:34:50.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:34:50.744: INFO: namespace statefulset-4930 deletion completed in 6.379365598s

• [SLOW TEST:38.855 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:34:50.750: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 13 10:34:50.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 create -f - --namespace=kubectl-3867'
Jul 13 10:34:51.306: INFO: stderr: ""
Jul 13 10:34:51.306: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 10:34:51.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3867'
Jul 13 10:34:51.430: INFO: stderr: ""
Jul 13 10:34:51.430: INFO: stdout: "update-demo-nautilus-989c8 update-demo-nautilus-xv446 "
Jul 13 10:34:51.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-989c8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3867'
Jul 13 10:34:51.581: INFO: stderr: ""
Jul 13 10:34:51.581: INFO: stdout: ""
Jul 13 10:34:51.581: INFO: update-demo-nautilus-989c8 is created but not running
Jul 13 10:34:56.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3867'
Jul 13 10:34:56.731: INFO: stderr: ""
Jul 13 10:34:56.731: INFO: stdout: "update-demo-nautilus-989c8 update-demo-nautilus-xv446 "
Jul 13 10:34:56.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-989c8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3867'
Jul 13 10:34:56.875: INFO: stderr: ""
Jul 13 10:34:56.875: INFO: stdout: "true"
Jul 13 10:34:56.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-989c8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3867'
Jul 13 10:34:57.009: INFO: stderr: ""
Jul 13 10:34:57.009: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 10:34:57.009: INFO: validating pod update-demo-nautilus-989c8
Jul 13 10:34:57.024: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 10:34:57.024: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 10:34:57.024: INFO: update-demo-nautilus-989c8 is verified up and running
Jul 13 10:34:57.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-xv446 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3867'
Jul 13 10:34:57.157: INFO: stderr: ""
Jul 13 10:34:57.157: INFO: stdout: "true"
Jul 13 10:34:57.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods update-demo-nautilus-xv446 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3867'
Jul 13 10:34:57.293: INFO: stderr: ""
Jul 13 10:34:57.293: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 10:34:57.293: INFO: validating pod update-demo-nautilus-xv446
Jul 13 10:34:57.311: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 10:34:57.311: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 10:34:57.311: INFO: update-demo-nautilus-xv446 is verified up and running
STEP: using delete to clean up resources
Jul 13 10:34:57.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete --grace-period=0 --force -f - --namespace=kubectl-3867'
Jul 13 10:34:57.436: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 10:34:57.436: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 13 10:34:57.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3867'
Jul 13 10:34:57.589: INFO: stderr: "No resources found.\n"
Jul 13 10:34:57.589: INFO: stdout: ""
Jul 13 10:34:57.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -l name=update-demo --namespace=kubectl-3867 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 10:34:57.739: INFO: stderr: ""
Jul 13 10:34:57.739: INFO: stdout: "update-demo-nautilus-989c8\nupdate-demo-nautilus-xv446\n"
Jul 13 10:34:58.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3867'
Jul 13 10:34:58.410: INFO: stderr: "No resources found.\n"
Jul 13 10:34:58.410: INFO: stdout: ""
Jul 13 10:34:58.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -l name=update-demo --namespace=kubectl-3867 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 10:34:58.541: INFO: stderr: ""
Jul 13 10:34:58.541: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:34:58.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3867" for this suite.
Jul 13 10:35:22.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:35:22.915: INFO: namespace kubectl-3867 deletion completed in 24.360962834s

• [SLOW TEST:32.165 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:35:22.915: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e69314d8-6998-46bd-aae1-bcac1113fe26
STEP: Creating a pod to test consume configMaps
Jul 13 10:35:23.165: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-686ac08a-6170-44a4-bcb5-5f5595284dee" in namespace "projected-1701" to be "success or failure"
Jul 13 10:35:23.178: INFO: Pod "pod-projected-configmaps-686ac08a-6170-44a4-bcb5-5f5595284dee": Phase="Pending", Reason="", readiness=false. Elapsed: 13.012081ms
Jul 13 10:35:25.188: INFO: Pod "pod-projected-configmaps-686ac08a-6170-44a4-bcb5-5f5595284dee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023019471s
Jul 13 10:35:27.198: INFO: Pod "pod-projected-configmaps-686ac08a-6170-44a4-bcb5-5f5595284dee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03368367s
STEP: Saw pod success
Jul 13 10:35:27.198: INFO: Pod "pod-projected-configmaps-686ac08a-6170-44a4-bcb5-5f5595284dee" satisfied condition "success or failure"
Jul 13 10:35:27.206: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-configmaps-686ac08a-6170-44a4-bcb5-5f5595284dee container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 10:35:27.312: INFO: Waiting for pod pod-projected-configmaps-686ac08a-6170-44a4-bcb5-5f5595284dee to disappear
Jul 13 10:35:27.322: INFO: Pod pod-projected-configmaps-686ac08a-6170-44a4-bcb5-5f5595284dee no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:35:27.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1701" for this suite.
Jul 13 10:35:33.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:35:33.683: INFO: namespace projected-1701 deletion completed in 6.350178692s

• [SLOW TEST:10.768 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:35:33.684: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 10:35:33.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f5c994b-95a5-401b-ba0c-4a432b36eda6" in namespace "projected-8168" to be "success or failure"
Jul 13 10:35:33.886: INFO: Pod "downwardapi-volume-8f5c994b-95a5-401b-ba0c-4a432b36eda6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.298296ms
Jul 13 10:35:35.896: INFO: Pod "downwardapi-volume-8f5c994b-95a5-401b-ba0c-4a432b36eda6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019514002s
Jul 13 10:35:37.908: INFO: Pod "downwardapi-volume-8f5c994b-95a5-401b-ba0c-4a432b36eda6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031167164s
STEP: Saw pod success
Jul 13 10:35:37.908: INFO: Pod "downwardapi-volume-8f5c994b-95a5-401b-ba0c-4a432b36eda6" satisfied condition "success or failure"
Jul 13 10:35:37.917: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-8f5c994b-95a5-401b-ba0c-4a432b36eda6 container client-container: <nil>
STEP: delete the pod
Jul 13 10:35:37.973: INFO: Waiting for pod downwardapi-volume-8f5c994b-95a5-401b-ba0c-4a432b36eda6 to disappear
Jul 13 10:35:37.981: INFO: Pod downwardapi-volume-8f5c994b-95a5-401b-ba0c-4a432b36eda6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:35:37.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8168" for this suite.
Jul 13 10:35:44.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:35:44.367: INFO: namespace projected-8168 deletion completed in 6.374396353s

• [SLOW TEST:10.683 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:35:44.370: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jul 13 10:35:44.537: INFO: Waiting up to 5m0s for pod "client-containers-36dcf1a3-aa0e-4f51-936b-3e54ab430aa9" in namespace "containers-2383" to be "success or failure"
Jul 13 10:35:44.549: INFO: Pod "client-containers-36dcf1a3-aa0e-4f51-936b-3e54ab430aa9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.491635ms
Jul 13 10:35:46.559: INFO: Pod "client-containers-36dcf1a3-aa0e-4f51-936b-3e54ab430aa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021844773s
Jul 13 10:35:48.571: INFO: Pod "client-containers-36dcf1a3-aa0e-4f51-936b-3e54ab430aa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033942402s
STEP: Saw pod success
Jul 13 10:35:48.571: INFO: Pod "client-containers-36dcf1a3-aa0e-4f51-936b-3e54ab430aa9" satisfied condition "success or failure"
Jul 13 10:35:48.581: INFO: Trying to get logs from node cncf-1-15 pod client-containers-36dcf1a3-aa0e-4f51-936b-3e54ab430aa9 container test-container: <nil>
STEP: delete the pod
Jul 13 10:35:48.670: INFO: Waiting for pod client-containers-36dcf1a3-aa0e-4f51-936b-3e54ab430aa9 to disappear
Jul 13 10:35:48.679: INFO: Pod client-containers-36dcf1a3-aa0e-4f51-936b-3e54ab430aa9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:35:48.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2383" for this suite.
Jul 13 10:35:54.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:35:55.052: INFO: namespace containers-2383 deletion completed in 6.362564854s

• [SLOW TEST:10.682 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:35:55.052: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6179
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-6179
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6179
Jul 13 10:35:55.258: INFO: Found 0 stateful pods, waiting for 1
Jul 13 10:36:05.276: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 13 10:36:05.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-6179 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 10:36:05.673: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 10:36:05.673: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 10:36:05.673: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 13 10:36:05.685: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 13 10:36:15.696: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 10:36:15.696: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 10:36:15.744: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jul 13 10:36:15.744: INFO: ss-0  cncf-1-15  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:35:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:35:55 +0000 UTC  }]
Jul 13 10:36:15.745: INFO: 
Jul 13 10:36:15.745: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 13 10:36:16.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987441622s
Jul 13 10:36:17.770: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975447326s
Jul 13 10:36:18.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962447543s
Jul 13 10:36:19.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.948182554s
Jul 13 10:36:20.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.936598579s
Jul 13 10:36:21.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.925062001s
Jul 13 10:36:22.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.914070068s
Jul 13 10:36:23.840: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.902605054s
Jul 13 10:36:24.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 892.366802ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6179
Jul 13 10:36:25.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-6179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 13 10:36:26.236: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 13 10:36:26.236: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 13 10:36:26.236: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 13 10:36:26.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-6179 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 13 10:36:26.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 13 10:36:26.654: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 13 10:36:26.654: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 13 10:36:26.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-6179 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 13 10:36:27.035: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 13 10:36:27.036: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 13 10:36:27.036: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 13 10:36:27.047: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 10:36:27.047: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 10:36:27.047: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 13 10:36:27.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-6179 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 10:36:27.431: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 10:36:27.431: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 10:36:27.431: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 13 10:36:27.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-6179 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 10:36:27.850: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 10:36:27.850: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 10:36:27.850: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 13 10:36:27.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 exec --namespace=statefulset-6179 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 13 10:36:28.294: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 13 10:36:28.294: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 13 10:36:28.294: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 13 10:36:28.294: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 10:36:28.312: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 13 10:36:38.331: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 10:36:38.331: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 10:36:38.331: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 10:36:38.388: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jul 13 10:36:38.388: INFO: ss-0  cncf-1-15    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:35:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:35:55 +0000 UTC  }]
Jul 13 10:36:38.388: INFO: ss-1  cncf-1-15-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:15 +0000 UTC  }]
Jul 13 10:36:38.388: INFO: ss-2  cncf-1-15    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:15 +0000 UTC  }]
Jul 13 10:36:38.388: INFO: 
Jul 13 10:36:38.388: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 13 10:36:39.401: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jul 13 10:36:39.401: INFO: ss-0  cncf-1-15    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:35:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:35:55 +0000 UTC  }]
Jul 13 10:36:39.401: INFO: ss-1  cncf-1-15-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:15 +0000 UTC  }]
Jul 13 10:36:39.401: INFO: ss-2  cncf-1-15    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:36:15 +0000 UTC  }]
Jul 13 10:36:39.401: INFO: 
Jul 13 10:36:39.402: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 13 10:36:40.413: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.976227766s
Jul 13 10:36:41.423: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.96561776s
Jul 13 10:36:42.439: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.95486636s
Jul 13 10:36:43.449: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.938867635s
Jul 13 10:36:44.459: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.929518571s
Jul 13 10:36:45.469: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.919365216s
Jul 13 10:36:46.480: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.908661475s
Jul 13 10:36:47.491: INFO: Verifying statefulset ss doesn't scale past 0 for another 897.951314ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6179
Jul 13 10:36:48.502: INFO: Scaling statefulset ss to 0
Jul 13 10:36:48.540: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 13 10:36:48.547: INFO: Deleting all statefulset in ns statefulset-6179
Jul 13 10:36:48.555: INFO: Scaling statefulset ss to 0
Jul 13 10:36:48.585: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 10:36:48.594: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:36:48.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6179" for this suite.
Jul 13 10:36:54.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:36:55.048: INFO: namespace statefulset-6179 deletion completed in 6.370847796s

• [SLOW TEST:59.996 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:36:55.052: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0713 10:37:05.433530      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 13 10:37:05.433: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:37:05.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7589" for this suite.
Jul 13 10:37:13.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:37:13.851: INFO: namespace gc-7589 deletion completed in 8.407426703s

• [SLOW TEST:18.799 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:37:13.851: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 13 10:37:13.997: INFO: Waiting up to 5m0s for pod "pod-70cf3039-bb9a-4c82-a8b0-34e9653daba5" in namespace "emptydir-3026" to be "success or failure"
Jul 13 10:37:14.011: INFO: Pod "pod-70cf3039-bb9a-4c82-a8b0-34e9653daba5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.27057ms
Jul 13 10:37:16.021: INFO: Pod "pod-70cf3039-bb9a-4c82-a8b0-34e9653daba5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023678836s
Jul 13 10:37:18.031: INFO: Pod "pod-70cf3039-bb9a-4c82-a8b0-34e9653daba5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033993747s
STEP: Saw pod success
Jul 13 10:37:18.031: INFO: Pod "pod-70cf3039-bb9a-4c82-a8b0-34e9653daba5" satisfied condition "success or failure"
Jul 13 10:37:18.040: INFO: Trying to get logs from node cncf-1-15 pod pod-70cf3039-bb9a-4c82-a8b0-34e9653daba5 container test-container: <nil>
STEP: delete the pod
Jul 13 10:37:18.129: INFO: Waiting for pod pod-70cf3039-bb9a-4c82-a8b0-34e9653daba5 to disappear
Jul 13 10:37:18.138: INFO: Pod pod-70cf3039-bb9a-4c82-a8b0-34e9653daba5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:37:18.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3026" for this suite.
Jul 13 10:37:24.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:37:24.511: INFO: namespace emptydir-3026 deletion completed in 6.363863584s

• [SLOW TEST:10.660 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:37:24.517: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:37:24.687: INFO: Creating ReplicaSet my-hostname-basic-c8a1d701-59a1-4803-8146-26a841500ff2
Jul 13 10:37:24.717: INFO: Pod name my-hostname-basic-c8a1d701-59a1-4803-8146-26a841500ff2: Found 0 pods out of 1
Jul 13 10:37:29.731: INFO: Pod name my-hostname-basic-c8a1d701-59a1-4803-8146-26a841500ff2: Found 1 pods out of 1
Jul 13 10:37:29.731: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c8a1d701-59a1-4803-8146-26a841500ff2" is running
Jul 13 10:37:29.748: INFO: Pod "my-hostname-basic-c8a1d701-59a1-4803-8146-26a841500ff2-sj8vp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-13 10:37:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-13 10:37:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-13 10:37:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-13 10:37:24 +0000 UTC Reason: Message:}])
Jul 13 10:37:29.748: INFO: Trying to dial the pod
Jul 13 10:37:34.785: INFO: Controller my-hostname-basic-c8a1d701-59a1-4803-8146-26a841500ff2: Got expected result from replica 1 [my-hostname-basic-c8a1d701-59a1-4803-8146-26a841500ff2-sj8vp]: "my-hostname-basic-c8a1d701-59a1-4803-8146-26a841500ff2-sj8vp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:37:34.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6447" for this suite.
Jul 13 10:37:40.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:37:41.296: INFO: namespace replicaset-6447 deletion completed in 6.497478438s

• [SLOW TEST:16.780 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:37:41.297: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:37:41.470: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 13 10:37:41.511: INFO: Number of nodes with available pods: 0
Jul 13 10:37:41.511: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:37:42.531: INFO: Number of nodes with available pods: 0
Jul 13 10:37:42.531: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:37:43.534: INFO: Number of nodes with available pods: 0
Jul 13 10:37:43.534: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:37:44.533: INFO: Number of nodes with available pods: 0
Jul 13 10:37:44.533: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:37:45.535: INFO: Number of nodes with available pods: 2
Jul 13 10:37:45.535: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 13 10:37:45.642: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:45.642: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:46.665: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:46.665: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:47.662: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:47.662: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:48.665: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:48.665: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:48.665: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:49.663: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:49.663: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:49.663: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:50.663: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:50.663: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:50.663: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:51.662: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:51.662: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:51.662: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:52.663: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:52.663: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:52.663: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:53.663: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:53.663: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:53.663: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:54.666: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:54.666: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:54.666: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:55.662: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:55.663: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:55.663: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:56.663: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:56.663: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:56.664: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:57.662: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:57.662: INFO: Wrong image for pod: daemon-set-r4w96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:57.662: INFO: Pod daemon-set-r4w96 is not available
Jul 13 10:37:58.665: INFO: Pod daemon-set-2hf96 is not available
Jul 13 10:37:58.665: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:37:59.663: INFO: Pod daemon-set-2hf96 is not available
Jul 13 10:37:59.663: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:38:00.664: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:38:01.663: INFO: Wrong image for pod: daemon-set-6gq4r. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 13 10:38:01.663: INFO: Pod daemon-set-6gq4r is not available
Jul 13 10:38:02.663: INFO: Pod daemon-set-qsx4w is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 13 10:38:02.692: INFO: Number of nodes with available pods: 1
Jul 13 10:38:02.692: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:38:03.717: INFO: Number of nodes with available pods: 1
Jul 13 10:38:03.718: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:38:04.711: INFO: Number of nodes with available pods: 2
Jul 13 10:38:04.711: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4359, will wait for the garbage collector to delete the pods
Jul 13 10:38:04.862: INFO: Deleting DaemonSet.extensions daemon-set took: 50.906385ms
Jul 13 10:38:05.264: INFO: Terminating DaemonSet.extensions daemon-set pods took: 402.068596ms
Jul 13 10:38:17.775: INFO: Number of nodes with available pods: 0
Jul 13 10:38:17.775: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 10:38:17.783: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4359/daemonsets","resourceVersion":"223552449"},"items":null}

Jul 13 10:38:17.792: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4359/pods","resourceVersion":"223552449"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:38:17.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4359" for this suite.
Jul 13 10:38:23.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:38:24.205: INFO: namespace daemonsets-4359 deletion completed in 6.378363188s

• [SLOW TEST:42.908 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:38:24.205: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-559415e6-2321-44c7-b020-b462b46f51fd
STEP: Creating a pod to test consume secrets
Jul 13 10:38:24.400: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-23d9e4e6-862c-4d31-9480-58a053f529dc" in namespace "projected-317" to be "success or failure"
Jul 13 10:38:24.410: INFO: Pod "pod-projected-secrets-23d9e4e6-862c-4d31-9480-58a053f529dc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.328427ms
Jul 13 10:38:26.420: INFO: Pod "pod-projected-secrets-23d9e4e6-862c-4d31-9480-58a053f529dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019223005s
Jul 13 10:38:28.430: INFO: Pod "pod-projected-secrets-23d9e4e6-862c-4d31-9480-58a053f529dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02943536s
STEP: Saw pod success
Jul 13 10:38:28.430: INFO: Pod "pod-projected-secrets-23d9e4e6-862c-4d31-9480-58a053f529dc" satisfied condition "success or failure"
Jul 13 10:38:28.438: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-secrets-23d9e4e6-862c-4d31-9480-58a053f529dc container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 10:38:28.532: INFO: Waiting for pod pod-projected-secrets-23d9e4e6-862c-4d31-9480-58a053f529dc to disappear
Jul 13 10:38:28.541: INFO: Pod pod-projected-secrets-23d9e4e6-862c-4d31-9480-58a053f529dc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:38:28.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-317" for this suite.
Jul 13 10:38:34.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:38:34.999: INFO: namespace projected-317 deletion completed in 6.447305811s

• [SLOW TEST:10.794 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:38:34.999: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-5257890a-1c77-4b65-a387-b1f8cf4e1fde
STEP: Creating a pod to test consume secrets
Jul 13 10:38:35.161: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fae80303-089d-42a7-9bf1-114da218f915" in namespace "projected-5773" to be "success or failure"
Jul 13 10:38:35.170: INFO: Pod "pod-projected-secrets-fae80303-089d-42a7-9bf1-114da218f915": Phase="Pending", Reason="", readiness=false. Elapsed: 8.605954ms
Jul 13 10:38:37.182: INFO: Pod "pod-projected-secrets-fae80303-089d-42a7-9bf1-114da218f915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020520267s
Jul 13 10:38:39.192: INFO: Pod "pod-projected-secrets-fae80303-089d-42a7-9bf1-114da218f915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031051689s
STEP: Saw pod success
Jul 13 10:38:39.192: INFO: Pod "pod-projected-secrets-fae80303-089d-42a7-9bf1-114da218f915" satisfied condition "success or failure"
Jul 13 10:38:39.200: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-secrets-fae80303-089d-42a7-9bf1-114da218f915 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 10:38:39.254: INFO: Waiting for pod pod-projected-secrets-fae80303-089d-42a7-9bf1-114da218f915 to disappear
Jul 13 10:38:39.266: INFO: Pod pod-projected-secrets-fae80303-089d-42a7-9bf1-114da218f915 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:38:39.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5773" for this suite.
Jul 13 10:38:45.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:38:45.635: INFO: namespace projected-5773 deletion completed in 6.353812012s

• [SLOW TEST:10.636 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:38:45.636: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-e4e6fb5c-3bab-4c04-9e7c-cfecc56953b1
STEP: Creating a pod to test consume secrets
Jul 13 10:38:45.848: INFO: Waiting up to 5m0s for pod "pod-secrets-11136290-9492-4dc2-9e64-d1070e7b1b15" in namespace "secrets-5263" to be "success or failure"
Jul 13 10:38:45.861: INFO: Pod "pod-secrets-11136290-9492-4dc2-9e64-d1070e7b1b15": Phase="Pending", Reason="", readiness=false. Elapsed: 13.49301ms
Jul 13 10:38:47.871: INFO: Pod "pod-secrets-11136290-9492-4dc2-9e64-d1070e7b1b15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023635207s
Jul 13 10:38:49.882: INFO: Pod "pod-secrets-11136290-9492-4dc2-9e64-d1070e7b1b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034293178s
STEP: Saw pod success
Jul 13 10:38:49.882: INFO: Pod "pod-secrets-11136290-9492-4dc2-9e64-d1070e7b1b15" satisfied condition "success or failure"
Jul 13 10:38:49.890: INFO: Trying to get logs from node cncf-1-15 pod pod-secrets-11136290-9492-4dc2-9e64-d1070e7b1b15 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 10:38:49.942: INFO: Waiting for pod pod-secrets-11136290-9492-4dc2-9e64-d1070e7b1b15 to disappear
Jul 13 10:38:49.951: INFO: Pod pod-secrets-11136290-9492-4dc2-9e64-d1070e7b1b15 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:38:49.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5263" for this suite.
Jul 13 10:38:55.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:38:56.349: INFO: namespace secrets-5263 deletion completed in 6.386200592s

• [SLOW TEST:10.713 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:38:56.350: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:38:56.478: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:38:57.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1665" for this suite.
Jul 13 10:39:03.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:39:04.011: INFO: namespace custom-resource-definition-1665 deletion completed in 6.371116401s

• [SLOW TEST:7.661 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:39:04.012: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 13 10:39:04.148: INFO: Waiting up to 5m0s for pod "pod-0822cade-0caf-4677-8b92-f05f6f05120d" in namespace "emptydir-4984" to be "success or failure"
Jul 13 10:39:04.165: INFO: Pod "pod-0822cade-0caf-4677-8b92-f05f6f05120d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.879743ms
Jul 13 10:39:06.176: INFO: Pod "pod-0822cade-0caf-4677-8b92-f05f6f05120d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027332734s
Jul 13 10:39:08.186: INFO: Pod "pod-0822cade-0caf-4677-8b92-f05f6f05120d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037279493s
STEP: Saw pod success
Jul 13 10:39:08.186: INFO: Pod "pod-0822cade-0caf-4677-8b92-f05f6f05120d" satisfied condition "success or failure"
Jul 13 10:39:08.196: INFO: Trying to get logs from node cncf-1-15 pod pod-0822cade-0caf-4677-8b92-f05f6f05120d container test-container: <nil>
STEP: delete the pod
Jul 13 10:39:08.276: INFO: Waiting for pod pod-0822cade-0caf-4677-8b92-f05f6f05120d to disappear
Jul 13 10:39:08.284: INFO: Pod pod-0822cade-0caf-4677-8b92-f05f6f05120d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:39:08.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4984" for this suite.
Jul 13 10:39:14.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:39:14.671: INFO: namespace emptydir-4984 deletion completed in 6.378345458s

• [SLOW TEST:10.659 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:39:14.672: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 13 10:39:14.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2042335a-5fac-4e75-8598-19238683965e" in namespace "downward-api-1606" to be "success or failure"
Jul 13 10:39:14.848: INFO: Pod "downwardapi-volume-2042335a-5fac-4e75-8598-19238683965e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.95943ms
Jul 13 10:39:16.858: INFO: Pod "downwardapi-volume-2042335a-5fac-4e75-8598-19238683965e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018185908s
Jul 13 10:39:18.886: INFO: Pod "downwardapi-volume-2042335a-5fac-4e75-8598-19238683965e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046664329s
STEP: Saw pod success
Jul 13 10:39:18.886: INFO: Pod "downwardapi-volume-2042335a-5fac-4e75-8598-19238683965e" satisfied condition "success or failure"
Jul 13 10:39:18.895: INFO: Trying to get logs from node cncf-1-15 pod downwardapi-volume-2042335a-5fac-4e75-8598-19238683965e container client-container: <nil>
STEP: delete the pod
Jul 13 10:39:18.948: INFO: Waiting for pod downwardapi-volume-2042335a-5fac-4e75-8598-19238683965e to disappear
Jul 13 10:39:18.957: INFO: Pod downwardapi-volume-2042335a-5fac-4e75-8598-19238683965e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:39:18.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1606" for this suite.
Jul 13 10:39:25.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:39:25.332: INFO: namespace downward-api-1606 deletion completed in 6.364869124s

• [SLOW TEST:10.660 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:39:25.332: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 13 10:39:30.132: INFO: Successfully updated pod "labelsupdate4208746d-524f-4763-8d30-06fcfaec18ef"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:39:32.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4441" for this suite.
Jul 13 10:39:56.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:39:56.619: INFO: namespace projected-4441 deletion completed in 24.41846746s

• [SLOW TEST:31.287 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:39:56.620: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 13 10:40:01.375: INFO: Successfully updated pod "annotationupdatee068db84-0b67-4c79-99a9-a92e0680db3f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:40:03.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3638" for this suite.
Jul 13 10:40:27.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:40:27.785: INFO: namespace projected-3638 deletion completed in 24.350292899s

• [SLOW TEST:31.164 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:40:27.785: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jul 13 10:40:27.922: INFO: Waiting up to 5m0s for pod "client-containers-588736d9-2325-4755-b301-5500fd14e41b" in namespace "containers-5306" to be "success or failure"
Jul 13 10:40:27.934: INFO: Pod "client-containers-588736d9-2325-4755-b301-5500fd14e41b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.16507ms
Jul 13 10:40:29.944: INFO: Pod "client-containers-588736d9-2325-4755-b301-5500fd14e41b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021687278s
Jul 13 10:40:31.954: INFO: Pod "client-containers-588736d9-2325-4755-b301-5500fd14e41b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031683927s
STEP: Saw pod success
Jul 13 10:40:31.954: INFO: Pod "client-containers-588736d9-2325-4755-b301-5500fd14e41b" satisfied condition "success or failure"
Jul 13 10:40:31.963: INFO: Trying to get logs from node cncf-1-15 pod client-containers-588736d9-2325-4755-b301-5500fd14e41b container test-container: <nil>
STEP: delete the pod
Jul 13 10:40:32.017: INFO: Waiting for pod client-containers-588736d9-2325-4755-b301-5500fd14e41b to disappear
Jul 13 10:40:32.026: INFO: Pod client-containers-588736d9-2325-4755-b301-5500fd14e41b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:40:32.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5306" for this suite.
Jul 13 10:40:38.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:40:38.398: INFO: namespace containers-5306 deletion completed in 6.360953319s

• [SLOW TEST:10.613 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:40:38.400: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jul 13 10:40:38.593: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2700" to be "success or failure"
Jul 13 10:40:38.601: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.406656ms
Jul 13 10:40:40.611: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017453052s
Jul 13 10:40:42.622: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028275074s
STEP: Saw pod success
Jul 13 10:40:42.622: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 13 10:40:42.634: INFO: Trying to get logs from node cncf-1-15 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 13 10:40:42.721: INFO: Waiting for pod pod-host-path-test to disappear
Jul 13 10:40:42.729: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:40:42.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2700" for this suite.
Jul 13 10:40:48.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:40:49.102: INFO: namespace hostpath-2700 deletion completed in 6.363680362s

• [SLOW TEST:10.703 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:40:49.103: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:40:49.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4266" for this suite.
Jul 13 10:41:13.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:41:13.722: INFO: namespace pods-4266 deletion completed in 24.380915327s

• [SLOW TEST:24.619 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:41:13.722: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 13 10:41:19.990: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 13 10:41:19.998: INFO: Pod pod-with-poststart-http-hook still exists
Jul 13 10:41:21.999: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 13 10:41:22.008: INFO: Pod pod-with-poststart-http-hook still exists
Jul 13 10:41:23.999: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 13 10:41:24.009: INFO: Pod pod-with-poststart-http-hook still exists
Jul 13 10:41:25.999: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 13 10:41:26.011: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:41:26.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6818" for this suite.
Jul 13 10:41:50.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:41:50.404: INFO: namespace container-lifecycle-hook-6818 deletion completed in 24.384685941s

• [SLOW TEST:36.682 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:41:50.404: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 13 10:41:50.562: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:41:57.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9559" for this suite.
Jul 13 10:42:21.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:42:22.234: INFO: namespace init-container-9559 deletion completed in 24.361255944s

• [SLOW TEST:31.830 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:42:22.234: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 13 10:42:22.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6582'
Jul 13 10:42:22.509: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 10:42:22.509: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul 13 10:42:22.539: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul 13 10:42:22.546: INFO: scanned /root for discovery docs: <nil>
Jul 13 10:42:22.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6582'
Jul 13 10:42:38.574: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 13 10:42:38.574: INFO: stdout: "Created e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2\nScaling up e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul 13 10:42:38.574: INFO: stdout: "Created e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2\nScaling up e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 13 10:42:38.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-6582'
Jul 13 10:42:38.718: INFO: stderr: ""
Jul 13 10:42:38.718: INFO: stdout: "e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2-mprz5 "
Jul 13 10:42:38.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2-mprz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6582'
Jul 13 10:42:38.842: INFO: stderr: ""
Jul 13 10:42:38.842: INFO: stdout: "true"
Jul 13 10:42:38.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 get pods e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2-mprz5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6582'
Jul 13 10:42:38.955: INFO: stderr: ""
Jul 13 10:42:38.955: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul 13 10:42:38.955: INFO: e2e-test-nginx-rc-69af8446506c6a8fa6e8732a04f2d1d2-mprz5 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jul 13 10:42:38.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-018000938 delete rc e2e-test-nginx-rc --namespace=kubectl-6582'
Jul 13 10:42:39.102: INFO: stderr: ""
Jul 13 10:42:39.102: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:42:39.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6582" for this suite.
Jul 13 10:43:03.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:43:03.449: INFO: namespace kubectl-6582 deletion completed in 24.32867515s

• [SLOW TEST:41.215 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:43:03.451: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-50fc66fd-9046-4267-aef4-c63d8600bf00
STEP: Creating a pod to test consume configMaps
Jul 13 10:43:03.642: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd2cde03-b78f-48c1-934c-9a2b0d894a53" in namespace "projected-4227" to be "success or failure"
Jul 13 10:43:03.657: INFO: Pod "pod-projected-configmaps-dd2cde03-b78f-48c1-934c-9a2b0d894a53": Phase="Pending", Reason="", readiness=false. Elapsed: 14.979804ms
Jul 13 10:43:05.668: INFO: Pod "pod-projected-configmaps-dd2cde03-b78f-48c1-934c-9a2b0d894a53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025742973s
Jul 13 10:43:07.679: INFO: Pod "pod-projected-configmaps-dd2cde03-b78f-48c1-934c-9a2b0d894a53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036587951s
STEP: Saw pod success
Jul 13 10:43:07.679: INFO: Pod "pod-projected-configmaps-dd2cde03-b78f-48c1-934c-9a2b0d894a53" satisfied condition "success or failure"
Jul 13 10:43:07.688: INFO: Trying to get logs from node cncf-1-15 pod pod-projected-configmaps-dd2cde03-b78f-48c1-934c-9a2b0d894a53 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 10:43:07.747: INFO: Waiting for pod pod-projected-configmaps-dd2cde03-b78f-48c1-934c-9a2b0d894a53 to disappear
Jul 13 10:43:07.756: INFO: Pod pod-projected-configmaps-dd2cde03-b78f-48c1-934c-9a2b0d894a53 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:43:07.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4227" for this suite.
Jul 13 10:43:13.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:43:14.136: INFO: namespace projected-4227 deletion completed in 6.369128169s

• [SLOW TEST:10.685 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:43:14.137: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 13 10:43:14.284: INFO: Creating deployment "nginx-deployment"
Jul 13 10:43:14.298: INFO: Waiting for observed generation 1
Jul 13 10:43:16.322: INFO: Waiting for all required pods to come up
Jul 13 10:43:16.340: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 13 10:43:24.375: INFO: Waiting for deployment "nginx-deployment" to complete
Jul 13 10:43:24.393: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul 13 10:43:24.422: INFO: Updating deployment nginx-deployment
Jul 13 10:43:24.422: INFO: Waiting for observed generation 2
Jul 13 10:43:26.439: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 13 10:43:26.454: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 13 10:43:26.474: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 13 10:43:26.499: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 13 10:43:26.499: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 13 10:43:26.509: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 13 10:43:26.539: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul 13 10:43:26.539: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul 13 10:43:26.558: INFO: Updating deployment nginx-deployment
Jul 13 10:43:26.558: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul 13 10:43:26.581: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 13 10:43:26.598: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 13 10:43:26.631: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3348,SelfLink:/apis/apps/v1/namespaces/deployment-3348/deployments/nginx-deployment,UID:ffd37ef2-da4f-4a07-b2fb-4de498c2df37,ResourceVersion:223555041,Generation:3,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-07-13 10:43:24 +0000 UTC 2019-07-13 10:43:14 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-07-13 10:43:26 +0000 UTC 2019-07-13 10:43:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul 13 10:43:26.668: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-3348,SelfLink:/apis/apps/v1/namespaces/deployment-3348/replicasets/nginx-deployment-55fb7cb77f,UID:8609af9b-5c7c-49df-a278-928fd72c6ec1,ResourceVersion:223555029,Generation:3,CreationTimestamp:2019-07-13 10:43:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ffd37ef2-da4f-4a07-b2fb-4de498c2df37 0xc003669b07 0xc003669b08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 13 10:43:26.668: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul 13 10:43:26.669: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-3348,SelfLink:/apis/apps/v1/namespaces/deployment-3348/replicasets/nginx-deployment-7b8c6f4498,UID:83111ce7-af78-40a4-95b9-47db16cfe5f7,ResourceVersion:223555026,Generation:3,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ffd37ef2-da4f-4a07-b2fb-4de498c2df37 0xc003669bd7 0xc003669bd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul 13 10:43:26.695: INFO: Pod "nginx-deployment-55fb7cb77f-4k4h9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4k4h9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-4k4h9,UID:97362451-a511-4286-9cb8-5f4bcba1a042,ResourceVersion:223555050,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d08597 0xc000d08598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d08610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d08630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.695: INFO: Pod "nginx-deployment-55fb7cb77f-5lsmv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5lsmv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-5lsmv,UID:5f0b3b7e-94ea-4e88-8ed7-67319ed23cb9,ResourceVersion:223555004,Generation:0,CreationTimestamp:2019-07-13 10:43:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.28/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d086c7 0xc000d086c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d08740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d08760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:,StartTime:2019-07-13 10:43:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.695: INFO: Pod "nginx-deployment-55fb7cb77f-7pzm4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7pzm4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-7pzm4,UID:bfe0b0f2-9a8b-4381-aa6b-886ed0721181,ResourceVersion:223555084,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d08847 0xc000d08848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d088c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d088e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.696: INFO: Pod "nginx-deployment-55fb7cb77f-88grh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-88grh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-88grh,UID:411ffd8e-c647-4351-a618-3e9f6d7e2484,ResourceVersion:223555051,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d08967 0xc000d08968}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d089e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d08a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.696: INFO: Pod "nginx-deployment-55fb7cb77f-cmhj2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cmhj2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-cmhj2,UID:bfd5defc-0ae5-4072-ba76-004e13ad2356,ResourceVersion:223555001,Generation:0,CreationTimestamp:2019-07-13 10:43:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.0.57/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d08a97 0xc000d08a98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d08b20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d08b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  }],Message:,Reason:,HostIP:51.68.115.96,PodIP:,StartTime:2019-07-13 10:43:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.696: INFO: Pod "nginx-deployment-55fb7cb77f-hl9xv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hl9xv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-hl9xv,UID:6998825c-e82d-4568-bf3d-a388aefdbf77,ResourceVersion:223555068,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d08c17 0xc000d08c18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d08c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d08cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.696: INFO: Pod "nginx-deployment-55fb7cb77f-ldmtl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ldmtl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-ldmtl,UID:c372a4f9-d8c7-4f29-8711-482393b1b518,ResourceVersion:223555009,Generation:0,CreationTimestamp:2019-07-13 10:43:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.31/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d08d47 0xc000d08d48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d08dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d08de0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:,StartTime:2019-07-13 10:43:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.696: INFO: Pod "nginx-deployment-55fb7cb77f-lpkjk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lpkjk,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-lpkjk,UID:956407d8-ffa8-40eb-b911-66b254e4db9a,ResourceVersion:223555076,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d08eb7 0xc000d08eb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d08f30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d08f50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.696: INFO: Pod "nginx-deployment-55fb7cb77f-qdq7f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qdq7f,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-qdq7f,UID:79243d44-e6ed-4f0a-b489-9933cbad5190,ResourceVersion:223555008,Generation:0,CreationTimestamp:2019-07-13 10:43:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.30/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d08fe7 0xc000d08fe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d09060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d09080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:,StartTime:2019-07-13 10:43:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.696: INFO: Pod "nginx-deployment-55fb7cb77f-rj7dz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rj7dz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-rj7dz,UID:6de69aea-6f5d-40e1-a46a-0decaf058914,ResourceVersion:223555006,Generation:0,CreationTimestamp:2019-07-13 10:43:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.29/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d09167 0xc000d09168}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d091e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d09200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:24 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:,StartTime:2019-07-13 10:43:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.697: INFO: Pod "nginx-deployment-55fb7cb77f-t6njk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-t6njk,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-t6njk,UID:f4345c02-3d2c-4da3-850e-fc3cfaa0149c,ResourceVersion:223555071,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d092d7 0xc000d092d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d09360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d09380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.697: INFO: Pod "nginx-deployment-55fb7cb77f-w68k4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-w68k4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-w68k4,UID:9291af2c-7b08-4b88-9784-d169f9682cfb,ResourceVersion:223555074,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d09417 0xc000d09418}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d09490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d094b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.697: INFO: Pod "nginx-deployment-55fb7cb77f-zwb5s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zwb5s,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-55fb7cb77f-zwb5s,UID:6314ac5e-9a5c-478d-a1a9-bd13193761c1,ResourceVersion:223555039,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 8609af9b-5c7c-49df-a278-928fd72c6ec1 0xc000d09537 0xc000d09538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d095b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d095d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.697: INFO: Pod "nginx-deployment-7b8c6f4498-22cr9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-22cr9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-22cr9,UID:8dd543cb-5391-452e-9599-5267a81f7d4a,ResourceVersion:223555079,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc000d09657 0xc000d09658}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d096c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d096e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.697: INFO: Pod "nginx-deployment-7b8c6f4498-448kv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-448kv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-448kv,UID:56a6b219-1c94-4ce5-a150-d5e5e8055698,ResourceVersion:223555058,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc000d09767 0xc000d09768}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d097d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d097f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:51.68.115.96,PodIP:,StartTime:2019-07-13 10:43:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.698: INFO: Pod "nginx-deployment-7b8c6f4498-4hzjr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4hzjr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-4hzjr,UID:75eafb9a-0b8e-4a8e-b7d0-811cbf2fda31,ResourceVersion:223554911,Generation:0,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.0.56/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc000d098d7 0xc000d098d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d09950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d09970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  }],Message:,Reason:,HostIP:51.68.115.96,PodIP:10.2.0.56,StartTime:2019-07-13 10:43:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-13 10:43:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a972c05fbd708b090d52d01e03c58597173f5e50abd41ccba77787e5cfa49890}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.698: INFO: Pod "nginx-deployment-7b8c6f4498-6dr98" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6dr98,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-6dr98,UID:65beba4f-b7d8-43d7-8d5d-17e390c9aa71,ResourceVersion:223554914,Generation:0,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.25/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc000d09a67 0xc000d09a68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d09ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d09af0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:10.2.1.25,StartTime:2019-07-13 10:43:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-13 10:43:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://bfe14a35dae87e8dbfea6733a269047c15ebb65b0dc487f5e6fb85f7d08eb012}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.698: INFO: Pod "nginx-deployment-7b8c6f4498-bksqg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bksqg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-bksqg,UID:dc5428af-76fd-4f6a-bc0e-dce234677ef0,ResourceVersion:223554900,Generation:0,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.0.55/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc000d09bd7 0xc000d09bd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d09c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d09c80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  }],Message:,Reason:,HostIP:51.68.115.96,PodIP:10.2.0.55,StartTime:2019-07-13 10:43:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-13 10:43:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9c921650b5290571349f9c80a0f7459052fb70632c1db4d45689fa26e49b0b6b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.698: INFO: Pod "nginx-deployment-7b8c6f4498-c22d6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-c22d6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-c22d6,UID:1bf8dba0-4dde-44c9-81ba-bfab92d12a18,ResourceVersion:223554876,Generation:0,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.23/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc000d09d67 0xc000d09d68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d09dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d09df0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:10.2.1.23,StartTime:2019-07-13 10:43:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-13 10:43:18 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e4935a3232cd2b51d853ddc53bad1444f5e2095dd5a251dfcd79ad353b9dd407}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.698: INFO: Pod "nginx-deployment-7b8c6f4498-d9qd7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d9qd7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-d9qd7,UID:248e2ec5-c1b7-4fcc-b55c-e51abb556519,ResourceVersion:223554873,Generation:0,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.0.54/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc000d09ed7 0xc000d09ed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d09f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d09f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  }],Message:,Reason:,HostIP:51.68.115.96,PodIP:10.2.0.54,StartTime:2019-07-13 10:43:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-13 10:43:18 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6043499c1e3734e02b5859be8d86840b53fd1758a5d1c96c866fb52ecee4861a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.698: INFO: Pod "nginx-deployment-7b8c6f4498-fq7pd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fq7pd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-fq7pd,UID:7a57a6a2-9d51-4bc3-8db1-97dbf1de5153,ResourceVersion:223555077,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027ca047 0xc0027ca048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ca0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ca0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.698: INFO: Pod "nginx-deployment-7b8c6f4498-hk958" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hk958,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-hk958,UID:aadeb35f-995a-4d1e-a4a6-30dff7af8ef8,ResourceVersion:223555081,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027ca157 0xc0027ca158}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ca1c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ca1e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.698: INFO: Pod "nginx-deployment-7b8c6f4498-kfzf2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kfzf2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-kfzf2,UID:aba89933-2cc0-4b02-9cc9-09cb13bab546,ResourceVersion:223555069,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027ca267 0xc0027ca268}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ca2d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ca2f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:,StartTime:2019-07-13 10:43:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-mbf98" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mbf98,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-mbf98,UID:0052c601-9f0d-487f-82f0-d1c1217a8704,ResourceVersion:223555080,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027ca3b7 0xc0027ca3b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ca420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ca440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-mkkdc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mkkdc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-mkkdc,UID:ae9b3e04-cae3-48cd-8994-7a6f3ef4e3ce,ResourceVersion:223554885,Generation:0,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.24/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027ca4d7 0xc0027ca4d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ca540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ca560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:10.2.1.24,StartTime:2019-07-13 10:43:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-13 10:43:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://fb6552e51a00bbb9225dae498c59cbd6123e8fa79332576ab941fa5c90226437}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-pcgxq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pcgxq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-pcgxq,UID:4c9e02a4-de6f-4517-a8d4-563ca931e677,ResourceVersion:223555060,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027ca637 0xc0027ca638}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ca6a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ca6c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-qqs46" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qqs46,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-qqs46,UID:0b01cf39-7bc8-42ee-b126-45805ae1a3ea,ResourceVersion:223555038,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027ca747 0xc0027ca748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ca7b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ca7d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-rj62n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rj62n,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-rj62n,UID:ed30913c-5d0b-43f5-a896-50a6cedd7e71,ResourceVersion:223555057,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027ca857 0xc0027ca858}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ca8c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ca8e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-s2lj2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-s2lj2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-s2lj2,UID:c1b4972b-04d3-4c26-8a7f-c9eaf941a34d,ResourceVersion:223555053,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027ca967 0xc0027ca968}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ca9d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ca9f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-ssrxj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ssrxj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-ssrxj,UID:7e1c3bc5-cb76-4e04-b4f0-75e0ef667b7c,ResourceVersion:223555075,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027caa77 0xc0027caa78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027caae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027cab00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-vwcrb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vwcrb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-vwcrb,UID:94560b55-d9c2-47ff-8b41-8da6917d2caf,ResourceVersion:223555059,Generation:0,CreationTimestamp:2019-07-13 10:43:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027cab87 0xc0027cab88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027cabf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027cac10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-xp5w7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xp5w7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-xp5w7,UID:ff808c58-d8da-4e1b-ba4f-afce769832b3,ResourceVersion:223554854,Generation:0,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.1.22/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027caca7 0xc0027caca8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027cad10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027cad30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  }],Message:,Reason:,HostIP:51.68.114.204,PodIP:10.2.1.22,StartTime:2019-07-13 10:43:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-13 10:43:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b1652bdd224775d0c56d19cadc76ff5d30c20a3587b361d493646bed4322877b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 13 10:43:26.699: INFO: Pod "nginx-deployment-7b8c6f4498-xx2z7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xx2z7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3348,SelfLink:/api/v1/namespaces/deployment-3348/pods/nginx-deployment-7b8c6f4498-xx2z7,UID:93ac548d-f141-4087-82dd-a1e2196d2c1f,ResourceVersion:223554850,Generation:0,CreationTimestamp:2019-07-13 10:43:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.0.53/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 83111ce7-af78-40a4-95b9-47db16cfe5f7 0xc0027cae17 0xc0027cae18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jkwvt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jkwvt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jkwvt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1-15-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027cae90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027caeb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-13 10:43:14 +0000 UTC  }],Message:,Reason:,HostIP:51.68.115.96,PodIP:10.2.0.53,StartTime:2019-07-13 10:43:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-13 10:43:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9ae3535479baf8c4cdaffa3911073c79990d99661d01ebb1a791862b0f8861e5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:43:26.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3348" for this suite.
Jul 13 10:43:34.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:43:35.025: INFO: namespace deployment-3348 deletion completed in 8.320358052s

• [SLOW TEST:20.888 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:43:35.026: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 13 10:43:35.187: INFO: Waiting up to 5m0s for pod "pod-b5709647-eaae-400f-b874-8e6d9a348188" in namespace "emptydir-7170" to be "success or failure"
Jul 13 10:43:35.206: INFO: Pod "pod-b5709647-eaae-400f-b874-8e6d9a348188": Phase="Pending", Reason="", readiness=false. Elapsed: 18.273754ms
Jul 13 10:43:37.217: INFO: Pod "pod-b5709647-eaae-400f-b874-8e6d9a348188": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029760895s
Jul 13 10:43:39.228: INFO: Pod "pod-b5709647-eaae-400f-b874-8e6d9a348188": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040855728s
Jul 13 10:43:41.238: INFO: Pod "pod-b5709647-eaae-400f-b874-8e6d9a348188": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051025099s
Jul 13 10:43:43.250: INFO: Pod "pod-b5709647-eaae-400f-b874-8e6d9a348188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.062232211s
STEP: Saw pod success
Jul 13 10:43:43.250: INFO: Pod "pod-b5709647-eaae-400f-b874-8e6d9a348188" satisfied condition "success or failure"
Jul 13 10:43:43.259: INFO: Trying to get logs from node cncf-1-15 pod pod-b5709647-eaae-400f-b874-8e6d9a348188 container test-container: <nil>
STEP: delete the pod
Jul 13 10:43:43.355: INFO: Waiting for pod pod-b5709647-eaae-400f-b874-8e6d9a348188 to disappear
Jul 13 10:43:43.363: INFO: Pod pod-b5709647-eaae-400f-b874-8e6d9a348188 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:43:43.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7170" for this suite.
Jul 13 10:43:49.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:43:49.731: INFO: namespace emptydir-7170 deletion completed in 6.356943428s

• [SLOW TEST:14.705 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 13 10:43:49.731: INFO: >>> kubeConfig: /tmp/kubeconfig-018000938
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 13 10:43:49.995: INFO: Number of nodes with available pods: 0
Jul 13 10:43:49.995: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:43:51.018: INFO: Number of nodes with available pods: 0
Jul 13 10:43:51.018: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:43:52.015: INFO: Number of nodes with available pods: 0
Jul 13 10:43:52.015: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:43:53.015: INFO: Number of nodes with available pods: 1
Jul 13 10:43:53.015: INFO: Node cncf-1-15-2 is running more than one daemon pod
Jul 13 10:43:54.016: INFO: Number of nodes with available pods: 2
Jul 13 10:43:54.016: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 13 10:43:54.076: INFO: Number of nodes with available pods: 1
Jul 13 10:43:54.076: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:43:55.091: INFO: Number of nodes with available pods: 1
Jul 13 10:43:55.091: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:43:56.098: INFO: Number of nodes with available pods: 1
Jul 13 10:43:56.098: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:43:57.098: INFO: Number of nodes with available pods: 1
Jul 13 10:43:57.098: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:43:58.098: INFO: Number of nodes with available pods: 1
Jul 13 10:43:58.098: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:43:59.101: INFO: Number of nodes with available pods: 1
Jul 13 10:43:59.101: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:00.097: INFO: Number of nodes with available pods: 1
Jul 13 10:44:00.097: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:01.100: INFO: Number of nodes with available pods: 1
Jul 13 10:44:01.100: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:02.096: INFO: Number of nodes with available pods: 1
Jul 13 10:44:02.096: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:03.095: INFO: Number of nodes with available pods: 1
Jul 13 10:44:03.095: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:04.095: INFO: Number of nodes with available pods: 1
Jul 13 10:44:04.095: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:05.102: INFO: Number of nodes with available pods: 1
Jul 13 10:44:05.102: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:06.107: INFO: Number of nodes with available pods: 1
Jul 13 10:44:06.107: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:07.097: INFO: Number of nodes with available pods: 1
Jul 13 10:44:07.097: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:08.101: INFO: Number of nodes with available pods: 1
Jul 13 10:44:08.102: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:09.097: INFO: Number of nodes with available pods: 1
Jul 13 10:44:09.097: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:10.100: INFO: Number of nodes with available pods: 1
Jul 13 10:44:10.100: INFO: Node cncf-1-15 is running more than one daemon pod
Jul 13 10:44:11.095: INFO: Number of nodes with available pods: 2
Jul 13 10:44:11.095: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4204, will wait for the garbage collector to delete the pods
Jul 13 10:44:11.181: INFO: Deleting DaemonSet.extensions daemon-set took: 19.132182ms
Jul 13 10:44:11.581: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.543723ms
Jul 13 10:44:17.792: INFO: Number of nodes with available pods: 0
Jul 13 10:44:17.792: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 10:44:17.800: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4204/daemonsets","resourceVersion":"223555720"},"items":null}

Jul 13 10:44:17.808: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4204/pods","resourceVersion":"223555720"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 13 10:44:17.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4204" for this suite.
Jul 13 10:44:23.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 13 10:44:24.213: INFO: namespace daemonsets-4204 deletion completed in 6.366050485s

• [SLOW TEST:34.481 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSJul 13 10:44:24.213: INFO: Running AfterSuite actions on all nodes
Jul 13 10:44:24.214: INFO: Running AfterSuite actions on node 1
Jul 13 10:44:24.214: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 6036.812 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h40m38.191689272s
Test Suite Passed
