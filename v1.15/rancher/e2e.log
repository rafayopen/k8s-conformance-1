I0822 01:36:44.195105      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-089099426
I0822 01:36:44.195448      15 e2e.go:241] Starting e2e run "52cae05f-a082-442b-b0e1-f1288668964a" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1566437801 - Will randomize all specs
Will run 215 of 4413 specs

Aug 22 01:36:44.573: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 01:36:44.575: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 22 01:36:44.597: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 22 01:36:44.645: INFO: The status of Pod rke-coredns-addon-deploy-job-pldgp is Succeeded, skipping waiting
Aug 22 01:36:44.645: INFO: The status of Pod rke-ingress-controller-deploy-job-64vk6 is Succeeded, skipping waiting
Aug 22 01:36:44.646: INFO: The status of Pod rke-metrics-addon-deploy-job-c28kb is Succeeded, skipping waiting
Aug 22 01:36:44.646: INFO: The status of Pod rke-network-plugin-deploy-job-qckrh is Succeeded, skipping waiting
Aug 22 01:36:44.646: INFO: 6 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 22 01:36:44.646: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Aug 22 01:36:44.646: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 22 01:36:44.656: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Aug 22 01:36:44.656: INFO: e2e test version: v1.15.3
Aug 22 01:36:44.659: INFO: kube-apiserver version: v1.15.3
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:36:44.660: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
Aug 22 01:36:44.699: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-e573b126-47a1-4717-a765-e0a884a64eb0
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:36:44.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1688" for this suite.
Aug 22 01:36:50.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:36:50.836: INFO: namespace secrets-1688 deletion completed in 6.127394424s

• [SLOW TEST:6.177 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:36:50.837: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-a6ebba97-bff2-4c8a-94db-43d9c0b3b2f6
STEP: Creating a pod to test consume secrets
Aug 22 01:36:50.876: INFO: Waiting up to 5m0s for pod "pod-secrets-c6c46b4a-9a1b-4d8a-b916-6446f48063d8" in namespace "secrets-9312" to be "success or failure"
Aug 22 01:36:50.880: INFO: Pod "pod-secrets-c6c46b4a-9a1b-4d8a-b916-6446f48063d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174934ms
Aug 22 01:36:52.884: INFO: Pod "pod-secrets-c6c46b4a-9a1b-4d8a-b916-6446f48063d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007435033s
Aug 22 01:36:54.888: INFO: Pod "pod-secrets-c6c46b4a-9a1b-4d8a-b916-6446f48063d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011357668s
STEP: Saw pod success
Aug 22 01:36:54.888: INFO: Pod "pod-secrets-c6c46b4a-9a1b-4d8a-b916-6446f48063d8" satisfied condition "success or failure"
Aug 22 01:36:54.891: INFO: Trying to get logs from node melsayed-conformance3 pod pod-secrets-c6c46b4a-9a1b-4d8a-b916-6446f48063d8 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 01:36:54.925: INFO: Waiting for pod pod-secrets-c6c46b4a-9a1b-4d8a-b916-6446f48063d8 to disappear
Aug 22 01:36:54.928: INFO: Pod pod-secrets-c6c46b4a-9a1b-4d8a-b916-6446f48063d8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:36:54.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9312" for this suite.
Aug 22 01:37:00.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:37:01.026: INFO: namespace secrets-9312 deletion completed in 6.09516222s

• [SLOW TEST:10.189 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:37:01.026: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6156
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6156
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6156
Aug 22 01:37:01.074: INFO: Found 0 stateful pods, waiting for 1
Aug 22 01:37:11.082: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 22 01:37:11.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-6156 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 01:37:11.642: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 01:37:11.642: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 01:37:11.642: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 22 01:37:11.645: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 22 01:37:21.651: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 01:37:21.651: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 01:37:21.668: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999519s
Aug 22 01:37:22.674: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996125613s
Aug 22 01:37:23.681: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990916821s
Aug 22 01:37:24.687: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98392861s
Aug 22 01:37:25.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977619179s
Aug 22 01:37:26.698: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971416497s
Aug 22 01:37:27.702: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.966653413s
Aug 22 01:37:28.707: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.962320076s
Aug 22 01:37:29.713: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.957442195s
Aug 22 01:37:30.717: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.374504ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6156
Aug 22 01:37:31.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-6156 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 22 01:37:32.078: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 22 01:37:32.078: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 22 01:37:32.078: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 22 01:37:32.083: INFO: Found 2 stateful pods, waiting for 3
Aug 22 01:37:42.116: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 01:37:42.116: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 01:37:42.116: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 22 01:37:42.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-6156 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 01:37:42.520: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 01:37:42.520: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 01:37:42.520: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 22 01:37:42.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-6156 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 01:37:42.803: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 01:37:42.803: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 01:37:42.803: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 22 01:37:42.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-6156 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 01:37:43.220: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 01:37:43.220: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 01:37:43.220: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 22 01:37:43.220: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 01:37:43.223: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 22 01:37:53.232: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 01:37:53.232: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 01:37:53.232: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 01:37:53.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999327s
Aug 22 01:37:54.523: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.727535905s
Aug 22 01:37:55.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.721332831s
Aug 22 01:37:56.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.716757692s
Aug 22 01:37:57.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.711005714s
Aug 22 01:37:58.644: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.707038687s
Aug 22 01:38:00.714: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.599997706s
Aug 22 01:38:01.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.530294882s
Aug 22 01:38:02.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 524.483486ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6156
Aug 22 01:38:03.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-6156 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 22 01:38:04.079: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 22 01:38:04.079: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 22 01:38:04.079: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 22 01:38:04.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-6156 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 22 01:38:04.417: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 22 01:38:04.417: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 22 01:38:04.417: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 22 01:38:04.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-6156 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 22 01:38:04.764: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 22 01:38:04.764: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 22 01:38:04.764: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 22 01:38:04.764: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 22 01:38:24.780: INFO: Deleting all statefulset in ns statefulset-6156
Aug 22 01:38:24.783: INFO: Scaling statefulset ss to 0
Aug 22 01:38:24.789: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 01:38:24.790: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:38:24.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6156" for this suite.
Aug 22 01:38:30.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:38:30.938: INFO: namespace statefulset-6156 deletion completed in 6.125267223s

• [SLOW TEST:89.912 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:38:30.944: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 01:38:31.055: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 22 01:38:31.066: INFO: Number of nodes with available pods: 0
Aug 22 01:38:31.066: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 22 01:38:31.088: INFO: Number of nodes with available pods: 0
Aug 22 01:38:31.088: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:33.745: INFO: Number of nodes with available pods: 0
Aug 22 01:38:33.745: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:34.094: INFO: Number of nodes with available pods: 0
Aug 22 01:38:34.094: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:35.092: INFO: Number of nodes with available pods: 0
Aug 22 01:38:35.092: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:36.093: INFO: Number of nodes with available pods: 1
Aug 22 01:38:36.093: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 22 01:38:36.117: INFO: Number of nodes with available pods: 0
Aug 22 01:38:36.117: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 22 01:38:36.155: INFO: Number of nodes with available pods: 0
Aug 22 01:38:36.155: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:37.160: INFO: Number of nodes with available pods: 0
Aug 22 01:38:37.160: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:38.158: INFO: Number of nodes with available pods: 0
Aug 22 01:38:38.158: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:39.160: INFO: Number of nodes with available pods: 0
Aug 22 01:38:39.161: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:40.158: INFO: Number of nodes with available pods: 0
Aug 22 01:38:40.158: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:41.159: INFO: Number of nodes with available pods: 0
Aug 22 01:38:41.159: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:42.158: INFO: Number of nodes with available pods: 0
Aug 22 01:38:42.158: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:43.160: INFO: Number of nodes with available pods: 0
Aug 22 01:38:43.160: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:44.158: INFO: Number of nodes with available pods: 0
Aug 22 01:38:44.158: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:45.161: INFO: Number of nodes with available pods: 0
Aug 22 01:38:45.161: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:46.182: INFO: Number of nodes with available pods: 0
Aug 22 01:38:46.182: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:47.181: INFO: Number of nodes with available pods: 0
Aug 22 01:38:47.181: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 01:38:48.159: INFO: Number of nodes with available pods: 1
Aug 22 01:38:48.159: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1135, will wait for the garbage collector to delete the pods
Aug 22 01:38:48.228: INFO: Deleting DaemonSet.extensions daemon-set took: 7.250038ms
Aug 22 01:38:48.729: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.255088ms
Aug 22 01:38:56.233: INFO: Number of nodes with available pods: 0
Aug 22 01:38:56.234: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 01:38:56.238: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1135/daemonsets","resourceVersion":"2513"},"items":null}

Aug 22 01:38:56.242: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1135/pods","resourceVersion":"2513"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:38:56.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1135" for this suite.
Aug 22 01:39:02.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:39:02.399: INFO: namespace daemonsets-1135 deletion completed in 6.122852418s

• [SLOW TEST:31.454 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:39:02.399: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 22 01:39:02.445: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8145,SelfLink:/api/v1/namespaces/watch-8145/configmaps/e2e-watch-test-watch-closed,UID:f7ecfb85-a11b-4039-8d2b-852dadb71fdd,ResourceVersion:2547,Generation:0,CreationTimestamp:2019-08-22 01:39:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 22 01:39:02.445: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8145,SelfLink:/api/v1/namespaces/watch-8145/configmaps/e2e-watch-test-watch-closed,UID:f7ecfb85-a11b-4039-8d2b-852dadb71fdd,ResourceVersion:2548,Generation:0,CreationTimestamp:2019-08-22 01:39:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 22 01:39:02.459: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8145,SelfLink:/api/v1/namespaces/watch-8145/configmaps/e2e-watch-test-watch-closed,UID:f7ecfb85-a11b-4039-8d2b-852dadb71fdd,ResourceVersion:2549,Generation:0,CreationTimestamp:2019-08-22 01:39:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 22 01:39:02.460: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8145,SelfLink:/api/v1/namespaces/watch-8145/configmaps/e2e-watch-test-watch-closed,UID:f7ecfb85-a11b-4039-8d2b-852dadb71fdd,ResourceVersion:2550,Generation:0,CreationTimestamp:2019-08-22 01:39:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:39:02.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8145" for this suite.
Aug 22 01:39:08.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:39:08.579: INFO: namespace watch-8145 deletion completed in 6.115795631s

• [SLOW TEST:6.180 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:39:08.580: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 22 01:39:12.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec pod-sharedvolume-6f7a8ec7-0220-4f29-b170-01188410df3a -c busybox-main-container --namespace=emptydir-2518 -- cat /usr/share/volumeshare/shareddata.txt'
Aug 22 01:39:12.967: INFO: stderr: ""
Aug 22 01:39:12.967: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:39:12.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2518" for this suite.
Aug 22 01:39:18.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:39:19.088: INFO: namespace emptydir-2518 deletion completed in 6.114926776s

• [SLOW TEST:10.508 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:39:19.089: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 22 01:39:19.125: INFO: Waiting up to 5m0s for pod "pod-274ff849-c62a-4087-aada-5fed82ddf1e0" in namespace "emptydir-4451" to be "success or failure"
Aug 22 01:39:19.129: INFO: Pod "pod-274ff849-c62a-4087-aada-5fed82ddf1e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025529ms
Aug 22 01:39:21.136: INFO: Pod "pod-274ff849-c62a-4087-aada-5fed82ddf1e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01025385s
STEP: Saw pod success
Aug 22 01:39:21.136: INFO: Pod "pod-274ff849-c62a-4087-aada-5fed82ddf1e0" satisfied condition "success or failure"
Aug 22 01:39:21.138: INFO: Trying to get logs from node melsayed-conformance3 pod pod-274ff849-c62a-4087-aada-5fed82ddf1e0 container test-container: <nil>
STEP: delete the pod
Aug 22 01:39:21.156: INFO: Waiting for pod pod-274ff849-c62a-4087-aada-5fed82ddf1e0 to disappear
Aug 22 01:39:21.158: INFO: Pod pod-274ff849-c62a-4087-aada-5fed82ddf1e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:39:21.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4451" for this suite.
Aug 22 01:39:27.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:39:27.282: INFO: namespace emptydir-4451 deletion completed in 6.121706196s

• [SLOW TEST:8.194 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:39:27.283: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 22 01:39:35.362: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:35.440: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:37.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:37.444: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:39.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:39.446: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:41.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:41.444: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:43.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:43.445: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:45.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:45.446: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:47.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:47.446: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:49.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:49.445: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:51.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:51.445: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:53.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:53.444: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:55.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:55.444: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 22 01:39:57.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 22 01:39:57.447: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:39:57.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9331" for this suite.
Aug 22 01:40:19.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:40:19.574: INFO: namespace container-lifecycle-hook-9331 deletion completed in 22.101738747s

• [SLOW TEST:52.291 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:40:19.574: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-ad2342ee-276b-4218-94e2-e6baf5afd9ba
STEP: Creating a pod to test consume secrets
Aug 22 01:40:19.609: INFO: Waiting up to 5m0s for pod "pod-secrets-196ddc91-d83b-4dc0-a190-01b7ecf37982" in namespace "secrets-9373" to be "success or failure"
Aug 22 01:40:19.612: INFO: Pod "pod-secrets-196ddc91-d83b-4dc0-a190-01b7ecf37982": Phase="Pending", Reason="", readiness=false. Elapsed: 3.053858ms
Aug 22 01:40:21.617: INFO: Pod "pod-secrets-196ddc91-d83b-4dc0-a190-01b7ecf37982": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007697946s
STEP: Saw pod success
Aug 22 01:40:21.617: INFO: Pod "pod-secrets-196ddc91-d83b-4dc0-a190-01b7ecf37982" satisfied condition "success or failure"
Aug 22 01:40:21.622: INFO: Trying to get logs from node melsayed-conformance3 pod pod-secrets-196ddc91-d83b-4dc0-a190-01b7ecf37982 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 01:40:21.648: INFO: Waiting for pod pod-secrets-196ddc91-d83b-4dc0-a190-01b7ecf37982 to disappear
Aug 22 01:40:21.653: INFO: Pod pod-secrets-196ddc91-d83b-4dc0-a190-01b7ecf37982 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:40:21.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9373" for this suite.
Aug 22 01:40:27.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:40:27.839: INFO: namespace secrets-9373 deletion completed in 6.183637053s

• [SLOW TEST:8.265 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:40:27.840: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Aug 22 01:40:27.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 api-versions'
Aug 22 01:40:27.990: INFO: stderr: ""
Aug 22 01:40:27.990: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.cattle.io/v3\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:40:27.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8637" for this suite.
Aug 22 01:40:36.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:40:36.094: INFO: namespace kubectl-8637 deletion completed in 8.100079913s

• [SLOW TEST:8.255 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:40:36.097: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 22 01:40:36.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7038'
Aug 22 01:40:36.321: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 22 01:40:36.321: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Aug 22 01:40:36.337: INFO: scanned /root for discovery docs: <nil>
Aug 22 01:40:36.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7038'
Aug 22 01:40:52.205: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 22 01:40:52.205: INFO: stdout: "Created e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080\nScaling up e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug 22 01:40:52.205: INFO: stdout: "Created e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080\nScaling up e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug 22 01:40:52.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7038'
Aug 22 01:40:52.351: INFO: stderr: ""
Aug 22 01:40:52.351: INFO: stdout: "e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080-x6lvs "
Aug 22 01:40:52.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080-x6lvs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7038'
Aug 22 01:40:52.475: INFO: stderr: ""
Aug 22 01:40:52.475: INFO: stdout: "true"
Aug 22 01:40:52.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080-x6lvs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7038'
Aug 22 01:40:52.646: INFO: stderr: ""
Aug 22 01:40:52.646: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug 22 01:40:52.646: INFO: e2e-test-nginx-rc-35356e96835ca5f24ad32332fc4b7080-x6lvs is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Aug 22 01:40:52.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete rc e2e-test-nginx-rc --namespace=kubectl-7038'
Aug 22 01:40:52.775: INFO: stderr: ""
Aug 22 01:40:52.775: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:40:52.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7038" for this suite.
Aug 22 01:41:00.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:41:00.894: INFO: namespace kubectl-7038 deletion completed in 8.114616556s

• [SLOW TEST:24.797 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:41:00.895: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:41:28.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8429" for this suite.
Aug 22 01:41:34.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:41:34.232: INFO: namespace namespaces-8429 deletion completed in 6.128891692s
STEP: Destroying namespace "nsdeletetest-6840" for this suite.
Aug 22 01:41:34.236: INFO: Namespace nsdeletetest-6840 was already deleted
STEP: Destroying namespace "nsdeletetest-6501" for this suite.
Aug 22 01:41:40.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:41:40.481: INFO: namespace nsdeletetest-6501 deletion completed in 6.244629865s

• [SLOW TEST:39.587 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:41:40.487: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-8d3a1310-791b-4b83-a915-d2cceb320c73
STEP: Creating a pod to test consume configMaps
Aug 22 01:41:40.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-74fd9f55-082a-4734-b897-a876c1099a6f" in namespace "projected-935" to be "success or failure"
Aug 22 01:41:40.561: INFO: Pod "pod-projected-configmaps-74fd9f55-082a-4734-b897-a876c1099a6f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.764556ms
Aug 22 01:41:42.571: INFO: Pod "pod-projected-configmaps-74fd9f55-082a-4734-b897-a876c1099a6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026398209s
STEP: Saw pod success
Aug 22 01:41:42.571: INFO: Pod "pod-projected-configmaps-74fd9f55-082a-4734-b897-a876c1099a6f" satisfied condition "success or failure"
Aug 22 01:41:42.575: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-configmaps-74fd9f55-082a-4734-b897-a876c1099a6f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 01:41:42.604: INFO: Waiting for pod pod-projected-configmaps-74fd9f55-082a-4734-b897-a876c1099a6f to disappear
Aug 22 01:41:42.606: INFO: Pod pod-projected-configmaps-74fd9f55-082a-4734-b897-a876c1099a6f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:41:42.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-935" for this suite.
Aug 22 01:41:48.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:41:48.741: INFO: namespace projected-935 deletion completed in 6.131950363s

• [SLOW TEST:8.254 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:41:48.745: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2348
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 22 01:41:48.782: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 22 01:42:12.905: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.2.5 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2348 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 01:42:12.905: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 01:42:14.106: INFO: Found all expected endpoints: [netserver-0]
Aug 22 01:42:14.111: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.0.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2348 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 01:42:14.111: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 01:42:15.342: INFO: Found all expected endpoints: [netserver-1]
Aug 22 01:42:15.347: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.1.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2348 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 01:42:15.347: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 01:42:16.545: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:42:16.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2348" for this suite.
Aug 22 01:42:40.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:42:40.247: INFO: namespace pod-network-test-2348 deletion completed in 22.148906788s

• [SLOW TEST:51.503 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:42:40.250: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-bafcb14f-4085-4117-a61e-d927949f1ebc
STEP: Creating a pod to test consume configMaps
Aug 22 01:42:40.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-d433d08f-3afe-4528-a3e2-fcebc2347139" in namespace "configmap-1762" to be "success or failure"
Aug 22 01:42:40.314: INFO: Pod "pod-configmaps-d433d08f-3afe-4528-a3e2-fcebc2347139": Phase="Pending", Reason="", readiness=false. Elapsed: 4.535365ms
Aug 22 01:42:42.318: INFO: Pod "pod-configmaps-d433d08f-3afe-4528-a3e2-fcebc2347139": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008904164s
Aug 22 01:42:44.322: INFO: Pod "pod-configmaps-d433d08f-3afe-4528-a3e2-fcebc2347139": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01275579s
STEP: Saw pod success
Aug 22 01:42:44.322: INFO: Pod "pod-configmaps-d433d08f-3afe-4528-a3e2-fcebc2347139" satisfied condition "success or failure"
Aug 22 01:42:44.325: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-d433d08f-3afe-4528-a3e2-fcebc2347139 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 01:42:44.357: INFO: Waiting for pod pod-configmaps-d433d08f-3afe-4528-a3e2-fcebc2347139 to disappear
Aug 22 01:42:44.362: INFO: Pod pod-configmaps-d433d08f-3afe-4528-a3e2-fcebc2347139 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:42:44.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1762" for this suite.
Aug 22 01:42:50.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:42:50.465: INFO: namespace configmap-1762 deletion completed in 6.100224636s

• [SLOW TEST:10.215 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:42:50.466: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 22 01:42:50.512: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2811,SelfLink:/api/v1/namespaces/watch-2811/configmaps/e2e-watch-test-label-changed,UID:7afa811b-69d5-4298-8f05-bee574b018a1,ResourceVersion:3392,Generation:0,CreationTimestamp:2019-08-22 01:42:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 22 01:42:50.512: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2811,SelfLink:/api/v1/namespaces/watch-2811/configmaps/e2e-watch-test-label-changed,UID:7afa811b-69d5-4298-8f05-bee574b018a1,ResourceVersion:3393,Generation:0,CreationTimestamp:2019-08-22 01:42:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 22 01:42:50.513: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2811,SelfLink:/api/v1/namespaces/watch-2811/configmaps/e2e-watch-test-label-changed,UID:7afa811b-69d5-4298-8f05-bee574b018a1,ResourceVersion:3394,Generation:0,CreationTimestamp:2019-08-22 01:42:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 22 01:43:00.621: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2811,SelfLink:/api/v1/namespaces/watch-2811/configmaps/e2e-watch-test-label-changed,UID:7afa811b-69d5-4298-8f05-bee574b018a1,ResourceVersion:3415,Generation:0,CreationTimestamp:2019-08-22 01:42:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 22 01:43:00.622: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2811,SelfLink:/api/v1/namespaces/watch-2811/configmaps/e2e-watch-test-label-changed,UID:7afa811b-69d5-4298-8f05-bee574b018a1,ResourceVersion:3416,Generation:0,CreationTimestamp:2019-08-22 01:42:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 22 01:43:00.622: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2811,SelfLink:/api/v1/namespaces/watch-2811/configmaps/e2e-watch-test-label-changed,UID:7afa811b-69d5-4298-8f05-bee574b018a1,ResourceVersion:3417,Generation:0,CreationTimestamp:2019-08-22 01:42:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:43:00.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2811" for this suite.
Aug 22 01:43:06.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:43:06.761: INFO: namespace watch-2811 deletion completed in 6.130386486s

• [SLOW TEST:16.296 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:43:06.761: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-9a4b4497-d0fa-4698-8280-afdf200a05e1
STEP: Creating a pod to test consume secrets
Aug 22 01:43:06.808: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2438fc37-fe31-4cda-b79a-90a5f1494a5d" in namespace "projected-2574" to be "success or failure"
Aug 22 01:43:06.812: INFO: Pod "pod-projected-secrets-2438fc37-fe31-4cda-b79a-90a5f1494a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.671309ms
Aug 22 01:43:08.816: INFO: Pod "pod-projected-secrets-2438fc37-fe31-4cda-b79a-90a5f1494a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007859973s
Aug 22 01:43:10.821: INFO: Pod "pod-projected-secrets-2438fc37-fe31-4cda-b79a-90a5f1494a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012599214s
STEP: Saw pod success
Aug 22 01:43:10.821: INFO: Pod "pod-projected-secrets-2438fc37-fe31-4cda-b79a-90a5f1494a5d" satisfied condition "success or failure"
Aug 22 01:43:10.825: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-secrets-2438fc37-fe31-4cda-b79a-90a5f1494a5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 01:43:10.851: INFO: Waiting for pod pod-projected-secrets-2438fc37-fe31-4cda-b79a-90a5f1494a5d to disappear
Aug 22 01:43:10.856: INFO: Pod pod-projected-secrets-2438fc37-fe31-4cda-b79a-90a5f1494a5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:43:10.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2574" for this suite.
Aug 22 01:43:16.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:43:16.982: INFO: namespace projected-2574 deletion completed in 6.119794773s

• [SLOW TEST:10.221 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:43:16.984: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 01:43:17.016: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:43:21.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7009" for this suite.
Aug 22 01:43:59.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:43:59.215: INFO: namespace pods-7009 deletion completed in 38.145209373s

• [SLOW TEST:42.231 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:43:59.218: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 22 01:44:07.797: INFO: Successfully updated pod "pod-update-5642a187-d41f-4fa1-9d5e-c107a2718a29"
STEP: verifying the updated pod is in kubernetes
Aug 22 01:44:07.876: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:44:07.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6163" for this suite.
Aug 22 01:44:29.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:44:30.016: INFO: namespace pods-6163 deletion completed in 22.133806087s

• [SLOW TEST:30.799 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:44:30.018: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-88d93014-a560-49f6-8bfc-56f3e8406ee3
STEP: Creating configMap with name cm-test-opt-upd-17b6b516-7ee9-4524-8641-f192ace25fef
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-88d93014-a560-49f6-8bfc-56f3e8406ee3
STEP: Updating configmap cm-test-opt-upd-17b6b516-7ee9-4524-8641-f192ace25fef
STEP: Creating configMap with name cm-test-opt-create-7e9080da-b8c6-407d-9735-12122f44a29e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:44:34.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3857" for this suite.
Aug 22 01:44:56.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:44:56.332: INFO: namespace projected-3857 deletion completed in 22.123044244s

• [SLOW TEST:26.314 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:44:56.332: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 01:44:56.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e4a5684-b1f5-4fc1-b0d5-4434fdf86f2a" in namespace "projected-3705" to be "success or failure"
Aug 22 01:44:56.385: INFO: Pod "downwardapi-volume-1e4a5684-b1f5-4fc1-b0d5-4434fdf86f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.647459ms
Aug 22 01:44:58.389: INFO: Pod "downwardapi-volume-1e4a5684-b1f5-4fc1-b0d5-4434fdf86f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020265925s
Aug 22 01:45:00.392: INFO: Pod "downwardapi-volume-1e4a5684-b1f5-4fc1-b0d5-4434fdf86f2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023612018s
STEP: Saw pod success
Aug 22 01:45:00.392: INFO: Pod "downwardapi-volume-1e4a5684-b1f5-4fc1-b0d5-4434fdf86f2a" satisfied condition "success or failure"
Aug 22 01:45:00.394: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-1e4a5684-b1f5-4fc1-b0d5-4434fdf86f2a container client-container: <nil>
STEP: delete the pod
Aug 22 01:45:00.413: INFO: Waiting for pod downwardapi-volume-1e4a5684-b1f5-4fc1-b0d5-4434fdf86f2a to disappear
Aug 22 01:45:00.417: INFO: Pod downwardapi-volume-1e4a5684-b1f5-4fc1-b0d5-4434fdf86f2a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:45:00.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3705" for this suite.
Aug 22 01:45:06.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:45:06.554: INFO: namespace projected-3705 deletion completed in 6.133550908s

• [SLOW TEST:10.222 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:45:06.555: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 22 01:45:06.594: INFO: Waiting up to 5m0s for pod "downward-api-d22a8a6f-1ad2-4df5-a7d3-eff1eb346f36" in namespace "downward-api-4752" to be "success or failure"
Aug 22 01:45:06.601: INFO: Pod "downward-api-d22a8a6f-1ad2-4df5-a7d3-eff1eb346f36": Phase="Pending", Reason="", readiness=false. Elapsed: 6.763795ms
Aug 22 01:45:09.358: INFO: Pod "downward-api-d22a8a6f-1ad2-4df5-a7d3-eff1eb346f36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.763902303s
Aug 22 01:45:11.362: INFO: Pod "downward-api-d22a8a6f-1ad2-4df5-a7d3-eff1eb346f36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.768028704s
STEP: Saw pod success
Aug 22 01:45:11.362: INFO: Pod "downward-api-d22a8a6f-1ad2-4df5-a7d3-eff1eb346f36" satisfied condition "success or failure"
Aug 22 01:45:11.365: INFO: Trying to get logs from node melsayed-conformance3 pod downward-api-d22a8a6f-1ad2-4df5-a7d3-eff1eb346f36 container dapi-container: <nil>
STEP: delete the pod
Aug 22 01:45:11.391: INFO: Waiting for pod downward-api-d22a8a6f-1ad2-4df5-a7d3-eff1eb346f36 to disappear
Aug 22 01:45:11.393: INFO: Pod downward-api-d22a8a6f-1ad2-4df5-a7d3-eff1eb346f36 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:45:11.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4752" for this suite.
Aug 22 01:45:17.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:45:17.521: INFO: namespace downward-api-4752 deletion completed in 6.125034586s

• [SLOW TEST:10.967 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:45:17.522: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9365
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 22 01:45:17.580: INFO: Found 0 stateful pods, waiting for 3
Aug 22 01:45:27.586: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 01:45:27.586: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 01:45:27.586: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 01:45:27.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-9365 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 01:45:27.959: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 01:45:27.959: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 01:45:27.959: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 22 01:45:38.549: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 22 01:45:48.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-9365 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 22 01:45:48.901: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 22 01:45:48.901: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 22 01:45:48.901: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 22 01:45:58.925: INFO: Waiting for StatefulSet statefulset-9365/ss2 to complete update
Aug 22 01:45:58.925: INFO: Waiting for Pod statefulset-9365/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 22 01:45:58.925: INFO: Waiting for Pod statefulset-9365/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 22 01:46:08.933: INFO: Waiting for StatefulSet statefulset-9365/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 22 01:46:18.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-9365 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 01:46:19.277: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 01:46:19.277: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 01:46:19.277: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 22 01:46:29.313: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 22 01:46:39.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-9365 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 22 01:46:39.755: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 22 01:46:39.755: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 22 01:46:39.755: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 22 01:46:59.776: INFO: Waiting for StatefulSet statefulset-9365/ss2 to complete update
Aug 22 01:46:59.776: INFO: Waiting for Pod statefulset-9365/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 22 01:47:09.786: INFO: Deleting all statefulset in ns statefulset-9365
Aug 22 01:47:09.790: INFO: Scaling statefulset ss2 to 0
Aug 22 01:47:39.807: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 01:47:39.810: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:47:39.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9365" for this suite.
Aug 22 01:47:45.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:47:45.928: INFO: namespace statefulset-9365 deletion completed in 6.102748329s

• [SLOW TEST:148.406 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:47:45.928: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-gk8q
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 01:47:45.967: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gk8q" in namespace "subpath-7243" to be "success or failure"
Aug 22 01:47:45.974: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.599638ms
Aug 22 01:47:47.978: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011258249s
Aug 22 01:47:49.982: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 4.014809987s
Aug 22 01:47:51.987: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 6.020052412s
Aug 22 01:47:53.991: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 8.024108384s
Aug 22 01:47:55.995: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 10.027550553s
Aug 22 01:47:57.999: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 12.031469776s
Aug 22 01:48:00.004: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 14.03653861s
Aug 22 01:48:02.008: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 16.040630987s
Aug 22 01:48:04.012: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 18.044758309s
Aug 22 01:48:06.017: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 20.049312613s
Aug 22 01:48:08.020: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Running", Reason="", readiness=true. Elapsed: 22.053219182s
Aug 22 01:48:10.025: INFO: Pod "pod-subpath-test-downwardapi-gk8q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.057528211s
STEP: Saw pod success
Aug 22 01:48:10.025: INFO: Pod "pod-subpath-test-downwardapi-gk8q" satisfied condition "success or failure"
Aug 22 01:48:10.028: INFO: Trying to get logs from node melsayed-conformance3 pod pod-subpath-test-downwardapi-gk8q container test-container-subpath-downwardapi-gk8q: <nil>
STEP: delete the pod
Aug 22 01:48:10.053: INFO: Waiting for pod pod-subpath-test-downwardapi-gk8q to disappear
Aug 22 01:48:10.057: INFO: Pod pod-subpath-test-downwardapi-gk8q no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-gk8q
Aug 22 01:48:10.057: INFO: Deleting pod "pod-subpath-test-downwardapi-gk8q" in namespace "subpath-7243"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:48:10.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7243" for this suite.
Aug 22 01:48:16.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:48:16.203: INFO: namespace subpath-7243 deletion completed in 6.140241014s

• [SLOW TEST:30.274 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:48:16.204: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 01:48:16.239: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5b29373-1193-4aca-9fd4-fc762982a9ef" in namespace "downward-api-2747" to be "success or failure"
Aug 22 01:48:16.243: INFO: Pod "downwardapi-volume-c5b29373-1193-4aca-9fd4-fc762982a9ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.973093ms
Aug 22 01:48:18.248: INFO: Pod "downwardapi-volume-c5b29373-1193-4aca-9fd4-fc762982a9ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008883884s
STEP: Saw pod success
Aug 22 01:48:18.249: INFO: Pod "downwardapi-volume-c5b29373-1193-4aca-9fd4-fc762982a9ef" satisfied condition "success or failure"
Aug 22 01:48:18.253: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-c5b29373-1193-4aca-9fd4-fc762982a9ef container client-container: <nil>
STEP: delete the pod
Aug 22 01:48:18.269: INFO: Waiting for pod downwardapi-volume-c5b29373-1193-4aca-9fd4-fc762982a9ef to disappear
Aug 22 01:48:18.282: INFO: Pod downwardapi-volume-c5b29373-1193-4aca-9fd4-fc762982a9ef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:48:18.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2747" for this suite.
Aug 22 01:48:24.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:48:24.415: INFO: namespace downward-api-2747 deletion completed in 6.12964865s

• [SLOW TEST:8.212 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:48:24.421: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:48:24.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6989" for this suite.
Aug 22 01:48:30.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:48:30.626: INFO: namespace kubelet-test-6989 deletion completed in 6.139306707s

• [SLOW TEST:6.206 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:48:30.635: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 01:48:30.690: INFO: (0) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 23.965069ms)
Aug 22 01:48:30.693: INFO: (1) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.392127ms)
Aug 22 01:48:30.697: INFO: (2) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.260989ms)
Aug 22 01:48:30.703: INFO: (3) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.158422ms)
Aug 22 01:48:30.708: INFO: (4) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.649798ms)
Aug 22 01:48:30.712: INFO: (5) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.501413ms)
Aug 22 01:48:30.722: INFO: (6) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 10.039989ms)
Aug 22 01:48:30.725: INFO: (7) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.344167ms)
Aug 22 01:48:30.729: INFO: (8) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.435152ms)
Aug 22 01:48:30.732: INFO: (9) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.117748ms)
Aug 22 01:48:30.735: INFO: (10) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.883171ms)
Aug 22 01:48:30.738: INFO: (11) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.67941ms)
Aug 22 01:48:30.740: INFO: (12) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.57825ms)
Aug 22 01:48:30.743: INFO: (13) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.877868ms)
Aug 22 01:48:30.747: INFO: (14) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.605783ms)
Aug 22 01:48:30.752: INFO: (15) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.008716ms)
Aug 22 01:48:30.756: INFO: (16) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.503531ms)
Aug 22 01:48:30.759: INFO: (17) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.027923ms)
Aug 22 01:48:30.763: INFO: (18) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.33484ms)
Aug 22 01:48:30.768: INFO: (19) /api/v1/nodes/melsayed-conformance1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.300684ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:48:30.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-992" for this suite.
Aug 22 01:48:36.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:48:36.885: INFO: namespace proxy-992 deletion completed in 6.111722805s

• [SLOW TEST:6.250 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:48:36.886: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 22 01:48:37.187: INFO: Pod name wrapped-volume-race-9dc47069-58b7-4c6a-9690-d56ec12df4e3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9dc47069-58b7-4c6a-9690-d56ec12df4e3 in namespace emptydir-wrapper-4843, will wait for the garbage collector to delete the pods
Aug 22 01:48:53.318: INFO: Deleting ReplicationController wrapped-volume-race-9dc47069-58b7-4c6a-9690-d56ec12df4e3 took: 7.200194ms
Aug 22 01:48:53.918: INFO: Terminating ReplicationController wrapped-volume-race-9dc47069-58b7-4c6a-9690-d56ec12df4e3 pods took: 600.399577ms
STEP: Creating RC which spawns configmap-volume pods
Aug 22 01:49:36.335: INFO: Pod name wrapped-volume-race-885e9bfa-9d89-4d41-adc4-3489add693e3: Found 0 pods out of 5
Aug 22 01:49:41.342: INFO: Pod name wrapped-volume-race-885e9bfa-9d89-4d41-adc4-3489add693e3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-885e9bfa-9d89-4d41-adc4-3489add693e3 in namespace emptydir-wrapper-4843, will wait for the garbage collector to delete the pods
Aug 22 01:49:53.439: INFO: Deleting ReplicationController wrapped-volume-race-885e9bfa-9d89-4d41-adc4-3489add693e3 took: 9.561515ms
Aug 22 01:49:54.040: INFO: Terminating ReplicationController wrapped-volume-race-885e9bfa-9d89-4d41-adc4-3489add693e3 pods took: 600.351266ms
STEP: Creating RC which spawns configmap-volume pods
Aug 22 01:50:36.664: INFO: Pod name wrapped-volume-race-1f0ff93f-1229-4555-a8b8-d3e983f951f8: Found 0 pods out of 5
Aug 22 01:50:41.672: INFO: Pod name wrapped-volume-race-1f0ff93f-1229-4555-a8b8-d3e983f951f8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1f0ff93f-1229-4555-a8b8-d3e983f951f8 in namespace emptydir-wrapper-4843, will wait for the garbage collector to delete the pods
Aug 22 01:50:53.799: INFO: Deleting ReplicationController wrapped-volume-race-1f0ff93f-1229-4555-a8b8-d3e983f951f8 took: 15.083861ms
Aug 22 01:50:54.500: INFO: Terminating ReplicationController wrapped-volume-race-1f0ff93f-1229-4555-a8b8-d3e983f951f8 pods took: 700.456744ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:51:36.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4843" for this suite.
Aug 22 01:51:42.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:51:42.958: INFO: namespace emptydir-wrapper-4843 deletion completed in 6.112589347s

• [SLOW TEST:186.073 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:51:42.962: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Aug 22 01:51:42.992: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 22 01:51:42.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-9147'
Aug 22 01:51:43.416: INFO: stderr: ""
Aug 22 01:51:43.416: INFO: stdout: "service/redis-slave created\n"
Aug 22 01:51:43.416: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 22 01:51:43.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-9147'
Aug 22 01:51:43.648: INFO: stderr: ""
Aug 22 01:51:43.648: INFO: stdout: "service/redis-master created\n"
Aug 22 01:51:43.648: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 22 01:51:43.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-9147'
Aug 22 01:51:43.899: INFO: stderr: ""
Aug 22 01:51:43.899: INFO: stdout: "service/frontend created\n"
Aug 22 01:51:43.899: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 22 01:51:43.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-9147'
Aug 22 01:51:44.171: INFO: stderr: ""
Aug 22 01:51:44.171: INFO: stdout: "deployment.apps/frontend created\n"
Aug 22 01:51:44.171: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 22 01:51:44.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-9147'
Aug 22 01:51:44.393: INFO: stderr: ""
Aug 22 01:51:44.393: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 22 01:51:44.393: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 22 01:51:44.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-9147'
Aug 22 01:51:44.642: INFO: stderr: ""
Aug 22 01:51:44.642: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 22 01:51:44.642: INFO: Waiting for all frontend pods to be Running.
Aug 22 01:51:59.694: INFO: Waiting for frontend to serve content.
Aug 22 01:52:04.720: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Aug 22 01:52:09.737: INFO: Trying to add a new entry to the guestbook.
Aug 22 01:52:09.751: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 22 01:52:09.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-9147'
Aug 22 01:52:09.913: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 01:52:09.913: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 01:52:09.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-9147'
Aug 22 01:52:10.047: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 01:52:10.047: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 01:52:10.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-9147'
Aug 22 01:52:10.182: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 01:52:10.182: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 01:52:10.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-9147'
Aug 22 01:52:10.313: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 01:52:10.313: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 01:52:10.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-9147'
Aug 22 01:52:10.422: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 01:52:10.422: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 22 01:52:10.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-9147'
Aug 22 01:52:10.541: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 01:52:10.541: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:52:10.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9147" for this suite.
Aug 22 01:52:56.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:52:56.636: INFO: namespace kubectl-9147 deletion completed in 46.090615461s

• [SLOW TEST:73.675 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:52:56.637: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 22 01:52:56.730: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Aug 22 01:52:57.557: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 22 01:52:59.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 01:53:01.620: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 01:53:03.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 01:53:05.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 01:53:07.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702035577, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 01:53:10.766: INFO: Waited 1.132886186s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:53:11.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8947" for this suite.
Aug 22 01:53:17.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:53:17.608: INFO: namespace aggregator-8947 deletion completed in 6.200856979s

• [SLOW TEST:20.971 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:53:17.610: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 01:53:17.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-4969'
Aug 22 01:53:18.026: INFO: stderr: ""
Aug 22 01:53:18.026: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 22 01:53:18.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-4969'
Aug 22 01:53:18.404: INFO: stderr: ""
Aug 22 01:53:18.404: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 22 01:53:19.410: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 01:53:19.410: INFO: Found 0 / 1
Aug 22 01:53:20.407: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 01:53:20.407: INFO: Found 0 / 1
Aug 22 01:53:21.408: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 01:53:21.408: INFO: Found 0 / 1
Aug 22 01:53:22.410: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 01:53:22.410: INFO: Found 1 / 1
Aug 22 01:53:22.411: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 22 01:53:22.415: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 01:53:22.415: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 22 01:53:22.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 describe pod redis-master-62jxq --namespace=kubectl-4969'
Aug 22 01:53:22.549: INFO: stderr: ""
Aug 22 01:53:22.549: INFO: stdout: "Name:           redis-master-62jxq\nNamespace:      kubectl-4969\nPriority:       0\nNode:           melsayed-conformance3/165.22.32.172\nStart Time:     Thu, 22 Aug 2019 01:53:18 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 10.42.1.30/32\nStatus:         Running\nIP:             10.42.1.30\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://0fdc3968e4c64a53b383626b5d8a7340a729c69ead1fcdcae8423772f09f891d\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 22 Aug 2019 01:53:21 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n9flq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-n9flq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-n9flq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                            Message\n  ----    ------     ----  ----                            -------\n  Normal  Scheduled  4s    default-scheduler               Successfully assigned kubectl-4969/redis-master-62jxq to melsayed-conformance3\n  Normal  Pulling    3s    kubelet, melsayed-conformance3  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     2s    kubelet, melsayed-conformance3  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    2s    kubelet, melsayed-conformance3  Created container redis-master\n  Normal  Started    1s    kubelet, melsayed-conformance3  Started container redis-master\n"
Aug 22 01:53:22.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 describe rc redis-master --namespace=kubectl-4969'
Aug 22 01:53:22.664: INFO: stderr: ""
Aug 22 01:53:22.664: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4969\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-62jxq\n"
Aug 22 01:53:22.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 describe service redis-master --namespace=kubectl-4969'
Aug 22 01:53:22.765: INFO: stderr: ""
Aug 22 01:53:22.765: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4969\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.43.200.161\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.42.1.30:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 22 01:53:22.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 describe node melsayed-conformance1'
Aug 22 01:53:22.877: INFO: stderr: ""
Aug 22 01:53:22.877: INFO: stdout: "Name:               melsayed-conformance1\nRoles:              controlplane,etcd,worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cattle.io/creator=norman\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=melsayed-conformance1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/controlplane=true\n                    node-role.kubernetes.io/etcd=true\n                    node-role.kubernetes.io/worker=true\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"5e:0d:68:16:98:84\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 167.71.96.182\n                    node.alpha.kubernetes.io/ttl: 0\n                    rke.cattle.io/external-ip: 167.71.96.182\n                    rke.cattle.io/internal-ip: 167.71.96.182\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 22 Aug 2019 01:27:16 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 22 Aug 2019 01:53:20 +0000   Thu, 22 Aug 2019 01:27:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 22 Aug 2019 01:53:20 +0000   Thu, 22 Aug 2019 01:27:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 22 Aug 2019 01:53:20 +0000   Thu, 22 Aug 2019 01:27:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 22 Aug 2019 01:53:20 +0000   Thu, 22 Aug 2019 01:27:46 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  167.71.96.182\n  Hostname:    melsayed-conformance1\nCapacity:\n cpu:                2\n ephemeral-storage:  60795880Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             2047992Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  56029482916\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             1945592Ki\n pods:               110\nSystem Info:\n Machine ID:                 311936e2303b034fe7ef70182235b8cb\n System UUID:                1F55970D-3AB5-429C-8567-33DDA568AD03\n Boot ID:                    31cf8d6a-7157-42e6-8ded-3b49e31e9f4c\n Kernel Version:             4.4.0-157-generic\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.8\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nPodCIDR:                     10.42.0.0/24\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  cattle-system              cattle-cluster-agent-55977bf459-5lfn6                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  cattle-system              cattle-node-agent-d2s4f                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  cattle-system              kube-api-auth-87zdm                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-b6mbk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  ingress-nginx              default-http-backend-8456b47798-h7zvx                      10m (0%)      10m (0%)    20Mi (1%)        20Mi (1%)      25m\n  ingress-nginx              nginx-ingress-controller-44rfm                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  kube-system                canal-lwlb2                                                250m (12%)    0 (0%)      0 (0%)           0 (0%)         25m\n  kube-system                coredns-5678df9bcc-74zgx                                   100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     25m\n  kube-system                coredns-autoscaler-57bc9c9bd-2dczs                         20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         25m\n  kube-system                metrics-server-dbcdc5bc9-pjqjt                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                380m (19%)  10m (0%)\n  memory             100Mi (5%)  190Mi (10%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age   From                               Message\n  ----    ------                   ----  ----                               -------\n  Normal  Starting                 26m   kubelet, melsayed-conformance1     Starting kubelet.\n  Normal  NodeHasSufficientMemory  26m   kubelet, melsayed-conformance1     Node melsayed-conformance1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    26m   kubelet, melsayed-conformance1     Node melsayed-conformance1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     26m   kubelet, melsayed-conformance1     Node melsayed-conformance1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  26m   kubelet, melsayed-conformance1     Updated Node Allocatable limit across pods\n  Normal  Starting                 26m   kube-proxy, melsayed-conformance1  Starting kube-proxy.\n  Normal  NodeReady                25m   kubelet, melsayed-conformance1     Node melsayed-conformance1 status is now: NodeReady\n"
Aug 22 01:53:22.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 describe namespace kubectl-4969'
Aug 22 01:53:22.995: INFO: stderr: ""
Aug 22 01:53:22.995: INFO: stdout: "Name:         kubectl-4969\nLabels:       e2e-framework=kubectl\n              e2e-run=52cae05f-a082-442b-b0e1-f1288668964a\nAnnotations:  cattle.io/status:\n                {\"Conditions\":[{\"Type\":\"ResourceQuotaInit\",\"Status\":\"True\",\"Message\":\"\",\"LastUpdateTime\":\"2019-08-22T01:53:18Z\"},{\"Type\":\"InitialRolesPopu...\n              lifecycle.cattle.io/create.namespace-auth: true\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:53:22.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4969" for this suite.
Aug 22 01:53:45.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:53:45.084: INFO: namespace kubectl-4969 deletion completed in 22.085561788s

• [SLOW TEST:27.474 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:53:45.085: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 01:53:45.120: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c49a8f9-40e8-4729-83b4-6e942801d161" in namespace "downward-api-8271" to be "success or failure"
Aug 22 01:53:45.125: INFO: Pod "downwardapi-volume-5c49a8f9-40e8-4729-83b4-6e942801d161": Phase="Pending", Reason="", readiness=false. Elapsed: 4.842982ms
Aug 22 01:53:47.129: INFO: Pod "downwardapi-volume-5c49a8f9-40e8-4729-83b4-6e942801d161": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008825286s
STEP: Saw pod success
Aug 22 01:53:47.129: INFO: Pod "downwardapi-volume-5c49a8f9-40e8-4729-83b4-6e942801d161" satisfied condition "success or failure"
Aug 22 01:53:47.132: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-5c49a8f9-40e8-4729-83b4-6e942801d161 container client-container: <nil>
STEP: delete the pod
Aug 22 01:53:47.157: INFO: Waiting for pod downwardapi-volume-5c49a8f9-40e8-4729-83b4-6e942801d161 to disappear
Aug 22 01:53:47.159: INFO: Pod downwardapi-volume-5c49a8f9-40e8-4729-83b4-6e942801d161 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:53:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8271" for this suite.
Aug 22 01:53:53.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:53:53.269: INFO: namespace downward-api-8271 deletion completed in 6.106531908s

• [SLOW TEST:8.183 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:53:53.270: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-c386dfba-13c8-4b05-804e-08374bff1778
STEP: Creating configMap with name cm-test-opt-upd-aa4aabf1-9ad7-4585-8440-9dd530c02102
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c386dfba-13c8-4b05-804e-08374bff1778
STEP: Updating configmap cm-test-opt-upd-aa4aabf1-9ad7-4585-8440-9dd530c02102
STEP: Creating configMap with name cm-test-opt-create-f8601ff7-7944-408a-91e5-b3c1ce9e00b0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:53:57.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6819" for this suite.
Aug 22 01:54:19.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:54:19.537: INFO: namespace configmap-6819 deletion completed in 22.126687685s

• [SLOW TEST:26.267 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:54:19.538: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 22 01:54:19.586: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:54:23.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1484" for this suite.
Aug 22 01:54:29.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:54:29.690: INFO: namespace init-container-1484 deletion completed in 6.10196763s

• [SLOW TEST:10.152 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:54:29.690: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 01:54:29.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0252b0c-9844-41c1-8a52-3d4c10a8b66e" in namespace "downward-api-4213" to be "success or failure"
Aug 22 01:54:29.809: INFO: Pod "downwardapi-volume-f0252b0c-9844-41c1-8a52-3d4c10a8b66e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.299014ms
Aug 22 01:54:31.813: INFO: Pod "downwardapi-volume-f0252b0c-9844-41c1-8a52-3d4c10a8b66e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018791223s
Aug 22 01:54:33.819: INFO: Pod "downwardapi-volume-f0252b0c-9844-41c1-8a52-3d4c10a8b66e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024275175s
STEP: Saw pod success
Aug 22 01:54:33.819: INFO: Pod "downwardapi-volume-f0252b0c-9844-41c1-8a52-3d4c10a8b66e" satisfied condition "success or failure"
Aug 22 01:54:33.822: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-f0252b0c-9844-41c1-8a52-3d4c10a8b66e container client-container: <nil>
STEP: delete the pod
Aug 22 01:54:33.859: INFO: Waiting for pod downwardapi-volume-f0252b0c-9844-41c1-8a52-3d4c10a8b66e to disappear
Aug 22 01:54:33.864: INFO: Pod downwardapi-volume-f0252b0c-9844-41c1-8a52-3d4c10a8b66e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:54:33.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4213" for this suite.
Aug 22 01:54:39.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:54:39.968: INFO: namespace downward-api-4213 deletion completed in 6.100718374s

• [SLOW TEST:10.278 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:54:39.970: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8561/configmap-test-a3244ceb-4b58-4440-9c82-d4ed52768d60
STEP: Creating a pod to test consume configMaps
Aug 22 01:54:40.037: INFO: Waiting up to 5m0s for pod "pod-configmaps-5955ba12-82b1-4268-808d-cdd76d465f91" in namespace "configmap-8561" to be "success or failure"
Aug 22 01:54:40.044: INFO: Pod "pod-configmaps-5955ba12-82b1-4268-808d-cdd76d465f91": Phase="Pending", Reason="", readiness=false. Elapsed: 6.551251ms
Aug 22 01:54:42.048: INFO: Pod "pod-configmaps-5955ba12-82b1-4268-808d-cdd76d465f91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01067481s
Aug 22 01:54:44.054: INFO: Pod "pod-configmaps-5955ba12-82b1-4268-808d-cdd76d465f91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016839565s
STEP: Saw pod success
Aug 22 01:54:44.055: INFO: Pod "pod-configmaps-5955ba12-82b1-4268-808d-cdd76d465f91" satisfied condition "success or failure"
Aug 22 01:54:44.059: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-5955ba12-82b1-4268-808d-cdd76d465f91 container env-test: <nil>
STEP: delete the pod
Aug 22 01:54:44.084: INFO: Waiting for pod pod-configmaps-5955ba12-82b1-4268-808d-cdd76d465f91 to disappear
Aug 22 01:54:44.086: INFO: Pod pod-configmaps-5955ba12-82b1-4268-808d-cdd76d465f91 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:54:44.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8561" for this suite.
Aug 22 01:54:50.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:54:50.214: INFO: namespace configmap-8561 deletion completed in 6.124253082s

• [SLOW TEST:10.244 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:54:50.214: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:54:50.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-721" for this suite.
Aug 22 01:54:56.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:54:56.354: INFO: namespace services-721 deletion completed in 6.10515135s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.140 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:54:56.357: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 01:54:56.446: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:54:57.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7755" for this suite.
Aug 22 01:55:03.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:55:03.611: INFO: namespace custom-resource-definition-7755 deletion completed in 6.094092641s

• [SLOW TEST:7.255 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:55:03.611: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 22 01:55:07.669: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-e9d3b610-7b07-43d4-ab22-0f82a30abed9,GenerateName:,Namespace:events-6931,SelfLink:/api/v1/namespaces/events-6931/pods/send-events-e9d3b610-7b07-43d4-ab22-0f82a30abed9,UID:f27b04b0-d751-4b4a-b315-a62cc8f36d59,ResourceVersion:6675,Generation:0,CreationTimestamp:2019-08-22 01:55:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 646217180,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.1.36/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mcwmm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mcwmm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-mcwmm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c73dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c73de0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 01:55:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 01:55:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 01:55:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 01:55:03 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:10.42.1.36,StartTime:2019-08-22 01:55:03 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-22 01:55:06 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://01b49b8547c148ec36f717922ef7db2e11cb3433548867433df216276b094064}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug 22 01:55:09.677: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 22 01:55:11.682: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:55:11.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6931" for this suite.
Aug 22 01:55:49.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:55:49.814: INFO: namespace events-6931 deletion completed in 38.121206686s

• [SLOW TEST:46.203 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:55:49.815: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Aug 22 01:55:49.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-9419'
Aug 22 01:55:50.161: INFO: stderr: ""
Aug 22 01:55:50.161: INFO: stdout: "pod/pause created\n"
Aug 22 01:55:50.161: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 22 01:55:50.161: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9419" to be "running and ready"
Aug 22 01:55:50.172: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.973829ms
Aug 22 01:55:52.177: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.015847348s
Aug 22 01:55:52.177: INFO: Pod "pause" satisfied condition "running and ready"
Aug 22 01:55:52.177: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 22 01:55:52.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 label pods pause testing-label=testing-label-value --namespace=kubectl-9419'
Aug 22 01:55:52.269: INFO: stderr: ""
Aug 22 01:55:52.269: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 22 01:55:52.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pod pause -L testing-label --namespace=kubectl-9419'
Aug 22 01:55:52.377: INFO: stderr: ""
Aug 22 01:55:52.377: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 22 01:55:52.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 label pods pause testing-label- --namespace=kubectl-9419'
Aug 22 01:55:52.513: INFO: stderr: ""
Aug 22 01:55:52.513: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 22 01:55:52.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pod pause -L testing-label --namespace=kubectl-9419'
Aug 22 01:55:52.605: INFO: stderr: ""
Aug 22 01:55:52.605: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Aug 22 01:55:52.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-9419'
Aug 22 01:55:52.717: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 01:55:52.717: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 22 01:55:52.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get rc,svc -l name=pause --no-headers --namespace=kubectl-9419'
Aug 22 01:55:52.821: INFO: stderr: "No resources found.\n"
Aug 22 01:55:52.822: INFO: stdout: ""
Aug 22 01:55:52.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -l name=pause --namespace=kubectl-9419 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 22 01:55:52.909: INFO: stderr: ""
Aug 22 01:55:52.909: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:55:52.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9419" for this suite.
Aug 22 01:55:59.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:55:59.179: INFO: namespace kubectl-9419 deletion completed in 6.26440836s

• [SLOW TEST:9.364 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:55:59.183: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Aug 22 01:55:59.226: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-089099426 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:55:59.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-172" for this suite.
Aug 22 01:56:05.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:56:05.426: INFO: namespace kubectl-172 deletion completed in 6.092616642s

• [SLOW TEST:6.243 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:56:05.427: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-826625c2-a847-44d3-b3e6-8165c133f0f5
STEP: Creating a pod to test consume configMaps
Aug 22 01:56:05.468: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d97021cf-5e13-4a0d-9226-0d09fd59e5a2" in namespace "projected-8522" to be "success or failure"
Aug 22 01:56:05.471: INFO: Pod "pod-projected-configmaps-d97021cf-5e13-4a0d-9226-0d09fd59e5a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.919611ms
Aug 22 01:56:07.477: INFO: Pod "pod-projected-configmaps-d97021cf-5e13-4a0d-9226-0d09fd59e5a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008221597s
STEP: Saw pod success
Aug 22 01:56:07.477: INFO: Pod "pod-projected-configmaps-d97021cf-5e13-4a0d-9226-0d09fd59e5a2" satisfied condition "success or failure"
Aug 22 01:56:07.480: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-configmaps-d97021cf-5e13-4a0d-9226-0d09fd59e5a2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 01:56:07.506: INFO: Waiting for pod pod-projected-configmaps-d97021cf-5e13-4a0d-9226-0d09fd59e5a2 to disappear
Aug 22 01:56:07.508: INFO: Pod pod-projected-configmaps-d97021cf-5e13-4a0d-9226-0d09fd59e5a2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:56:07.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8522" for this suite.
Aug 22 01:56:13.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:56:13.623: INFO: namespace projected-8522 deletion completed in 6.113114262s

• [SLOW TEST:8.197 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:56:13.625: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 22 01:56:15.751: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:56:15.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9937" for this suite.
Aug 22 01:56:21.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:56:21.876: INFO: namespace container-runtime-9937 deletion completed in 6.10676444s

• [SLOW TEST:8.251 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:56:21.877: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:56:25.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5803" for this suite.
Aug 22 01:56:31.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:56:32.028: INFO: namespace kubelet-test-5803 deletion completed in 6.090644041s

• [SLOW TEST:10.151 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:56:32.028: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-dd28f277-3c34-4cb8-ba69-617efa03aaed
STEP: Creating a pod to test consume configMaps
Aug 22 01:56:32.080: INFO: Waiting up to 5m0s for pod "pod-configmaps-f1d186d1-1dd8-4df4-97c8-9da587d19306" in namespace "configmap-9770" to be "success or failure"
Aug 22 01:56:32.086: INFO: Pod "pod-configmaps-f1d186d1-1dd8-4df4-97c8-9da587d19306": Phase="Pending", Reason="", readiness=false. Elapsed: 6.475443ms
Aug 22 01:56:34.091: INFO: Pod "pod-configmaps-f1d186d1-1dd8-4df4-97c8-9da587d19306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011236692s
STEP: Saw pod success
Aug 22 01:56:34.091: INFO: Pod "pod-configmaps-f1d186d1-1dd8-4df4-97c8-9da587d19306" satisfied condition "success or failure"
Aug 22 01:56:34.094: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-f1d186d1-1dd8-4df4-97c8-9da587d19306 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 01:56:34.121: INFO: Waiting for pod pod-configmaps-f1d186d1-1dd8-4df4-97c8-9da587d19306 to disappear
Aug 22 01:56:34.126: INFO: Pod pod-configmaps-f1d186d1-1dd8-4df4-97c8-9da587d19306 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:56:34.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9770" for this suite.
Aug 22 01:56:40.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:56:40.234: INFO: namespace configmap-9770 deletion completed in 6.103212895s

• [SLOW TEST:8.206 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:56:40.235: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0822 01:56:50.347557      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 22 01:56:50.347: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:56:50.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8650" for this suite.
Aug 22 01:56:56.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:56:56.468: INFO: namespace gc-8650 deletion completed in 6.114830332s

• [SLOW TEST:16.233 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:56:56.469: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 22 01:57:01.109: INFO: Successfully updated pod "annotationupdate9906a07f-db39-4911-a9a7-d6499fb2c483"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:57:03.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2916" for this suite.
Aug 22 01:57:23.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:57:23.250: INFO: namespace downward-api-2916 deletion completed in 20.107174391s

• [SLOW TEST:26.781 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:57:23.251: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 01:57:25.309: INFO: Waiting up to 5m0s for pod "client-envvars-b7409421-4106-469a-a6a6-e99f7d611926" in namespace "pods-1726" to be "success or failure"
Aug 22 01:57:25.322: INFO: Pod "client-envvars-b7409421-4106-469a-a6a6-e99f7d611926": Phase="Pending", Reason="", readiness=false. Elapsed: 12.531528ms
Aug 22 01:57:27.344: INFO: Pod "client-envvars-b7409421-4106-469a-a6a6-e99f7d611926": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034162281s
Aug 22 01:57:29.349: INFO: Pod "client-envvars-b7409421-4106-469a-a6a6-e99f7d611926": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039500749s
STEP: Saw pod success
Aug 22 01:57:29.349: INFO: Pod "client-envvars-b7409421-4106-469a-a6a6-e99f7d611926" satisfied condition "success or failure"
Aug 22 01:57:29.352: INFO: Trying to get logs from node melsayed-conformance2 pod client-envvars-b7409421-4106-469a-a6a6-e99f7d611926 container env3cont: <nil>
STEP: delete the pod
Aug 22 01:57:29.385: INFO: Waiting for pod client-envvars-b7409421-4106-469a-a6a6-e99f7d611926 to disappear
Aug 22 01:57:29.387: INFO: Pod client-envvars-b7409421-4106-469a-a6a6-e99f7d611926 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:57:29.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1726" for this suite.
Aug 22 01:58:07.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:58:07.487: INFO: namespace pods-1726 deletion completed in 38.09709239s

• [SLOW TEST:44.236 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:58:07.488: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7036.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7036.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7036.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7036.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 01:58:33.561: INFO: DNS probes using dns-test-f0c30507-a123-47de-bbda-9a3946b4d3cf succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7036.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7036.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7036.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7036.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 01:58:53.609: INFO: DNS probes using dns-test-45943e7e-4073-48e6-8f15-b15657da691a succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7036.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7036.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7036.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7036.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 01:58:55.696: INFO: DNS probes using dns-test-1c90904a-c429-465e-934f-8a4ea2e72bb3 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:58:55.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7036" for this suite.
Aug 22 01:59:01.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:59:01.837: INFO: namespace dns-7036 deletion completed in 6.107980456s

• [SLOW TEST:54.350 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:59:01.838: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Aug 22 01:59:01.877: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4875" to be "success or failure"
Aug 22 01:59:01.884: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.975864ms
Aug 22 01:59:03.889: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012171084s
Aug 22 01:59:05.893: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016687596s
Aug 22 01:59:07.897: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0201164s
Aug 22 01:59:09.902: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02543725s
Aug 22 01:59:11.907: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029896867s
Aug 22 01:59:13.912: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035033135s
Aug 22 01:59:15.916: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.03937804s
Aug 22 01:59:17.921: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.044676599s
STEP: Saw pod success
Aug 22 01:59:17.922: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 22 01:59:17.927: INFO: Trying to get logs from node melsayed-conformance3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 22 01:59:17.949: INFO: Waiting for pod pod-host-path-test to disappear
Aug 22 01:59:17.952: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:59:17.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4875" for this suite.
Aug 22 01:59:23.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:59:24.066: INFO: namespace hostpath-4875 deletion completed in 6.111896442s

• [SLOW TEST:22.229 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:59:24.067: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 01:59:24.099: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 22 01:59:26.128: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:59:27.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1771" for this suite.
Aug 22 01:59:33.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:59:33.250: INFO: namespace replication-controller-1771 deletion completed in 6.108195074s

• [SLOW TEST:9.183 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:59:33.253: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 22 01:59:35.329: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:59:35.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7643" for this suite.
Aug 22 01:59:41.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 01:59:41.442: INFO: namespace container-runtime-7643 deletion completed in 6.093462581s

• [SLOW TEST:8.190 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 01:59:41.447: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 22 01:59:41.475: INFO: namespace kubectl-3282
Aug 22 01:59:41.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-3282'
Aug 22 01:59:41.669: INFO: stderr: ""
Aug 22 01:59:41.669: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 22 01:59:42.673: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 01:59:42.673: INFO: Found 0 / 1
Aug 22 01:59:43.673: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 01:59:43.673: INFO: Found 1 / 1
Aug 22 01:59:43.673: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 22 01:59:43.675: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 01:59:43.675: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 22 01:59:43.675: INFO: wait on redis-master startup in kubectl-3282 
Aug 22 01:59:43.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 logs redis-master-psjpr redis-master --namespace=kubectl-3282'
Aug 22 01:59:43.770: INFO: stderr: ""
Aug 22 01:59:43.770: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Aug 01:59:42.721 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Aug 01:59:42.722 # Server started, Redis version 3.2.12\n1:M 22 Aug 01:59:42.722 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Aug 01:59:42.722 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug 22 01:59:43.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3282'
Aug 22 01:59:43.882: INFO: stderr: ""
Aug 22 01:59:43.882: INFO: stdout: "service/rm2 exposed\n"
Aug 22 01:59:43.888: INFO: Service rm2 in namespace kubectl-3282 found.
STEP: exposing service
Aug 22 01:59:45.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3282'
Aug 22 01:59:46.022: INFO: stderr: ""
Aug 22 01:59:46.022: INFO: stdout: "service/rm3 exposed\n"
Aug 22 01:59:46.037: INFO: Service rm3 in namespace kubectl-3282 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 01:59:48.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3282" for this suite.
Aug 22 02:00:10.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:00:10.179: INFO: namespace kubectl-3282 deletion completed in 22.133919247s

• [SLOW TEST:28.732 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:00:10.181: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Aug 22 02:00:10.785: INFO: created pod pod-service-account-defaultsa
Aug 22 02:00:10.786: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 22 02:00:10.792: INFO: created pod pod-service-account-mountsa
Aug 22 02:00:10.792: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 22 02:00:10.800: INFO: created pod pod-service-account-nomountsa
Aug 22 02:00:10.800: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 22 02:00:10.808: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 22 02:00:10.808: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 22 02:00:10.823: INFO: created pod pod-service-account-mountsa-mountspec
Aug 22 02:00:10.823: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 22 02:00:10.838: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 22 02:00:10.838: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 22 02:00:10.859: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 22 02:00:10.859: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 22 02:00:10.873: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 22 02:00:10.873: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 22 02:00:10.890: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 22 02:00:10.890: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:00:10.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7649" for this suite.
Aug 22 02:00:16.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:00:17.055: INFO: namespace svcaccounts-7649 deletion completed in 6.138339516s

• [SLOW TEST:6.875 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:00:17.055: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9656.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9656.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9656.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9656.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9656.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9656.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 02:00:21.189: INFO: DNS probes using dns-9656/dns-test-969114cf-fbf8-41e0-bb84-382bc7cef83a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:00:21.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9656" for this suite.
Aug 22 02:00:27.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:00:27.442: INFO: namespace dns-9656 deletion completed in 6.234883369s

• [SLOW TEST:10.387 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:00:27.446: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:00:34.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5977" for this suite.
Aug 22 02:00:56.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:00:56.603: INFO: namespace replication-controller-5977 deletion completed in 22.093095052s

• [SLOW TEST:29.158 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:00:56.608: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 22 02:00:58.661: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:00:58.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6903" for this suite.
Aug 22 02:01:04.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:01:04.786: INFO: namespace container-runtime-6903 deletion completed in 6.094123469s

• [SLOW TEST:8.178 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:01:04.790: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:01:04.823: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 22 02:01:04.839: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 22 02:01:09.844: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 22 02:01:09.844: INFO: Creating deployment "test-rolling-update-deployment"
Aug 22 02:01:09.850: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 22 02:01:09.855: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 22 02:01:11.862: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 22 02:01:11.864: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 22 02:01:11.874: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4875,SelfLink:/apis/apps/v1/namespaces/deployment-4875/deployments/test-rolling-update-deployment,UID:1237d910-e99c-400a-9930-c609012d6d3c,ResourceVersion:8360,Generation:1,CreationTimestamp:2019-08-22 02:01:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-22 02:01:09 +0000 UTC 2019-08-22 02:01:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-22 02:01:11 +0000 UTC 2019-08-22 02:01:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 22 02:01:11.877: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-4875,SelfLink:/apis/apps/v1/namespaces/deployment-4875/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:47cedcd4-4c54-4802-b910-fd27ceada8c3,ResourceVersion:8350,Generation:1,CreationTimestamp:2019-08-22 02:01:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1237d910-e99c-400a-9930-c609012d6d3c 0xc00349b097 0xc00349b098}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 22 02:01:11.877: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 22 02:01:11.877: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4875,SelfLink:/apis/apps/v1/namespaces/deployment-4875/replicasets/test-rolling-update-controller,UID:690b924f-c1b2-44a4-8e20-d0288b897006,ResourceVersion:8359,Generation:2,CreationTimestamp:2019-08-22 02:01:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1237d910-e99c-400a-9930-c609012d6d3c 0xc00349afbf 0xc00349afd0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 22 02:01:11.880: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-x4fdq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-x4fdq,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-4875,SelfLink:/api/v1/namespaces/deployment-4875/pods/test-rolling-update-deployment-79f6b9d75c-x4fdq,UID:390ed946-2761-4de7-b5d6-9adb3756d24b,ResourceVersion:8349,Generation:0,CreationTimestamp:2019-08-22 02:01:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.1.63/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 47cedcd4-4c54-4802-b910-fd27ceada8c3 0xc00349b9b7 0xc00349b9b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k7c7q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k7c7q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-k7c7q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00349ba20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00349ba40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:01:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:01:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:01:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:01:09 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:10.42.1.63,StartTime:2019-08-22 02:01:09 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-22 02:01:11 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://b9cd45d44e7819117d45c60088b01cc54bedf2498e7844aab1eacebcc2b43ca0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:01:11.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4875" for this suite.
Aug 22 02:01:17.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:01:17.995: INFO: namespace deployment-4875 deletion completed in 6.110474995s

• [SLOW TEST:13.205 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:01:17.995: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2800
I0822 02:01:18.028027      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2800, replica count: 1
I0822 02:01:19.079168      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:20.079504      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:21.079820      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:22.080100      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:23.080456      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:24.081019      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:25.081349      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:26.081604      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:27.081839      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:28.082164      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:01:29.082525      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 02:01:29.196: INFO: Created: latency-svc-w74jm
Aug 22 02:01:29.207: INFO: Got endpoints: latency-svc-w74jm [25.043073ms]
Aug 22 02:01:29.224: INFO: Created: latency-svc-nfrmk
Aug 22 02:01:29.230: INFO: Created: latency-svc-kzlbk
Aug 22 02:01:29.233: INFO: Got endpoints: latency-svc-nfrmk [24.859475ms]
Aug 22 02:01:29.241: INFO: Got endpoints: latency-svc-kzlbk [31.095961ms]
Aug 22 02:01:29.252: INFO: Created: latency-svc-hkv9g
Aug 22 02:01:29.253: INFO: Created: latency-svc-n5px4
Aug 22 02:01:29.268: INFO: Got endpoints: latency-svc-hkv9g [60.105774ms]
Aug 22 02:01:29.276: INFO: Got endpoints: latency-svc-n5px4 [67.604633ms]
Aug 22 02:01:29.278: INFO: Created: latency-svc-jrln2
Aug 22 02:01:29.281: INFO: Created: latency-svc-56rsh
Aug 22 02:01:29.292: INFO: Got endpoints: latency-svc-jrln2 [83.568736ms]
Aug 22 02:01:29.294: INFO: Created: latency-svc-kdcrd
Aug 22 02:01:29.294: INFO: Got endpoints: latency-svc-56rsh [85.473201ms]
Aug 22 02:01:29.301: INFO: Got endpoints: latency-svc-kdcrd [91.517561ms]
Aug 22 02:01:29.302: INFO: Created: latency-svc-8tqjx
Aug 22 02:01:29.312: INFO: Created: latency-svc-4m5vc
Aug 22 02:01:29.315: INFO: Got endpoints: latency-svc-8tqjx [105.490271ms]
Aug 22 02:01:29.318: INFO: Got endpoints: latency-svc-4m5vc [107.707759ms]
Aug 22 02:01:29.324: INFO: Created: latency-svc-srl5p
Aug 22 02:01:29.327: INFO: Got endpoints: latency-svc-srl5p [116.236843ms]
Aug 22 02:01:29.331: INFO: Created: latency-svc-dlnxc
Aug 22 02:01:29.336: INFO: Created: latency-svc-lxgn2
Aug 22 02:01:29.348: INFO: Created: latency-svc-bx2q8
Aug 22 02:01:29.360: INFO: Got endpoints: latency-svc-dlnxc [148.804175ms]
Aug 22 02:01:29.366: INFO: Got endpoints: latency-svc-lxgn2 [155.221317ms]
Aug 22 02:01:29.367: INFO: Created: latency-svc-drhkk
Aug 22 02:01:29.367: INFO: Got endpoints: latency-svc-bx2q8 [155.458425ms]
Aug 22 02:01:29.377: INFO: Created: latency-svc-smjkm
Aug 22 02:01:29.389: INFO: Got endpoints: latency-svc-smjkm [177.877253ms]
Aug 22 02:01:29.390: INFO: Got endpoints: latency-svc-drhkk [178.13828ms]
Aug 22 02:01:29.469: INFO: Created: latency-svc-j2g82
Aug 22 02:01:29.478: INFO: Got endpoints: latency-svc-j2g82 [245.049671ms]
Aug 22 02:01:29.479: INFO: Created: latency-svc-shcjw
Aug 22 02:01:29.482: INFO: Created: latency-svc-wnbb7
Aug 22 02:01:29.492: INFO: Got endpoints: latency-svc-wnbb7 [223.122595ms]
Aug 22 02:01:29.492: INFO: Got endpoints: latency-svc-shcjw [250.299764ms]
Aug 22 02:01:29.501: INFO: Created: latency-svc-rqz96
Aug 22 02:01:29.508: INFO: Created: latency-svc-jffrb
Aug 22 02:01:29.512: INFO: Got endpoints: latency-svc-rqz96 [235.331324ms]
Aug 22 02:01:29.516: INFO: Created: latency-svc-5qvsk
Aug 22 02:01:29.521: INFO: Got endpoints: latency-svc-jffrb [228.414087ms]
Aug 22 02:01:29.528: INFO: Got endpoints: latency-svc-5qvsk [233.339666ms]
Aug 22 02:01:29.530: INFO: Created: latency-svc-n9f4h
Aug 22 02:01:29.579: INFO: Created: latency-svc-cdj98
Aug 22 02:01:29.580: INFO: Got endpoints: latency-svc-n9f4h [278.805734ms]
Aug 22 02:01:29.586: INFO: Got endpoints: latency-svc-cdj98 [270.326103ms]
Aug 22 02:01:29.588: INFO: Created: latency-svc-r786b
Aug 22 02:01:29.596: INFO: Got endpoints: latency-svc-r786b [278.243175ms]
Aug 22 02:01:29.604: INFO: Created: latency-svc-8h9qj
Aug 22 02:01:29.604: INFO: Created: latency-svc-4fvsh
Aug 22 02:01:29.615: INFO: Created: latency-svc-5ngcq
Aug 22 02:01:29.619: INFO: Got endpoints: latency-svc-4fvsh [291.695085ms]
Aug 22 02:01:29.619: INFO: Got endpoints: latency-svc-8h9qj [259.302664ms]
Aug 22 02:01:29.619: INFO: Created: latency-svc-hmvx2
Aug 22 02:01:29.627: INFO: Created: latency-svc-p4gtg
Aug 22 02:01:29.635: INFO: Got endpoints: latency-svc-5ngcq [268.372953ms]
Aug 22 02:01:29.643: INFO: Got endpoints: latency-svc-p4gtg [253.665477ms]
Aug 22 02:01:29.644: INFO: Got endpoints: latency-svc-hmvx2 [276.646806ms]
Aug 22 02:01:29.644: INFO: Created: latency-svc-58l7h
Aug 22 02:01:29.646: INFO: Got endpoints: latency-svc-58l7h [256.623915ms]
Aug 22 02:01:29.651: INFO: Created: latency-svc-2r5fm
Aug 22 02:01:29.656: INFO: Created: latency-svc-84jvn
Aug 22 02:01:29.657: INFO: Got endpoints: latency-svc-2r5fm [179.602706ms]
Aug 22 02:01:29.669: INFO: Created: latency-svc-gbhcg
Aug 22 02:01:29.676: INFO: Got endpoints: latency-svc-84jvn [184.042468ms]
Aug 22 02:01:29.676: INFO: Got endpoints: latency-svc-gbhcg [183.488301ms]
Aug 22 02:01:29.682: INFO: Created: latency-svc-xwnc7
Aug 22 02:01:29.687: INFO: Created: latency-svc-llt7z
Aug 22 02:01:29.689: INFO: Got endpoints: latency-svc-xwnc7 [177.240895ms]
Aug 22 02:01:29.699: INFO: Created: latency-svc-kzcxn
Aug 22 02:01:29.700: INFO: Got endpoints: latency-svc-llt7z [179.429847ms]
Aug 22 02:01:29.704: INFO: Got endpoints: latency-svc-kzcxn [176.346991ms]
Aug 22 02:01:29.717: INFO: Created: latency-svc-w69vh
Aug 22 02:01:29.721: INFO: Created: latency-svc-mfl6x
Aug 22 02:01:29.723: INFO: Got endpoints: latency-svc-w69vh [143.212851ms]
Aug 22 02:01:29.729: INFO: Got endpoints: latency-svc-mfl6x [143.593086ms]
Aug 22 02:01:29.733: INFO: Created: latency-svc-z2ct6
Aug 22 02:01:29.739: INFO: Created: latency-svc-prtq8
Aug 22 02:01:29.744: INFO: Got endpoints: latency-svc-z2ct6 [146.993758ms]
Aug 22 02:01:29.754: INFO: Got endpoints: latency-svc-prtq8 [135.663835ms]
Aug 22 02:01:29.756: INFO: Created: latency-svc-rf6k6
Aug 22 02:01:29.765: INFO: Created: latency-svc-v8bsl
Aug 22 02:01:29.772: INFO: Created: latency-svc-cppbh
Aug 22 02:01:29.773: INFO: Created: latency-svc-kpsfh
Aug 22 02:01:29.779: INFO: Created: latency-svc-l2mr2
Aug 22 02:01:29.786: INFO: Created: latency-svc-b2295
Aug 22 02:01:29.790: INFO: Created: latency-svc-9lwvw
Aug 22 02:01:29.795: INFO: Created: latency-svc-v4nr5
Aug 22 02:01:29.800: INFO: Got endpoints: latency-svc-rf6k6 [181.305845ms]
Aug 22 02:01:29.803: INFO: Created: latency-svc-pjd4x
Aug 22 02:01:29.808: INFO: Created: latency-svc-rbdcw
Aug 22 02:01:29.817: INFO: Created: latency-svc-9mdcr
Aug 22 02:01:29.824: INFO: Created: latency-svc-tsr58
Aug 22 02:01:29.829: INFO: Created: latency-svc-kc29s
Aug 22 02:01:29.834: INFO: Created: latency-svc-v6qpg
Aug 22 02:01:29.840: INFO: Created: latency-svc-jlwh2
Aug 22 02:01:29.847: INFO: Created: latency-svc-tdr9t
Aug 22 02:01:29.851: INFO: Got endpoints: latency-svc-v8bsl [215.772604ms]
Aug 22 02:01:29.858: INFO: Created: latency-svc-p7s7j
Aug 22 02:01:29.900: INFO: Got endpoints: latency-svc-cppbh [256.75468ms]
Aug 22 02:01:29.909: INFO: Created: latency-svc-5rmzk
Aug 22 02:01:29.951: INFO: Got endpoints: latency-svc-kpsfh [306.995908ms]
Aug 22 02:01:29.959: INFO: Created: latency-svc-rlwn9
Aug 22 02:01:30.002: INFO: Got endpoints: latency-svc-l2mr2 [355.733971ms]
Aug 22 02:01:30.010: INFO: Created: latency-svc-6tgss
Aug 22 02:01:30.052: INFO: Got endpoints: latency-svc-b2295 [394.368986ms]
Aug 22 02:01:30.059: INFO: Created: latency-svc-jxphj
Aug 22 02:01:30.100: INFO: Got endpoints: latency-svc-9lwvw [424.155867ms]
Aug 22 02:01:30.108: INFO: Created: latency-svc-8rw2w
Aug 22 02:01:30.155: INFO: Got endpoints: latency-svc-v4nr5 [478.5531ms]
Aug 22 02:01:30.161: INFO: Created: latency-svc-gvzlt
Aug 22 02:01:30.202: INFO: Got endpoints: latency-svc-pjd4x [512.499979ms]
Aug 22 02:01:30.210: INFO: Created: latency-svc-nzmgf
Aug 22 02:01:30.253: INFO: Got endpoints: latency-svc-rbdcw [552.725656ms]
Aug 22 02:01:30.265: INFO: Created: latency-svc-lcb8z
Aug 22 02:01:30.300: INFO: Got endpoints: latency-svc-9mdcr [595.56375ms]
Aug 22 02:01:30.307: INFO: Created: latency-svc-gq874
Aug 22 02:01:30.352: INFO: Got endpoints: latency-svc-tsr58 [629.331933ms]
Aug 22 02:01:30.400: INFO: Created: latency-svc-skbnj
Aug 22 02:01:30.402: INFO: Got endpoints: latency-svc-kc29s [672.107408ms]
Aug 22 02:01:30.408: INFO: Created: latency-svc-xdkv8
Aug 22 02:01:30.460: INFO: Got endpoints: latency-svc-v6qpg [715.758312ms]
Aug 22 02:01:30.467: INFO: Created: latency-svc-sl9nb
Aug 22 02:01:30.501: INFO: Got endpoints: latency-svc-jlwh2 [746.593519ms]
Aug 22 02:01:30.508: INFO: Created: latency-svc-twtp8
Aug 22 02:01:30.557: INFO: Got endpoints: latency-svc-tdr9t [756.317892ms]
Aug 22 02:01:30.569: INFO: Created: latency-svc-dptk6
Aug 22 02:01:30.602: INFO: Got endpoints: latency-svc-p7s7j [751.378518ms]
Aug 22 02:01:30.610: INFO: Created: latency-svc-vgq6t
Aug 22 02:01:30.649: INFO: Got endpoints: latency-svc-5rmzk [749.198152ms]
Aug 22 02:01:30.658: INFO: Created: latency-svc-2wjvd
Aug 22 02:01:30.701: INFO: Got endpoints: latency-svc-rlwn9 [750.710951ms]
Aug 22 02:01:30.711: INFO: Created: latency-svc-x4ghx
Aug 22 02:01:30.750: INFO: Got endpoints: latency-svc-6tgss [747.862828ms]
Aug 22 02:01:30.759: INFO: Created: latency-svc-r92qq
Aug 22 02:01:30.803: INFO: Got endpoints: latency-svc-jxphj [750.621722ms]
Aug 22 02:01:30.809: INFO: Created: latency-svc-th56l
Aug 22 02:01:30.849: INFO: Got endpoints: latency-svc-8rw2w [749.292502ms]
Aug 22 02:01:30.856: INFO: Created: latency-svc-st7bd
Aug 22 02:01:30.900: INFO: Got endpoints: latency-svc-gvzlt [745.608757ms]
Aug 22 02:01:30.910: INFO: Created: latency-svc-f6zlz
Aug 22 02:01:30.971: INFO: Got endpoints: latency-svc-nzmgf [769.126251ms]
Aug 22 02:01:30.986: INFO: Created: latency-svc-thr2t
Aug 22 02:01:31.001: INFO: Got endpoints: latency-svc-lcb8z [748.010145ms]
Aug 22 02:01:31.010: INFO: Created: latency-svc-jzp5j
Aug 22 02:01:31.052: INFO: Got endpoints: latency-svc-gq874 [751.599796ms]
Aug 22 02:01:31.062: INFO: Created: latency-svc-72q8r
Aug 22 02:01:31.102: INFO: Got endpoints: latency-svc-skbnj [749.430742ms]
Aug 22 02:01:31.112: INFO: Created: latency-svc-6j77l
Aug 22 02:01:31.150: INFO: Got endpoints: latency-svc-xdkv8 [747.993419ms]
Aug 22 02:01:31.157: INFO: Created: latency-svc-pww5q
Aug 22 02:01:31.200: INFO: Got endpoints: latency-svc-sl9nb [739.99022ms]
Aug 22 02:01:31.207: INFO: Created: latency-svc-67bll
Aug 22 02:01:31.250: INFO: Got endpoints: latency-svc-twtp8 [749.307887ms]
Aug 22 02:01:31.309: INFO: Got endpoints: latency-svc-dptk6 [751.785015ms]
Aug 22 02:01:31.312: INFO: Created: latency-svc-nj82n
Aug 22 02:01:31.503: INFO: Created: latency-svc-6xzg8
Aug 22 02:01:31.511: INFO: Got endpoints: latency-svc-x4ghx [809.593668ms]
Aug 22 02:01:31.511: INFO: Got endpoints: latency-svc-2wjvd [861.860912ms]
Aug 22 02:01:31.512: INFO: Got endpoints: latency-svc-vgq6t [909.449517ms]
Aug 22 02:01:31.512: INFO: Got endpoints: latency-svc-r92qq [762.257532ms]
Aug 22 02:01:31.612: INFO: Got endpoints: latency-svc-st7bd [762.547058ms]
Aug 22 02:01:31.612: INFO: Got endpoints: latency-svc-th56l [809.229675ms]
Aug 22 02:01:31.625: INFO: Created: latency-svc-qdj4h
Aug 22 02:01:31.645: INFO: Created: latency-svc-pbl5q
Aug 22 02:01:31.654: INFO: Created: latency-svc-phjlh
Aug 22 02:01:31.656: INFO: Got endpoints: latency-svc-f6zlz [755.8499ms]
Aug 22 02:01:31.665: INFO: Created: latency-svc-492sh
Aug 22 02:01:31.679: INFO: Created: latency-svc-lhmxl
Aug 22 02:01:31.688: INFO: Created: latency-svc-2nklg
Aug 22 02:01:31.692: INFO: Created: latency-svc-j97tf
Aug 22 02:01:31.699: INFO: Got endpoints: latency-svc-thr2t [728.374227ms]
Aug 22 02:01:31.709: INFO: Created: latency-svc-nbfbd
Aug 22 02:01:31.751: INFO: Got endpoints: latency-svc-jzp5j [750.055736ms]
Aug 22 02:01:31.758: INFO: Created: latency-svc-rzbr4
Aug 22 02:01:31.804: INFO: Got endpoints: latency-svc-72q8r [751.852116ms]
Aug 22 02:01:31.820: INFO: Created: latency-svc-qfq88
Aug 22 02:01:31.850: INFO: Got endpoints: latency-svc-6j77l [748.100937ms]
Aug 22 02:01:31.861: INFO: Created: latency-svc-jpfbz
Aug 22 02:01:31.911: INFO: Got endpoints: latency-svc-pww5q [761.063443ms]
Aug 22 02:01:31.918: INFO: Created: latency-svc-ctcwt
Aug 22 02:01:31.954: INFO: Got endpoints: latency-svc-67bll [754.289841ms]
Aug 22 02:01:31.965: INFO: Created: latency-svc-cgccb
Aug 22 02:01:32.002: INFO: Got endpoints: latency-svc-nj82n [751.600289ms]
Aug 22 02:01:32.011: INFO: Created: latency-svc-gtzt2
Aug 22 02:01:32.052: INFO: Got endpoints: latency-svc-6xzg8 [743.051193ms]
Aug 22 02:01:32.060: INFO: Created: latency-svc-wxz8n
Aug 22 02:01:32.105: INFO: Got endpoints: latency-svc-qdj4h [594.009682ms]
Aug 22 02:01:32.171: INFO: Got endpoints: latency-svc-pbl5q [658.839426ms]
Aug 22 02:01:32.172: INFO: Created: latency-svc-kj6t2
Aug 22 02:01:32.186: INFO: Created: latency-svc-bv4kx
Aug 22 02:01:32.200: INFO: Got endpoints: latency-svc-phjlh [688.442136ms]
Aug 22 02:01:32.211: INFO: Created: latency-svc-mmh86
Aug 22 02:01:32.251: INFO: Got endpoints: latency-svc-492sh [739.529757ms]
Aug 22 02:01:32.259: INFO: Created: latency-svc-kftwm
Aug 22 02:01:32.306: INFO: Got endpoints: latency-svc-lhmxl [694.483379ms]
Aug 22 02:01:32.320: INFO: Created: latency-svc-xfdfg
Aug 22 02:01:32.353: INFO: Got endpoints: latency-svc-2nklg [741.149165ms]
Aug 22 02:01:32.361: INFO: Created: latency-svc-nw7sh
Aug 22 02:01:32.400: INFO: Got endpoints: latency-svc-j97tf [743.801962ms]
Aug 22 02:01:32.410: INFO: Created: latency-svc-72wg2
Aug 22 02:01:32.449: INFO: Got endpoints: latency-svc-nbfbd [750.155182ms]
Aug 22 02:01:32.456: INFO: Created: latency-svc-p5cxw
Aug 22 02:01:32.501: INFO: Got endpoints: latency-svc-rzbr4 [749.319153ms]
Aug 22 02:01:32.514: INFO: Created: latency-svc-pvxcj
Aug 22 02:01:32.554: INFO: Got endpoints: latency-svc-qfq88 [750.184678ms]
Aug 22 02:01:32.565: INFO: Created: latency-svc-67n9l
Aug 22 02:01:32.602: INFO: Got endpoints: latency-svc-jpfbz [751.98193ms]
Aug 22 02:01:32.611: INFO: Created: latency-svc-zwsd2
Aug 22 02:01:32.655: INFO: Got endpoints: latency-svc-ctcwt [743.828346ms]
Aug 22 02:01:32.746: INFO: Got endpoints: latency-svc-cgccb [792.074244ms]
Aug 22 02:01:32.761: INFO: Created: latency-svc-bdcn5
Aug 22 02:01:32.766: INFO: Created: latency-svc-9qtfp
Aug 22 02:01:32.766: INFO: Got endpoints: latency-svc-gtzt2 [764.155333ms]
Aug 22 02:01:32.775: INFO: Created: latency-svc-h88tz
Aug 22 02:01:32.800: INFO: Got endpoints: latency-svc-wxz8n [748.106265ms]
Aug 22 02:01:32.844: INFO: Created: latency-svc-pnhk4
Aug 22 02:01:32.886: INFO: Got endpoints: latency-svc-kj6t2 [780.62226ms]
Aug 22 02:01:32.930: INFO: Got endpoints: latency-svc-bv4kx [758.026662ms]
Aug 22 02:01:32.935: INFO: Created: latency-svc-q58rr
Aug 22 02:01:32.941: INFO: Created: latency-svc-skkvg
Aug 22 02:01:32.950: INFO: Got endpoints: latency-svc-mmh86 [749.957145ms]
Aug 22 02:01:32.958: INFO: Created: latency-svc-pns9z
Aug 22 02:01:32.999: INFO: Got endpoints: latency-svc-kftwm [748.516885ms]
Aug 22 02:01:33.009: INFO: Created: latency-svc-rw2zp
Aug 22 02:01:33.051: INFO: Got endpoints: latency-svc-xfdfg [744.338152ms]
Aug 22 02:01:33.062: INFO: Created: latency-svc-vd5z2
Aug 22 02:01:33.101: INFO: Got endpoints: latency-svc-nw7sh [747.444804ms]
Aug 22 02:01:33.110: INFO: Created: latency-svc-t6k5n
Aug 22 02:01:33.155: INFO: Got endpoints: latency-svc-72wg2 [754.381669ms]
Aug 22 02:01:33.171: INFO: Created: latency-svc-kx7rc
Aug 22 02:01:33.203: INFO: Got endpoints: latency-svc-p5cxw [753.992966ms]
Aug 22 02:01:33.218: INFO: Created: latency-svc-t7fgq
Aug 22 02:01:33.250: INFO: Got endpoints: latency-svc-pvxcj [749.219012ms]
Aug 22 02:01:33.258: INFO: Created: latency-svc-5b5c4
Aug 22 02:01:33.300: INFO: Got endpoints: latency-svc-67n9l [745.978037ms]
Aug 22 02:01:33.313: INFO: Created: latency-svc-xtk9d
Aug 22 02:01:33.350: INFO: Got endpoints: latency-svc-zwsd2 [748.256012ms]
Aug 22 02:01:33.356: INFO: Created: latency-svc-rnkrm
Aug 22 02:01:33.399: INFO: Got endpoints: latency-svc-bdcn5 [744.62681ms]
Aug 22 02:01:33.414: INFO: Created: latency-svc-chgnr
Aug 22 02:01:33.450: INFO: Got endpoints: latency-svc-9qtfp [703.742763ms]
Aug 22 02:01:33.461: INFO: Created: latency-svc-5ncjd
Aug 22 02:01:33.501: INFO: Got endpoints: latency-svc-h88tz [734.630502ms]
Aug 22 02:01:33.569: INFO: Got endpoints: latency-svc-pnhk4 [769.083746ms]
Aug 22 02:01:33.578: INFO: Created: latency-svc-zj8pm
Aug 22 02:01:33.590: INFO: Created: latency-svc-4rnz6
Aug 22 02:01:33.601: INFO: Got endpoints: latency-svc-q58rr [714.868873ms]
Aug 22 02:01:33.607: INFO: Created: latency-svc-c696w
Aug 22 02:01:33.651: INFO: Got endpoints: latency-svc-skkvg [721.564253ms]
Aug 22 02:01:33.659: INFO: Created: latency-svc-xmmh2
Aug 22 02:01:33.701: INFO: Got endpoints: latency-svc-pns9z [751.091814ms]
Aug 22 02:01:33.711: INFO: Created: latency-svc-bxcq8
Aug 22 02:01:33.750: INFO: Got endpoints: latency-svc-rw2zp [750.977567ms]
Aug 22 02:01:33.760: INFO: Created: latency-svc-c9x7n
Aug 22 02:01:33.802: INFO: Got endpoints: latency-svc-vd5z2 [751.515882ms]
Aug 22 02:01:33.815: INFO: Created: latency-svc-55xtq
Aug 22 02:01:33.850: INFO: Got endpoints: latency-svc-t6k5n [749.551512ms]
Aug 22 02:01:33.859: INFO: Created: latency-svc-88p8r
Aug 22 02:01:33.913: INFO: Got endpoints: latency-svc-kx7rc [758.390902ms]
Aug 22 02:01:33.927: INFO: Created: latency-svc-glhv4
Aug 22 02:01:33.952: INFO: Got endpoints: latency-svc-t7fgq [748.173144ms]
Aug 22 02:01:33.962: INFO: Created: latency-svc-f9zzf
Aug 22 02:01:34.000: INFO: Got endpoints: latency-svc-5b5c4 [750.123516ms]
Aug 22 02:01:34.010: INFO: Created: latency-svc-dn7ng
Aug 22 02:01:34.051: INFO: Got endpoints: latency-svc-xtk9d [750.591823ms]
Aug 22 02:01:34.074: INFO: Created: latency-svc-9kg8v
Aug 22 02:01:34.101: INFO: Got endpoints: latency-svc-rnkrm [750.981436ms]
Aug 22 02:01:34.109: INFO: Created: latency-svc-nxh6t
Aug 22 02:01:34.152: INFO: Got endpoints: latency-svc-chgnr [752.977494ms]
Aug 22 02:01:34.164: INFO: Created: latency-svc-h6hc6
Aug 22 02:01:34.201: INFO: Got endpoints: latency-svc-5ncjd [751.175544ms]
Aug 22 02:01:34.210: INFO: Created: latency-svc-c7zp9
Aug 22 02:01:34.250: INFO: Got endpoints: latency-svc-zj8pm [748.673663ms]
Aug 22 02:01:34.257: INFO: Created: latency-svc-jpbp2
Aug 22 02:01:34.303: INFO: Got endpoints: latency-svc-4rnz6 [733.672985ms]
Aug 22 02:01:34.316: INFO: Created: latency-svc-f6n99
Aug 22 02:01:34.356: INFO: Got endpoints: latency-svc-c696w [754.641391ms]
Aug 22 02:01:34.369: INFO: Created: latency-svc-vqgf4
Aug 22 02:01:34.401: INFO: Got endpoints: latency-svc-xmmh2 [749.805169ms]
Aug 22 02:01:34.411: INFO: Created: latency-svc-xl79z
Aug 22 02:01:34.451: INFO: Got endpoints: latency-svc-bxcq8 [749.777106ms]
Aug 22 02:01:34.466: INFO: Created: latency-svc-w8k7f
Aug 22 02:01:34.503: INFO: Got endpoints: latency-svc-c9x7n [752.023693ms]
Aug 22 02:01:34.510: INFO: Created: latency-svc-2j57q
Aug 22 02:01:34.550: INFO: Got endpoints: latency-svc-55xtq [747.280329ms]
Aug 22 02:01:34.560: INFO: Created: latency-svc-xw5nj
Aug 22 02:01:34.603: INFO: Got endpoints: latency-svc-88p8r [753.079853ms]
Aug 22 02:01:34.615: INFO: Created: latency-svc-zsxlf
Aug 22 02:01:34.653: INFO: Got endpoints: latency-svc-glhv4 [740.188352ms]
Aug 22 02:01:34.662: INFO: Created: latency-svc-wjg4d
Aug 22 02:01:34.702: INFO: Got endpoints: latency-svc-f9zzf [749.716445ms]
Aug 22 02:01:34.710: INFO: Created: latency-svc-qxk7p
Aug 22 02:01:34.753: INFO: Got endpoints: latency-svc-dn7ng [753.306702ms]
Aug 22 02:01:34.765: INFO: Created: latency-svc-54jl4
Aug 22 02:01:34.800: INFO: Got endpoints: latency-svc-9kg8v [749.076896ms]
Aug 22 02:01:34.813: INFO: Created: latency-svc-dxzc9
Aug 22 02:01:34.852: INFO: Got endpoints: latency-svc-nxh6t [750.452302ms]
Aug 22 02:01:34.860: INFO: Created: latency-svc-ztcxx
Aug 22 02:01:34.901: INFO: Got endpoints: latency-svc-h6hc6 [748.069854ms]
Aug 22 02:01:34.989: INFO: Created: latency-svc-jxcbl
Aug 22 02:01:34.991: INFO: Got endpoints: latency-svc-c7zp9 [789.33011ms]
Aug 22 02:01:35.008: INFO: Created: latency-svc-2jz9d
Aug 22 02:01:35.011: INFO: Got endpoints: latency-svc-jpbp2 [760.897501ms]
Aug 22 02:01:35.019: INFO: Created: latency-svc-p22pn
Aug 22 02:01:35.051: INFO: Got endpoints: latency-svc-f6n99 [748.480851ms]
Aug 22 02:01:35.061: INFO: Created: latency-svc-fstww
Aug 22 02:01:35.100: INFO: Got endpoints: latency-svc-vqgf4 [744.511659ms]
Aug 22 02:01:35.108: INFO: Created: latency-svc-w2272
Aug 22 02:01:35.153: INFO: Got endpoints: latency-svc-xl79z [752.172704ms]
Aug 22 02:01:35.165: INFO: Created: latency-svc-bfxzv
Aug 22 02:01:35.202: INFO: Got endpoints: latency-svc-w8k7f [751.222782ms]
Aug 22 02:01:35.212: INFO: Created: latency-svc-7blqg
Aug 22 02:01:35.251: INFO: Got endpoints: latency-svc-2j57q [748.670409ms]
Aug 22 02:01:35.259: INFO: Created: latency-svc-sfwdx
Aug 22 02:01:35.302: INFO: Got endpoints: latency-svc-xw5nj [752.172889ms]
Aug 22 02:01:35.310: INFO: Created: latency-svc-hwqpk
Aug 22 02:01:35.355: INFO: Got endpoints: latency-svc-zsxlf [751.327639ms]
Aug 22 02:01:35.362: INFO: Created: latency-svc-ltkpz
Aug 22 02:01:35.407: INFO: Got endpoints: latency-svc-wjg4d [753.303302ms]
Aug 22 02:01:35.426: INFO: Created: latency-svc-jxdp5
Aug 22 02:01:35.451: INFO: Got endpoints: latency-svc-qxk7p [749.20765ms]
Aug 22 02:01:35.471: INFO: Created: latency-svc-l6j2f
Aug 22 02:01:35.502: INFO: Got endpoints: latency-svc-54jl4 [748.674402ms]
Aug 22 02:01:35.546: INFO: Created: latency-svc-xpzrm
Aug 22 02:01:35.554: INFO: Got endpoints: latency-svc-dxzc9 [754.090598ms]
Aug 22 02:01:35.563: INFO: Created: latency-svc-sjl6m
Aug 22 02:01:35.602: INFO: Got endpoints: latency-svc-ztcxx [750.373862ms]
Aug 22 02:01:35.615: INFO: Created: latency-svc-7r6hv
Aug 22 02:01:35.651: INFO: Got endpoints: latency-svc-jxcbl [750.698033ms]
Aug 22 02:01:35.665: INFO: Created: latency-svc-pvq6r
Aug 22 02:01:35.704: INFO: Got endpoints: latency-svc-2jz9d [713.158828ms]
Aug 22 02:01:35.713: INFO: Created: latency-svc-c2gxb
Aug 22 02:01:35.751: INFO: Got endpoints: latency-svc-p22pn [740.533934ms]
Aug 22 02:01:35.758: INFO: Created: latency-svc-mhq5l
Aug 22 02:01:35.801: INFO: Got endpoints: latency-svc-fstww [749.165078ms]
Aug 22 02:01:35.813: INFO: Created: latency-svc-ndwkc
Aug 22 02:01:35.854: INFO: Got endpoints: latency-svc-w2272 [753.989032ms]
Aug 22 02:01:35.875: INFO: Created: latency-svc-g2v8w
Aug 22 02:01:35.902: INFO: Got endpoints: latency-svc-bfxzv [748.901127ms]
Aug 22 02:01:35.915: INFO: Created: latency-svc-njnln
Aug 22 02:01:35.952: INFO: Got endpoints: latency-svc-7blqg [749.83358ms]
Aug 22 02:01:35.960: INFO: Created: latency-svc-8f55m
Aug 22 02:01:36.001: INFO: Got endpoints: latency-svc-sfwdx [749.369089ms]
Aug 22 02:01:36.027: INFO: Created: latency-svc-7qbv4
Aug 22 02:01:36.053: INFO: Got endpoints: latency-svc-hwqpk [750.837461ms]
Aug 22 02:01:36.060: INFO: Created: latency-svc-c8k4p
Aug 22 02:01:36.100: INFO: Got endpoints: latency-svc-ltkpz [744.97282ms]
Aug 22 02:01:36.108: INFO: Created: latency-svc-4gbvf
Aug 22 02:01:36.157: INFO: Got endpoints: latency-svc-jxdp5 [749.790833ms]
Aug 22 02:01:36.164: INFO: Created: latency-svc-fkkjq
Aug 22 02:01:36.203: INFO: Got endpoints: latency-svc-l6j2f [751.691034ms]
Aug 22 02:01:36.212: INFO: Created: latency-svc-64ll6
Aug 22 02:01:36.250: INFO: Got endpoints: latency-svc-xpzrm [747.663578ms]
Aug 22 02:01:36.265: INFO: Created: latency-svc-thwdd
Aug 22 02:01:36.302: INFO: Got endpoints: latency-svc-sjl6m [747.48159ms]
Aug 22 02:01:36.308: INFO: Created: latency-svc-dg9v2
Aug 22 02:01:36.351: INFO: Got endpoints: latency-svc-7r6hv [748.28303ms]
Aug 22 02:01:36.356: INFO: Created: latency-svc-n8w49
Aug 22 02:01:36.400: INFO: Got endpoints: latency-svc-pvq6r [748.697236ms]
Aug 22 02:01:36.406: INFO: Created: latency-svc-lmmpv
Aug 22 02:01:36.450: INFO: Got endpoints: latency-svc-c2gxb [745.729267ms]
Aug 22 02:01:36.457: INFO: Created: latency-svc-prnmz
Aug 22 02:01:36.500: INFO: Got endpoints: latency-svc-mhq5l [748.61558ms]
Aug 22 02:01:36.506: INFO: Created: latency-svc-w7zrm
Aug 22 02:01:36.550: INFO: Got endpoints: latency-svc-ndwkc [749.124888ms]
Aug 22 02:01:36.562: INFO: Created: latency-svc-77ng5
Aug 22 02:01:36.600: INFO: Got endpoints: latency-svc-g2v8w [745.669817ms]
Aug 22 02:01:36.616: INFO: Created: latency-svc-6fjl7
Aug 22 02:01:36.650: INFO: Got endpoints: latency-svc-njnln [747.336203ms]
Aug 22 02:01:36.658: INFO: Created: latency-svc-djbld
Aug 22 02:01:36.701: INFO: Got endpoints: latency-svc-8f55m [748.847401ms]
Aug 22 02:01:36.708: INFO: Created: latency-svc-9nv4g
Aug 22 02:01:36.750: INFO: Got endpoints: latency-svc-7qbv4 [748.63691ms]
Aug 22 02:01:36.756: INFO: Created: latency-svc-985dq
Aug 22 02:01:36.800: INFO: Got endpoints: latency-svc-c8k4p [747.284558ms]
Aug 22 02:01:36.809: INFO: Created: latency-svc-xnbwc
Aug 22 02:01:36.850: INFO: Got endpoints: latency-svc-4gbvf [750.230017ms]
Aug 22 02:01:36.857: INFO: Created: latency-svc-z4nrb
Aug 22 02:01:36.899: INFO: Got endpoints: latency-svc-fkkjq [742.67355ms]
Aug 22 02:01:36.907: INFO: Created: latency-svc-5tn5m
Aug 22 02:01:36.953: INFO: Got endpoints: latency-svc-64ll6 [750.129204ms]
Aug 22 02:01:36.964: INFO: Created: latency-svc-5mtsv
Aug 22 02:01:37.019: INFO: Got endpoints: latency-svc-thwdd [769.087653ms]
Aug 22 02:01:37.041: INFO: Created: latency-svc-7nvw7
Aug 22 02:01:37.049: INFO: Got endpoints: latency-svc-dg9v2 [746.850114ms]
Aug 22 02:01:37.100: INFO: Got endpoints: latency-svc-n8w49 [749.212982ms]
Aug 22 02:01:37.149: INFO: Got endpoints: latency-svc-lmmpv [749.160091ms]
Aug 22 02:01:37.202: INFO: Got endpoints: latency-svc-prnmz [752.672068ms]
Aug 22 02:01:37.253: INFO: Got endpoints: latency-svc-w7zrm [752.652275ms]
Aug 22 02:01:37.300: INFO: Got endpoints: latency-svc-77ng5 [749.570838ms]
Aug 22 02:01:37.353: INFO: Got endpoints: latency-svc-6fjl7 [753.237364ms]
Aug 22 02:01:37.401: INFO: Got endpoints: latency-svc-djbld [751.322032ms]
Aug 22 02:01:37.451: INFO: Got endpoints: latency-svc-9nv4g [749.971689ms]
Aug 22 02:01:37.501: INFO: Got endpoints: latency-svc-985dq [751.170438ms]
Aug 22 02:01:37.556: INFO: Got endpoints: latency-svc-xnbwc [755.687945ms]
Aug 22 02:01:37.601: INFO: Got endpoints: latency-svc-z4nrb [750.455539ms]
Aug 22 02:01:37.651: INFO: Got endpoints: latency-svc-5tn5m [751.631904ms]
Aug 22 02:01:37.701: INFO: Got endpoints: latency-svc-5mtsv [748.073016ms]
Aug 22 02:01:37.753: INFO: Got endpoints: latency-svc-7nvw7 [733.226121ms]
Aug 22 02:01:37.753: INFO: Latencies: [24.859475ms 31.095961ms 60.105774ms 67.604633ms 83.568736ms 85.473201ms 91.517561ms 105.490271ms 107.707759ms 116.236843ms 135.663835ms 143.212851ms 143.593086ms 146.993758ms 148.804175ms 155.221317ms 155.458425ms 176.346991ms 177.240895ms 177.877253ms 178.13828ms 179.429847ms 179.602706ms 181.305845ms 183.488301ms 184.042468ms 215.772604ms 223.122595ms 228.414087ms 233.339666ms 235.331324ms 245.049671ms 250.299764ms 253.665477ms 256.623915ms 256.75468ms 259.302664ms 268.372953ms 270.326103ms 276.646806ms 278.243175ms 278.805734ms 291.695085ms 306.995908ms 355.733971ms 394.368986ms 424.155867ms 478.5531ms 512.499979ms 552.725656ms 594.009682ms 595.56375ms 629.331933ms 658.839426ms 672.107408ms 688.442136ms 694.483379ms 703.742763ms 713.158828ms 714.868873ms 715.758312ms 721.564253ms 728.374227ms 733.226121ms 733.672985ms 734.630502ms 739.529757ms 739.99022ms 740.188352ms 740.533934ms 741.149165ms 742.67355ms 743.051193ms 743.801962ms 743.828346ms 744.338152ms 744.511659ms 744.62681ms 744.97282ms 745.608757ms 745.669817ms 745.729267ms 745.978037ms 746.593519ms 746.850114ms 747.280329ms 747.284558ms 747.336203ms 747.444804ms 747.48159ms 747.663578ms 747.862828ms 747.993419ms 748.010145ms 748.069854ms 748.073016ms 748.100937ms 748.106265ms 748.173144ms 748.256012ms 748.28303ms 748.480851ms 748.516885ms 748.61558ms 748.63691ms 748.670409ms 748.673663ms 748.674402ms 748.697236ms 748.847401ms 748.901127ms 749.076896ms 749.124888ms 749.160091ms 749.165078ms 749.198152ms 749.20765ms 749.212982ms 749.219012ms 749.292502ms 749.307887ms 749.319153ms 749.369089ms 749.430742ms 749.551512ms 749.570838ms 749.716445ms 749.777106ms 749.790833ms 749.805169ms 749.83358ms 749.957145ms 749.971689ms 750.055736ms 750.123516ms 750.129204ms 750.155182ms 750.184678ms 750.230017ms 750.373862ms 750.452302ms 750.455539ms 750.591823ms 750.621722ms 750.698033ms 750.710951ms 750.837461ms 750.977567ms 750.981436ms 751.091814ms 751.170438ms 751.175544ms 751.222782ms 751.322032ms 751.327639ms 751.378518ms 751.515882ms 751.599796ms 751.600289ms 751.631904ms 751.691034ms 751.785015ms 751.852116ms 751.98193ms 752.023693ms 752.172704ms 752.172889ms 752.652275ms 752.672068ms 752.977494ms 753.079853ms 753.237364ms 753.303302ms 753.306702ms 753.989032ms 753.992966ms 754.090598ms 754.289841ms 754.381669ms 754.641391ms 755.687945ms 755.8499ms 756.317892ms 758.026662ms 758.390902ms 760.897501ms 761.063443ms 762.257532ms 762.547058ms 764.155333ms 769.083746ms 769.087653ms 769.126251ms 780.62226ms 789.33011ms 792.074244ms 809.229675ms 809.593668ms 861.860912ms 909.449517ms]
Aug 22 02:01:37.753: INFO: 50 %ile: 748.28303ms
Aug 22 02:01:37.753: INFO: 90 %ile: 755.687945ms
Aug 22 02:01:37.753: INFO: 99 %ile: 861.860912ms
Aug 22 02:01:37.753: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:01:37.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2800" for this suite.
Aug 22 02:01:59.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:01:59.853: INFO: namespace svc-latency-2800 deletion completed in 22.09552092s

• [SLOW TEST:41.858 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:01:59.855: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 22 02:01:59.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-4453'
Aug 22 02:02:00.246: INFO: stderr: ""
Aug 22 02:02:00.246: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 02:02:00.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4453'
Aug 22 02:02:00.337: INFO: stderr: ""
Aug 22 02:02:00.337: INFO: stdout: "update-demo-nautilus-hlf42 update-demo-nautilus-shl4t "
Aug 22 02:02:00.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-hlf42 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4453'
Aug 22 02:02:00.425: INFO: stderr: ""
Aug 22 02:02:00.425: INFO: stdout: ""
Aug 22 02:02:00.425: INFO: update-demo-nautilus-hlf42 is created but not running
Aug 22 02:02:05.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4453'
Aug 22 02:02:05.515: INFO: stderr: ""
Aug 22 02:02:05.515: INFO: stdout: "update-demo-nautilus-hlf42 update-demo-nautilus-shl4t "
Aug 22 02:02:05.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-hlf42 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4453'
Aug 22 02:02:05.602: INFO: stderr: ""
Aug 22 02:02:05.602: INFO: stdout: ""
Aug 22 02:02:05.602: INFO: update-demo-nautilus-hlf42 is created but not running
Aug 22 02:02:10.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4453'
Aug 22 02:02:10.700: INFO: stderr: ""
Aug 22 02:02:10.700: INFO: stdout: "update-demo-nautilus-hlf42 update-demo-nautilus-shl4t "
Aug 22 02:02:10.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-hlf42 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4453'
Aug 22 02:02:10.806: INFO: stderr: ""
Aug 22 02:02:10.806: INFO: stdout: "true"
Aug 22 02:02:10.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-hlf42 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4453'
Aug 22 02:02:10.888: INFO: stderr: ""
Aug 22 02:02:10.888: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 22 02:02:10.888: INFO: validating pod update-demo-nautilus-hlf42
Aug 22 02:02:10.894: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 02:02:10.894: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 02:02:10.894: INFO: update-demo-nautilus-hlf42 is verified up and running
Aug 22 02:02:10.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-shl4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4453'
Aug 22 02:02:10.987: INFO: stderr: ""
Aug 22 02:02:10.987: INFO: stdout: "true"
Aug 22 02:02:10.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-shl4t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4453'
Aug 22 02:02:11.065: INFO: stderr: ""
Aug 22 02:02:11.065: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 22 02:02:11.065: INFO: validating pod update-demo-nautilus-shl4t
Aug 22 02:02:11.071: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 02:02:11.071: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 02:02:11.071: INFO: update-demo-nautilus-shl4t is verified up and running
STEP: using delete to clean up resources
Aug 22 02:02:11.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-4453'
Aug 22 02:02:11.171: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 02:02:11.171: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 22 02:02:11.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4453'
Aug 22 02:02:11.269: INFO: stderr: "No resources found.\n"
Aug 22 02:02:11.269: INFO: stdout: ""
Aug 22 02:02:11.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -l name=update-demo --namespace=kubectl-4453 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 22 02:02:11.351: INFO: stderr: ""
Aug 22 02:02:11.351: INFO: stdout: "update-demo-nautilus-hlf42\nupdate-demo-nautilus-shl4t\n"
Aug 22 02:02:11.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4453'
Aug 22 02:02:11.961: INFO: stderr: "No resources found.\n"
Aug 22 02:02:11.961: INFO: stdout: ""
Aug 22 02:02:11.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -l name=update-demo --namespace=kubectl-4453 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 22 02:02:12.067: INFO: stderr: ""
Aug 22 02:02:12.067: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:02:12.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4453" for this suite.
Aug 22 02:02:24.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:02:24.213: INFO: namespace kubectl-4453 deletion completed in 12.142297189s

• [SLOW TEST:24.358 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:02:24.216: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 22 02:02:24.265: INFO: Waiting up to 5m0s for pod "pod-7915f7a6-578f-410d-b47b-c9512ba344c3" in namespace "emptydir-1815" to be "success or failure"
Aug 22 02:02:24.271: INFO: Pod "pod-7915f7a6-578f-410d-b47b-c9512ba344c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.913941ms
Aug 22 02:02:26.275: INFO: Pod "pod-7915f7a6-578f-410d-b47b-c9512ba344c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009835998s
Aug 22 02:02:28.278: INFO: Pod "pod-7915f7a6-578f-410d-b47b-c9512ba344c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013197411s
STEP: Saw pod success
Aug 22 02:02:28.278: INFO: Pod "pod-7915f7a6-578f-410d-b47b-c9512ba344c3" satisfied condition "success or failure"
Aug 22 02:02:28.281: INFO: Trying to get logs from node melsayed-conformance3 pod pod-7915f7a6-578f-410d-b47b-c9512ba344c3 container test-container: <nil>
STEP: delete the pod
Aug 22 02:02:28.305: INFO: Waiting for pod pod-7915f7a6-578f-410d-b47b-c9512ba344c3 to disappear
Aug 22 02:02:28.308: INFO: Pod pod-7915f7a6-578f-410d-b47b-c9512ba344c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:02:28.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1815" for this suite.
Aug 22 02:02:34.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:02:34.407: INFO: namespace emptydir-1815 deletion completed in 6.09612502s

• [SLOW TEST:10.192 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:02:34.409: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3060
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3060
STEP: Creating statefulset with conflicting port in namespace statefulset-3060
STEP: Waiting until pod test-pod will start running in namespace statefulset-3060
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3060
Aug 22 02:02:40.209: INFO: Observed stateful pod in namespace: statefulset-3060, name: ss-0, uid: 4850b761-3309-4a1c-b512-968f10d295f2, status phase: Pending. Waiting for statefulset controller to delete.
Aug 22 02:02:40.544: INFO: Observed stateful pod in namespace: statefulset-3060, name: ss-0, uid: 4850b761-3309-4a1c-b512-968f10d295f2, status phase: Failed. Waiting for statefulset controller to delete.
Aug 22 02:02:40.553: INFO: Observed stateful pod in namespace: statefulset-3060, name: ss-0, uid: 4850b761-3309-4a1c-b512-968f10d295f2, status phase: Failed. Waiting for statefulset controller to delete.
Aug 22 02:02:40.557: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3060
STEP: Removing pod with conflicting port in namespace statefulset-3060
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3060 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 22 02:02:44.754: INFO: Deleting all statefulset in ns statefulset-3060
Aug 22 02:02:44.761: INFO: Scaling statefulset ss to 0
Aug 22 02:03:05.025: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 02:03:05.027: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:03:05.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3060" for this suite.
Aug 22 02:03:11.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:03:11.143: INFO: namespace statefulset-3060 deletion completed in 6.097388888s

• [SLOW TEST:36.734 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:03:11.143: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5876
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 22 02:03:11.187: INFO: Found 0 stateful pods, waiting for 3
Aug 22 02:03:21.193: INFO: Found 2 stateful pods, waiting for 3
Aug 22 02:03:31.192: INFO: Found 2 stateful pods, waiting for 3
Aug 22 02:03:41.191: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 02:03:41.191: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 02:03:41.191: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 22 02:03:41.218: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 22 02:03:51.265: INFO: Updating stateful set ss2
Aug 22 02:03:51.295: INFO: Waiting for Pod statefulset-5876/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Aug 22 02:04:03.539: INFO: Found 2 stateful pods, waiting for 3
Aug 22 02:04:13.690: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 02:04:13.690: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 02:04:13.691: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 22 02:04:13.730: INFO: Updating stateful set ss2
Aug 22 02:04:13.765: INFO: Waiting for Pod statefulset-5876/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 22 02:04:23.772: INFO: Waiting for Pod statefulset-5876/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 22 02:04:33.801: INFO: Updating stateful set ss2
Aug 22 02:04:33.813: INFO: Waiting for StatefulSet statefulset-5876/ss2 to complete update
Aug 22 02:04:33.813: INFO: Waiting for Pod statefulset-5876/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 22 02:04:43.824: INFO: Deleting all statefulset in ns statefulset-5876
Aug 22 02:04:43.827: INFO: Scaling statefulset ss2 to 0
Aug 22 02:05:13.844: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 02:05:13.847: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:05:13.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5876" for this suite.
Aug 22 02:05:19.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:05:19.965: INFO: namespace statefulset-5876 deletion completed in 6.102371263s

• [SLOW TEST:128.822 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:05:19.966: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0822 02:05:50.055207      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 22 02:05:50.055: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:05:50.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4343" for this suite.
Aug 22 02:05:56.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:05:56.202: INFO: namespace gc-4343 deletion completed in 6.145026471s

• [SLOW TEST:36.236 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:05:56.203: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:05:56.236: INFO: (0) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.389958ms)
Aug 22 02:05:56.239: INFO: (1) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.38681ms)
Aug 22 02:05:56.244: INFO: (2) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.47299ms)
Aug 22 02:05:56.248: INFO: (3) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.524892ms)
Aug 22 02:05:56.252: INFO: (4) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.025889ms)
Aug 22 02:05:56.255: INFO: (5) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.276048ms)
Aug 22 02:05:56.259: INFO: (6) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.39783ms)
Aug 22 02:05:56.262: INFO: (7) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.181684ms)
Aug 22 02:05:56.267: INFO: (8) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.317724ms)
Aug 22 02:05:56.271: INFO: (9) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.621682ms)
Aug 22 02:05:56.275: INFO: (10) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.144502ms)
Aug 22 02:05:56.277: INFO: (11) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.398564ms)
Aug 22 02:05:56.280: INFO: (12) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.09721ms)
Aug 22 02:05:56.286: INFO: (13) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.450877ms)
Aug 22 02:05:56.292: INFO: (14) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.585858ms)
Aug 22 02:05:56.307: INFO: (15) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 15.188221ms)
Aug 22 02:05:56.319: INFO: (16) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 11.564042ms)
Aug 22 02:05:56.322: INFO: (17) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.382461ms)
Aug 22 02:05:56.326: INFO: (18) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.0233ms)
Aug 22 02:05:56.329: INFO: (19) /api/v1/nodes/melsayed-conformance1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.880241ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:05:56.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2520" for this suite.
Aug 22 02:06:02.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:06:02.441: INFO: namespace proxy-2520 deletion completed in 6.107440703s

• [SLOW TEST:6.238 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:06:02.441: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:06:02.479: INFO: Creating deployment "test-recreate-deployment"
Aug 22 02:06:02.484: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 22 02:06:02.489: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Aug 22 02:06:04.494: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 22 02:06:04.497: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 22 02:06:04.509: INFO: Updating deployment test-recreate-deployment
Aug 22 02:06:04.509: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 22 02:06:04.611: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3064,SelfLink:/apis/apps/v1/namespaces/deployment-3064/deployments/test-recreate-deployment,UID:ea9920c8-584c-4b7f-9dac-f15df034cc1a,ResourceVersion:10805,Generation:2,CreationTimestamp:2019-08-22 02:06:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-22 02:06:04 +0000 UTC 2019-08-22 02:06:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-22 02:06:04 +0000 UTC 2019-08-22 02:06:02 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug 22 02:06:04.615: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-3064,SelfLink:/apis/apps/v1/namespaces/deployment-3064/replicasets/test-recreate-deployment-5c8c9cc69d,UID:703edebd-f0f3-4758-915b-26df49c9d1a0,ResourceVersion:10803,Generation:1,CreationTimestamp:2019-08-22 02:06:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ea9920c8-584c-4b7f-9dac-f15df034cc1a 0xc00161c357 0xc00161c358}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 22 02:06:04.615: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 22 02:06:04.615: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-3064,SelfLink:/apis/apps/v1/namespaces/deployment-3064/replicasets/test-recreate-deployment-6df85df6b9,UID:3f52c162-91cb-440b-9835-4f1713d0d0d2,ResourceVersion:10793,Generation:2,CreationTimestamp:2019-08-22 02:06:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ea9920c8-584c-4b7f-9dac-f15df034cc1a 0xc00161c427 0xc00161c428}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 22 02:06:04.620: INFO: Pod "test-recreate-deployment-5c8c9cc69d-64pmj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-64pmj,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-3064,SelfLink:/api/v1/namespaces/deployment-3064/pods/test-recreate-deployment-5c8c9cc69d-64pmj,UID:c2e641cf-006d-45e7-a68e-fe4061cd89ac,ResourceVersion:10806,Generation:0,CreationTimestamp:2019-08-22 02:06:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 703edebd-f0f3-4758-915b-26df49c9d1a0 0xc00161cd07 0xc00161cd08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7jm6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7jm6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7jm6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00161cd70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00161cda0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:04 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:,StartTime:2019-08-22 02:06:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:06:04.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3064" for this suite.
Aug 22 02:06:10.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:06:10.717: INFO: namespace deployment-3064 deletion completed in 6.093900138s

• [SLOW TEST:8.276 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:06:10.719: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 22 02:06:10.771: INFO: Waiting up to 5m0s for pod "pod-d3e51fd1-6dd4-4a8c-89d5-0f204a009fd5" in namespace "emptydir-9931" to be "success or failure"
Aug 22 02:06:10.779: INFO: Pod "pod-d3e51fd1-6dd4-4a8c-89d5-0f204a009fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.117409ms
Aug 22 02:06:12.782: INFO: Pod "pod-d3e51fd1-6dd4-4a8c-89d5-0f204a009fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010154168s
Aug 22 02:06:14.787: INFO: Pod "pod-d3e51fd1-6dd4-4a8c-89d5-0f204a009fd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015089949s
STEP: Saw pod success
Aug 22 02:06:14.787: INFO: Pod "pod-d3e51fd1-6dd4-4a8c-89d5-0f204a009fd5" satisfied condition "success or failure"
Aug 22 02:06:14.790: INFO: Trying to get logs from node melsayed-conformance3 pod pod-d3e51fd1-6dd4-4a8c-89d5-0f204a009fd5 container test-container: <nil>
STEP: delete the pod
Aug 22 02:06:14.813: INFO: Waiting for pod pod-d3e51fd1-6dd4-4a8c-89d5-0f204a009fd5 to disappear
Aug 22 02:06:14.816: INFO: Pod pod-d3e51fd1-6dd4-4a8c-89d5-0f204a009fd5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:06:14.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9931" for this suite.
Aug 22 02:06:20.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:06:20.915: INFO: namespace emptydir-9931 deletion completed in 6.096096671s

• [SLOW TEST:10.196 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:06:20.915: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Aug 22 02:06:20.965: INFO: Waiting up to 5m0s for pod "client-containers-7e599b77-9111-49c4-ba42-ef87976e0dd1" in namespace "containers-6406" to be "success or failure"
Aug 22 02:06:20.974: INFO: Pod "client-containers-7e599b77-9111-49c4-ba42-ef87976e0dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118319ms
Aug 22 02:06:22.981: INFO: Pod "client-containers-7e599b77-9111-49c4-ba42-ef87976e0dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015182458s
Aug 22 02:06:24.984: INFO: Pod "client-containers-7e599b77-9111-49c4-ba42-ef87976e0dd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018698977s
STEP: Saw pod success
Aug 22 02:06:24.984: INFO: Pod "client-containers-7e599b77-9111-49c4-ba42-ef87976e0dd1" satisfied condition "success or failure"
Aug 22 02:06:24.987: INFO: Trying to get logs from node melsayed-conformance3 pod client-containers-7e599b77-9111-49c4-ba42-ef87976e0dd1 container test-container: <nil>
STEP: delete the pod
Aug 22 02:06:25.005: INFO: Waiting for pod client-containers-7e599b77-9111-49c4-ba42-ef87976e0dd1 to disappear
Aug 22 02:06:25.008: INFO: Pod client-containers-7e599b77-9111-49c4-ba42-ef87976e0dd1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:06:25.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6406" for this suite.
Aug 22 02:06:31.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:06:31.119: INFO: namespace containers-6406 deletion completed in 6.10796311s

• [SLOW TEST:10.204 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:06:31.120: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-d99c4c24-dd7d-4de9-819e-b73cff818d0d
Aug 22 02:06:31.214: INFO: Pod name my-hostname-basic-d99c4c24-dd7d-4de9-819e-b73cff818d0d: Found 0 pods out of 1
Aug 22 02:06:36.219: INFO: Pod name my-hostname-basic-d99c4c24-dd7d-4de9-819e-b73cff818d0d: Found 1 pods out of 1
Aug 22 02:06:36.220: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d99c4c24-dd7d-4de9-819e-b73cff818d0d" are running
Aug 22 02:06:36.223: INFO: Pod "my-hostname-basic-d99c4c24-dd7d-4de9-819e-b73cff818d0d-pwv8f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-22 02:06:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-22 02:06:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-22 02:06:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-22 02:06:31 +0000 UTC Reason: Message:}])
Aug 22 02:06:36.223: INFO: Trying to dial the pod
Aug 22 02:06:41.237: INFO: Controller my-hostname-basic-d99c4c24-dd7d-4de9-819e-b73cff818d0d: Got expected result from replica 1 [my-hostname-basic-d99c4c24-dd7d-4de9-819e-b73cff818d0d-pwv8f]: "my-hostname-basic-d99c4c24-dd7d-4de9-819e-b73cff818d0d-pwv8f", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:06:41.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6208" for this suite.
Aug 22 02:06:47.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:06:47.367: INFO: namespace replication-controller-6208 deletion completed in 6.126107739s

• [SLOW TEST:16.247 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:06:47.372: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4606
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-4606
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4606
Aug 22 02:06:47.423: INFO: Found 0 stateful pods, waiting for 1
Aug 22 02:06:57.429: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 22 02:06:57.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-4606 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 02:06:58.046: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 02:06:58.046: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 02:06:58.046: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 22 02:06:58.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 22 02:07:08.055: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 02:07:08.055: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 02:07:08.066: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:08.066: INFO: ss-0  melsayed-conformance3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:08.066: INFO: 
Aug 22 02:07:08.066: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 22 02:07:09.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997689469s
Aug 22 02:07:10.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994147053s
Aug 22 02:07:11.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985329416s
Aug 22 02:07:12.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980172886s
Aug 22 02:07:13.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976611445s
Aug 22 02:07:14.098: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972085769s
Aug 22 02:07:15.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965575803s
Aug 22 02:07:16.108: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96038835s
Aug 22 02:07:17.113: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.301018ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4606
Aug 22 02:07:18.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-4606 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 22 02:07:18.383: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 22 02:07:18.383: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 22 02:07:18.383: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 22 02:07:18.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-4606 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 22 02:07:18.643: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 22 02:07:18.643: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 22 02:07:18.643: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 22 02:07:18.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-4606 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 22 02:07:18.991: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 22 02:07:18.991: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 22 02:07:18.991: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 22 02:07:18.995: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 02:07:18.995: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 22 02:07:18.995: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 22 02:07:18.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-4606 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 02:07:19.286: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 02:07:19.286: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 02:07:19.286: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 22 02:07:19.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-4606 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 02:07:19.555: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 02:07:19.555: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 02:07:19.555: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 22 02:07:19.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 exec --namespace=statefulset-4606 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 22 02:07:19.916: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 22 02:07:19.916: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 22 02:07:19.916: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 22 02:07:19.916: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 02:07:19.919: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 22 02:07:29.928: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 02:07:29.928: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 02:07:29.928: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 22 02:07:29.940: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:29.940: INFO: ss-0  melsayed-conformance3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:29.940: INFO: ss-1  melsayed-conformance2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:29.940: INFO: ss-2  melsayed-conformance1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:29.940: INFO: 
Aug 22 02:07:29.940: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 22 02:07:30.947: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:30.947: INFO: ss-0  melsayed-conformance3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:30.948: INFO: ss-1  melsayed-conformance2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:30.948: INFO: ss-2  melsayed-conformance1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:30.948: INFO: 
Aug 22 02:07:30.948: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 22 02:07:31.953: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:31.953: INFO: ss-0  melsayed-conformance3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:31.953: INFO: ss-1  melsayed-conformance2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:31.953: INFO: ss-2  melsayed-conformance1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:31.953: INFO: 
Aug 22 02:07:31.953: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 22 02:07:32.959: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:32.959: INFO: ss-0  melsayed-conformance3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:32.959: INFO: ss-2  melsayed-conformance1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:32.959: INFO: 
Aug 22 02:07:32.959: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 22 02:07:33.963: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:33.963: INFO: ss-0  melsayed-conformance3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:33.963: INFO: ss-2  melsayed-conformance1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:33.963: INFO: 
Aug 22 02:07:33.963: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 22 02:07:34.969: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:34.969: INFO: ss-0  melsayed-conformance3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:34.969: INFO: ss-2  melsayed-conformance1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:34.969: INFO: 
Aug 22 02:07:34.969: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 22 02:07:35.974: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:35.974: INFO: ss-0  melsayed-conformance3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:35.974: INFO: ss-2  melsayed-conformance1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:08 +0000 UTC  }]
Aug 22 02:07:35.974: INFO: 
Aug 22 02:07:35.974: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 22 02:07:36.980: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:36.980: INFO: ss-0  melsayed-conformance3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:36.980: INFO: 
Aug 22 02:07:36.980: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 22 02:07:37.985: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Aug 22 02:07:37.985: INFO: ss-0  melsayed-conformance3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:07:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:06:47 +0000 UTC  }]
Aug 22 02:07:37.985: INFO: 
Aug 22 02:07:37.985: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 22 02:07:38.989: INFO: Verifying statefulset ss doesn't scale past 0 for another 952.358094ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4606
Aug 22 02:07:39.992: INFO: Scaling statefulset ss to 0
Aug 22 02:07:40.003: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 22 02:07:40.006: INFO: Deleting all statefulset in ns statefulset-4606
Aug 22 02:07:40.008: INFO: Scaling statefulset ss to 0
Aug 22 02:07:40.014: INFO: Waiting for statefulset status.replicas updated to 0
Aug 22 02:07:40.016: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:07:40.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4606" for this suite.
Aug 22 02:07:46.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:07:46.130: INFO: namespace statefulset-4606 deletion completed in 6.100308096s

• [SLOW TEST:58.758 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:07:46.130: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-6hw7
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 02:07:46.189: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6hw7" in namespace "subpath-9689" to be "success or failure"
Aug 22 02:07:46.199: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.258696ms
Aug 22 02:07:48.203: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013648928s
Aug 22 02:07:50.208: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 4.018901776s
Aug 22 02:07:52.212: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 6.023181439s
Aug 22 02:07:54.218: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 8.02908381s
Aug 22 02:07:56.222: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 10.032740128s
Aug 22 02:07:58.226: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 12.03671622s
Aug 22 02:08:00.229: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 14.040187848s
Aug 22 02:08:02.234: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 16.045584814s
Aug 22 02:08:04.239: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 18.050300731s
Aug 22 02:08:06.245: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 20.056175057s
Aug 22 02:08:08.248: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Running", Reason="", readiness=true. Elapsed: 22.059358985s
Aug 22 02:08:10.253: INFO: Pod "pod-subpath-test-secret-6hw7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.063885318s
STEP: Saw pod success
Aug 22 02:08:10.253: INFO: Pod "pod-subpath-test-secret-6hw7" satisfied condition "success or failure"
Aug 22 02:08:10.256: INFO: Trying to get logs from node melsayed-conformance3 pod pod-subpath-test-secret-6hw7 container test-container-subpath-secret-6hw7: <nil>
STEP: delete the pod
Aug 22 02:08:10.281: INFO: Waiting for pod pod-subpath-test-secret-6hw7 to disappear
Aug 22 02:08:10.283: INFO: Pod pod-subpath-test-secret-6hw7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-6hw7
Aug 22 02:08:10.283: INFO: Deleting pod "pod-subpath-test-secret-6hw7" in namespace "subpath-9689"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:08:10.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9689" for this suite.
Aug 22 02:08:16.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:08:16.399: INFO: namespace subpath-9689 deletion completed in 6.111432297s

• [SLOW TEST:30.269 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:08:16.400: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-cde7c128-0ed7-45e5-be55-581a3c2b975b
STEP: Creating secret with name secret-projected-all-test-volume-2a16422d-c528-4322-aa68-b358b1693632
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 22 02:08:16.449: INFO: Waiting up to 5m0s for pod "projected-volume-7cc41e23-9419-4958-b03f-7b3a238eb4c1" in namespace "projected-4507" to be "success or failure"
Aug 22 02:08:16.459: INFO: Pod "projected-volume-7cc41e23-9419-4958-b03f-7b3a238eb4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.986913ms
Aug 22 02:08:18.462: INFO: Pod "projected-volume-7cc41e23-9419-4958-b03f-7b3a238eb4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013251729s
Aug 22 02:08:20.466: INFO: Pod "projected-volume-7cc41e23-9419-4958-b03f-7b3a238eb4c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017392546s
STEP: Saw pod success
Aug 22 02:08:20.467: INFO: Pod "projected-volume-7cc41e23-9419-4958-b03f-7b3a238eb4c1" satisfied condition "success or failure"
Aug 22 02:08:20.472: INFO: Trying to get logs from node melsayed-conformance3 pod projected-volume-7cc41e23-9419-4958-b03f-7b3a238eb4c1 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 22 02:08:20.490: INFO: Waiting for pod projected-volume-7cc41e23-9419-4958-b03f-7b3a238eb4c1 to disappear
Aug 22 02:08:20.494: INFO: Pod projected-volume-7cc41e23-9419-4958-b03f-7b3a238eb4c1 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:08:20.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4507" for this suite.
Aug 22 02:08:26.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:08:26.608: INFO: namespace projected-4507 deletion completed in 6.109859671s

• [SLOW TEST:10.208 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:08:26.611: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 22 02:08:26.725: INFO: Waiting up to 5m0s for pod "pod-fc166977-7d45-4456-a568-2dcb04daa499" in namespace "emptydir-470" to be "success or failure"
Aug 22 02:08:26.737: INFO: Pod "pod-fc166977-7d45-4456-a568-2dcb04daa499": Phase="Pending", Reason="", readiness=false. Elapsed: 11.901013ms
Aug 22 02:08:28.740: INFO: Pod "pod-fc166977-7d45-4456-a568-2dcb04daa499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015700213s
STEP: Saw pod success
Aug 22 02:08:28.741: INFO: Pod "pod-fc166977-7d45-4456-a568-2dcb04daa499" satisfied condition "success or failure"
Aug 22 02:08:28.744: INFO: Trying to get logs from node melsayed-conformance3 pod pod-fc166977-7d45-4456-a568-2dcb04daa499 container test-container: <nil>
STEP: delete the pod
Aug 22 02:08:28.772: INFO: Waiting for pod pod-fc166977-7d45-4456-a568-2dcb04daa499 to disappear
Aug 22 02:08:28.775: INFO: Pod pod-fc166977-7d45-4456-a568-2dcb04daa499 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:08:28.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-470" for this suite.
Aug 22 02:08:34.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:08:34.875: INFO: namespace emptydir-470 deletion completed in 6.096323252s

• [SLOW TEST:8.265 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:08:34.878: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-d89daeb4-ef6c-4bc4-abd7-89df8548d3dc
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d89daeb4-ef6c-4bc4-abd7-89df8548d3dc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:09:57.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4510" for this suite.
Aug 22 02:10:19.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:10:19.553: INFO: namespace configmap-4510 deletion completed in 22.140282582s

• [SLOW TEST:104.675 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:10:19.553: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 22 02:10:22.228: INFO: Successfully updated pod "labelsupdate24aa5d3c-1bcb-4b75-9f0a-ae5585ac1052"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:10:24.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5359" for this suite.
Aug 22 02:10:46.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:10:46.395: INFO: namespace downward-api-5359 deletion completed in 22.138595648s

• [SLOW TEST:26.842 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:10:46.395: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:10:46.435: INFO: Creating ReplicaSet my-hostname-basic-e9d1bac9-771b-4a69-9a09-d11a7446e24a
Aug 22 02:10:46.443: INFO: Pod name my-hostname-basic-e9d1bac9-771b-4a69-9a09-d11a7446e24a: Found 0 pods out of 1
Aug 22 02:10:51.448: INFO: Pod name my-hostname-basic-e9d1bac9-771b-4a69-9a09-d11a7446e24a: Found 1 pods out of 1
Aug 22 02:10:51.448: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e9d1bac9-771b-4a69-9a09-d11a7446e24a" is running
Aug 22 02:10:51.453: INFO: Pod "my-hostname-basic-e9d1bac9-771b-4a69-9a09-d11a7446e24a-db575" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-22 02:10:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-22 02:10:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-22 02:10:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-22 02:10:46 +0000 UTC Reason: Message:}])
Aug 22 02:10:51.453: INFO: Trying to dial the pod
Aug 22 02:10:56.470: INFO: Controller my-hostname-basic-e9d1bac9-771b-4a69-9a09-d11a7446e24a: Got expected result from replica 1 [my-hostname-basic-e9d1bac9-771b-4a69-9a09-d11a7446e24a-db575]: "my-hostname-basic-e9d1bac9-771b-4a69-9a09-d11a7446e24a-db575", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:10:56.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1721" for this suite.
Aug 22 02:11:02.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:11:02.583: INFO: namespace replicaset-1721 deletion completed in 6.109363143s

• [SLOW TEST:16.189 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:11:02.586: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-94ee0266-5e96-480c-9a8e-80b84d7ec966 in namespace container-probe-3578
Aug 22 02:11:06.633: INFO: Started pod liveness-94ee0266-5e96-480c-9a8e-80b84d7ec966 in namespace container-probe-3578
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 02:11:06.637: INFO: Initial restart count of pod liveness-94ee0266-5e96-480c-9a8e-80b84d7ec966 is 0
Aug 22 02:11:24.685: INFO: Restart count of pod container-probe-3578/liveness-94ee0266-5e96-480c-9a8e-80b84d7ec966 is now 1 (18.048207509s elapsed)
Aug 22 02:11:42.857: INFO: Restart count of pod container-probe-3578/liveness-94ee0266-5e96-480c-9a8e-80b84d7ec966 is now 2 (36.220453376s elapsed)
Aug 22 02:12:02.937: INFO: Restart count of pod container-probe-3578/liveness-94ee0266-5e96-480c-9a8e-80b84d7ec966 is now 3 (56.299996171s elapsed)
Aug 22 02:12:22.991: INFO: Restart count of pod container-probe-3578/liveness-94ee0266-5e96-480c-9a8e-80b84d7ec966 is now 4 (1m16.354732956s elapsed)
Aug 22 02:13:23.122: INFO: Restart count of pod container-probe-3578/liveness-94ee0266-5e96-480c-9a8e-80b84d7ec966 is now 5 (2m16.485696093s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:13:23.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3578" for this suite.
Aug 22 02:13:29.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:13:29.266: INFO: namespace container-probe-3578 deletion completed in 6.119255777s

• [SLOW TEST:146.680 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:13:29.269: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:13:29.366: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13580baa-ed8b-47af-a6e6-7da7f16aa8de" in namespace "projected-684" to be "success or failure"
Aug 22 02:13:29.377: INFO: Pod "downwardapi-volume-13580baa-ed8b-47af-a6e6-7da7f16aa8de": Phase="Pending", Reason="", readiness=false. Elapsed: 10.153808ms
Aug 22 02:13:31.380: INFO: Pod "downwardapi-volume-13580baa-ed8b-47af-a6e6-7da7f16aa8de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013892771s
Aug 22 02:13:33.384: INFO: Pod "downwardapi-volume-13580baa-ed8b-47af-a6e6-7da7f16aa8de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01763636s
STEP: Saw pod success
Aug 22 02:13:33.385: INFO: Pod "downwardapi-volume-13580baa-ed8b-47af-a6e6-7da7f16aa8de" satisfied condition "success or failure"
Aug 22 02:13:33.388: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-13580baa-ed8b-47af-a6e6-7da7f16aa8de container client-container: <nil>
STEP: delete the pod
Aug 22 02:13:33.407: INFO: Waiting for pod downwardapi-volume-13580baa-ed8b-47af-a6e6-7da7f16aa8de to disappear
Aug 22 02:13:33.415: INFO: Pod downwardapi-volume-13580baa-ed8b-47af-a6e6-7da7f16aa8de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:13:33.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-684" for this suite.
Aug 22 02:13:39.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:13:39.527: INFO: namespace projected-684 deletion completed in 6.108337259s

• [SLOW TEST:10.259 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:13:39.529: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:13:41.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4872" for this suite.
Aug 22 02:14:21.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:14:21.781: INFO: namespace kubelet-test-4872 deletion completed in 40.164420179s

• [SLOW TEST:42.252 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:14:21.782: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-8207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8207 to expose endpoints map[]
Aug 22 02:14:21.833: INFO: Get endpoints failed (6.641526ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Aug 22 02:14:22.838: INFO: successfully validated that service multi-endpoint-test in namespace services-8207 exposes endpoints map[] (1.011579765s elapsed)
STEP: Creating pod pod1 in namespace services-8207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8207 to expose endpoints map[pod1:[100]]
Aug 22 02:14:27.130: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.276056751s elapsed, will retry)
Aug 22 02:14:28.136: INFO: successfully validated that service multi-endpoint-test in namespace services-8207 exposes endpoints map[pod1:[100]] (5.281826483s elapsed)
STEP: Creating pod pod2 in namespace services-8207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8207 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 22 02:14:32.219: INFO: Unexpected endpoints: found map[59756b62-843f-4c0d-8ab6-f398ffd32a96:[100]], expected map[pod1:[100] pod2:[101]] (4.061456757s elapsed, will retry)
Aug 22 02:14:37.280: INFO: successfully validated that service multi-endpoint-test in namespace services-8207 exposes endpoints map[pod1:[100] pod2:[101]] (9.122968124s elapsed)
STEP: Deleting pod pod1 in namespace services-8207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8207 to expose endpoints map[pod2:[101]]
Aug 22 02:14:38.304: INFO: successfully validated that service multi-endpoint-test in namespace services-8207 exposes endpoints map[pod2:[101]] (1.019322744s elapsed)
STEP: Deleting pod pod2 in namespace services-8207
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8207 to expose endpoints map[]
Aug 22 02:14:39.321: INFO: successfully validated that service multi-endpoint-test in namespace services-8207 exposes endpoints map[] (1.007574184s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:14:39.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8207" for this suite.
Aug 22 02:15:01.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:15:01.475: INFO: namespace services-8207 deletion completed in 22.113836892s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:39.694 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:15:01.480: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 22 02:15:01.521: INFO: Waiting up to 5m0s for pod "downward-api-536bf6cb-1147-4566-b888-2ff8152afda8" in namespace "downward-api-6396" to be "success or failure"
Aug 22 02:15:01.527: INFO: Pod "downward-api-536bf6cb-1147-4566-b888-2ff8152afda8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.707697ms
Aug 22 02:15:03.531: INFO: Pod "downward-api-536bf6cb-1147-4566-b888-2ff8152afda8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009435997s
Aug 22 02:15:06.087: INFO: Pod "downward-api-536bf6cb-1147-4566-b888-2ff8152afda8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.566370613s
STEP: Saw pod success
Aug 22 02:15:06.088: INFO: Pod "downward-api-536bf6cb-1147-4566-b888-2ff8152afda8" satisfied condition "success or failure"
Aug 22 02:15:06.096: INFO: Trying to get logs from node melsayed-conformance3 pod downward-api-536bf6cb-1147-4566-b888-2ff8152afda8 container dapi-container: <nil>
STEP: delete the pod
Aug 22 02:15:06.131: INFO: Waiting for pod downward-api-536bf6cb-1147-4566-b888-2ff8152afda8 to disappear
Aug 22 02:15:06.133: INFO: Pod downward-api-536bf6cb-1147-4566-b888-2ff8152afda8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:15:06.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6396" for this suite.
Aug 22 02:15:12.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:15:12.312: INFO: namespace downward-api-6396 deletion completed in 6.176689613s

• [SLOW TEST:10.833 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:15:12.318: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-5139
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5139 to expose endpoints map[]
Aug 22 02:15:12.357: INFO: successfully validated that service endpoint-test2 in namespace services-5139 exposes endpoints map[] (9.016843ms elapsed)
STEP: Creating pod pod1 in namespace services-5139
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5139 to expose endpoints map[pod1:[80]]
Aug 22 02:15:14.391: INFO: successfully validated that service endpoint-test2 in namespace services-5139 exposes endpoints map[pod1:[80]] (2.026644806s elapsed)
STEP: Creating pod pod2 in namespace services-5139
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5139 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 22 02:15:16.428: INFO: successfully validated that service endpoint-test2 in namespace services-5139 exposes endpoints map[pod1:[80] pod2:[80]] (2.033793192s elapsed)
STEP: Deleting pod pod1 in namespace services-5139
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5139 to expose endpoints map[pod2:[80]]
Aug 22 02:15:17.447: INFO: successfully validated that service endpoint-test2 in namespace services-5139 exposes endpoints map[pod2:[80]] (1.015121392s elapsed)
STEP: Deleting pod pod2 in namespace services-5139
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5139 to expose endpoints map[]
Aug 22 02:15:17.485: INFO: successfully validated that service endpoint-test2 in namespace services-5139 exposes endpoints map[] (30.558558ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:15:17.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5139" for this suite.
Aug 22 02:15:39.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:15:39.600: INFO: namespace services-5139 deletion completed in 22.094113472s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:27.283 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:15:39.602: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 22 02:15:43.697: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 02:15:43.700: INFO: Pod pod-with-poststart-http-hook still exists
Aug 22 02:15:45.700: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 02:15:45.706: INFO: Pod pod-with-poststart-http-hook still exists
Aug 22 02:15:47.700: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 02:15:47.706: INFO: Pod pod-with-poststart-http-hook still exists
Aug 22 02:15:49.700: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 02:15:49.703: INFO: Pod pod-with-poststart-http-hook still exists
Aug 22 02:15:51.700: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 02:15:51.704: INFO: Pod pod-with-poststart-http-hook still exists
Aug 22 02:15:53.700: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 02:15:53.704: INFO: Pod pod-with-poststart-http-hook still exists
Aug 22 02:15:55.701: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 22 02:15:55.705: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:15:55.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6041" for this suite.
Aug 22 02:16:17.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:16:17.907: INFO: namespace container-lifecycle-hook-6041 deletion completed in 22.197583821s

• [SLOW TEST:38.305 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:16:17.907: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-9c14d5b1-fb16-480c-a3f5-c07c1af7c86a
STEP: Creating a pod to test consume configMaps
Aug 22 02:16:17.947: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f25d877-099e-4810-a811-549cb67e87ca" in namespace "projected-271" to be "success or failure"
Aug 22 02:16:17.951: INFO: Pod "pod-projected-configmaps-1f25d877-099e-4810-a811-549cb67e87ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.955372ms
Aug 22 02:16:19.955: INFO: Pod "pod-projected-configmaps-1f25d877-099e-4810-a811-549cb67e87ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007887019s
STEP: Saw pod success
Aug 22 02:16:19.955: INFO: Pod "pod-projected-configmaps-1f25d877-099e-4810-a811-549cb67e87ca" satisfied condition "success or failure"
Aug 22 02:16:19.958: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-configmaps-1f25d877-099e-4810-a811-549cb67e87ca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 02:16:19.976: INFO: Waiting for pod pod-projected-configmaps-1f25d877-099e-4810-a811-549cb67e87ca to disappear
Aug 22 02:16:19.980: INFO: Pod pod-projected-configmaps-1f25d877-099e-4810-a811-549cb67e87ca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:16:19.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-271" for this suite.
Aug 22 02:16:25.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:16:26.098: INFO: namespace projected-271 deletion completed in 6.11548627s

• [SLOW TEST:8.192 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:16:26.102: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 22 02:16:28.215: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-089099426 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 22 02:16:38.304: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:16:38.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7379" for this suite.
Aug 22 02:16:44.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:16:44.407: INFO: namespace pods-7379 deletion completed in 6.094956469s

• [SLOW TEST:18.306 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:16:44.407: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Aug 22 02:16:44.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 cluster-info'
Aug 22 02:16:44.719: INFO: stderr: ""
Aug 22 02:16:44.719: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:16:44.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5174" for this suite.
Aug 22 02:16:50.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:16:50.853: INFO: namespace kubectl-5174 deletion completed in 6.129778991s

• [SLOW TEST:6.445 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:16:50.853: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:16:50.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a881d9d-5898-46b5-8a74-d51d8339a6a8" in namespace "projected-5486" to be "success or failure"
Aug 22 02:16:50.952: INFO: Pod "downwardapi-volume-9a881d9d-5898-46b5-8a74-d51d8339a6a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666172ms
Aug 22 02:16:52.957: INFO: Pod "downwardapi-volume-9a881d9d-5898-46b5-8a74-d51d8339a6a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009760273s
STEP: Saw pod success
Aug 22 02:16:52.957: INFO: Pod "downwardapi-volume-9a881d9d-5898-46b5-8a74-d51d8339a6a8" satisfied condition "success or failure"
Aug 22 02:16:52.960: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-9a881d9d-5898-46b5-8a74-d51d8339a6a8 container client-container: <nil>
STEP: delete the pod
Aug 22 02:16:52.992: INFO: Waiting for pod downwardapi-volume-9a881d9d-5898-46b5-8a74-d51d8339a6a8 to disappear
Aug 22 02:16:53.001: INFO: Pod downwardapi-volume-9a881d9d-5898-46b5-8a74-d51d8339a6a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:16:53.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5486" for this suite.
Aug 22 02:16:59.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:16:59.115: INFO: namespace projected-5486 deletion completed in 6.110087882s

• [SLOW TEST:8.262 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:16:59.119: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:16:59.158: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38abeaa7-1aad-49b5-8d77-c7eb8c8dfaae" in namespace "downward-api-9549" to be "success or failure"
Aug 22 02:16:59.188: INFO: Pod "downwardapi-volume-38abeaa7-1aad-49b5-8d77-c7eb8c8dfaae": Phase="Pending", Reason="", readiness=false. Elapsed: 29.17191ms
Aug 22 02:17:01.191: INFO: Pod "downwardapi-volume-38abeaa7-1aad-49b5-8d77-c7eb8c8dfaae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032896921s
Aug 22 02:17:03.195: INFO: Pod "downwardapi-volume-38abeaa7-1aad-49b5-8d77-c7eb8c8dfaae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036920792s
STEP: Saw pod success
Aug 22 02:17:03.195: INFO: Pod "downwardapi-volume-38abeaa7-1aad-49b5-8d77-c7eb8c8dfaae" satisfied condition "success or failure"
Aug 22 02:17:03.199: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-38abeaa7-1aad-49b5-8d77-c7eb8c8dfaae container client-container: <nil>
STEP: delete the pod
Aug 22 02:17:03.227: INFO: Waiting for pod downwardapi-volume-38abeaa7-1aad-49b5-8d77-c7eb8c8dfaae to disappear
Aug 22 02:17:03.230: INFO: Pod downwardapi-volume-38abeaa7-1aad-49b5-8d77-c7eb8c8dfaae no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:17:03.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9549" for this suite.
Aug 22 02:17:09.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:17:09.382: INFO: namespace downward-api-9549 deletion completed in 6.149313947s

• [SLOW TEST:10.263 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:17:09.388: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 22 02:17:09.448: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9544,SelfLink:/api/v1/namespaces/watch-9544/configmaps/e2e-watch-test-resource-version,UID:da100109-1357-42f7-bf8e-b6df421d2ffb,ResourceVersion:12830,Generation:0,CreationTimestamp:2019-08-22 02:17:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 22 02:17:09.448: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9544,SelfLink:/api/v1/namespaces/watch-9544/configmaps/e2e-watch-test-resource-version,UID:da100109-1357-42f7-bf8e-b6df421d2ffb,ResourceVersion:12831,Generation:0,CreationTimestamp:2019-08-22 02:17:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:17:09.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9544" for this suite.
Aug 22 02:17:15.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:17:15.551: INFO: namespace watch-9544 deletion completed in 6.09927344s

• [SLOW TEST:6.163 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:17:15.561: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-ba63fc0d-555a-4559-ad92-429e8b3e6535
STEP: Creating a pod to test consume configMaps
Aug 22 02:17:15.657: INFO: Waiting up to 5m0s for pod "pod-configmaps-48281e6a-37b4-4378-9ad1-78dad7526afc" in namespace "configmap-950" to be "success or failure"
Aug 22 02:17:15.670: INFO: Pod "pod-configmaps-48281e6a-37b4-4378-9ad1-78dad7526afc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.317007ms
Aug 22 02:17:17.674: INFO: Pod "pod-configmaps-48281e6a-37b4-4378-9ad1-78dad7526afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017261478s
Aug 22 02:17:19.678: INFO: Pod "pod-configmaps-48281e6a-37b4-4378-9ad1-78dad7526afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020699317s
STEP: Saw pod success
Aug 22 02:17:19.678: INFO: Pod "pod-configmaps-48281e6a-37b4-4378-9ad1-78dad7526afc" satisfied condition "success or failure"
Aug 22 02:17:19.681: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-48281e6a-37b4-4378-9ad1-78dad7526afc container configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 02:17:19.701: INFO: Waiting for pod pod-configmaps-48281e6a-37b4-4378-9ad1-78dad7526afc to disappear
Aug 22 02:17:19.704: INFO: Pod pod-configmaps-48281e6a-37b4-4378-9ad1-78dad7526afc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:17:19.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-950" for this suite.
Aug 22 02:17:25.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:17:25.795: INFO: namespace configmap-950 deletion completed in 6.08769345s

• [SLOW TEST:10.234 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:17:25.795: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-47bcd0c6-0b15-4a9a-8701-7eaa93afc384
STEP: Creating secret with name s-test-opt-upd-7ab0e475-d2f0-4681-8d8b-dbc86ee7f677
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-47bcd0c6-0b15-4a9a-8701-7eaa93afc384
STEP: Updating secret s-test-opt-upd-7ab0e475-d2f0-4681-8d8b-dbc86ee7f677
STEP: Creating secret with name s-test-opt-create-ad38a2a9-cf88-45e4-a055-e8f973537303
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:18:38.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-435" for this suite.
Aug 22 02:19:00.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:19:00.472: INFO: namespace projected-435 deletion completed in 22.120377003s

• [SLOW TEST:94.677 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:19:00.472: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-3219fbe1-c42f-446d-9a5b-8c64fa6d85d8
STEP: Creating a pod to test consume secrets
Aug 22 02:19:00.516: INFO: Waiting up to 5m0s for pod "pod-secrets-182350db-5a6a-4bf0-a5d2-9d94c5ddd3cb" in namespace "secrets-447" to be "success or failure"
Aug 22 02:19:00.520: INFO: Pod "pod-secrets-182350db-5a6a-4bf0-a5d2-9d94c5ddd3cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.175573ms
Aug 22 02:19:02.524: INFO: Pod "pod-secrets-182350db-5a6a-4bf0-a5d2-9d94c5ddd3cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007458805s
Aug 22 02:19:04.528: INFO: Pod "pod-secrets-182350db-5a6a-4bf0-a5d2-9d94c5ddd3cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011125805s
STEP: Saw pod success
Aug 22 02:19:04.528: INFO: Pod "pod-secrets-182350db-5a6a-4bf0-a5d2-9d94c5ddd3cb" satisfied condition "success or failure"
Aug 22 02:19:04.530: INFO: Trying to get logs from node melsayed-conformance3 pod pod-secrets-182350db-5a6a-4bf0-a5d2-9d94c5ddd3cb container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 02:19:04.554: INFO: Waiting for pod pod-secrets-182350db-5a6a-4bf0-a5d2-9d94c5ddd3cb to disappear
Aug 22 02:19:04.559: INFO: Pod pod-secrets-182350db-5a6a-4bf0-a5d2-9d94c5ddd3cb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:19:04.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-447" for this suite.
Aug 22 02:19:10.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:19:10.653: INFO: namespace secrets-447 deletion completed in 6.091205937s

• [SLOW TEST:10.181 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:19:10.655: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Aug 22 02:19:10.686: INFO: Waiting up to 5m0s for pod "client-containers-00d53e15-8cb9-4d85-8475-2497d881e4fe" in namespace "containers-9557" to be "success or failure"
Aug 22 02:19:10.690: INFO: Pod "client-containers-00d53e15-8cb9-4d85-8475-2497d881e4fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.191008ms
Aug 22 02:19:12.694: INFO: Pod "client-containers-00d53e15-8cb9-4d85-8475-2497d881e4fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.007585008s
Aug 22 02:19:14.698: INFO: Pod "client-containers-00d53e15-8cb9-4d85-8475-2497d881e4fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011348611s
STEP: Saw pod success
Aug 22 02:19:14.698: INFO: Pod "client-containers-00d53e15-8cb9-4d85-8475-2497d881e4fe" satisfied condition "success or failure"
Aug 22 02:19:14.703: INFO: Trying to get logs from node melsayed-conformance3 pod client-containers-00d53e15-8cb9-4d85-8475-2497d881e4fe container test-container: <nil>
STEP: delete the pod
Aug 22 02:19:14.725: INFO: Waiting for pod client-containers-00d53e15-8cb9-4d85-8475-2497d881e4fe to disappear
Aug 22 02:19:14.727: INFO: Pod client-containers-00d53e15-8cb9-4d85-8475-2497d881e4fe no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:19:14.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9557" for this suite.
Aug 22 02:19:20.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:19:20.851: INFO: namespace containers-9557 deletion completed in 6.120925965s

• [SLOW TEST:10.197 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:19:20.855: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 22 02:19:20.903: INFO: Waiting up to 5m0s for pod "pod-34921714-f429-4077-9eb3-2b094b11b120" in namespace "emptydir-1842" to be "success or failure"
Aug 22 02:19:20.906: INFO: Pod "pod-34921714-f429-4077-9eb3-2b094b11b120": Phase="Pending", Reason="", readiness=false. Elapsed: 2.858048ms
Aug 22 02:19:22.909: INFO: Pod "pod-34921714-f429-4077-9eb3-2b094b11b120": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006429489s
Aug 22 02:19:24.914: INFO: Pod "pod-34921714-f429-4077-9eb3-2b094b11b120": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010790487s
STEP: Saw pod success
Aug 22 02:19:24.914: INFO: Pod "pod-34921714-f429-4077-9eb3-2b094b11b120" satisfied condition "success or failure"
Aug 22 02:19:24.917: INFO: Trying to get logs from node melsayed-conformance3 pod pod-34921714-f429-4077-9eb3-2b094b11b120 container test-container: <nil>
STEP: delete the pod
Aug 22 02:19:24.938: INFO: Waiting for pod pod-34921714-f429-4077-9eb3-2b094b11b120 to disappear
Aug 22 02:19:24.940: INFO: Pod pod-34921714-f429-4077-9eb3-2b094b11b120 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:19:24.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1842" for this suite.
Aug 22 02:19:30.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:19:31.052: INFO: namespace emptydir-1842 deletion completed in 6.109053126s

• [SLOW TEST:10.197 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:19:31.052: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 22 02:19:31.074: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 22 02:19:31.082: INFO: Waiting for terminating namespaces to be deleted...
Aug 22 02:19:31.085: INFO: 
Logging pods the kubelet thinks is on node melsayed-conformance1 before test
Aug 22 02:19:31.095: INFO: default-http-backend-8456b47798-h7zvx from ingress-nginx started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.095: INFO: 	Container default-http-backend ready: true, restart count 0
Aug 22 02:19:31.095: INFO: cattle-node-agent-d2s4f from cattle-system started at 2019-08-22 01:28:10 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.095: INFO: 	Container agent ready: true, restart count 0
Aug 22 02:19:31.095: INFO: sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-b6mbk from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:19:31.096: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 02:19:31.096: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 22 02:19:31.096: INFO: coredns-5678df9bcc-74zgx from kube-system started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.096: INFO: 	Container coredns ready: true, restart count 0
Aug 22 02:19:31.096: INFO: rke-metrics-addon-deploy-job-c28kb from kube-system started at 2019-08-22 01:27:40 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.096: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Aug 22 02:19:31.096: INFO: rke-ingress-controller-deploy-job-64vk6 from kube-system started at 2019-08-22 01:27:45 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.097: INFO: 	Container rke-ingress-controller-pod ready: false, restart count 0
Aug 22 02:19:31.097: INFO: metrics-server-dbcdc5bc9-pjqjt from kube-system started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.097: INFO: 	Container metrics-server ready: true, restart count 0
Aug 22 02:19:31.097: INFO: coredns-autoscaler-57bc9c9bd-2dczs from kube-system started at 2019-08-22 01:27:50 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.097: INFO: 	Container autoscaler ready: true, restart count 0
Aug 22 02:19:31.097: INFO: cattle-cluster-agent-55977bf459-5lfn6 from cattle-system started at 2019-08-22 01:28:10 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.097: INFO: 	Container cluster-register ready: true, restart count 0
Aug 22 02:19:31.097: INFO: kube-api-auth-87zdm from cattle-system started at 2019-08-22 01:28:10 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.097: INFO: 	Container kube-api-auth ready: true, restart count 0
Aug 22 02:19:31.097: INFO: rke-network-plugin-deploy-job-qckrh from kube-system started at 2019-08-22 01:27:29 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.097: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Aug 22 02:19:31.097: INFO: rke-coredns-addon-deploy-job-pldgp from kube-system started at 2019-08-22 01:27:34 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.097: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Aug 22 02:19:31.098: INFO: nginx-ingress-controller-44rfm from ingress-nginx started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.098: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 22 02:19:31.098: INFO: canal-lwlb2 from kube-system started at 2019-08-22 01:27:32 +0000 UTC (2 container statuses recorded)
Aug 22 02:19:31.098: INFO: 	Container calico-node ready: true, restart count 0
Aug 22 02:19:31.098: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 02:19:31.098: INFO: 
Logging pods the kubelet thinks is on node melsayed-conformance2 before test
Aug 22 02:19:31.108: INFO: cattle-node-agent-x849w from cattle-system started at 2019-08-22 01:28:45 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.108: INFO: 	Container agent ready: true, restart count 0
Aug 22 02:19:31.108: INFO: nginx-ingress-controller-wsr46 from ingress-nginx started at 2019-08-22 01:29:06 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.108: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 22 02:19:31.108: INFO: sonobuoy-e2e-job-58e145a7c72f46b3 from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:19:31.108: INFO: 	Container e2e ready: true, restart count 0
Aug 22 02:19:31.108: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 02:19:31.108: INFO: sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-bs4pn from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:19:31.108: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 02:19:31.108: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 22 02:19:31.108: INFO: canal-2fsgt from kube-system started at 2019-08-22 01:28:45 +0000 UTC (2 container statuses recorded)
Aug 22 02:19:31.108: INFO: 	Container calico-node ready: true, restart count 0
Aug 22 02:19:31.108: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 02:19:31.108: INFO: 
Logging pods the kubelet thinks is on node melsayed-conformance3 before test
Aug 22 02:19:31.115: INFO: sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-p5mld from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:19:31.115: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 02:19:31.115: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 22 02:19:31.115: INFO: nginx-ingress-controller-h65vt from ingress-nginx started at 2019-08-22 01:28:58 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.115: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 22 02:19:31.115: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-22 01:35:54 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.115: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 22 02:19:31.115: INFO: cattle-node-agent-r7lmz from cattle-system started at 2019-08-22 01:28:38 +0000 UTC (1 container statuses recorded)
Aug 22 02:19:31.115: INFO: 	Container agent ready: true, restart count 0
Aug 22 02:19:31.115: INFO: canal-kcl25 from kube-system started at 2019-08-22 01:28:38 +0000 UTC (2 container statuses recorded)
Aug 22 02:19:31.115: INFO: 	Container calico-node ready: true, restart count 0
Aug 22 02:19:31.115: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c5c28ddb-de92-4b43-a9ae-387dd678580a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c5c28ddb-de92-4b43-a9ae-387dd678580a off the node melsayed-conformance3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c5c28ddb-de92-4b43-a9ae-387dd678580a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:19:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4607" for this suite.
Aug 22 02:19:47.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:19:47.322: INFO: namespace sched-pred-4607 deletion completed in 8.117924599s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:16.271 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:19:47.328: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-30d3d3ee-4520-4d67-a8ad-03b6c4ec7f75
STEP: Creating a pod to test consume secrets
Aug 22 02:19:47.371: INFO: Waiting up to 5m0s for pod "pod-secrets-d0a43b15-7289-45b3-9cec-4cd8ed650587" in namespace "secrets-5349" to be "success or failure"
Aug 22 02:19:47.383: INFO: Pod "pod-secrets-d0a43b15-7289-45b3-9cec-4cd8ed650587": Phase="Pending", Reason="", readiness=false. Elapsed: 12.891432ms
Aug 22 02:19:49.387: INFO: Pod "pod-secrets-d0a43b15-7289-45b3-9cec-4cd8ed650587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016337065s
Aug 22 02:19:51.391: INFO: Pod "pod-secrets-d0a43b15-7289-45b3-9cec-4cd8ed650587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020155211s
STEP: Saw pod success
Aug 22 02:19:51.391: INFO: Pod "pod-secrets-d0a43b15-7289-45b3-9cec-4cd8ed650587" satisfied condition "success or failure"
Aug 22 02:19:51.394: INFO: Trying to get logs from node melsayed-conformance3 pod pod-secrets-d0a43b15-7289-45b3-9cec-4cd8ed650587 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 02:19:51.415: INFO: Waiting for pod pod-secrets-d0a43b15-7289-45b3-9cec-4cd8ed650587 to disappear
Aug 22 02:19:51.419: INFO: Pod pod-secrets-d0a43b15-7289-45b3-9cec-4cd8ed650587 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:19:51.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5349" for this suite.
Aug 22 02:19:57.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:19:57.514: INFO: namespace secrets-5349 deletion completed in 6.091876162s

• [SLOW TEST:10.186 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:19:57.519: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-264d
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 02:19:57.564: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-264d" in namespace "subpath-6153" to be "success or failure"
Aug 22 02:19:57.570: INFO: Pod "pod-subpath-test-projected-264d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378474ms
Aug 22 02:19:59.574: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009576064s
Aug 22 02:20:01.578: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 4.01384457s
Aug 22 02:20:03.597: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 6.032582475s
Aug 22 02:20:05.601: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 8.036936465s
Aug 22 02:20:07.606: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 10.041746578s
Aug 22 02:20:09.610: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 12.046135131s
Aug 22 02:20:11.615: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 14.050585493s
Aug 22 02:20:13.619: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 16.054721244s
Aug 22 02:20:15.622: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 18.057992976s
Aug 22 02:20:17.627: INFO: Pod "pod-subpath-test-projected-264d": Phase="Running", Reason="", readiness=true. Elapsed: 20.062472384s
Aug 22 02:20:19.631: INFO: Pod "pod-subpath-test-projected-264d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.066684566s
STEP: Saw pod success
Aug 22 02:20:19.631: INFO: Pod "pod-subpath-test-projected-264d" satisfied condition "success or failure"
Aug 22 02:20:19.635: INFO: Trying to get logs from node melsayed-conformance3 pod pod-subpath-test-projected-264d container test-container-subpath-projected-264d: <nil>
STEP: delete the pod
Aug 22 02:20:19.653: INFO: Waiting for pod pod-subpath-test-projected-264d to disappear
Aug 22 02:20:19.660: INFO: Pod pod-subpath-test-projected-264d no longer exists
STEP: Deleting pod pod-subpath-test-projected-264d
Aug 22 02:20:19.661: INFO: Deleting pod "pod-subpath-test-projected-264d" in namespace "subpath-6153"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:20:19.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6153" for this suite.
Aug 22 02:20:25.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:20:25.789: INFO: namespace subpath-6153 deletion completed in 6.11631866s

• [SLOW TEST:28.271 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:20:25.792: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 22 02:20:25.830: INFO: Waiting up to 5m0s for pod "downward-api-c0e825c6-14a3-46a4-badf-7c89d3639ce0" in namespace "downward-api-4709" to be "success or failure"
Aug 22 02:20:25.844: INFO: Pod "downward-api-c0e825c6-14a3-46a4-badf-7c89d3639ce0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.956474ms
Aug 22 02:20:27.847: INFO: Pod "downward-api-c0e825c6-14a3-46a4-badf-7c89d3639ce0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017804924s
STEP: Saw pod success
Aug 22 02:20:27.847: INFO: Pod "downward-api-c0e825c6-14a3-46a4-badf-7c89d3639ce0" satisfied condition "success or failure"
Aug 22 02:20:27.850: INFO: Trying to get logs from node melsayed-conformance3 pod downward-api-c0e825c6-14a3-46a4-badf-7c89d3639ce0 container dapi-container: <nil>
STEP: delete the pod
Aug 22 02:20:27.878: INFO: Waiting for pod downward-api-c0e825c6-14a3-46a4-badf-7c89d3639ce0 to disappear
Aug 22 02:20:27.883: INFO: Pod downward-api-c0e825c6-14a3-46a4-badf-7c89d3639ce0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:20:27.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4709" for this suite.
Aug 22 02:20:33.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:20:34.012: INFO: namespace downward-api-4709 deletion completed in 6.125135856s

• [SLOW TEST:8.220 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:20:34.012: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5076/configmap-test-e11a045f-a384-401e-9f59-06c81b7a27bc
STEP: Creating a pod to test consume configMaps
Aug 22 02:20:34.078: INFO: Waiting up to 5m0s for pod "pod-configmaps-9caa1324-8558-4b3f-bd5c-d52d20d2aaac" in namespace "configmap-5076" to be "success or failure"
Aug 22 02:20:34.100: INFO: Pod "pod-configmaps-9caa1324-8558-4b3f-bd5c-d52d20d2aaac": Phase="Pending", Reason="", readiness=false. Elapsed: 21.541597ms
Aug 22 02:20:36.104: INFO: Pod "pod-configmaps-9caa1324-8558-4b3f-bd5c-d52d20d2aaac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025244869s
Aug 22 02:20:38.108: INFO: Pod "pod-configmaps-9caa1324-8558-4b3f-bd5c-d52d20d2aaac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030028654s
STEP: Saw pod success
Aug 22 02:20:38.108: INFO: Pod "pod-configmaps-9caa1324-8558-4b3f-bd5c-d52d20d2aaac" satisfied condition "success or failure"
Aug 22 02:20:38.111: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-9caa1324-8558-4b3f-bd5c-d52d20d2aaac container env-test: <nil>
STEP: delete the pod
Aug 22 02:20:38.135: INFO: Waiting for pod pod-configmaps-9caa1324-8558-4b3f-bd5c-d52d20d2aaac to disappear
Aug 22 02:20:38.143: INFO: Pod pod-configmaps-9caa1324-8558-4b3f-bd5c-d52d20d2aaac no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:20:38.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5076" for this suite.
Aug 22 02:20:44.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:20:44.267: INFO: namespace configmap-5076 deletion completed in 6.121262916s

• [SLOW TEST:10.255 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:20:44.268: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 22 02:20:44.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-8901'
Aug 22 02:20:44.509: INFO: stderr: ""
Aug 22 02:20:44.509: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 22 02:20:45.515: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 02:20:45.515: INFO: Found 0 / 1
Aug 22 02:20:46.518: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 02:20:46.525: INFO: Found 0 / 1
Aug 22 02:20:47.513: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 02:20:47.514: INFO: Found 0 / 1
Aug 22 02:20:48.514: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 02:20:48.514: INFO: Found 1 / 1
Aug 22 02:20:48.515: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 22 02:20:48.519: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 02:20:48.519: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 22 02:20:48.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 patch pod redis-master-djwbw --namespace=kubectl-8901 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 22 02:20:48.616: INFO: stderr: ""
Aug 22 02:20:48.616: INFO: stdout: "pod/redis-master-djwbw patched\n"
STEP: checking annotations
Aug 22 02:20:48.619: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 02:20:48.620: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:20:48.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8901" for this suite.
Aug 22 02:21:10.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:21:10.719: INFO: namespace kubectl-8901 deletion completed in 22.095000577s

• [SLOW TEST:26.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:21:10.723: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:21:10.758: INFO: Waiting up to 5m0s for pod "downwardapi-volume-809a88aa-9932-43e4-8130-6dcafedd1d70" in namespace "projected-8451" to be "success or failure"
Aug 22 02:21:10.767: INFO: Pod "downwardapi-volume-809a88aa-9932-43e4-8130-6dcafedd1d70": Phase="Pending", Reason="", readiness=false. Elapsed: 9.165038ms
Aug 22 02:21:12.771: INFO: Pod "downwardapi-volume-809a88aa-9932-43e4-8130-6dcafedd1d70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013401838s
STEP: Saw pod success
Aug 22 02:21:12.771: INFO: Pod "downwardapi-volume-809a88aa-9932-43e4-8130-6dcafedd1d70" satisfied condition "success or failure"
Aug 22 02:21:12.773: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-809a88aa-9932-43e4-8130-6dcafedd1d70 container client-container: <nil>
STEP: delete the pod
Aug 22 02:21:12.794: INFO: Waiting for pod downwardapi-volume-809a88aa-9932-43e4-8130-6dcafedd1d70 to disappear
Aug 22 02:21:12.800: INFO: Pod downwardapi-volume-809a88aa-9932-43e4-8130-6dcafedd1d70 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:21:12.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8451" for this suite.
Aug 22 02:21:18.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:21:18.906: INFO: namespace projected-8451 deletion completed in 6.103132489s

• [SLOW TEST:8.183 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:21:18.907: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Aug 22 02:21:18.944: INFO: Waiting up to 5m0s for pod "var-expansion-e15970b4-9f88-4034-9264-463175428617" in namespace "var-expansion-5511" to be "success or failure"
Aug 22 02:21:18.950: INFO: Pod "var-expansion-e15970b4-9f88-4034-9264-463175428617": Phase="Pending", Reason="", readiness=false. Elapsed: 5.399337ms
Aug 22 02:21:20.955: INFO: Pod "var-expansion-e15970b4-9f88-4034-9264-463175428617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010387839s
STEP: Saw pod success
Aug 22 02:21:20.955: INFO: Pod "var-expansion-e15970b4-9f88-4034-9264-463175428617" satisfied condition "success or failure"
Aug 22 02:21:20.958: INFO: Trying to get logs from node melsayed-conformance3 pod var-expansion-e15970b4-9f88-4034-9264-463175428617 container dapi-container: <nil>
STEP: delete the pod
Aug 22 02:21:20.984: INFO: Waiting for pod var-expansion-e15970b4-9f88-4034-9264-463175428617 to disappear
Aug 22 02:21:20.986: INFO: Pod var-expansion-e15970b4-9f88-4034-9264-463175428617 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:21:20.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5511" for this suite.
Aug 22 02:21:27.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:21:27.104: INFO: namespace var-expansion-5511 deletion completed in 6.115253759s

• [SLOW TEST:8.197 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:21:27.107: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 22 02:21:31.692: INFO: Successfully updated pod "pod-update-activedeadlineseconds-28a0a082-1e59-432f-ac62-81113b522d94"
Aug 22 02:21:31.692: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-28a0a082-1e59-432f-ac62-81113b522d94" in namespace "pods-4872" to be "terminated due to deadline exceeded"
Aug 22 02:21:31.699: INFO: Pod "pod-update-activedeadlineseconds-28a0a082-1e59-432f-ac62-81113b522d94": Phase="Running", Reason="", readiness=true. Elapsed: 6.24874ms
Aug 22 02:21:33.702: INFO: Pod "pod-update-activedeadlineseconds-28a0a082-1e59-432f-ac62-81113b522d94": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.009977891s
Aug 22 02:21:33.702: INFO: Pod "pod-update-activedeadlineseconds-28a0a082-1e59-432f-ac62-81113b522d94" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:21:33.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4872" for this suite.
Aug 22 02:21:39.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:21:39.829: INFO: namespace pods-4872 deletion completed in 6.117682173s

• [SLOW TEST:12.722 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:21:39.830: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:22:05.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2955" for this suite.
Aug 22 02:22:11.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:22:11.196: INFO: namespace container-runtime-2955 deletion completed in 6.106256371s

• [SLOW TEST:31.367 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:22:11.199: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-b293311c-80ff-4c40-a11d-6700cbf2ccf5
STEP: Creating a pod to test consume configMaps
Aug 22 02:22:11.295: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a9ed4a3-8f83-4f36-bc78-12185e28b754" in namespace "configmap-9353" to be "success or failure"
Aug 22 02:22:11.305: INFO: Pod "pod-configmaps-8a9ed4a3-8f83-4f36-bc78-12185e28b754": Phase="Pending", Reason="", readiness=false. Elapsed: 9.497339ms
Aug 22 02:22:13.308: INFO: Pod "pod-configmaps-8a9ed4a3-8f83-4f36-bc78-12185e28b754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012918727s
STEP: Saw pod success
Aug 22 02:22:13.308: INFO: Pod "pod-configmaps-8a9ed4a3-8f83-4f36-bc78-12185e28b754" satisfied condition "success or failure"
Aug 22 02:22:13.310: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-8a9ed4a3-8f83-4f36-bc78-12185e28b754 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 02:22:13.327: INFO: Waiting for pod pod-configmaps-8a9ed4a3-8f83-4f36-bc78-12185e28b754 to disappear
Aug 22 02:22:13.332: INFO: Pod pod-configmaps-8a9ed4a3-8f83-4f36-bc78-12185e28b754 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:22:13.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9353" for this suite.
Aug 22 02:22:19.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:22:19.449: INFO: namespace configmap-9353 deletion completed in 6.112068878s

• [SLOW TEST:8.250 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:22:19.450: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6437.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6437.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 02:22:23.524: INFO: DNS probes using dns-6437/dns-test-351d5dbf-280c-46df-99da-2ac8a90a1579 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:22:23.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6437" for this suite.
Aug 22 02:22:29.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:22:29.652: INFO: namespace dns-6437 deletion completed in 6.112543225s

• [SLOW TEST:10.203 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:22:29.655: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-6cbef150-564b-4361-995b-fdd90ecc9083
STEP: Creating a pod to test consume configMaps
Aug 22 02:22:29.702: INFO: Waiting up to 5m0s for pod "pod-configmaps-47b73196-b1bf-4072-8bf5-2d0f21590f47" in namespace "configmap-7188" to be "success or failure"
Aug 22 02:22:29.709: INFO: Pod "pod-configmaps-47b73196-b1bf-4072-8bf5-2d0f21590f47": Phase="Pending", Reason="", readiness=false. Elapsed: 6.490398ms
Aug 22 02:22:31.712: INFO: Pod "pod-configmaps-47b73196-b1bf-4072-8bf5-2d0f21590f47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010107921s
STEP: Saw pod success
Aug 22 02:22:31.712: INFO: Pod "pod-configmaps-47b73196-b1bf-4072-8bf5-2d0f21590f47" satisfied condition "success or failure"
Aug 22 02:22:31.715: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-47b73196-b1bf-4072-8bf5-2d0f21590f47 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 02:22:31.735: INFO: Waiting for pod pod-configmaps-47b73196-b1bf-4072-8bf5-2d0f21590f47 to disappear
Aug 22 02:22:31.741: INFO: Pod pod-configmaps-47b73196-b1bf-4072-8bf5-2d0f21590f47 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:22:31.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7188" for this suite.
Aug 22 02:22:37.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:22:37.844: INFO: namespace configmap-7188 deletion completed in 6.099119288s

• [SLOW TEST:8.189 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:22:37.845: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Aug 22 02:22:37.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 --namespace=kubectl-1791 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 22 02:22:39.978: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 22 02:22:39.978: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:22:41.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1791" for this suite.
Aug 22 02:22:50.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:22:50.100: INFO: namespace kubectl-1791 deletion completed in 8.106823519s

• [SLOW TEST:12.256 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:22:50.101: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-b7414227-b95c-4de7-a093-7fac83075a04
STEP: Creating a pod to test consume configMaps
Aug 22 02:22:50.156: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4" in namespace "projected-916" to be "success or failure"
Aug 22 02:22:50.161: INFO: Pod "pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.043889ms
Aug 22 02:22:52.164: INFO: Pod "pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008581906s
Aug 22 02:22:54.171: INFO: Pod "pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014849339s
Aug 22 02:22:56.174: INFO: Pod "pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017902174s
Aug 22 02:22:58.177: INFO: Pod "pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021276227s
STEP: Saw pod success
Aug 22 02:22:58.177: INFO: Pod "pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4" satisfied condition "success or failure"
Aug 22 02:22:58.181: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 02:22:58.198: INFO: Waiting for pod pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4 to disappear
Aug 22 02:22:58.200: INFO: Pod pod-projected-configmaps-2b7c4ae0-e976-4d16-99e1-7205e61aa9d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:22:58.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-916" for this suite.
Aug 22 02:23:04.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:23:04.321: INFO: namespace projected-916 deletion completed in 6.116147812s

• [SLOW TEST:14.220 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:23:04.322: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 22 02:23:10.424: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:10.428: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:12.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:12.433: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:14.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:14.433: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:16.433: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:16.438: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:18.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:18.434: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:20.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:20.432: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:22.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:22.434: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:24.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:24.433: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:26.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:26.725: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:28.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:28.433: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:30.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:30.434: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:32.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:32.432: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:34.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:34.435: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 22 02:23:36.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 22 02:23:36.432: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:23:36.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7471" for this suite.
Aug 22 02:23:58.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:23:58.544: INFO: namespace container-lifecycle-hook-7471 deletion completed in 22.107619184s

• [SLOW TEST:54.222 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:23:58.546: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 22 02:23:58.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-5840'
Aug 22 02:23:58.691: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 22 02:23:58.691: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Aug 22 02:24:00.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5840'
Aug 22 02:24:00.806: INFO: stderr: ""
Aug 22 02:24:00.806: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:24:00.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5840" for this suite.
Aug 22 02:24:06.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:24:06.910: INFO: namespace kubectl-5840 deletion completed in 6.100125018s

• [SLOW TEST:8.364 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:24:06.910: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 22 02:24:11.471: INFO: Successfully updated pod "labelsupdate46b34be8-4857-4b27-bd19-acd7c61d33e5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:24:13.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5769" for this suite.
Aug 22 02:24:35.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:24:35.605: INFO: namespace projected-5769 deletion completed in 22.099495085s

• [SLOW TEST:28.695 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:24:35.606: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:24:35.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a51e579a-9120-417b-a2db-984078c4ac0b" in namespace "projected-7028" to be "success or failure"
Aug 22 02:24:35.665: INFO: Pod "downwardapi-volume-a51e579a-9120-417b-a2db-984078c4ac0b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.789635ms
Aug 22 02:24:37.670: INFO: Pod "downwardapi-volume-a51e579a-9120-417b-a2db-984078c4ac0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015704013s
Aug 22 02:24:39.675: INFO: Pod "downwardapi-volume-a51e579a-9120-417b-a2db-984078c4ac0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020652085s
STEP: Saw pod success
Aug 22 02:24:39.675: INFO: Pod "downwardapi-volume-a51e579a-9120-417b-a2db-984078c4ac0b" satisfied condition "success or failure"
Aug 22 02:24:39.678: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-a51e579a-9120-417b-a2db-984078c4ac0b container client-container: <nil>
STEP: delete the pod
Aug 22 02:24:39.699: INFO: Waiting for pod downwardapi-volume-a51e579a-9120-417b-a2db-984078c4ac0b to disappear
Aug 22 02:24:39.705: INFO: Pod downwardapi-volume-a51e579a-9120-417b-a2db-984078c4ac0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:24:39.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7028" for this suite.
Aug 22 02:24:45.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:24:45.802: INFO: namespace projected-7028 deletion completed in 6.094001322s

• [SLOW TEST:10.196 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:24:45.803: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-d5a0d82e-6079-4191-b22f-a1caa216a8de
STEP: Creating a pod to test consume secrets
Aug 22 02:24:45.887: INFO: Waiting up to 5m0s for pod "pod-secrets-757bcda2-9fc1-42e7-b0b5-8d1efae5e0db" in namespace "secrets-3906" to be "success or failure"
Aug 22 02:24:45.892: INFO: Pod "pod-secrets-757bcda2-9fc1-42e7-b0b5-8d1efae5e0db": Phase="Pending", Reason="", readiness=false. Elapsed: 5.179291ms
Aug 22 02:24:47.902: INFO: Pod "pod-secrets-757bcda2-9fc1-42e7-b0b5-8d1efae5e0db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015310811s
Aug 22 02:24:49.907: INFO: Pod "pod-secrets-757bcda2-9fc1-42e7-b0b5-8d1efae5e0db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020405191s
STEP: Saw pod success
Aug 22 02:24:49.907: INFO: Pod "pod-secrets-757bcda2-9fc1-42e7-b0b5-8d1efae5e0db" satisfied condition "success or failure"
Aug 22 02:24:49.911: INFO: Trying to get logs from node melsayed-conformance3 pod pod-secrets-757bcda2-9fc1-42e7-b0b5-8d1efae5e0db container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 02:24:49.930: INFO: Waiting for pod pod-secrets-757bcda2-9fc1-42e7-b0b5-8d1efae5e0db to disappear
Aug 22 02:24:49.934: INFO: Pod pod-secrets-757bcda2-9fc1-42e7-b0b5-8d1efae5e0db no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:24:49.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3906" for this suite.
Aug 22 02:24:55.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:24:56.052: INFO: namespace secrets-3906 deletion completed in 6.115458178s
STEP: Destroying namespace "secret-namespace-7476" for this suite.
Aug 22 02:25:02.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:25:02.175: INFO: namespace secret-namespace-7476 deletion completed in 6.123384645s

• [SLOW TEST:16.373 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:25:02.177: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f5cc06dc-6412-4b4f-a9eb-4cb5ee8e75f8
STEP: Creating a pod to test consume configMaps
Aug 22 02:25:02.219: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-57a64d4b-e445-4dfe-862a-dbe6c145db72" in namespace "projected-9523" to be "success or failure"
Aug 22 02:25:02.233: INFO: Pod "pod-projected-configmaps-57a64d4b-e445-4dfe-862a-dbe6c145db72": Phase="Pending", Reason="", readiness=false. Elapsed: 14.049268ms
Aug 22 02:25:04.239: INFO: Pod "pod-projected-configmaps-57a64d4b-e445-4dfe-862a-dbe6c145db72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019495847s
Aug 22 02:25:06.242: INFO: Pod "pod-projected-configmaps-57a64d4b-e445-4dfe-862a-dbe6c145db72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022534702s
STEP: Saw pod success
Aug 22 02:25:06.242: INFO: Pod "pod-projected-configmaps-57a64d4b-e445-4dfe-862a-dbe6c145db72" satisfied condition "success or failure"
Aug 22 02:25:06.244: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-configmaps-57a64d4b-e445-4dfe-862a-dbe6c145db72 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 02:25:06.267: INFO: Waiting for pod pod-projected-configmaps-57a64d4b-e445-4dfe-862a-dbe6c145db72 to disappear
Aug 22 02:25:06.270: INFO: Pod pod-projected-configmaps-57a64d4b-e445-4dfe-862a-dbe6c145db72 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:25:06.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9523" for this suite.
Aug 22 02:25:12.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:25:12.357: INFO: namespace projected-9523 deletion completed in 6.084562166s

• [SLOW TEST:10.180 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:25:12.365: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 22 02:25:18.440: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:18.440: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:18.639: INFO: Exec stderr: ""
Aug 22 02:25:18.639: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:18.639: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:18.869: INFO: Exec stderr: ""
Aug 22 02:25:18.869: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:18.869: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:19.055: INFO: Exec stderr: ""
Aug 22 02:25:19.056: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:19.056: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:19.289: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 22 02:25:19.289: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:19.289: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:19.472: INFO: Exec stderr: ""
Aug 22 02:25:19.472: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:19.472: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:19.650: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 22 02:25:19.650: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:19.650: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:19.862: INFO: Exec stderr: ""
Aug 22 02:25:19.863: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:19.863: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:20.062: INFO: Exec stderr: ""
Aug 22 02:25:20.062: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:20.062: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:20.257: INFO: Exec stderr: ""
Aug 22 02:25:20.257: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7873 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:25:20.257: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:25:20.457: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:25:20.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7873" for this suite.
Aug 22 02:26:10.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:26:10.570: INFO: namespace e2e-kubelet-etc-hosts-7873 deletion completed in 50.107157674s

• [SLOW TEST:58.205 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:26:10.570: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-df1d793f-7519-4ee3-b983-7dc52ec10a21
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:26:12.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2658" for this suite.
Aug 22 02:26:34.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:26:34.825: INFO: namespace configmap-2658 deletion completed in 22.103079498s

• [SLOW TEST:24.255 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:26:34.825: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:26:34.857: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1dba31e9-a816-40fc-871e-b5566cd62ace" in namespace "projected-8264" to be "success or failure"
Aug 22 02:26:34.861: INFO: Pod "downwardapi-volume-1dba31e9-a816-40fc-871e-b5566cd62ace": Phase="Pending", Reason="", readiness=false. Elapsed: 4.668336ms
Aug 22 02:26:36.865: INFO: Pod "downwardapi-volume-1dba31e9-a816-40fc-871e-b5566cd62ace": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008482288s
Aug 22 02:26:38.871: INFO: Pod "downwardapi-volume-1dba31e9-a816-40fc-871e-b5566cd62ace": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01399036s
Aug 22 02:26:40.875: INFO: Pod "downwardapi-volume-1dba31e9-a816-40fc-871e-b5566cd62ace": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018135755s
STEP: Saw pod success
Aug 22 02:26:40.875: INFO: Pod "downwardapi-volume-1dba31e9-a816-40fc-871e-b5566cd62ace" satisfied condition "success or failure"
Aug 22 02:26:40.878: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-1dba31e9-a816-40fc-871e-b5566cd62ace container client-container: <nil>
STEP: delete the pod
Aug 22 02:26:40.899: INFO: Waiting for pod downwardapi-volume-1dba31e9-a816-40fc-871e-b5566cd62ace to disappear
Aug 22 02:26:40.901: INFO: Pod downwardapi-volume-1dba31e9-a816-40fc-871e-b5566cd62ace no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:26:40.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8264" for this suite.
Aug 22 02:26:46.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:26:47.049: INFO: namespace projected-8264 deletion completed in 6.145136439s

• [SLOW TEST:12.224 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:26:47.059: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Aug 22 02:26:47.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-8400'
Aug 22 02:26:47.530: INFO: stderr: ""
Aug 22 02:26:47.530: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Aug 22 02:26:48.536: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 02:26:48.536: INFO: Found 0 / 1
Aug 22 02:26:49.537: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 02:26:49.537: INFO: Found 1 / 1
Aug 22 02:26:49.537: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 22 02:26:49.539: INFO: Selector matched 1 pods for map[app:redis]
Aug 22 02:26:49.540: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug 22 02:26:49.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 logs redis-master-wplvh redis-master --namespace=kubectl-8400'
Aug 22 02:26:49.641: INFO: stderr: ""
Aug 22 02:26:49.641: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Aug 02:26:48.701 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Aug 02:26:48.701 # Server started, Redis version 3.2.12\n1:M 22 Aug 02:26:48.701 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Aug 02:26:48.701 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug 22 02:26:49.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 log redis-master-wplvh redis-master --namespace=kubectl-8400 --tail=1'
Aug 22 02:26:49.772: INFO: stderr: ""
Aug 22 02:26:49.772: INFO: stdout: "1:M 22 Aug 02:26:48.701 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug 22 02:26:49.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 log redis-master-wplvh redis-master --namespace=kubectl-8400 --limit-bytes=1'
Aug 22 02:26:49.875: INFO: stderr: ""
Aug 22 02:26:49.875: INFO: stdout: " "
STEP: exposing timestamps
Aug 22 02:26:49.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 log redis-master-wplvh redis-master --namespace=kubectl-8400 --tail=1 --timestamps'
Aug 22 02:26:49.968: INFO: stderr: ""
Aug 22 02:26:49.968: INFO: stdout: "2019-08-22T02:26:48.701831902Z 1:M 22 Aug 02:26:48.701 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug 22 02:26:52.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 log redis-master-wplvh redis-master --namespace=kubectl-8400 --since=1s'
Aug 22 02:26:52.572: INFO: stderr: ""
Aug 22 02:26:52.572: INFO: stdout: ""
Aug 22 02:26:52.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 log redis-master-wplvh redis-master --namespace=kubectl-8400 --since=24h'
Aug 22 02:26:52.682: INFO: stderr: ""
Aug 22 02:26:52.682: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Aug 02:26:48.701 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Aug 02:26:48.701 # Server started, Redis version 3.2.12\n1:M 22 Aug 02:26:48.701 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Aug 02:26:48.701 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Aug 22 02:26:52.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-8400'
Aug 22 02:26:52.777: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 02:26:52.777: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug 22 02:26:52.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get rc,svc -l name=nginx --no-headers --namespace=kubectl-8400'
Aug 22 02:26:52.875: INFO: stderr: "No resources found.\n"
Aug 22 02:26:52.875: INFO: stdout: ""
Aug 22 02:26:52.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -l name=nginx --namespace=kubectl-8400 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 22 02:26:52.984: INFO: stderr: ""
Aug 22 02:26:52.984: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:26:52.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8400" for this suite.
Aug 22 02:26:59.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:26:59.099: INFO: namespace kubectl-8400 deletion completed in 6.110914544s

• [SLOW TEST:12.041 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:26:59.101: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Aug 22 02:27:01.182: INFO: Pod pod-hostip-6fde74da-a9fa-4477-ae62-58b67531acba has hostIP: 165.22.32.172
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:27:01.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3315" for this suite.
Aug 22 02:27:23.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:27:23.307: INFO: namespace pods-3315 deletion completed in 22.121457156s

• [SLOW TEST:24.206 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:27:23.307: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 22 02:27:23.346: INFO: Waiting up to 5m0s for pod "pod-9bce8e69-3f51-4943-9cbb-3c50fc7e7b40" in namespace "emptydir-2374" to be "success or failure"
Aug 22 02:27:23.356: INFO: Pod "pod-9bce8e69-3f51-4943-9cbb-3c50fc7e7b40": Phase="Pending", Reason="", readiness=false. Elapsed: 9.487015ms
Aug 22 02:27:25.359: INFO: Pod "pod-9bce8e69-3f51-4943-9cbb-3c50fc7e7b40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013234153s
STEP: Saw pod success
Aug 22 02:27:25.359: INFO: Pod "pod-9bce8e69-3f51-4943-9cbb-3c50fc7e7b40" satisfied condition "success or failure"
Aug 22 02:27:25.363: INFO: Trying to get logs from node melsayed-conformance3 pod pod-9bce8e69-3f51-4943-9cbb-3c50fc7e7b40 container test-container: <nil>
STEP: delete the pod
Aug 22 02:27:25.388: INFO: Waiting for pod pod-9bce8e69-3f51-4943-9cbb-3c50fc7e7b40 to disappear
Aug 22 02:27:25.425: INFO: Pod pod-9bce8e69-3f51-4943-9cbb-3c50fc7e7b40 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:27:25.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2374" for this suite.
Aug 22 02:27:31.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:27:31.521: INFO: namespace emptydir-2374 deletion completed in 6.089534062s

• [SLOW TEST:8.214 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:27:31.522: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:27:31.559: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:27:33.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5520" for this suite.
Aug 22 02:28:23.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:28:23.876: INFO: namespace pods-5520 deletion completed in 50.10310959s

• [SLOW TEST:52.354 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:28:23.876: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 22 02:28:23.911: INFO: Waiting up to 5m0s for pod "pod-d26bcc4e-458b-4561-aaac-ac9145e7eb9d" in namespace "emptydir-9507" to be "success or failure"
Aug 22 02:28:23.922: INFO: Pod "pod-d26bcc4e-458b-4561-aaac-ac9145e7eb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.807998ms
Aug 22 02:28:25.925: INFO: Pod "pod-d26bcc4e-458b-4561-aaac-ac9145e7eb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014074663s
Aug 22 02:28:27.929: INFO: Pod "pod-d26bcc4e-458b-4561-aaac-ac9145e7eb9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017849022s
STEP: Saw pod success
Aug 22 02:28:27.929: INFO: Pod "pod-d26bcc4e-458b-4561-aaac-ac9145e7eb9d" satisfied condition "success or failure"
Aug 22 02:28:27.932: INFO: Trying to get logs from node melsayed-conformance3 pod pod-d26bcc4e-458b-4561-aaac-ac9145e7eb9d container test-container: <nil>
STEP: delete the pod
Aug 22 02:28:27.963: INFO: Waiting for pod pod-d26bcc4e-458b-4561-aaac-ac9145e7eb9d to disappear
Aug 22 02:28:27.965: INFO: Pod pod-d26bcc4e-458b-4561-aaac-ac9145e7eb9d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:28:27.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9507" for this suite.
Aug 22 02:28:33.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:28:34.072: INFO: namespace emptydir-9507 deletion completed in 6.103082556s

• [SLOW TEST:10.196 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:28:34.076: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-b9a04c5b-cb33-4bd0-b6c9-0c80e5c65323
STEP: Creating a pod to test consume secrets
Aug 22 02:28:34.214: INFO: Waiting up to 5m0s for pod "pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae" in namespace "secrets-381" to be "success or failure"
Aug 22 02:28:34.223: INFO: Pod "pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.950324ms
Aug 22 02:28:36.227: INFO: Pod "pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01182585s
Aug 22 02:28:38.231: INFO: Pod "pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01625661s
Aug 22 02:28:40.237: INFO: Pod "pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02229709s
Aug 22 02:28:42.240: INFO: Pod "pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025259963s
Aug 22 02:28:44.244: INFO: Pod "pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029454628s
STEP: Saw pod success
Aug 22 02:28:44.244: INFO: Pod "pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae" satisfied condition "success or failure"
Aug 22 02:28:44.249: INFO: Trying to get logs from node melsayed-conformance3 pod pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 02:28:45.907: INFO: Waiting for pod pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae to disappear
Aug 22 02:28:45.910: INFO: Pod pod-secrets-2d0de968-f72f-4b7e-923c-9c38e36590ae no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:28:45.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-381" for this suite.
Aug 22 02:28:51.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:28:52.023: INFO: namespace secrets-381 deletion completed in 6.110364716s

• [SLOW TEST:17.948 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:28:52.029: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 22 02:28:52.092: INFO: Number of nodes with available pods: 0
Aug 22 02:28:52.092: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:28:53.105: INFO: Number of nodes with available pods: 0
Aug 22 02:28:53.106: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:28:54.339: INFO: Number of nodes with available pods: 0
Aug 22 02:28:54.339: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:28:55.100: INFO: Number of nodes with available pods: 1
Aug 22 02:28:55.100: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:28:56.099: INFO: Number of nodes with available pods: 2
Aug 22 02:28:56.099: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:28:57.100: INFO: Number of nodes with available pods: 2
Aug 22 02:28:57.100: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:28:58.101: INFO: Number of nodes with available pods: 3
Aug 22 02:28:58.101: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 22 02:28:58.135: INFO: Number of nodes with available pods: 2
Aug 22 02:28:58.135: INFO: Node melsayed-conformance3 is running more than one daemon pod
Aug 22 02:28:59.147: INFO: Number of nodes with available pods: 2
Aug 22 02:28:59.147: INFO: Node melsayed-conformance3 is running more than one daemon pod
Aug 22 02:29:00.150: INFO: Number of nodes with available pods: 2
Aug 22 02:29:00.150: INFO: Node melsayed-conformance3 is running more than one daemon pod
Aug 22 02:29:01.143: INFO: Number of nodes with available pods: 2
Aug 22 02:29:01.143: INFO: Node melsayed-conformance3 is running more than one daemon pod
Aug 22 02:29:02.147: INFO: Number of nodes with available pods: 3
Aug 22 02:29:02.148: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1599, will wait for the garbage collector to delete the pods
Aug 22 02:29:02.219: INFO: Deleting DaemonSet.extensions daemon-set took: 5.738954ms
Aug 22 02:29:02.719: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.317379ms
Aug 22 02:29:15.722: INFO: Number of nodes with available pods: 0
Aug 22 02:29:15.722: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 02:29:15.725: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1599/daemonsets","resourceVersion":"15362"},"items":null}

Aug 22 02:29:15.728: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1599/pods","resourceVersion":"15362"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:29:15.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1599" for this suite.
Aug 22 02:29:21.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:29:21.906: INFO: namespace daemonsets-1599 deletion completed in 6.149448447s

• [SLOW TEST:29.877 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:29:21.908: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Aug 22 02:29:21.940: INFO: Waiting up to 5m0s for pod "var-expansion-621e9c8c-0a4e-4601-adc1-74cbac5c4430" in namespace "var-expansion-7368" to be "success or failure"
Aug 22 02:29:21.944: INFO: Pod "var-expansion-621e9c8c-0a4e-4601-adc1-74cbac5c4430": Phase="Pending", Reason="", readiness=false. Elapsed: 3.97753ms
Aug 22 02:29:23.948: INFO: Pod "var-expansion-621e9c8c-0a4e-4601-adc1-74cbac5c4430": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007909196s
STEP: Saw pod success
Aug 22 02:29:23.948: INFO: Pod "var-expansion-621e9c8c-0a4e-4601-adc1-74cbac5c4430" satisfied condition "success or failure"
Aug 22 02:29:23.952: INFO: Trying to get logs from node melsayed-conformance3 pod var-expansion-621e9c8c-0a4e-4601-adc1-74cbac5c4430 container dapi-container: <nil>
STEP: delete the pod
Aug 22 02:29:23.977: INFO: Waiting for pod var-expansion-621e9c8c-0a4e-4601-adc1-74cbac5c4430 to disappear
Aug 22 02:29:23.979: INFO: Pod var-expansion-621e9c8c-0a4e-4601-adc1-74cbac5c4430 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:29:23.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7368" for this suite.
Aug 22 02:29:30.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:29:30.217: INFO: namespace var-expansion-7368 deletion completed in 6.227277419s

• [SLOW TEST:8.309 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:29:30.217: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:29:30.328: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc8a73c0-b1ee-4614-bd09-d38a6a42ccbf" in namespace "projected-8007" to be "success or failure"
Aug 22 02:29:30.335: INFO: Pod "downwardapi-volume-cc8a73c0-b1ee-4614-bd09-d38a6a42ccbf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.276609ms
Aug 22 02:29:32.339: INFO: Pod "downwardapi-volume-cc8a73c0-b1ee-4614-bd09-d38a6a42ccbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011271547s
STEP: Saw pod success
Aug 22 02:29:32.339: INFO: Pod "downwardapi-volume-cc8a73c0-b1ee-4614-bd09-d38a6a42ccbf" satisfied condition "success or failure"
Aug 22 02:29:32.343: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-cc8a73c0-b1ee-4614-bd09-d38a6a42ccbf container client-container: <nil>
STEP: delete the pod
Aug 22 02:29:32.371: INFO: Waiting for pod downwardapi-volume-cc8a73c0-b1ee-4614-bd09-d38a6a42ccbf to disappear
Aug 22 02:29:32.375: INFO: Pod downwardapi-volume-cc8a73c0-b1ee-4614-bd09-d38a6a42ccbf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:29:32.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8007" for this suite.
Aug 22 02:29:38.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:29:38.497: INFO: namespace projected-8007 deletion completed in 6.117825872s

• [SLOW TEST:8.280 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:29:38.497: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 22 02:29:44.552: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0822 02:29:44.552279      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:29:44.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2693" for this suite.
Aug 22 02:29:50.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:29:50.686: INFO: namespace gc-2693 deletion completed in 6.13081511s

• [SLOW TEST:12.189 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:29:50.686: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-4c01f4af-f559-438c-b094-8766f5d32dbe
STEP: Creating a pod to test consume secrets
Aug 22 02:29:50.721: INFO: Waiting up to 5m0s for pod "pod-secrets-7c04b6b5-a486-44d1-8596-116db0773960" in namespace "secrets-9692" to be "success or failure"
Aug 22 02:29:50.729: INFO: Pod "pod-secrets-7c04b6b5-a486-44d1-8596-116db0773960": Phase="Pending", Reason="", readiness=false. Elapsed: 7.397956ms
Aug 22 02:29:52.733: INFO: Pod "pod-secrets-7c04b6b5-a486-44d1-8596-116db0773960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011734964s
Aug 22 02:29:54.738: INFO: Pod "pod-secrets-7c04b6b5-a486-44d1-8596-116db0773960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017216435s
STEP: Saw pod success
Aug 22 02:29:54.739: INFO: Pod "pod-secrets-7c04b6b5-a486-44d1-8596-116db0773960" satisfied condition "success or failure"
Aug 22 02:29:54.742: INFO: Trying to get logs from node melsayed-conformance3 pod pod-secrets-7c04b6b5-a486-44d1-8596-116db0773960 container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 02:29:54.770: INFO: Waiting for pod pod-secrets-7c04b6b5-a486-44d1-8596-116db0773960 to disappear
Aug 22 02:29:54.774: INFO: Pod pod-secrets-7c04b6b5-a486-44d1-8596-116db0773960 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:29:54.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9692" for this suite.
Aug 22 02:30:00.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:30:00.891: INFO: namespace secrets-9692 deletion completed in 6.112282933s

• [SLOW TEST:10.205 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:30:00.893: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Aug 22 02:30:00.921: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-089099426 proxy --unix-socket=/tmp/kubectl-proxy-unix718564829/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:30:00.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9912" for this suite.
Aug 22 02:30:07.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:30:07.097: INFO: namespace kubectl-9912 deletion completed in 6.097314086s

• [SLOW TEST:6.204 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:30:07.099: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-4f5aca0e-e2c5-4fa5-abe6-00232b68d3e4
STEP: Creating a pod to test consume configMaps
Aug 22 02:30:07.137: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac279f2c-32ac-447f-b458-52c7e1bbfa57" in namespace "projected-1112" to be "success or failure"
Aug 22 02:30:07.147: INFO: Pod "pod-projected-configmaps-ac279f2c-32ac-447f-b458-52c7e1bbfa57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025244ms
Aug 22 02:30:09.150: INFO: Pod "pod-projected-configmaps-ac279f2c-32ac-447f-b458-52c7e1bbfa57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013542975s
Aug 22 02:30:11.154: INFO: Pod "pod-projected-configmaps-ac279f2c-32ac-447f-b458-52c7e1bbfa57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017292811s
STEP: Saw pod success
Aug 22 02:30:11.154: INFO: Pod "pod-projected-configmaps-ac279f2c-32ac-447f-b458-52c7e1bbfa57" satisfied condition "success or failure"
Aug 22 02:30:11.157: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-configmaps-ac279f2c-32ac-447f-b458-52c7e1bbfa57 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 02:30:11.175: INFO: Waiting for pod pod-projected-configmaps-ac279f2c-32ac-447f-b458-52c7e1bbfa57 to disappear
Aug 22 02:30:11.179: INFO: Pod pod-projected-configmaps-ac279f2c-32ac-447f-b458-52c7e1bbfa57 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:30:11.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1112" for this suite.
Aug 22 02:30:17.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:30:17.285: INFO: namespace projected-1112 deletion completed in 6.102860246s

• [SLOW TEST:10.186 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:30:17.285: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:30:37.333: INFO: Container started at 2019-08-22 02:30:18 +0000 UTC, pod became ready at 2019-08-22 02:30:35 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:30:37.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3592" for this suite.
Aug 22 02:30:59.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:30:59.436: INFO: namespace container-probe-3592 deletion completed in 22.098590945s

• [SLOW TEST:42.151 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:30:59.438: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0822 02:31:39.504498      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 22 02:31:39.505: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:31:39.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6516" for this suite.
Aug 22 02:31:45.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:31:45.678: INFO: namespace gc-6516 deletion completed in 6.169185952s

• [SLOW TEST:46.240 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:31:45.679: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:31:45.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df087222-1033-4fc4-840a-7000022c20d7" in namespace "downward-api-7103" to be "success or failure"
Aug 22 02:31:45.838: INFO: Pod "downwardapi-volume-df087222-1033-4fc4-840a-7000022c20d7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.184293ms
Aug 22 02:31:47.843: INFO: Pod "downwardapi-volume-df087222-1033-4fc4-840a-7000022c20d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01908761s
STEP: Saw pod success
Aug 22 02:31:47.843: INFO: Pod "downwardapi-volume-df087222-1033-4fc4-840a-7000022c20d7" satisfied condition "success or failure"
Aug 22 02:31:47.846: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-df087222-1033-4fc4-840a-7000022c20d7 container client-container: <nil>
STEP: delete the pod
Aug 22 02:31:47.893: INFO: Waiting for pod downwardapi-volume-df087222-1033-4fc4-840a-7000022c20d7 to disappear
Aug 22 02:31:47.897: INFO: Pod downwardapi-volume-df087222-1033-4fc4-840a-7000022c20d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:31:47.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7103" for this suite.
Aug 22 02:31:53.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:31:54.000: INFO: namespace downward-api-7103 deletion completed in 6.099524705s

• [SLOW TEST:8.321 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:31:54.000: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 22 02:31:56.576: INFO: Successfully updated pod "annotationupdate00071dac-98d9-42fe-93e6-eb064fa6e27a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:32:00.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3633" for this suite.
Aug 22 02:32:22.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:32:22.724: INFO: namespace projected-3633 deletion completed in 22.112787152s

• [SLOW TEST:28.723 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:32:22.726: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:32:22.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277" in namespace "projected-3127" to be "success or failure"
Aug 22 02:32:22.783: INFO: Pod "downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277": Phase="Pending", Reason="", readiness=false. Elapsed: 13.874742ms
Aug 22 02:32:24.797: INFO: Pod "downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027232293s
Aug 22 02:32:26.801: INFO: Pod "downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030998123s
Aug 22 02:32:28.804: INFO: Pod "downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034784148s
Aug 22 02:32:30.808: INFO: Pod "downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038601433s
Aug 22 02:32:32.812: INFO: Pod "downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042730928s
Aug 22 02:32:34.819: INFO: Pod "downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.049330009s
STEP: Saw pod success
Aug 22 02:32:34.819: INFO: Pod "downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277" satisfied condition "success or failure"
Aug 22 02:32:34.823: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277 container client-container: <nil>
STEP: delete the pod
Aug 22 02:32:34.849: INFO: Waiting for pod downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277 to disappear
Aug 22 02:32:34.853: INFO: Pod downwardapi-volume-9f9281f1-46d2-417e-be51-75f0607aa277 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:32:34.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3127" for this suite.
Aug 22 02:32:40.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:32:40.955: INFO: namespace projected-3127 deletion completed in 6.098731663s

• [SLOW TEST:18.230 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:32:40.957: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6016
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 22 02:32:40.994: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 22 02:33:34.081: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.150:8080/dial?request=hostName&protocol=http&host=10.42.0.49&port=8080&tries=1'] Namespace:pod-network-test-6016 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:33:34.081: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:33:34.682: INFO: Waiting for endpoints: map[]
Aug 22 02:33:34.685: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.150:8080/dial?request=hostName&protocol=http&host=10.42.1.149&port=8080&tries=1'] Namespace:pod-network-test-6016 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:33:34.685: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:33:34.916: INFO: Waiting for endpoints: map[]
Aug 22 02:33:34.920: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.150:8080/dial?request=hostName&protocol=http&host=10.42.2.36&port=8080&tries=1'] Namespace:pod-network-test-6016 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:33:34.920: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:33:35.166: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:33:35.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6016" for this suite.
Aug 22 02:33:59.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:33:59.894: INFO: namespace pod-network-test-6016 deletion completed in 24.722911147s

• [SLOW TEST:78.937 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:33:59.894: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:34:59.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8912" for this suite.
Aug 22 02:35:21.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:35:22.047: INFO: namespace container-probe-8912 deletion completed in 22.103803672s

• [SLOW TEST:82.153 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:35:22.050: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 22 02:35:22.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9525'
Aug 22 02:35:22.236: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 22 02:35:22.236: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug 22 02:35:22.256: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-twqbc]
Aug 22 02:35:22.256: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-twqbc" in namespace "kubectl-9525" to be "running and ready"
Aug 22 02:35:22.263: INFO: Pod "e2e-test-nginx-rc-twqbc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.518358ms
Aug 22 02:35:24.267: INFO: Pod "e2e-test-nginx-rc-twqbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.010714133s
Aug 22 02:35:24.267: INFO: Pod "e2e-test-nginx-rc-twqbc" satisfied condition "running and ready"
Aug 22 02:35:24.267: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-twqbc]
Aug 22 02:35:24.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 logs rc/e2e-test-nginx-rc --namespace=kubectl-9525'
Aug 22 02:35:24.404: INFO: stderr: ""
Aug 22 02:35:24.404: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Aug 22 02:35:24.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete rc e2e-test-nginx-rc --namespace=kubectl-9525'
Aug 22 02:35:24.495: INFO: stderr: ""
Aug 22 02:35:24.495: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:35:24.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9525" for this suite.
Aug 22 02:35:48.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:35:48.611: INFO: namespace kubectl-9525 deletion completed in 24.111920375s

• [SLOW TEST:26.561 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:35:48.611: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 22 02:35:48.675: INFO: Waiting up to 5m0s for pod "pod-13d5a878-0319-457d-9991-d2e1f69eb4a7" in namespace "emptydir-2237" to be "success or failure"
Aug 22 02:35:48.679: INFO: Pod "pod-13d5a878-0319-457d-9991-d2e1f69eb4a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021759ms
Aug 22 02:35:50.683: INFO: Pod "pod-13d5a878-0319-457d-9991-d2e1f69eb4a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007361662s
Aug 22 02:35:52.687: INFO: Pod "pod-13d5a878-0319-457d-9991-d2e1f69eb4a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011895599s
STEP: Saw pod success
Aug 22 02:35:52.687: INFO: Pod "pod-13d5a878-0319-457d-9991-d2e1f69eb4a7" satisfied condition "success or failure"
Aug 22 02:35:52.690: INFO: Trying to get logs from node melsayed-conformance3 pod pod-13d5a878-0319-457d-9991-d2e1f69eb4a7 container test-container: <nil>
STEP: delete the pod
Aug 22 02:35:52.718: INFO: Waiting for pod pod-13d5a878-0319-457d-9991-d2e1f69eb4a7 to disappear
Aug 22 02:35:52.722: INFO: Pod pod-13d5a878-0319-457d-9991-d2e1f69eb4a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:35:52.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2237" for this suite.
Aug 22 02:35:58.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:35:58.845: INFO: namespace emptydir-2237 deletion completed in 6.119489067s

• [SLOW TEST:10.233 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:35:58.847: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Aug 22 02:35:58.897: INFO: Waiting up to 5m0s for pod "client-containers-10461927-debc-47aa-b501-3305187bcdbd" in namespace "containers-7216" to be "success or failure"
Aug 22 02:35:58.902: INFO: Pod "client-containers-10461927-debc-47aa-b501-3305187bcdbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.94684ms
Aug 22 02:36:00.906: INFO: Pod "client-containers-10461927-debc-47aa-b501-3305187bcdbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009101352s
Aug 22 02:36:02.911: INFO: Pod "client-containers-10461927-debc-47aa-b501-3305187bcdbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01333335s
STEP: Saw pod success
Aug 22 02:36:02.911: INFO: Pod "client-containers-10461927-debc-47aa-b501-3305187bcdbd" satisfied condition "success or failure"
Aug 22 02:36:02.915: INFO: Trying to get logs from node melsayed-conformance3 pod client-containers-10461927-debc-47aa-b501-3305187bcdbd container test-container: <nil>
STEP: delete the pod
Aug 22 02:36:02.938: INFO: Waiting for pod client-containers-10461927-debc-47aa-b501-3305187bcdbd to disappear
Aug 22 02:36:02.943: INFO: Pod client-containers-10461927-debc-47aa-b501-3305187bcdbd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:36:02.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7216" for this suite.
Aug 22 02:36:08.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:36:09.095: INFO: namespace containers-7216 deletion completed in 6.146559344s

• [SLOW TEST:10.248 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:36:09.097: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-36817857-8a6e-481e-8828-c33dd678114b
STEP: Creating a pod to test consume secrets
Aug 22 02:36:09.170: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-00d5d7af-4376-4778-8e6a-125115aade3b" in namespace "projected-9510" to be "success or failure"
Aug 22 02:36:09.192: INFO: Pod "pod-projected-secrets-00d5d7af-4376-4778-8e6a-125115aade3b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.938793ms
Aug 22 02:36:11.200: INFO: Pod "pod-projected-secrets-00d5d7af-4376-4778-8e6a-125115aade3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029448892s
STEP: Saw pod success
Aug 22 02:36:11.200: INFO: Pod "pod-projected-secrets-00d5d7af-4376-4778-8e6a-125115aade3b" satisfied condition "success or failure"
Aug 22 02:36:11.203: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-secrets-00d5d7af-4376-4778-8e6a-125115aade3b container secret-volume-test: <nil>
STEP: delete the pod
Aug 22 02:36:11.225: INFO: Waiting for pod pod-projected-secrets-00d5d7af-4376-4778-8e6a-125115aade3b to disappear
Aug 22 02:36:11.229: INFO: Pod pod-projected-secrets-00d5d7af-4376-4778-8e6a-125115aade3b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:36:11.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9510" for this suite.
Aug 22 02:36:17.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:36:17.348: INFO: namespace projected-9510 deletion completed in 6.113961947s

• [SLOW TEST:8.251 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:36:17.348: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2nwvb in namespace proxy-7798
I0822 02:36:17.401594      15 runners.go:180] Created replication controller with name: proxy-service-2nwvb, namespace: proxy-7798, replica count: 1
I0822 02:36:18.452132      15 runners.go:180] proxy-service-2nwvb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:36:19.452425      15 runners.go:180] proxy-service-2nwvb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0822 02:36:20.452696      15 runners.go:180] proxy-service-2nwvb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0822 02:36:21.452967      15 runners.go:180] proxy-service-2nwvb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0822 02:36:22.453251      15 runners.go:180] proxy-service-2nwvb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0822 02:36:23.453523      15 runners.go:180] proxy-service-2nwvb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 22 02:36:23.456: INFO: setup took 6.081190273s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 22 02:36:23.473: INFO: (0) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 15.820297ms)
Aug 22 02:36:23.473: INFO: (0) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 16.206423ms)
Aug 22 02:36:23.478: INFO: (0) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 21.029236ms)
Aug 22 02:36:23.478: INFO: (0) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 21.167261ms)
Aug 22 02:36:23.478: INFO: (0) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 21.124559ms)
Aug 22 02:36:23.479: INFO: (0) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 21.897508ms)
Aug 22 02:36:23.479: INFO: (0) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 21.877821ms)
Aug 22 02:36:23.479: INFO: (0) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 22.260134ms)
Aug 22 02:36:23.479: INFO: (0) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 21.879373ms)
Aug 22 02:36:23.479: INFO: (0) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 22.048597ms)
Aug 22 02:36:23.479: INFO: (0) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 21.92305ms)
Aug 22 02:36:23.488: INFO: (0) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 30.947716ms)
Aug 22 02:36:23.496: INFO: (0) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 39.414215ms)
Aug 22 02:36:23.496: INFO: (0) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 39.559179ms)
Aug 22 02:36:23.500: INFO: (0) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 43.444946ms)
Aug 22 02:36:23.502: INFO: (0) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 45.410599ms)
Aug 22 02:36:23.530: INFO: (1) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 27.091279ms)
Aug 22 02:36:23.536: INFO: (1) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 31.381688ms)
Aug 22 02:36:23.537: INFO: (1) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 33.411089ms)
Aug 22 02:36:23.539: INFO: (1) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 35.525869ms)
Aug 22 02:36:23.547: INFO: (1) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 43.545364ms)
Aug 22 02:36:23.547: INFO: (1) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 43.403854ms)
Aug 22 02:36:23.547: INFO: (1) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 42.949033ms)
Aug 22 02:36:23.548: INFO: (1) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 44.639992ms)
Aug 22 02:36:23.548: INFO: (1) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 45.199911ms)
Aug 22 02:36:23.552: INFO: (1) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 47.941581ms)
Aug 22 02:36:23.552: INFO: (1) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 48.273795ms)
Aug 22 02:36:23.552: INFO: (1) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 48.495353ms)
Aug 22 02:36:23.552: INFO: (1) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 48.24413ms)
Aug 22 02:36:23.553: INFO: (1) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 49.095571ms)
Aug 22 02:36:23.553: INFO: (1) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 48.862276ms)
Aug 22 02:36:23.553: INFO: (1) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 50.260751ms)
Aug 22 02:36:23.593: INFO: (2) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 38.746756ms)
Aug 22 02:36:23.593: INFO: (2) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 39.8771ms)
Aug 22 02:36:23.595: INFO: (2) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 40.846923ms)
Aug 22 02:36:23.595: INFO: (2) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 41.337145ms)
Aug 22 02:36:23.595: INFO: (2) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 40.788922ms)
Aug 22 02:36:23.597: INFO: (2) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 42.684955ms)
Aug 22 02:36:23.597: INFO: (2) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 43.464708ms)
Aug 22 02:36:23.597: INFO: (2) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 43.969673ms)
Aug 22 02:36:23.597: INFO: (2) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 43.694945ms)
Aug 22 02:36:23.597: INFO: (2) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 43.357975ms)
Aug 22 02:36:23.598: INFO: (2) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 43.87545ms)
Aug 22 02:36:23.598: INFO: (2) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 44.115061ms)
Aug 22 02:36:23.599: INFO: (2) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 44.729324ms)
Aug 22 02:36:23.599: INFO: (2) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 44.843069ms)
Aug 22 02:36:23.599: INFO: (2) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 44.9139ms)
Aug 22 02:36:23.599: INFO: (2) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 45.187094ms)
Aug 22 02:36:23.625: INFO: (3) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 25.470238ms)
Aug 22 02:36:23.625: INFO: (3) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 25.400707ms)
Aug 22 02:36:23.625: INFO: (3) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 25.763955ms)
Aug 22 02:36:23.625: INFO: (3) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 25.041443ms)
Aug 22 02:36:23.626: INFO: (3) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 25.236461ms)
Aug 22 02:36:23.628: INFO: (3) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 27.464737ms)
Aug 22 02:36:23.628: INFO: (3) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 27.36281ms)
Aug 22 02:36:23.628: INFO: (3) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 27.737408ms)
Aug 22 02:36:23.629: INFO: (3) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 28.342935ms)
Aug 22 02:36:23.629: INFO: (3) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 28.218897ms)
Aug 22 02:36:23.629: INFO: (3) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 29.096406ms)
Aug 22 02:36:23.629: INFO: (3) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 27.942132ms)
Aug 22 02:36:23.629: INFO: (3) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 28.938397ms)
Aug 22 02:36:23.629: INFO: (3) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 28.862922ms)
Aug 22 02:36:23.629: INFO: (3) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 29.206985ms)
Aug 22 02:36:23.629: INFO: (3) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 29.058514ms)
Aug 22 02:36:23.638: INFO: (4) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 8.622122ms)
Aug 22 02:36:23.645: INFO: (4) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 15.493262ms)
Aug 22 02:36:23.647: INFO: (4) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 17.617312ms)
Aug 22 02:36:23.649: INFO: (4) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 19.175563ms)
Aug 22 02:36:23.652: INFO: (4) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 22.637881ms)
Aug 22 02:36:23.654: INFO: (4) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 24.800092ms)
Aug 22 02:36:23.654: INFO: (4) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 24.809314ms)
Aug 22 02:36:23.655: INFO: (4) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 25.278945ms)
Aug 22 02:36:23.654: INFO: (4) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 24.850962ms)
Aug 22 02:36:23.655: INFO: (4) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 25.839946ms)
Aug 22 02:36:23.669: INFO: (4) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 39.295831ms)
Aug 22 02:36:23.669: INFO: (4) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 39.539364ms)
Aug 22 02:36:23.669: INFO: (4) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 39.324223ms)
Aug 22 02:36:23.669: INFO: (4) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 39.593778ms)
Aug 22 02:36:23.669: INFO: (4) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 39.447853ms)
Aug 22 02:36:23.669: INFO: (4) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 39.559748ms)
Aug 22 02:36:23.698: INFO: (5) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 28.774955ms)
Aug 22 02:36:23.717: INFO: (5) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 46.952562ms)
Aug 22 02:36:23.717: INFO: (5) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 47.655596ms)
Aug 22 02:36:23.717: INFO: (5) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 46.96183ms)
Aug 22 02:36:23.717: INFO: (5) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 47.367777ms)
Aug 22 02:36:23.717: INFO: (5) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 47.896893ms)
Aug 22 02:36:23.717: INFO: (5) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 47.753701ms)
Aug 22 02:36:23.717: INFO: (5) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 48.313476ms)
Aug 22 02:36:23.717: INFO: (5) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 48.175558ms)
Aug 22 02:36:23.718: INFO: (5) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 48.08518ms)
Aug 22 02:36:23.719: INFO: (5) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 49.542986ms)
Aug 22 02:36:23.719: INFO: (5) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 49.508353ms)
Aug 22 02:36:23.720: INFO: (5) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 50.220137ms)
Aug 22 02:36:23.722: INFO: (5) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 52.236111ms)
Aug 22 02:36:23.722: INFO: (5) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 51.926148ms)
Aug 22 02:36:23.722: INFO: (5) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 51.850879ms)
Aug 22 02:36:23.735: INFO: (6) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 13.408063ms)
Aug 22 02:36:23.735: INFO: (6) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 13.309392ms)
Aug 22 02:36:23.735: INFO: (6) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 13.554797ms)
Aug 22 02:36:23.736: INFO: (6) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 13.322473ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 16.320284ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 16.313423ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 16.226216ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 16.117976ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 16.158033ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 16.213136ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 16.377485ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 16.692677ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 16.488355ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 17.026103ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 16.942015ms)
Aug 22 02:36:23.739: INFO: (6) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 16.900874ms)
Aug 22 02:36:23.757: INFO: (7) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 17.80779ms)
Aug 22 02:36:23.759: INFO: (7) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 19.577107ms)
Aug 22 02:36:23.759: INFO: (7) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 19.514611ms)
Aug 22 02:36:23.759: INFO: (7) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 19.898365ms)
Aug 22 02:36:23.760: INFO: (7) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 19.305562ms)
Aug 22 02:36:23.760: INFO: (7) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 19.250306ms)
Aug 22 02:36:23.760: INFO: (7) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 20.257678ms)
Aug 22 02:36:23.760: INFO: (7) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 20.173294ms)
Aug 22 02:36:23.760: INFO: (7) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 19.908611ms)
Aug 22 02:36:23.760: INFO: (7) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 20.209111ms)
Aug 22 02:36:23.760: INFO: (7) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 20.56405ms)
Aug 22 02:36:23.761: INFO: (7) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 20.725049ms)
Aug 22 02:36:23.761: INFO: (7) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 20.123425ms)
Aug 22 02:36:23.761: INFO: (7) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 20.822631ms)
Aug 22 02:36:23.761: INFO: (7) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 20.842476ms)
Aug 22 02:36:23.761: INFO: (7) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 20.423056ms)
Aug 22 02:36:23.772: INFO: (8) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 10.566279ms)
Aug 22 02:36:23.772: INFO: (8) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 10.369505ms)
Aug 22 02:36:23.772: INFO: (8) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 11.107717ms)
Aug 22 02:36:23.774: INFO: (8) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 12.779324ms)
Aug 22 02:36:23.776: INFO: (8) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 14.537655ms)
Aug 22 02:36:23.777: INFO: (8) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 15.478857ms)
Aug 22 02:36:23.777: INFO: (8) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 16.392212ms)
Aug 22 02:36:23.777: INFO: (8) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 16.047687ms)
Aug 22 02:36:23.778: INFO: (8) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 16.228893ms)
Aug 22 02:36:23.778: INFO: (8) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 16.558127ms)
Aug 22 02:36:23.778: INFO: (8) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 16.513077ms)
Aug 22 02:36:23.779: INFO: (8) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 16.815337ms)
Aug 22 02:36:23.779: INFO: (8) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 16.733188ms)
Aug 22 02:36:23.779: INFO: (8) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 17.221142ms)
Aug 22 02:36:23.779: INFO: (8) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 17.274884ms)
Aug 22 02:36:23.779: INFO: (8) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 17.27631ms)
Aug 22 02:36:23.793: INFO: (9) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 13.97156ms)
Aug 22 02:36:23.794: INFO: (9) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 14.162412ms)
Aug 22 02:36:23.803: INFO: (9) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 23.389332ms)
Aug 22 02:36:23.803: INFO: (9) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 23.217326ms)
Aug 22 02:36:23.803: INFO: (9) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 23.481426ms)
Aug 22 02:36:23.804: INFO: (9) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 23.716282ms)
Aug 22 02:36:23.804: INFO: (9) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 23.956844ms)
Aug 22 02:36:23.804: INFO: (9) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 24.170195ms)
Aug 22 02:36:23.804: INFO: (9) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 24.978261ms)
Aug 22 02:36:23.804: INFO: (9) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 24.2831ms)
Aug 22 02:36:23.804: INFO: (9) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 24.666808ms)
Aug 22 02:36:23.809: INFO: (9) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 28.953604ms)
Aug 22 02:36:23.809: INFO: (9) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 29.067124ms)
Aug 22 02:36:23.809: INFO: (9) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 29.672052ms)
Aug 22 02:36:23.809: INFO: (9) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 29.410891ms)
Aug 22 02:36:23.809: INFO: (9) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 29.846364ms)
Aug 22 02:36:23.816: INFO: (10) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 6.008159ms)
Aug 22 02:36:23.817: INFO: (10) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 6.597434ms)
Aug 22 02:36:23.817: INFO: (10) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 7.524032ms)
Aug 22 02:36:23.818: INFO: (10) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 7.286171ms)
Aug 22 02:36:23.818: INFO: (10) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 7.770753ms)
Aug 22 02:36:23.818: INFO: (10) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 7.967723ms)
Aug 22 02:36:23.818: INFO: (10) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 7.535461ms)
Aug 22 02:36:23.818: INFO: (10) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 7.454839ms)
Aug 22 02:36:23.818: INFO: (10) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 8.475068ms)
Aug 22 02:36:23.820: INFO: (10) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 9.1062ms)
Aug 22 02:36:23.820: INFO: (10) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 9.402363ms)
Aug 22 02:36:23.820: INFO: (10) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 9.727756ms)
Aug 22 02:36:23.820: INFO: (10) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 9.152445ms)
Aug 22 02:36:23.820: INFO: (10) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 9.77495ms)
Aug 22 02:36:23.820: INFO: (10) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 9.972849ms)
Aug 22 02:36:23.820: INFO: (10) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 9.534465ms)
Aug 22 02:36:23.828: INFO: (11) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 7.09608ms)
Aug 22 02:36:23.829: INFO: (11) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 7.719795ms)
Aug 22 02:36:23.829: INFO: (11) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 8.080274ms)
Aug 22 02:36:23.829: INFO: (11) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 8.843287ms)
Aug 22 02:36:23.829: INFO: (11) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 8.82458ms)
Aug 22 02:36:23.830: INFO: (11) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 8.861212ms)
Aug 22 02:36:23.830: INFO: (11) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 9.586516ms)
Aug 22 02:36:23.830: INFO: (11) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 9.892668ms)
Aug 22 02:36:23.830: INFO: (11) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 9.438296ms)
Aug 22 02:36:23.830: INFO: (11) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 9.473044ms)
Aug 22 02:36:23.830: INFO: (11) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 9.893433ms)
Aug 22 02:36:23.831: INFO: (11) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 11.168526ms)
Aug 22 02:36:23.832: INFO: (11) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 11.132783ms)
Aug 22 02:36:23.832: INFO: (11) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 11.432823ms)
Aug 22 02:36:23.832: INFO: (11) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 11.558407ms)
Aug 22 02:36:23.832: INFO: (11) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 11.456367ms)
Aug 22 02:36:23.841: INFO: (12) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 7.975809ms)
Aug 22 02:36:23.841: INFO: (12) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 8.158605ms)
Aug 22 02:36:23.842: INFO: (12) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 8.573688ms)
Aug 22 02:36:23.842: INFO: (12) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 8.63575ms)
Aug 22 02:36:23.842: INFO: (12) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 9.195205ms)
Aug 22 02:36:23.842: INFO: (12) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 9.279187ms)
Aug 22 02:36:23.843: INFO: (12) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 10.171997ms)
Aug 22 02:36:23.843: INFO: (12) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 10.274611ms)
Aug 22 02:36:23.844: INFO: (12) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 10.948326ms)
Aug 22 02:36:23.844: INFO: (12) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 11.019073ms)
Aug 22 02:36:23.844: INFO: (12) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 10.914362ms)
Aug 22 02:36:23.844: INFO: (12) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 10.749503ms)
Aug 22 02:36:23.844: INFO: (12) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 11.249111ms)
Aug 22 02:36:23.844: INFO: (12) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 11.534811ms)
Aug 22 02:36:23.844: INFO: (12) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 11.697603ms)
Aug 22 02:36:23.845: INFO: (12) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 11.728321ms)
Aug 22 02:36:23.853: INFO: (13) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 7.814227ms)
Aug 22 02:36:23.853: INFO: (13) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 8.501718ms)
Aug 22 02:36:23.855: INFO: (13) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 10.059388ms)
Aug 22 02:36:23.855: INFO: (13) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 10.264266ms)
Aug 22 02:36:23.856: INFO: (13) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 10.84787ms)
Aug 22 02:36:23.856: INFO: (13) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 10.927624ms)
Aug 22 02:36:23.856: INFO: (13) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 10.478708ms)
Aug 22 02:36:23.856: INFO: (13) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 10.753406ms)
Aug 22 02:36:23.856: INFO: (13) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 11.106468ms)
Aug 22 02:36:23.856: INFO: (13) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 10.918164ms)
Aug 22 02:36:23.856: INFO: (13) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 10.85299ms)
Aug 22 02:36:23.858: INFO: (13) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 12.969751ms)
Aug 22 02:36:23.858: INFO: (13) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 13.443558ms)
Aug 22 02:36:23.859: INFO: (13) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 13.515868ms)
Aug 22 02:36:23.859: INFO: (13) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 13.336368ms)
Aug 22 02:36:23.859: INFO: (13) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 13.621326ms)
Aug 22 02:36:23.865: INFO: (14) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 6.355849ms)
Aug 22 02:36:23.868: INFO: (14) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 8.508182ms)
Aug 22 02:36:23.869: INFO: (14) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 8.901268ms)
Aug 22 02:36:23.869: INFO: (14) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 8.920242ms)
Aug 22 02:36:23.869: INFO: (14) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 8.928284ms)
Aug 22 02:36:23.869: INFO: (14) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 8.676908ms)
Aug 22 02:36:23.869: INFO: (14) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 8.832602ms)
Aug 22 02:36:23.869: INFO: (14) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 9.120979ms)
Aug 22 02:36:23.869: INFO: (14) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 8.930855ms)
Aug 22 02:36:23.870: INFO: (14) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 9.605293ms)
Aug 22 02:36:23.870: INFO: (14) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 10.675538ms)
Aug 22 02:36:23.870: INFO: (14) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 9.763696ms)
Aug 22 02:36:23.870: INFO: (14) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 10.601092ms)
Aug 22 02:36:23.870: INFO: (14) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 10.752827ms)
Aug 22 02:36:23.871: INFO: (14) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 11.414993ms)
Aug 22 02:36:23.871: INFO: (14) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 11.723395ms)
Aug 22 02:36:23.879: INFO: (15) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 6.889193ms)
Aug 22 02:36:23.879: INFO: (15) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 7.093144ms)
Aug 22 02:36:23.879: INFO: (15) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 6.323974ms)
Aug 22 02:36:23.879: INFO: (15) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 7.034539ms)
Aug 22 02:36:23.880: INFO: (15) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 7.454912ms)
Aug 22 02:36:23.880: INFO: (15) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 8.310672ms)
Aug 22 02:36:23.881: INFO: (15) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 7.963068ms)
Aug 22 02:36:23.881: INFO: (15) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 8.51829ms)
Aug 22 02:36:23.882: INFO: (15) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 9.526034ms)
Aug 22 02:36:23.882: INFO: (15) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 9.16724ms)
Aug 22 02:36:23.882: INFO: (15) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 9.594499ms)
Aug 22 02:36:23.882: INFO: (15) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 10.136107ms)
Aug 22 02:36:23.882: INFO: (15) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 9.395836ms)
Aug 22 02:36:23.883: INFO: (15) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 10.230303ms)
Aug 22 02:36:23.882: INFO: (15) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 9.474571ms)
Aug 22 02:36:23.883: INFO: (15) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 10.139575ms)
Aug 22 02:36:23.887: INFO: (16) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 4.484118ms)
Aug 22 02:36:23.890: INFO: (16) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 6.875136ms)
Aug 22 02:36:23.891: INFO: (16) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 7.548586ms)
Aug 22 02:36:23.891: INFO: (16) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 7.415031ms)
Aug 22 02:36:23.891: INFO: (16) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 7.501461ms)
Aug 22 02:36:23.891: INFO: (16) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 7.629835ms)
Aug 22 02:36:23.892: INFO: (16) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 8.274816ms)
Aug 22 02:36:23.892: INFO: (16) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 8.840367ms)
Aug 22 02:36:23.892: INFO: (16) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 9.178974ms)
Aug 22 02:36:23.892: INFO: (16) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 8.901055ms)
Aug 22 02:36:23.893: INFO: (16) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 9.450415ms)
Aug 22 02:36:23.893: INFO: (16) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 9.979494ms)
Aug 22 02:36:23.893: INFO: (16) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 10.044723ms)
Aug 22 02:36:23.893: INFO: (16) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 10.080083ms)
Aug 22 02:36:23.893: INFO: (16) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 10.187118ms)
Aug 22 02:36:23.894: INFO: (16) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 10.599197ms)
Aug 22 02:36:23.902: INFO: (17) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 8.036794ms)
Aug 22 02:36:23.903: INFO: (17) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 8.423143ms)
Aug 22 02:36:23.907: INFO: (17) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 11.400416ms)
Aug 22 02:36:23.910: INFO: (17) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 15.759822ms)
Aug 22 02:36:23.911: INFO: (17) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 15.679504ms)
Aug 22 02:36:23.911: INFO: (17) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 16.291222ms)
Aug 22 02:36:23.911: INFO: (17) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 16.476385ms)
Aug 22 02:36:23.912: INFO: (17) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 16.657575ms)
Aug 22 02:36:23.912: INFO: (17) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 17.421284ms)
Aug 22 02:36:23.912: INFO: (17) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 17.676202ms)
Aug 22 02:36:23.912: INFO: (17) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 17.468143ms)
Aug 22 02:36:23.912: INFO: (17) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 17.667203ms)
Aug 22 02:36:23.912: INFO: (17) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 17.500861ms)
Aug 22 02:36:23.913: INFO: (17) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 17.794415ms)
Aug 22 02:36:23.913: INFO: (17) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 17.850852ms)
Aug 22 02:36:23.913: INFO: (17) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 18.063442ms)
Aug 22 02:36:23.918: INFO: (18) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 4.87301ms)
Aug 22 02:36:23.918: INFO: (18) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 5.155928ms)
Aug 22 02:36:23.918: INFO: (18) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 5.110986ms)
Aug 22 02:36:23.918: INFO: (18) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 5.050758ms)
Aug 22 02:36:23.919: INFO: (18) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 5.828265ms)
Aug 22 02:36:23.919: INFO: (18) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 5.457452ms)
Aug 22 02:36:23.921: INFO: (18) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 7.238636ms)
Aug 22 02:36:23.925: INFO: (18) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 11.325689ms)
Aug 22 02:36:23.926: INFO: (18) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 11.621097ms)
Aug 22 02:36:23.926: INFO: (18) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 12.092471ms)
Aug 22 02:36:23.926: INFO: (18) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 12.189704ms)
Aug 22 02:36:23.926: INFO: (18) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 12.437542ms)
Aug 22 02:36:23.926: INFO: (18) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 12.243855ms)
Aug 22 02:36:23.927: INFO: (18) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 12.734342ms)
Aug 22 02:36:23.927: INFO: (18) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 12.878447ms)
Aug 22 02:36:23.927: INFO: (18) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 13.150779ms)
Aug 22 02:36:23.931: INFO: (19) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">test<... (200; 3.487504ms)
Aug 22 02:36:23.932: INFO: (19) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:460/proxy/: tls baz (200; 4.020199ms)
Aug 22 02:36:23.934: INFO: (19) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:1080/proxy/rewriteme">... (200; 5.862464ms)
Aug 22 02:36:23.934: INFO: (19) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 5.828829ms)
Aug 22 02:36:23.934: INFO: (19) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 6.124033ms)
Aug 22 02:36:23.938: INFO: (19) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:443/proxy/tlsrewritem... (200; 9.534878ms)
Aug 22 02:36:23.940: INFO: (19) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname2/proxy/: bar (200; 12.194648ms)
Aug 22 02:36:23.941: INFO: (19) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname2/proxy/: tls qux (200; 12.519478ms)
Aug 22 02:36:23.941: INFO: (19) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb:160/proxy/: foo (200; 12.44625ms)
Aug 22 02:36:23.941: INFO: (19) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname2/proxy/: bar (200; 12.861928ms)
Aug 22 02:36:23.941: INFO: (19) /api/v1/namespaces/proxy-7798/services/http:proxy-service-2nwvb:portname1/proxy/: foo (200; 12.959085ms)
Aug 22 02:36:23.942: INFO: (19) /api/v1/namespaces/proxy-7798/pods/http:proxy-service-2nwvb-d4qjb:162/proxy/: bar (200; 13.006034ms)
Aug 22 02:36:23.942: INFO: (19) /api/v1/namespaces/proxy-7798/pods/https:proxy-service-2nwvb-d4qjb:462/proxy/: tls qux (200; 13.587585ms)
Aug 22 02:36:23.942: INFO: (19) /api/v1/namespaces/proxy-7798/services/proxy-service-2nwvb:portname1/proxy/: foo (200; 13.620467ms)
Aug 22 02:36:23.942: INFO: (19) /api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/: <a href="/api/v1/namespaces/proxy-7798/pods/proxy-service-2nwvb-d4qjb/proxy/rewriteme">test</a> (200; 13.657689ms)
Aug 22 02:36:23.942: INFO: (19) /api/v1/namespaces/proxy-7798/services/https:proxy-service-2nwvb:tlsportname1/proxy/: tls baz (200; 13.907119ms)
STEP: deleting ReplicationController proxy-service-2nwvb in namespace proxy-7798, will wait for the garbage collector to delete the pods
Aug 22 02:36:24.004: INFO: Deleting ReplicationController proxy-service-2nwvb took: 8.891676ms
Aug 22 02:36:24.505: INFO: Terminating ReplicationController proxy-service-2nwvb pods took: 500.363698ms
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:36:26.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7798" for this suite.
Aug 22 02:36:32.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:36:32.517: INFO: namespace proxy-7798 deletion completed in 6.108695714s

• [SLOW TEST:15.169 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:36:32.517: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 22 02:36:32.555: INFO: Waiting up to 5m0s for pod "pod-0e8f27cf-7ba8-4207-aada-94b0c3d1f5cf" in namespace "emptydir-2341" to be "success or failure"
Aug 22 02:36:32.561: INFO: Pod "pod-0e8f27cf-7ba8-4207-aada-94b0c3d1f5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124043ms
Aug 22 02:36:34.564: INFO: Pod "pod-0e8f27cf-7ba8-4207-aada-94b0c3d1f5cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009339388s
STEP: Saw pod success
Aug 22 02:36:34.564: INFO: Pod "pod-0e8f27cf-7ba8-4207-aada-94b0c3d1f5cf" satisfied condition "success or failure"
Aug 22 02:36:34.567: INFO: Trying to get logs from node melsayed-conformance3 pod pod-0e8f27cf-7ba8-4207-aada-94b0c3d1f5cf container test-container: <nil>
STEP: delete the pod
Aug 22 02:36:34.593: INFO: Waiting for pod pod-0e8f27cf-7ba8-4207-aada-94b0c3d1f5cf to disappear
Aug 22 02:36:34.597: INFO: Pod pod-0e8f27cf-7ba8-4207-aada-94b0c3d1f5cf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:36:34.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2341" for this suite.
Aug 22 02:36:40.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:36:40.713: INFO: namespace emptydir-2341 deletion completed in 6.112669092s

• [SLOW TEST:8.196 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:36:40.713: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 22 02:36:40.743: INFO: Waiting up to 5m0s for pod "pod-a521132b-c6e6-484d-84d2-53bc4ba28a18" in namespace "emptydir-9284" to be "success or failure"
Aug 22 02:36:40.747: INFO: Pod "pod-a521132b-c6e6-484d-84d2-53bc4ba28a18": Phase="Pending", Reason="", readiness=false. Elapsed: 3.897406ms
Aug 22 02:36:42.751: INFO: Pod "pod-a521132b-c6e6-484d-84d2-53bc4ba28a18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007534502s
STEP: Saw pod success
Aug 22 02:36:42.751: INFO: Pod "pod-a521132b-c6e6-484d-84d2-53bc4ba28a18" satisfied condition "success or failure"
Aug 22 02:36:42.753: INFO: Trying to get logs from node melsayed-conformance3 pod pod-a521132b-c6e6-484d-84d2-53bc4ba28a18 container test-container: <nil>
STEP: delete the pod
Aug 22 02:36:42.776: INFO: Waiting for pod pod-a521132b-c6e6-484d-84d2-53bc4ba28a18 to disappear
Aug 22 02:36:42.779: INFO: Pod pod-a521132b-c6e6-484d-84d2-53bc4ba28a18 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:36:42.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9284" for this suite.
Aug 22 02:36:48.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:36:48.878: INFO: namespace emptydir-9284 deletion completed in 6.091909446s

• [SLOW TEST:8.165 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:36:48.878: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:36:50.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6876" for this suite.
Aug 22 02:36:56.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:36:57.061: INFO: namespace emptydir-wrapper-6876 deletion completed in 6.103504855s

• [SLOW TEST:8.183 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:36:57.061: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:36:57.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76481ea4-f532-4252-8701-84c81b2bd5cb" in namespace "downward-api-6582" to be "success or failure"
Aug 22 02:36:57.164: INFO: Pod "downwardapi-volume-76481ea4-f532-4252-8701-84c81b2bd5cb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.561544ms
Aug 22 02:36:59.168: INFO: Pod "downwardapi-volume-76481ea4-f532-4252-8701-84c81b2bd5cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024496818s
Aug 22 02:37:01.172: INFO: Pod "downwardapi-volume-76481ea4-f532-4252-8701-84c81b2bd5cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028263628s
Aug 22 02:37:03.175: INFO: Pod "downwardapi-volume-76481ea4-f532-4252-8701-84c81b2bd5cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031583142s
STEP: Saw pod success
Aug 22 02:37:03.175: INFO: Pod "downwardapi-volume-76481ea4-f532-4252-8701-84c81b2bd5cb" satisfied condition "success or failure"
Aug 22 02:37:03.178: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-76481ea4-f532-4252-8701-84c81b2bd5cb container client-container: <nil>
STEP: delete the pod
Aug 22 02:37:03.202: INFO: Waiting for pod downwardapi-volume-76481ea4-f532-4252-8701-84c81b2bd5cb to disappear
Aug 22 02:37:03.208: INFO: Pod downwardapi-volume-76481ea4-f532-4252-8701-84c81b2bd5cb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:37:03.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6582" for this suite.
Aug 22 02:37:09.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:37:09.322: INFO: namespace downward-api-6582 deletion completed in 6.109569651s

• [SLOW TEST:12.261 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:37:09.322: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-1ba7e7ee-5bd3-4854-b6ac-64defad8abb2
STEP: Creating a pod to test consume secrets
Aug 22 02:37:09.378: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4602b12c-d777-4a50-b73f-6615f5edb163" in namespace "projected-6357" to be "success or failure"
Aug 22 02:37:09.383: INFO: Pod "pod-projected-secrets-4602b12c-d777-4a50-b73f-6615f5edb163": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877413ms
Aug 22 02:37:11.388: INFO: Pod "pod-projected-secrets-4602b12c-d777-4a50-b73f-6615f5edb163": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010260117s
STEP: Saw pod success
Aug 22 02:37:11.388: INFO: Pod "pod-projected-secrets-4602b12c-d777-4a50-b73f-6615f5edb163" satisfied condition "success or failure"
Aug 22 02:37:11.391: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-secrets-4602b12c-d777-4a50-b73f-6615f5edb163 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 02:37:11.413: INFO: Waiting for pod pod-projected-secrets-4602b12c-d777-4a50-b73f-6615f5edb163 to disappear
Aug 22 02:37:11.417: INFO: Pod pod-projected-secrets-4602b12c-d777-4a50-b73f-6615f5edb163 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:37:11.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6357" for this suite.
Aug 22 02:37:17.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:37:17.551: INFO: namespace projected-6357 deletion completed in 6.129684783s

• [SLOW TEST:8.229 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:37:17.551: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 22 02:37:19.608: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:37:19.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3234" for this suite.
Aug 22 02:37:25.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:37:25.732: INFO: namespace container-runtime-3234 deletion completed in 6.104744942s

• [SLOW TEST:8.180 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:37:25.734: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 22 02:37:25.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3542'
Aug 22 02:37:26.050: INFO: stderr: ""
Aug 22 02:37:26.050: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Aug 22 02:37:26.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete pods e2e-test-nginx-pod --namespace=kubectl-3542'
Aug 22 02:37:32.187: INFO: stderr: ""
Aug 22 02:37:32.187: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:37:32.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3542" for this suite.
Aug 22 02:37:38.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:37:38.285: INFO: namespace kubectl-3542 deletion completed in 6.094632577s

• [SLOW TEST:12.552 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:37:38.287: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-814e086b-f4fc-4860-91d9-b69e5f68e7ad
STEP: Creating a pod to test consume secrets
Aug 22 02:37:38.333: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341" in namespace "projected-3758" to be "success or failure"
Aug 22 02:37:38.340: INFO: Pod "pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341": Phase="Pending", Reason="", readiness=false. Elapsed: 6.186448ms
Aug 22 02:37:40.343: INFO: Pod "pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009757704s
Aug 22 02:37:42.347: INFO: Pod "pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013791342s
Aug 22 02:37:44.352: INFO: Pod "pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018568033s
Aug 22 02:37:46.357: INFO: Pod "pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02295724s
STEP: Saw pod success
Aug 22 02:37:46.357: INFO: Pod "pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341" satisfied condition "success or failure"
Aug 22 02:37:46.360: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 02:37:46.386: INFO: Waiting for pod pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341 to disappear
Aug 22 02:37:46.389: INFO: Pod pod-projected-secrets-7fd031d9-fb7d-4e47-a341-e8e2fcce9341 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:37:46.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3758" for this suite.
Aug 22 02:37:52.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:37:52.481: INFO: namespace projected-3758 deletion completed in 6.088406927s

• [SLOW TEST:14.194 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:37:52.483: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8776
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 22 02:37:52.509: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 22 02:38:14.625: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.165:8080/dial?request=hostName&protocol=udp&host=10.42.0.50&port=8081&tries=1'] Namespace:pod-network-test-8776 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:38:14.625: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:38:14.818: INFO: Waiting for endpoints: map[]
Aug 22 02:38:14.822: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.165:8080/dial?request=hostName&protocol=udp&host=10.42.2.37&port=8081&tries=1'] Namespace:pod-network-test-8776 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:38:14.822: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:38:15.061: INFO: Waiting for endpoints: map[]
Aug 22 02:38:15.065: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.1.165:8080/dial?request=hostName&protocol=udp&host=10.42.1.164&port=8081&tries=1'] Namespace:pod-network-test-8776 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:38:15.065: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:38:15.303: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:38:15.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8776" for this suite.
Aug 22 02:38:37.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:38:37.439: INFO: namespace pod-network-test-8776 deletion completed in 22.130350324s

• [SLOW TEST:44.957 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:38:37.446: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 22 02:38:37.479: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 22 02:38:37.485: INFO: Waiting for terminating namespaces to be deleted...
Aug 22 02:38:37.489: INFO: 
Logging pods the kubelet thinks is on node melsayed-conformance1 before test
Aug 22 02:38:37.500: INFO: coredns-5678df9bcc-74zgx from kube-system started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.500: INFO: 	Container coredns ready: true, restart count 0
Aug 22 02:38:37.500: INFO: default-http-backend-8456b47798-h7zvx from ingress-nginx started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.500: INFO: 	Container default-http-backend ready: true, restart count 0
Aug 22 02:38:37.501: INFO: cattle-node-agent-d2s4f from cattle-system started at 2019-08-22 01:28:10 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.501: INFO: 	Container agent ready: true, restart count 0
Aug 22 02:38:37.501: INFO: sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-b6mbk from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:38:37.501: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 22 02:38:37.501: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 22 02:38:37.501: INFO: cattle-cluster-agent-55977bf459-5lfn6 from cattle-system started at 2019-08-22 01:28:10 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.501: INFO: 	Container cluster-register ready: true, restart count 0
Aug 22 02:38:37.501: INFO: kube-api-auth-87zdm from cattle-system started at 2019-08-22 01:28:10 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.501: INFO: 	Container kube-api-auth ready: true, restart count 0
Aug 22 02:38:37.501: INFO: rke-network-plugin-deploy-job-qckrh from kube-system started at 2019-08-22 01:27:29 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.501: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Aug 22 02:38:37.501: INFO: rke-metrics-addon-deploy-job-c28kb from kube-system started at 2019-08-22 01:27:40 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.502: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Aug 22 02:38:37.502: INFO: rke-ingress-controller-deploy-job-64vk6 from kube-system started at 2019-08-22 01:27:45 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.502: INFO: 	Container rke-ingress-controller-pod ready: false, restart count 0
Aug 22 02:38:37.502: INFO: metrics-server-dbcdc5bc9-pjqjt from kube-system started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.502: INFO: 	Container metrics-server ready: true, restart count 0
Aug 22 02:38:37.502: INFO: coredns-autoscaler-57bc9c9bd-2dczs from kube-system started at 2019-08-22 01:27:50 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.502: INFO: 	Container autoscaler ready: true, restart count 0
Aug 22 02:38:37.502: INFO: canal-lwlb2 from kube-system started at 2019-08-22 01:27:32 +0000 UTC (2 container statuses recorded)
Aug 22 02:38:37.502: INFO: 	Container calico-node ready: true, restart count 0
Aug 22 02:38:37.502: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 02:38:37.502: INFO: rke-coredns-addon-deploy-job-pldgp from kube-system started at 2019-08-22 01:27:34 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.502: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Aug 22 02:38:37.502: INFO: nginx-ingress-controller-44rfm from ingress-nginx started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.502: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 22 02:38:37.503: INFO: 
Logging pods the kubelet thinks is on node melsayed-conformance2 before test
Aug 22 02:38:37.511: INFO: canal-2fsgt from kube-system started at 2019-08-22 01:28:45 +0000 UTC (2 container statuses recorded)
Aug 22 02:38:37.511: INFO: 	Container calico-node ready: true, restart count 0
Aug 22 02:38:37.511: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 02:38:37.511: INFO: cattle-node-agent-x849w from cattle-system started at 2019-08-22 01:28:45 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.511: INFO: 	Container agent ready: true, restart count 0
Aug 22 02:38:37.511: INFO: nginx-ingress-controller-wsr46 from ingress-nginx started at 2019-08-22 01:29:06 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.511: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 22 02:38:37.511: INFO: sonobuoy-e2e-job-58e145a7c72f46b3 from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:38:37.511: INFO: 	Container e2e ready: true, restart count 0
Aug 22 02:38:37.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 02:38:37.511: INFO: sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-bs4pn from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:38:37.511: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 22 02:38:37.511: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 22 02:38:37.511: INFO: 
Logging pods the kubelet thinks is on node melsayed-conformance3 before test
Aug 22 02:38:37.524: INFO: nginx-ingress-controller-h65vt from ingress-nginx started at 2019-08-22 01:28:58 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.525: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 22 02:38:37.525: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-22 01:35:54 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.525: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 22 02:38:37.525: INFO: sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-p5mld from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:38:37.525: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 22 02:38:37.526: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 22 02:38:37.526: INFO: cattle-node-agent-r7lmz from cattle-system started at 2019-08-22 01:28:38 +0000 UTC (1 container statuses recorded)
Aug 22 02:38:37.526: INFO: 	Container agent ready: true, restart count 0
Aug 22 02:38:37.526: INFO: canal-kcl25 from kube-system started at 2019-08-22 01:28:38 +0000 UTC (2 container statuses recorded)
Aug 22 02:38:37.526: INFO: 	Container calico-node ready: true, restart count 0
Aug 22 02:38:37.526: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node melsayed-conformance1
STEP: verifying the node has the label node melsayed-conformance2
STEP: verifying the node has the label node melsayed-conformance3
Aug 22 02:38:37.592: INFO: Pod cattle-cluster-agent-55977bf459-5lfn6 requesting resource cpu=0m on Node melsayed-conformance1
Aug 22 02:38:37.592: INFO: Pod cattle-node-agent-d2s4f requesting resource cpu=0m on Node melsayed-conformance1
Aug 22 02:38:37.593: INFO: Pod cattle-node-agent-r7lmz requesting resource cpu=0m on Node melsayed-conformance3
Aug 22 02:38:37.593: INFO: Pod cattle-node-agent-x849w requesting resource cpu=0m on Node melsayed-conformance2
Aug 22 02:38:37.593: INFO: Pod kube-api-auth-87zdm requesting resource cpu=0m on Node melsayed-conformance1
Aug 22 02:38:37.593: INFO: Pod sonobuoy requesting resource cpu=0m on Node melsayed-conformance3
Aug 22 02:38:37.593: INFO: Pod sonobuoy-e2e-job-58e145a7c72f46b3 requesting resource cpu=0m on Node melsayed-conformance2
Aug 22 02:38:37.593: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-b6mbk requesting resource cpu=0m on Node melsayed-conformance1
Aug 22 02:38:37.593: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-bs4pn requesting resource cpu=0m on Node melsayed-conformance2
Aug 22 02:38:37.593: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-p5mld requesting resource cpu=0m on Node melsayed-conformance3
Aug 22 02:38:37.593: INFO: Pod default-http-backend-8456b47798-h7zvx requesting resource cpu=10m on Node melsayed-conformance1
Aug 22 02:38:37.594: INFO: Pod nginx-ingress-controller-44rfm requesting resource cpu=0m on Node melsayed-conformance1
Aug 22 02:38:37.594: INFO: Pod nginx-ingress-controller-h65vt requesting resource cpu=0m on Node melsayed-conformance3
Aug 22 02:38:37.594: INFO: Pod nginx-ingress-controller-wsr46 requesting resource cpu=0m on Node melsayed-conformance2
Aug 22 02:38:37.594: INFO: Pod canal-2fsgt requesting resource cpu=250m on Node melsayed-conformance2
Aug 22 02:38:37.594: INFO: Pod canal-kcl25 requesting resource cpu=250m on Node melsayed-conformance3
Aug 22 02:38:37.594: INFO: Pod canal-lwlb2 requesting resource cpu=250m on Node melsayed-conformance1
Aug 22 02:38:37.594: INFO: Pod coredns-5678df9bcc-74zgx requesting resource cpu=100m on Node melsayed-conformance1
Aug 22 02:38:37.595: INFO: Pod coredns-autoscaler-57bc9c9bd-2dczs requesting resource cpu=20m on Node melsayed-conformance1
Aug 22 02:38:37.595: INFO: Pod metrics-server-dbcdc5bc9-pjqjt requesting resource cpu=0m on Node melsayed-conformance1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19805918-a683-44cb-bd9a-59b0a716a1b8.15bd1e42caace360], Reason = [Scheduled], Message = [Successfully assigned sched-pred-724/filler-pod-19805918-a683-44cb-bd9a-59b0a716a1b8 to melsayed-conformance3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19805918-a683-44cb-bd9a-59b0a716a1b8.15bd1e430172cd16], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19805918-a683-44cb-bd9a-59b0a716a1b8.15bd1e4305a3fb4a], Reason = [Created], Message = [Created container filler-pod-19805918-a683-44cb-bd9a-59b0a716a1b8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19805918-a683-44cb-bd9a-59b0a716a1b8.15bd1e43162d92c0], Reason = [Started], Message = [Started container filler-pod-19805918-a683-44cb-bd9a-59b0a716a1b8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b5f8a76-f2d2-427e-9b48-32d0d77ef5ba.15bd1e42cab23d68], Reason = [Scheduled], Message = [Successfully assigned sched-pred-724/filler-pod-2b5f8a76-f2d2-427e-9b48-32d0d77ef5ba to melsayed-conformance1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b5f8a76-f2d2-427e-9b48-32d0d77ef5ba.15bd1e4307e59197], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b5f8a76-f2d2-427e-9b48-32d0d77ef5ba.15bd1e432ff4fdb3], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b5f8a76-f2d2-427e-9b48-32d0d77ef5ba.15bd1e4333df5721], Reason = [Created], Message = [Created container filler-pod-2b5f8a76-f2d2-427e-9b48-32d0d77ef5ba]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b5f8a76-f2d2-427e-9b48-32d0d77ef5ba.15bd1e43457c62bf], Reason = [Started], Message = [Started container filler-pod-2b5f8a76-f2d2-427e-9b48-32d0d77ef5ba]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b6d4ad2-55da-442f-bbed-dc31bb5041f5.15bd1e42cc11fa19], Reason = [Scheduled], Message = [Successfully assigned sched-pred-724/filler-pod-6b6d4ad2-55da-442f-bbed-dc31bb5041f5 to melsayed-conformance2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b6d4ad2-55da-442f-bbed-dc31bb5041f5.15bd1e458c9cfcb5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b6d4ad2-55da-442f-bbed-dc31bb5041f5.15bd1e458ffe93e6], Reason = [Created], Message = [Created container filler-pod-6b6d4ad2-55da-442f-bbed-dc31bb5041f5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b6d4ad2-55da-442f-bbed-dc31bb5041f5.15bd1e45a07ba40e], Reason = [Started], Message = [Started container filler-pod-6b6d4ad2-55da-442f-bbed-dc31bb5041f5]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15bd1e460fb29256], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node melsayed-conformance1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node melsayed-conformance2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node melsayed-conformance3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:38:52.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-724" for this suite.
Aug 22 02:38:58.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:38:58.812: INFO: namespace sched-pred-724 deletion completed in 6.104322486s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:21.367 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:38:58.813: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-917.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-917.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-917.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-917.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-917.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-917.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-917.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-917.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-917.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-917.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-917.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 66.149.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.149.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.149.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.149.66_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-917.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-917.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-917.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-917.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-917.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-917.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-917.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-917.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-917.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-917.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-917.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 66.149.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.149.66_udp@PTR;check="$$(dig +tcp +noall +answer +search 66.149.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.149.66_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 22 02:39:02.905: INFO: Unable to read wheezy_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:02.908: INFO: Unable to read wheezy_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:02.911: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:02.914: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:02.946: INFO: Unable to read jessie_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:02.950: INFO: Unable to read jessie_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:02.954: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:02.957: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:02.981: INFO: Lookups using dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35 failed for: [wheezy_udp@dns-test-service.dns-917.svc.cluster.local wheezy_tcp@dns-test-service.dns-917.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_udp@dns-test-service.dns-917.svc.cluster.local jessie_tcp@dns-test-service.dns-917.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local]

Aug 22 02:39:07.987: INFO: Unable to read wheezy_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:07.991: INFO: Unable to read wheezy_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:07.995: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:07.998: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:08.040: INFO: Unable to read jessie_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:08.044: INFO: Unable to read jessie_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:08.048: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:08.052: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:08.074: INFO: Lookups using dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35 failed for: [wheezy_udp@dns-test-service.dns-917.svc.cluster.local wheezy_tcp@dns-test-service.dns-917.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_udp@dns-test-service.dns-917.svc.cluster.local jessie_tcp@dns-test-service.dns-917.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local]

Aug 22 02:39:12.988: INFO: Unable to read wheezy_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:12.993: INFO: Unable to read wheezy_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:12.998: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:13.001: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:13.031: INFO: Unable to read jessie_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:13.035: INFO: Unable to read jessie_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:13.039: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:13.043: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:13.067: INFO: Lookups using dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35 failed for: [wheezy_udp@dns-test-service.dns-917.svc.cluster.local wheezy_tcp@dns-test-service.dns-917.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_udp@dns-test-service.dns-917.svc.cluster.local jessie_tcp@dns-test-service.dns-917.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local]

Aug 22 02:39:17.986: INFO: Unable to read wheezy_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:17.991: INFO: Unable to read wheezy_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:17.995: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:17.999: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:18.028: INFO: Unable to read jessie_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:18.030: INFO: Unable to read jessie_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:18.034: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:18.036: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:18.058: INFO: Lookups using dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35 failed for: [wheezy_udp@dns-test-service.dns-917.svc.cluster.local wheezy_tcp@dns-test-service.dns-917.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_udp@dns-test-service.dns-917.svc.cluster.local jessie_tcp@dns-test-service.dns-917.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local]

Aug 22 02:39:22.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:22.994: INFO: Unable to read wheezy_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:22.997: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:23.001: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:23.026: INFO: Unable to read jessie_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:23.029: INFO: Unable to read jessie_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:23.032: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:23.035: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:23.055: INFO: Lookups using dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35 failed for: [wheezy_udp@dns-test-service.dns-917.svc.cluster.local wheezy_tcp@dns-test-service.dns-917.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_udp@dns-test-service.dns-917.svc.cluster.local jessie_tcp@dns-test-service.dns-917.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local]

Aug 22 02:39:27.987: INFO: Unable to read wheezy_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:27.991: INFO: Unable to read wheezy_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:27.995: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:27.999: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:28.034: INFO: Unable to read jessie_udp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:28.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:28.041: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:28.045: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local from pod dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35: the server could not find the requested resource (get pods dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35)
Aug 22 02:39:28.068: INFO: Lookups using dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35 failed for: [wheezy_udp@dns-test-service.dns-917.svc.cluster.local wheezy_tcp@dns-test-service.dns-917.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_udp@dns-test-service.dns-917.svc.cluster.local jessie_tcp@dns-test-service.dns-917.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-917.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-917.svc.cluster.local]

Aug 22 02:39:33.064: INFO: DNS probes using dns-917/dns-test-a4ce1c89-d39e-4068-9e9b-643a753a4c35 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:39:33.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-917" for this suite.
Aug 22 02:39:39.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:39:39.265: INFO: namespace dns-917 deletion completed in 6.096758241s

• [SLOW TEST:40.452 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:39:39.265: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-6c1f9d2b-7b70-4b7c-bfa2-da322f459b13
STEP: Creating a pod to test consume secrets
Aug 22 02:39:39.305: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32" in namespace "projected-1089" to be "success or failure"
Aug 22 02:39:39.315: INFO: Pod "pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32": Phase="Pending", Reason="", readiness=false. Elapsed: 9.942441ms
Aug 22 02:39:41.320: INFO: Pod "pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015038571s
Aug 22 02:39:43.324: INFO: Pod "pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018881923s
Aug 22 02:39:45.336: INFO: Pod "pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030757682s
Aug 22 02:39:47.340: INFO: Pod "pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.034803117s
STEP: Saw pod success
Aug 22 02:39:47.340: INFO: Pod "pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32" satisfied condition "success or failure"
Aug 22 02:39:47.344: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 02:39:47.375: INFO: Waiting for pod pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32 to disappear
Aug 22 02:39:47.379: INFO: Pod pod-projected-secrets-1ec20188-0fb0-4d55-890d-80df38f62e32 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:39:47.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1089" for this suite.
Aug 22 02:39:53.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:39:53.516: INFO: namespace projected-1089 deletion completed in 6.133639687s

• [SLOW TEST:14.252 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:39:53.517: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 02:39:53.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27b8db06-12d1-4005-bdc8-9de0556d2735" in namespace "downward-api-8979" to be "success or failure"
Aug 22 02:39:53.566: INFO: Pod "downwardapi-volume-27b8db06-12d1-4005-bdc8-9de0556d2735": Phase="Pending", Reason="", readiness=false. Elapsed: 3.415018ms
Aug 22 02:39:55.570: INFO: Pod "downwardapi-volume-27b8db06-12d1-4005-bdc8-9de0556d2735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00726271s
STEP: Saw pod success
Aug 22 02:39:55.570: INFO: Pod "downwardapi-volume-27b8db06-12d1-4005-bdc8-9de0556d2735" satisfied condition "success or failure"
Aug 22 02:39:55.572: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-27b8db06-12d1-4005-bdc8-9de0556d2735 container client-container: <nil>
STEP: delete the pod
Aug 22 02:39:55.607: INFO: Waiting for pod downwardapi-volume-27b8db06-12d1-4005-bdc8-9de0556d2735 to disappear
Aug 22 02:39:55.620: INFO: Pod downwardapi-volume-27b8db06-12d1-4005-bdc8-9de0556d2735 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:39:55.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8979" for this suite.
Aug 22 02:40:01.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:40:01.790: INFO: namespace downward-api-8979 deletion completed in 6.166541619s

• [SLOW TEST:8.273 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:40:01.791: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-99da4661-009a-48da-a611-472ee5c238f1
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:40:01.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8687" for this suite.
Aug 22 02:40:07.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:40:07.926: INFO: namespace configmap-8687 deletion completed in 6.090849934s

• [SLOW TEST:6.135 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:40:07.929: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 22 02:40:07.995: INFO: Waiting up to 5m0s for pod "downward-api-deb2bbc2-c984-4f34-aa36-14a5c043151e" in namespace "downward-api-8103" to be "success or failure"
Aug 22 02:40:08.014: INFO: Pod "downward-api-deb2bbc2-c984-4f34-aa36-14a5c043151e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.860603ms
Aug 22 02:40:10.018: INFO: Pod "downward-api-deb2bbc2-c984-4f34-aa36-14a5c043151e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022409821s
Aug 22 02:40:12.023: INFO: Pod "downward-api-deb2bbc2-c984-4f34-aa36-14a5c043151e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027142406s
STEP: Saw pod success
Aug 22 02:40:12.023: INFO: Pod "downward-api-deb2bbc2-c984-4f34-aa36-14a5c043151e" satisfied condition "success or failure"
Aug 22 02:40:12.027: INFO: Trying to get logs from node melsayed-conformance3 pod downward-api-deb2bbc2-c984-4f34-aa36-14a5c043151e container dapi-container: <nil>
STEP: delete the pod
Aug 22 02:40:12.047: INFO: Waiting for pod downward-api-deb2bbc2-c984-4f34-aa36-14a5c043151e to disappear
Aug 22 02:40:12.052: INFO: Pod downward-api-deb2bbc2-c984-4f34-aa36-14a5c043151e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:40:12.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8103" for this suite.
Aug 22 02:40:18.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:40:18.155: INFO: namespace downward-api-8103 deletion completed in 6.099811728s

• [SLOW TEST:10.227 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:40:18.157: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-7cc9731c-dd5f-471c-a44d-5cbe205d3003 in namespace container-probe-7190
Aug 22 02:40:20.246: INFO: Started pod busybox-7cc9731c-dd5f-471c-a44d-5cbe205d3003 in namespace container-probe-7190
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 02:40:20.249: INFO: Initial restart count of pod busybox-7cc9731c-dd5f-471c-a44d-5cbe205d3003 is 0
Aug 22 02:41:12.369: INFO: Restart count of pod container-probe-7190/busybox-7cc9731c-dd5f-471c-a44d-5cbe205d3003 is now 1 (52.119936527s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:41:12.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7190" for this suite.
Aug 22 02:41:18.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:41:18.513: INFO: namespace container-probe-7190 deletion completed in 6.120181641s

• [SLOW TEST:60.356 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:41:18.514: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 22 02:41:18.591: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 22 02:41:25.817: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:41:25.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7616" for this suite.
Aug 22 02:41:31.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:41:31.942: INFO: namespace pods-7616 deletion completed in 6.118356034s

• [SLOW TEST:13.428 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:41:31.942: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c2c0fabe-9d73-47fd-a4bb-aa50010bed63
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-c2c0fabe-9d73-47fd-a4bb-aa50010bed63
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:43:03.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6554" for this suite.
Aug 22 02:44:25.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:44:25.666: INFO: namespace projected-6554 deletion completed in 1m22.100616629s

• [SLOW TEST:173.724 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:44:25.671: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 22 02:44:25.735: INFO: Number of nodes with available pods: 0
Aug 22 02:44:25.735: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:44:26.744: INFO: Number of nodes with available pods: 0
Aug 22 02:44:26.745: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:44:27.756: INFO: Number of nodes with available pods: 1
Aug 22 02:44:27.756: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:44:28.742: INFO: Number of nodes with available pods: 3
Aug 22 02:44:28.742: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 22 02:44:28.780: INFO: Number of nodes with available pods: 2
Aug 22 02:44:28.780: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:44:29.792: INFO: Number of nodes with available pods: 2
Aug 22 02:44:29.792: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:44:30.788: INFO: Number of nodes with available pods: 2
Aug 22 02:44:30.788: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:44:31.788: INFO: Number of nodes with available pods: 2
Aug 22 02:44:31.788: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:44:32.787: INFO: Number of nodes with available pods: 2
Aug 22 02:44:32.787: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:44:33.789: INFO: Number of nodes with available pods: 2
Aug 22 02:44:33.789: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:44:34.791: INFO: Number of nodes with available pods: 2
Aug 22 02:44:34.791: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:44:35.787: INFO: Number of nodes with available pods: 2
Aug 22 02:44:35.787: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:44:36.790: INFO: Number of nodes with available pods: 2
Aug 22 02:44:36.790: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:44:37.789: INFO: Number of nodes with available pods: 3
Aug 22 02:44:37.789: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6123, will wait for the garbage collector to delete the pods
Aug 22 02:44:37.860: INFO: Deleting DaemonSet.extensions daemon-set took: 15.066076ms
Aug 22 02:44:38.360: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.582679ms
Aug 22 02:44:48.064: INFO: Number of nodes with available pods: 0
Aug 22 02:44:48.064: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 02:44:48.067: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6123/daemonsets","resourceVersion":"18672"},"items":null}

Aug 22 02:44:48.070: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6123/pods","resourceVersion":"18672"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:44:48.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6123" for this suite.
Aug 22 02:44:54.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:44:54.211: INFO: namespace daemonsets-6123 deletion completed in 6.127521054s

• [SLOW TEST:28.539 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:44:54.211: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0822 02:45:04.340839      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 22 02:45:04.340: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:45:04.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5153" for this suite.
Aug 22 02:45:10.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:45:10.453: INFO: namespace gc-5153 deletion completed in 6.106523348s

• [SLOW TEST:16.242 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:45:10.456: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:45:10.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 version'
Aug 22 02:45:10.616: INFO: stderr: ""
Aug 22 02:45:10.616: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:45:10.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9644" for this suite.
Aug 22 02:45:16.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:45:16.727: INFO: namespace kubectl-9644 deletion completed in 6.106624981s

• [SLOW TEST:6.271 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:45:16.727: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-6407d7b5-c3af-4c7f-800b-7da6c4aaf003
STEP: Creating a pod to test consume configMaps
Aug 22 02:45:16.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec3fc0dc-2244-49e4-8251-4a42b2c82e2f" in namespace "configmap-1151" to be "success or failure"
Aug 22 02:45:16.781: INFO: Pod "pod-configmaps-ec3fc0dc-2244-49e4-8251-4a42b2c82e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044546ms
Aug 22 02:45:18.785: INFO: Pod "pod-configmaps-ec3fc0dc-2244-49e4-8251-4a42b2c82e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011858435s
Aug 22 02:45:20.790: INFO: Pod "pod-configmaps-ec3fc0dc-2244-49e4-8251-4a42b2c82e2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016815262s
STEP: Saw pod success
Aug 22 02:45:20.790: INFO: Pod "pod-configmaps-ec3fc0dc-2244-49e4-8251-4a42b2c82e2f" satisfied condition "success or failure"
Aug 22 02:45:20.794: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-ec3fc0dc-2244-49e4-8251-4a42b2c82e2f container configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 02:45:20.818: INFO: Waiting for pod pod-configmaps-ec3fc0dc-2244-49e4-8251-4a42b2c82e2f to disappear
Aug 22 02:45:20.821: INFO: Pod pod-configmaps-ec3fc0dc-2244-49e4-8251-4a42b2c82e2f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:45:20.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1151" for this suite.
Aug 22 02:45:26.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:45:26.935: INFO: namespace configmap-1151 deletion completed in 6.110476678s

• [SLOW TEST:10.208 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:45:26.935: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 22 02:45:26.965: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:45:30.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6651" for this suite.
Aug 22 02:45:36.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:45:36.291: INFO: namespace init-container-6651 deletion completed in 6.124882232s

• [SLOW TEST:9.356 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:45:36.292: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 22 02:45:36.380: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:45:40.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8583" for this suite.
Aug 22 02:46:02.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:46:02.602: INFO: namespace init-container-8583 deletion completed in 22.103284847s

• [SLOW TEST:26.311 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:46:02.613: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9580
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 22 02:46:02.647: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 22 02:46:24.753: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.1.179:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9580 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:46:24.753: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:46:24.988: INFO: Found all expected endpoints: [netserver-0]
Aug 22 02:46:24.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.53:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9580 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:46:24.991: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:46:25.230: INFO: Found all expected endpoints: [netserver-1]
Aug 22 02:46:25.234: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.2.42:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9580 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 22 02:46:25.234: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
Aug 22 02:46:25.494: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:46:25.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9580" for this suite.
Aug 22 02:46:47.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:46:47.621: INFO: namespace pod-network-test-9580 deletion completed in 22.121202197s

• [SLOW TEST:45.008 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:46:47.625: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 22 02:46:47.654: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 22 02:46:47.663: INFO: Waiting for terminating namespaces to be deleted...
Aug 22 02:46:47.668: INFO: 
Logging pods the kubelet thinks is on node melsayed-conformance1 before test
Aug 22 02:46:47.676: INFO: coredns-autoscaler-57bc9c9bd-2dczs from kube-system started at 2019-08-22 01:27:50 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.676: INFO: 	Container autoscaler ready: true, restart count 0
Aug 22 02:46:47.677: INFO: cattle-cluster-agent-55977bf459-5lfn6 from cattle-system started at 2019-08-22 01:28:10 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.677: INFO: 	Container cluster-register ready: true, restart count 0
Aug 22 02:46:47.677: INFO: kube-api-auth-87zdm from cattle-system started at 2019-08-22 01:28:10 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.677: INFO: 	Container kube-api-auth ready: true, restart count 0
Aug 22 02:46:47.677: INFO: rke-network-plugin-deploy-job-qckrh from kube-system started at 2019-08-22 01:27:29 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.677: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Aug 22 02:46:47.677: INFO: rke-metrics-addon-deploy-job-c28kb from kube-system started at 2019-08-22 01:27:40 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.677: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Aug 22 02:46:47.677: INFO: rke-ingress-controller-deploy-job-64vk6 from kube-system started at 2019-08-22 01:27:45 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.677: INFO: 	Container rke-ingress-controller-pod ready: false, restart count 0
Aug 22 02:46:47.677: INFO: metrics-server-dbcdc5bc9-pjqjt from kube-system started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.677: INFO: 	Container metrics-server ready: true, restart count 0
Aug 22 02:46:47.677: INFO: canal-lwlb2 from kube-system started at 2019-08-22 01:27:32 +0000 UTC (2 container statuses recorded)
Aug 22 02:46:47.677: INFO: 	Container calico-node ready: true, restart count 0
Aug 22 02:46:47.677: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 02:46:47.677: INFO: rke-coredns-addon-deploy-job-pldgp from kube-system started at 2019-08-22 01:27:34 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.677: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Aug 22 02:46:47.678: INFO: nginx-ingress-controller-44rfm from ingress-nginx started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.678: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 22 02:46:47.678: INFO: coredns-5678df9bcc-74zgx from kube-system started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.678: INFO: 	Container coredns ready: true, restart count 0
Aug 22 02:46:47.678: INFO: default-http-backend-8456b47798-h7zvx from ingress-nginx started at 2019-08-22 01:27:47 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.678: INFO: 	Container default-http-backend ready: true, restart count 0
Aug 22 02:46:47.678: INFO: cattle-node-agent-d2s4f from cattle-system started at 2019-08-22 01:28:10 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.678: INFO: 	Container agent ready: true, restart count 0
Aug 22 02:46:47.678: INFO: sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-b6mbk from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:46:47.678: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 22 02:46:47.678: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 22 02:46:47.678: INFO: 
Logging pods the kubelet thinks is on node melsayed-conformance2 before test
Aug 22 02:46:47.685: INFO: sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-bs4pn from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:46:47.685: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 22 02:46:47.685: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 22 02:46:47.685: INFO: canal-2fsgt from kube-system started at 2019-08-22 01:28:45 +0000 UTC (2 container statuses recorded)
Aug 22 02:46:47.685: INFO: 	Container calico-node ready: true, restart count 0
Aug 22 02:46:47.685: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 02:46:47.685: INFO: cattle-node-agent-x849w from cattle-system started at 2019-08-22 01:28:45 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.685: INFO: 	Container agent ready: true, restart count 0
Aug 22 02:46:47.685: INFO: nginx-ingress-controller-wsr46 from ingress-nginx started at 2019-08-22 01:29:06 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.685: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 22 02:46:47.685: INFO: sonobuoy-e2e-job-58e145a7c72f46b3 from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:46:47.685: INFO: 	Container e2e ready: true, restart count 0
Aug 22 02:46:47.685: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 22 02:46:47.685: INFO: 
Logging pods the kubelet thinks is on node melsayed-conformance3 before test
Aug 22 02:46:47.695: INFO: cattle-node-agent-r7lmz from cattle-system started at 2019-08-22 01:28:38 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.695: INFO: 	Container agent ready: true, restart count 0
Aug 22 02:46:47.695: INFO: canal-kcl25 from kube-system started at 2019-08-22 01:28:38 +0000 UTC (2 container statuses recorded)
Aug 22 02:46:47.695: INFO: 	Container calico-node ready: true, restart count 0
Aug 22 02:46:47.695: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 22 02:46:47.695: INFO: nginx-ingress-controller-h65vt from ingress-nginx started at 2019-08-22 01:28:58 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.695: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 22 02:46:47.695: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-22 01:35:54 +0000 UTC (1 container statuses recorded)
Aug 22 02:46:47.695: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 22 02:46:47.695: INFO: sonobuoy-systemd-logs-daemon-set-cf369fd0dc06424b-p5mld from heptio-sonobuoy started at 2019-08-22 01:35:58 +0000 UTC (2 container statuses recorded)
Aug 22 02:46:47.695: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 22 02:46:47.695: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15bd1eb4e689d732], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:46:48.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2222" for this suite.
Aug 22 02:46:54.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:46:54.834: INFO: namespace sched-pred-2222 deletion completed in 6.110516591s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.210 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:46:54.838: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:47:00.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9937" for this suite.
Aug 22 02:47:06.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:47:06.545: INFO: namespace watch-9937 deletion completed in 6.200174573s

• [SLOW TEST:11.708 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:47:06.545: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 22 02:47:09.662: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:47:10.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3144" for this suite.
Aug 22 02:47:40.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:47:40.791: INFO: namespace replicaset-3144 deletion completed in 30.109214773s

• [SLOW TEST:34.246 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:47:40.796: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 22 02:47:40.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-4756'
Aug 22 02:47:41.289: INFO: stderr: ""
Aug 22 02:47:41.289: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug 22 02:47:46.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pod e2e-test-nginx-pod --namespace=kubectl-4756 -o json'
Aug 22 02:47:46.432: INFO: stderr: ""
Aug 22 02:47:46.432: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.42.1.183/32\"\n        },\n        \"creationTimestamp\": \"2019-08-22T02:47:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-4756\",\n        \"resourceVersion\": \"19469\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4756/pods/e2e-test-nginx-pod\",\n        \"uid\": \"b5b0f798-6155-4a9d-980a-27191e42ff2e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-fnbf2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"melsayed-conformance3\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-fnbf2\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-fnbf2\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-22T02:47:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-22T02:47:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-22T02:47:43Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-22T02:47:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://1a0a445004b4f2733d9f796199424d920eb400c31aa642e3ca372836926531a5\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-22T02:47:42Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"165.22.32.172\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.1.183\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-22T02:47:41Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 22 02:47:46.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 replace -f - --namespace=kubectl-4756'
Aug 22 02:47:46.644: INFO: stderr: ""
Aug 22 02:47:46.644: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Aug 22 02:47:46.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete pods e2e-test-nginx-pod --namespace=kubectl-4756'
Aug 22 02:47:48.610: INFO: stderr: ""
Aug 22 02:47:48.610: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:47:48.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4756" for this suite.
Aug 22 02:47:54.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:47:54.720: INFO: namespace kubectl-4756 deletion completed in 6.106436705s

• [SLOW TEST:13.924 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:47:54.721: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-6bkp
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 02:47:54.771: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6bkp" in namespace "subpath-7695" to be "success or failure"
Aug 22 02:47:54.784: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Pending", Reason="", readiness=false. Elapsed: 13.152209ms
Aug 22 02:47:56.787: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.016724514s
Aug 22 02:47:58.792: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 4.02097934s
Aug 22 02:48:00.796: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 6.025289857s
Aug 22 02:48:02.800: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 8.029016784s
Aug 22 02:48:04.805: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 10.034549785s
Aug 22 02:48:06.809: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 12.038749923s
Aug 22 02:48:08.815: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 14.043979534s
Aug 22 02:48:10.818: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 16.047739296s
Aug 22 02:48:12.822: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 18.051819839s
Aug 22 02:48:14.827: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 20.056832739s
Aug 22 02:48:16.831: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Running", Reason="", readiness=true. Elapsed: 22.060675796s
Aug 22 02:48:18.835: INFO: Pod "pod-subpath-test-configmap-6bkp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.064215634s
STEP: Saw pod success
Aug 22 02:48:18.835: INFO: Pod "pod-subpath-test-configmap-6bkp" satisfied condition "success or failure"
Aug 22 02:48:18.842: INFO: Trying to get logs from node melsayed-conformance3 pod pod-subpath-test-configmap-6bkp container test-container-subpath-configmap-6bkp: <nil>
STEP: delete the pod
Aug 22 02:48:18.869: INFO: Waiting for pod pod-subpath-test-configmap-6bkp to disappear
Aug 22 02:48:18.871: INFO: Pod pod-subpath-test-configmap-6bkp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6bkp
Aug 22 02:48:18.871: INFO: Deleting pod "pod-subpath-test-configmap-6bkp" in namespace "subpath-7695"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:48:18.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7695" for this suite.
Aug 22 02:48:24.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:48:25.018: INFO: namespace subpath-7695 deletion completed in 6.140434722s

• [SLOW TEST:30.297 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:48:25.023: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 22 02:48:25.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7925'
Aug 22 02:48:25.189: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 22 02:48:25.189: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Aug 22 02:48:27.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7925'
Aug 22 02:48:27.301: INFO: stderr: ""
Aug 22 02:48:27.301: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:48:27.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7925" for this suite.
Aug 22 02:48:49.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:48:49.412: INFO: namespace kubectl-7925 deletion completed in 22.107519129s

• [SLOW TEST:24.389 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:48:49.416: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-982/secret-test-31429633-deab-4ea4-bf7b-8dd3184b91a8
STEP: Creating a pod to test consume secrets
Aug 22 02:48:49.465: INFO: Waiting up to 5m0s for pod "pod-configmaps-2666679d-2914-432d-afd1-f0486082bd96" in namespace "secrets-982" to be "success or failure"
Aug 22 02:48:49.473: INFO: Pod "pod-configmaps-2666679d-2914-432d-afd1-f0486082bd96": Phase="Pending", Reason="", readiness=false. Elapsed: 7.300471ms
Aug 22 02:48:51.477: INFO: Pod "pod-configmaps-2666679d-2914-432d-afd1-f0486082bd96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01172811s
STEP: Saw pod success
Aug 22 02:48:51.478: INFO: Pod "pod-configmaps-2666679d-2914-432d-afd1-f0486082bd96" satisfied condition "success or failure"
Aug 22 02:48:51.481: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-2666679d-2914-432d-afd1-f0486082bd96 container env-test: <nil>
STEP: delete the pod
Aug 22 02:48:51.505: INFO: Waiting for pod pod-configmaps-2666679d-2914-432d-afd1-f0486082bd96 to disappear
Aug 22 02:48:51.507: INFO: Pod pod-configmaps-2666679d-2914-432d-afd1-f0486082bd96 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:48:51.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-982" for this suite.
Aug 22 02:48:57.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:48:57.607: INFO: namespace secrets-982 deletion completed in 6.096545969s

• [SLOW TEST:8.191 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:48:57.608: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 22 02:48:57.653: INFO: Waiting up to 5m0s for pod "pod-28eb4b3c-b160-4885-b738-1c8706000fcc" in namespace "emptydir-6916" to be "success or failure"
Aug 22 02:48:57.667: INFO: Pod "pod-28eb4b3c-b160-4885-b738-1c8706000fcc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.335127ms
Aug 22 02:48:59.677: INFO: Pod "pod-28eb4b3c-b160-4885-b738-1c8706000fcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024507387s
STEP: Saw pod success
Aug 22 02:48:59.677: INFO: Pod "pod-28eb4b3c-b160-4885-b738-1c8706000fcc" satisfied condition "success or failure"
Aug 22 02:48:59.683: INFO: Trying to get logs from node melsayed-conformance3 pod pod-28eb4b3c-b160-4885-b738-1c8706000fcc container test-container: <nil>
STEP: delete the pod
Aug 22 02:48:59.709: INFO: Waiting for pod pod-28eb4b3c-b160-4885-b738-1c8706000fcc to disappear
Aug 22 02:48:59.714: INFO: Pod pod-28eb4b3c-b160-4885-b738-1c8706000fcc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:48:59.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6916" for this suite.
Aug 22 02:49:05.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:49:05.850: INFO: namespace emptydir-6916 deletion completed in 6.132797169s

• [SLOW TEST:8.243 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:49:05.850: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-68221342-3e44-4cc7-a65e-61c682b01fb1
STEP: Creating a pod to test consume configMaps
Aug 22 02:49:05.962: INFO: Waiting up to 5m0s for pod "pod-configmaps-484c7aec-c588-46c7-856a-f1a069f4b2cc" in namespace "configmap-1985" to be "success or failure"
Aug 22 02:49:05.981: INFO: Pod "pod-configmaps-484c7aec-c588-46c7-856a-f1a069f4b2cc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.850402ms
Aug 22 02:49:07.985: INFO: Pod "pod-configmaps-484c7aec-c588-46c7-856a-f1a069f4b2cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022246062s
STEP: Saw pod success
Aug 22 02:49:07.985: INFO: Pod "pod-configmaps-484c7aec-c588-46c7-856a-f1a069f4b2cc" satisfied condition "success or failure"
Aug 22 02:49:07.988: INFO: Trying to get logs from node melsayed-conformance3 pod pod-configmaps-484c7aec-c588-46c7-856a-f1a069f4b2cc container configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 02:49:08.011: INFO: Waiting for pod pod-configmaps-484c7aec-c588-46c7-856a-f1a069f4b2cc to disappear
Aug 22 02:49:08.014: INFO: Pod pod-configmaps-484c7aec-c588-46c7-856a-f1a069f4b2cc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:49:08.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1985" for this suite.
Aug 22 02:49:14.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:49:14.146: INFO: namespace configmap-1985 deletion completed in 6.127523058s

• [SLOW TEST:8.296 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:49:14.147: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:49:16.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6492" for this suite.
Aug 22 02:49:56.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:49:56.369: INFO: namespace kubelet-test-6492 deletion completed in 40.152390534s

• [SLOW TEST:42.223 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:49:56.375: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:49:56.427: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 22 02:49:56.441: INFO: Number of nodes with available pods: 0
Aug 22 02:49:56.441: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:49:57.448: INFO: Number of nodes with available pods: 0
Aug 22 02:49:57.448: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:49:58.452: INFO: Number of nodes with available pods: 1
Aug 22 02:49:58.452: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:49:59.450: INFO: Number of nodes with available pods: 2
Aug 22 02:49:59.450: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:00.449: INFO: Number of nodes with available pods: 2
Aug 22 02:50:00.449: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:01.452: INFO: Number of nodes with available pods: 2
Aug 22 02:50:01.452: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:02.448: INFO: Number of nodes with available pods: 2
Aug 22 02:50:02.448: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:03.448: INFO: Number of nodes with available pods: 2
Aug 22 02:50:03.448: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:04.449: INFO: Number of nodes with available pods: 2
Aug 22 02:50:04.450: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:05.449: INFO: Number of nodes with available pods: 2
Aug 22 02:50:05.449: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:06.452: INFO: Number of nodes with available pods: 2
Aug 22 02:50:06.452: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:07.450: INFO: Number of nodes with available pods: 2
Aug 22 02:50:07.450: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:08.449: INFO: Number of nodes with available pods: 2
Aug 22 02:50:08.449: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:09.450: INFO: Number of nodes with available pods: 2
Aug 22 02:50:09.450: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:10.448: INFO: Number of nodes with available pods: 2
Aug 22 02:50:10.448: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:11.452: INFO: Number of nodes with available pods: 2
Aug 22 02:50:11.452: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:12.449: INFO: Number of nodes with available pods: 2
Aug 22 02:50:12.449: INFO: Node melsayed-conformance2 is running more than one daemon pod
Aug 22 02:50:13.670: INFO: Number of nodes with available pods: 3
Aug 22 02:50:13.670: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 22 02:50:13.847: INFO: Wrong image for pod: daemon-set-gjmxx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:13.847: INFO: Wrong image for pod: daemon-set-gxw9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:13.847: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:14.876: INFO: Wrong image for pod: daemon-set-gjmxx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:14.876: INFO: Wrong image for pod: daemon-set-gxw9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:14.876: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:15.873: INFO: Wrong image for pod: daemon-set-gjmxx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:15.873: INFO: Pod daemon-set-gjmxx is not available
Aug 22 02:50:15.873: INFO: Wrong image for pod: daemon-set-gxw9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:15.873: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:16.873: INFO: Wrong image for pod: daemon-set-gxw9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:16.873: INFO: Pod daemon-set-x6s78 is not available
Aug 22 02:50:16.873: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:17.872: INFO: Wrong image for pod: daemon-set-gxw9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:17.873: INFO: Pod daemon-set-x6s78 is not available
Aug 22 02:50:17.873: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:18.872: INFO: Wrong image for pod: daemon-set-gxw9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:18.872: INFO: Pod daemon-set-x6s78 is not available
Aug 22 02:50:18.872: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:19.875: INFO: Wrong image for pod: daemon-set-gxw9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:19.875: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:20.873: INFO: Wrong image for pod: daemon-set-gxw9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:20.873: INFO: Pod daemon-set-gxw9w is not available
Aug 22 02:50:20.873: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:21.873: INFO: Pod daemon-set-mfkdh is not available
Aug 22 02:50:21.873: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:22.872: INFO: Pod daemon-set-mfkdh is not available
Aug 22 02:50:22.872: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:23.873: INFO: Pod daemon-set-mfkdh is not available
Aug 22 02:50:23.874: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:24.875: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:25.874: INFO: Wrong image for pod: daemon-set-zr8bn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 22 02:50:25.874: INFO: Pod daemon-set-zr8bn is not available
Aug 22 02:50:26.874: INFO: Pod daemon-set-6zlfx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 22 02:50:26.889: INFO: Number of nodes with available pods: 2
Aug 22 02:50:26.889: INFO: Node melsayed-conformance3 is running more than one daemon pod
Aug 22 02:50:27.898: INFO: Number of nodes with available pods: 2
Aug 22 02:50:27.899: INFO: Node melsayed-conformance3 is running more than one daemon pod
Aug 22 02:50:28.899: INFO: Number of nodes with available pods: 3
Aug 22 02:50:28.899: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4982, will wait for the garbage collector to delete the pods
Aug 22 02:50:28.975: INFO: Deleting DaemonSet.extensions daemon-set took: 9.956356ms
Aug 22 02:50:29.475: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.234135ms
Aug 22 02:50:36.280: INFO: Number of nodes with available pods: 0
Aug 22 02:50:36.280: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 02:50:36.283: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4982/daemonsets","resourceVersion":"20092"},"items":null}

Aug 22 02:50:36.285: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4982/pods","resourceVersion":"20092"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:50:36.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4982" for this suite.
Aug 22 02:50:42.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:50:42.403: INFO: namespace daemonsets-4982 deletion completed in 6.101284294s

• [SLOW TEST:46.029 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:50:42.404: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 22 02:50:42.435: INFO: Waiting up to 5m0s for pod "pod-d45b2561-399e-44fa-b7ba-2f3d794cc5a1" in namespace "emptydir-8264" to be "success or failure"
Aug 22 02:50:42.440: INFO: Pod "pod-d45b2561-399e-44fa-b7ba-2f3d794cc5a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.364697ms
Aug 22 02:50:44.444: INFO: Pod "pod-d45b2561-399e-44fa-b7ba-2f3d794cc5a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007915299s
STEP: Saw pod success
Aug 22 02:50:44.444: INFO: Pod "pod-d45b2561-399e-44fa-b7ba-2f3d794cc5a1" satisfied condition "success or failure"
Aug 22 02:50:44.447: INFO: Trying to get logs from node melsayed-conformance3 pod pod-d45b2561-399e-44fa-b7ba-2f3d794cc5a1 container test-container: <nil>
STEP: delete the pod
Aug 22 02:50:44.598: INFO: Waiting for pod pod-d45b2561-399e-44fa-b7ba-2f3d794cc5a1 to disappear
Aug 22 02:50:44.605: INFO: Pod pod-d45b2561-399e-44fa-b7ba-2f3d794cc5a1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:50:44.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8264" for this suite.
Aug 22 02:50:50.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:50:50.717: INFO: namespace emptydir-8264 deletion completed in 6.10777319s

• [SLOW TEST:8.313 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:50:50.718: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 22 02:50:50.807: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 22 02:50:55.812: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:50:56.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9827" for this suite.
Aug 22 02:51:02.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:51:02.936: INFO: namespace replication-controller-9827 deletion completed in 6.099778493s

• [SLOW TEST:12.219 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:51:02.938: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 22 02:51:03.133: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-a,UID:81c6abaa-3040-44c7-99ee-be7660d96c9a,ResourceVersion:20271,Generation:0,CreationTimestamp:2019-08-22 02:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 22 02:51:03.133: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-a,UID:81c6abaa-3040-44c7-99ee-be7660d96c9a,ResourceVersion:20271,Generation:0,CreationTimestamp:2019-08-22 02:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 22 02:51:13.141: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-a,UID:81c6abaa-3040-44c7-99ee-be7660d96c9a,ResourceVersion:20290,Generation:0,CreationTimestamp:2019-08-22 02:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 22 02:51:13.142: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-a,UID:81c6abaa-3040-44c7-99ee-be7660d96c9a,ResourceVersion:20290,Generation:0,CreationTimestamp:2019-08-22 02:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 22 02:51:23.151: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-a,UID:81c6abaa-3040-44c7-99ee-be7660d96c9a,ResourceVersion:20306,Generation:0,CreationTimestamp:2019-08-22 02:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 22 02:51:23.151: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-a,UID:81c6abaa-3040-44c7-99ee-be7660d96c9a,ResourceVersion:20306,Generation:0,CreationTimestamp:2019-08-22 02:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 22 02:51:33.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-a,UID:81c6abaa-3040-44c7-99ee-be7660d96c9a,ResourceVersion:20323,Generation:0,CreationTimestamp:2019-08-22 02:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 22 02:51:33.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-a,UID:81c6abaa-3040-44c7-99ee-be7660d96c9a,ResourceVersion:20323,Generation:0,CreationTimestamp:2019-08-22 02:51:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 22 02:51:43.167: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-b,UID:9b5abb32-756a-462c-8883-72d23fd6d4ca,ResourceVersion:20340,Generation:0,CreationTimestamp:2019-08-22 02:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 22 02:51:43.167: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-b,UID:9b5abb32-756a-462c-8883-72d23fd6d4ca,ResourceVersion:20340,Generation:0,CreationTimestamp:2019-08-22 02:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 22 02:51:53.176: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-b,UID:9b5abb32-756a-462c-8883-72d23fd6d4ca,ResourceVersion:20358,Generation:0,CreationTimestamp:2019-08-22 02:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 22 02:51:53.176: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7710,SelfLink:/api/v1/namespaces/watch-7710/configmaps/e2e-watch-test-configmap-b,UID:9b5abb32-756a-462c-8883-72d23fd6d4ca,ResourceVersion:20358,Generation:0,CreationTimestamp:2019-08-22 02:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:52:03.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7710" for this suite.
Aug 22 02:52:09.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:52:09.271: INFO: namespace watch-7710 deletion completed in 6.089809581s

• [SLOW TEST:66.333 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:52:09.272: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Aug 22 02:52:09.912: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:52:09.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0822 02:52:09.911993      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7250" for this suite.
Aug 22 02:52:15.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:52:16.021: INFO: namespace gc-7250 deletion completed in 6.105453944s

• [SLOW TEST:6.749 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:52:16.025: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-7168ce13-fb27-41fe-9682-c231246b74f7 in namespace container-probe-1164
Aug 22 02:52:20.123: INFO: Started pod liveness-7168ce13-fb27-41fe-9682-c231246b74f7 in namespace container-probe-1164
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 02:52:20.126: INFO: Initial restart count of pod liveness-7168ce13-fb27-41fe-9682-c231246b74f7 is 0
Aug 22 02:52:42.196: INFO: Restart count of pod container-probe-1164/liveness-7168ce13-fb27-41fe-9682-c231246b74f7 is now 1 (22.069794668s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:52:42.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1164" for this suite.
Aug 22 02:52:48.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:52:48.366: INFO: namespace container-probe-1164 deletion completed in 6.153640557s

• [SLOW TEST:32.341 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:52:48.366: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-0b4dceb5-beae-44f5-85d9-07b1db72fa67
STEP: Creating secret with name s-test-opt-upd-b44aa50e-f08e-42b3-835c-e3d1d342529d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0b4dceb5-beae-44f5-85d9-07b1db72fa67
STEP: Updating secret s-test-opt-upd-b44aa50e-f08e-42b3-835c-e3d1d342529d
STEP: Creating secret with name s-test-opt-create-ac91b446-d8ac-4912-a77e-20b947d4e077
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:54:06.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4707" for this suite.
Aug 22 02:54:29.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:54:29.094: INFO: namespace secrets-4707 deletion completed in 22.090903112s

• [SLOW TEST:100.728 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:54:29.096: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 22 02:54:29.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7535'
Aug 22 02:54:29.265: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 22 02:54:29.265: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Aug 22 02:54:29.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete jobs e2e-test-nginx-job --namespace=kubectl-7535'
Aug 22 02:54:29.389: INFO: stderr: ""
Aug 22 02:54:29.389: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:54:29.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7535" for this suite.
Aug 22 02:54:35.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:54:35.499: INFO: namespace kubectl-7535 deletion completed in 6.102537773s

• [SLOW TEST:6.403 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:54:35.499: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:54:35.558: INFO: Create a RollingUpdate DaemonSet
Aug 22 02:54:35.562: INFO: Check that daemon pods launch on every node of the cluster
Aug 22 02:54:35.570: INFO: Number of nodes with available pods: 0
Aug 22 02:54:35.570: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:54:36.582: INFO: Number of nodes with available pods: 0
Aug 22 02:54:36.582: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:54:37.601: INFO: Number of nodes with available pods: 1
Aug 22 02:54:37.601: INFO: Node melsayed-conformance1 is running more than one daemon pod
Aug 22 02:54:38.588: INFO: Number of nodes with available pods: 3
Aug 22 02:54:38.589: INFO: Number of running nodes: 3, number of available pods: 3
Aug 22 02:54:38.589: INFO: Update the DaemonSet to trigger a rollout
Aug 22 02:54:38.600: INFO: Updating DaemonSet daemon-set
Aug 22 02:54:48.621: INFO: Roll back the DaemonSet before rollout is complete
Aug 22 02:54:48.626: INFO: Updating DaemonSet daemon-set
Aug 22 02:54:48.626: INFO: Make sure DaemonSet rollback is complete
Aug 22 02:54:48.629: INFO: Wrong image for pod: daemon-set-mc8h4. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 22 02:54:48.629: INFO: Pod daemon-set-mc8h4 is not available
Aug 22 02:54:49.638: INFO: Wrong image for pod: daemon-set-mc8h4. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 22 02:54:49.638: INFO: Pod daemon-set-mc8h4 is not available
Aug 22 02:54:50.636: INFO: Wrong image for pod: daemon-set-mc8h4. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 22 02:54:50.637: INFO: Pod daemon-set-mc8h4 is not available
Aug 22 02:54:51.639: INFO: Pod daemon-set-wtztl is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4378, will wait for the garbage collector to delete the pods
Aug 22 02:54:51.712: INFO: Deleting DaemonSet.extensions daemon-set took: 9.311991ms
Aug 22 02:54:52.212: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.255235ms
Aug 22 02:56:18.316: INFO: Number of nodes with available pods: 0
Aug 22 02:56:18.316: INFO: Number of running nodes: 0, number of available pods: 0
Aug 22 02:56:18.319: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4378/daemonsets","resourceVersion":"21048"},"items":null}

Aug 22 02:56:18.321: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4378/pods","resourceVersion":"21048"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:56:18.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4378" for this suite.
Aug 22 02:56:24.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:56:24.460: INFO: namespace daemonsets-4378 deletion completed in 6.124110935s

• [SLOW TEST:108.962 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:56:24.461: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:56:26.201: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 22 02:56:31.207: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 22 02:56:31.207: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 22 02:56:33.212: INFO: Creating deployment "test-rollover-deployment"
Aug 22 02:56:33.229: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 22 02:56:35.243: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 22 02:56:35.254: INFO: Ensure that both replica sets have 1 created replica
Aug 22 02:56:35.259: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 22 02:56:35.267: INFO: Updating deployment test-rollover-deployment
Aug 22 02:56:35.267: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 22 02:56:39.231: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 22 02:56:39.255: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 22 02:56:39.261: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 02:56:39.261: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039395, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 02:56:41.270: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 02:56:41.270: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039395, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 02:56:43.272: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 02:56:43.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039395, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 02:56:45.270: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 02:56:45.270: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039403, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 02:56:47.267: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 02:56:47.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039403, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 02:56:49.268: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 02:56:49.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039403, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 02:56:51.274: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 02:56:51.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039403, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 02:56:53.270: INFO: all replica sets need to contain the pod-template-hash label
Aug 22 02:56:53.270: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039403, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039393, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 22 02:56:55.270: INFO: 
Aug 22 02:56:55.270: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 22 02:56:55.495: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-461,SelfLink:/apis/apps/v1/namespaces/deployment-461/deployments/test-rollover-deployment,UID:85ae2917-500a-4381-94cf-0213ee3ce4be,ResourceVersion:21224,Generation:2,CreationTimestamp:2019-08-22 02:56:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-22 02:56:33 +0000 UTC 2019-08-22 02:56:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-22 02:56:54 +0000 UTC 2019-08-22 02:56:33 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 22 02:56:55.498: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-461,SelfLink:/apis/apps/v1/namespaces/deployment-461/replicasets/test-rollover-deployment-854595fc44,UID:4bc182ff-39e1-476e-be6d-7728747654d3,ResourceVersion:21211,Generation:2,CreationTimestamp:2019-08-22 02:56:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 85ae2917-500a-4381-94cf-0213ee3ce4be 0xc002c0cf07 0xc002c0cf08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 22 02:56:55.499: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 22 02:56:55.499: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-461,SelfLink:/apis/apps/v1/namespaces/deployment-461/replicasets/test-rollover-controller,UID:57953016-7385-4d27-950f-7bdc77a2a53b,ResourceVersion:21223,Generation:2,CreationTimestamp:2019-08-22 02:56:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 85ae2917-500a-4381-94cf-0213ee3ce4be 0xc002c0ce27 0xc002c0ce28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 22 02:56:55.499: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-461,SelfLink:/apis/apps/v1/namespaces/deployment-461/replicasets/test-rollover-deployment-9b8b997cf,UID:55dd3bdc-1c56-4194-b195-7d3ef5b2e590,ResourceVersion:21168,Generation:2,CreationTimestamp:2019-08-22 02:56:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 85ae2917-500a-4381-94cf-0213ee3ce4be 0xc002c0cff0 0xc002c0cff1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 22 02:56:55.503: INFO: Pod "test-rollover-deployment-854595fc44-tkhxc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-tkhxc,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-461,SelfLink:/api/v1/namespaces/deployment-461/pods/test-rollover-deployment-854595fc44-tkhxc,UID:f2371a2c-1067-4388-a2bd-ab09048967bd,ResourceVersion:21192,Generation:0,CreationTimestamp:2019-08-22 02:56:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.1.201/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 4bc182ff-39e1-476e-be6d-7728747654d3 0xc002c0dbe7 0xc002c0dbe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vbp7p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vbp7p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-vbp7p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c0dc50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c0dc70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:56:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:56:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:56:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:56:35 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:10.42.1.201,StartTime:2019-08-22 02:56:35 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-22 02:56:42 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://4a3e68d12b565f2794e1f3d1b851ee13ec516b33ee349fcef472d7c2aa0aa898}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:56:55.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-461" for this suite.
Aug 22 02:57:01.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:57:01.643: INFO: namespace deployment-461 deletion completed in 6.13620733s

• [SLOW TEST:37.182 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:57:01.644: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Aug 22 02:57:01.695: INFO: Waiting up to 5m0s for pod "client-containers-d18b140d-01db-430f-b43c-7ff16922dcb5" in namespace "containers-3006" to be "success or failure"
Aug 22 02:57:01.704: INFO: Pod "client-containers-d18b140d-01db-430f-b43c-7ff16922dcb5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.772987ms
Aug 22 02:57:03.709: INFO: Pod "client-containers-d18b140d-01db-430f-b43c-7ff16922dcb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013433663s
STEP: Saw pod success
Aug 22 02:57:03.709: INFO: Pod "client-containers-d18b140d-01db-430f-b43c-7ff16922dcb5" satisfied condition "success or failure"
Aug 22 02:57:03.712: INFO: Trying to get logs from node melsayed-conformance3 pod client-containers-d18b140d-01db-430f-b43c-7ff16922dcb5 container test-container: <nil>
STEP: delete the pod
Aug 22 02:57:03.740: INFO: Waiting for pod client-containers-d18b140d-01db-430f-b43c-7ff16922dcb5 to disappear
Aug 22 02:57:03.743: INFO: Pod client-containers-d18b140d-01db-430f-b43c-7ff16922dcb5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:57:03.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3006" for this suite.
Aug 22 02:57:09.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:57:09.855: INFO: namespace containers-3006 deletion completed in 6.105694623s

• [SLOW TEST:8.212 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:57:09.856: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 02:57:10.192: INFO: Creating deployment "nginx-deployment"
Aug 22 02:57:10.196: INFO: Waiting for observed generation 1
Aug 22 02:57:12.204: INFO: Waiting for all required pods to come up
Aug 22 02:57:12.208: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 22 02:57:14.222: INFO: Waiting for deployment "nginx-deployment" to complete
Aug 22 02:57:14.228: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug 22 02:57:14.239: INFO: Updating deployment nginx-deployment
Aug 22 02:57:14.239: INFO: Waiting for observed generation 2
Aug 22 02:57:16.245: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 22 02:57:16.248: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 22 02:57:16.251: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 22 02:57:16.259: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 22 02:57:16.259: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 22 02:57:16.261: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 22 02:57:16.266: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug 22 02:57:16.266: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug 22 02:57:16.272: INFO: Updating deployment nginx-deployment
Aug 22 02:57:16.272: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug 22 02:57:16.280: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 22 02:57:18.296: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 22 02:57:18.309: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7303,SelfLink:/apis/apps/v1/namespaces/deployment-7303/deployments/nginx-deployment,UID:3f824e3e-0d8f-4368-a5db-0ef594afb503,ResourceVersion:21623,Generation:3,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-08-22 02:57:16 +0000 UTC 2019-08-22 02:57:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-22 02:57:16 +0000 UTC 2019-08-22 02:57:10 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug 22 02:57:18.319: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-7303,SelfLink:/apis/apps/v1/namespaces/deployment-7303/replicasets/nginx-deployment-55fb7cb77f,UID:4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6,ResourceVersion:21620,Generation:3,CreationTimestamp:2019-08-22 02:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 3f824e3e-0d8f-4368-a5db-0ef594afb503 0xc001ed8217 0xc001ed8218}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 22 02:57:18.319: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug 22 02:57:18.319: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-7303,SelfLink:/apis/apps/v1/namespaces/deployment-7303/replicasets/nginx-deployment-7b8c6f4498,UID:089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a,ResourceVersion:21622,Generation:3,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 3f824e3e-0d8f-4368-a5db-0ef594afb503 0xc001ed82e7 0xc001ed82e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug 22 02:57:18.328: INFO: Pod "nginx-deployment-55fb7cb77f-26spb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-26spb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-26spb,UID:200b2bfa-c9f4-4b6a-9489-7812e4e4c1ef,ResourceVersion:21580,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c4ae7 0xc0029c4ae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c4b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c4b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.328: INFO: Pod "nginx-deployment-55fb7cb77f-2jvv6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2jvv6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-2jvv6,UID:1097076b-31f3-46a6-8dd2-af044b8b8555,ResourceVersion:21509,Generation:0,CreationTimestamp:2019-08-22 02:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c4bf0 0xc0029c4bf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c4c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c4c80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:,StartTime:2019-08-22 02:57:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.329: INFO: Pod "nginx-deployment-55fb7cb77f-4ftqq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4ftqq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-4ftqq,UID:fccaa6c9-bdff-440d-9994-2366e2f11ae1,ResourceVersion:21582,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c4d50 0xc0029c4d51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c4dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c4de0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:,StartTime:2019-08-22 02:57:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.329: INFO: Pod "nginx-deployment-55fb7cb77f-5hzqh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5hzqh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-5hzqh,UID:9992cf58-223b-43cd-b1da-1759ca254249,ResourceVersion:21608,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c4eb0 0xc0029c4eb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c4f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c4f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:,StartTime:2019-08-22 02:57:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.329: INFO: Pod "nginx-deployment-55fb7cb77f-jhcz8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jhcz8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-jhcz8,UID:25a055e5-dfc9-4fc4-bd33-88aa52627833,ResourceVersion:21571,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c5010 0xc0029c5011}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c5080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c50a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.329: INFO: Pod "nginx-deployment-55fb7cb77f-kv94b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kv94b,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-kv94b,UID:f4865743-8a21-4e6b-9bf4-045213a26580,ResourceVersion:21626,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c5120 0xc0029c5121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c5190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c51b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:,StartTime:2019-08-22 02:57:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.330: INFO: Pod "nginx-deployment-55fb7cb77f-l2p9h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-l2p9h,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-l2p9h,UID:88895e77-a8e4-448f-a212-cde20f2bad10,ResourceVersion:21645,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c5280 0xc0029c5281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c52f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c5310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:,StartTime:2019-08-22 02:57:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.330: INFO: Pod "nginx-deployment-55fb7cb77f-l7tvw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-l7tvw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-l7tvw,UID:5424ed80-a693-49aa-a03a-d97c3b87f0a6,ResourceVersion:21493,Generation:0,CreationTimestamp:2019-08-22 02:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c53e0 0xc0029c53e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c5450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c5470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:,StartTime:2019-08-22 02:57:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.330: INFO: Pod "nginx-deployment-55fb7cb77f-mkl2q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mkl2q,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-mkl2q,UID:55e78f0e-b96a-40d4-b3ad-f10aa21a2535,ResourceVersion:21584,Generation:0,CreationTimestamp:2019-08-22 02:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.2.51/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c5550 0xc0029c5551}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c55c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c55e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  }],Message:,Reason:,HostIP:167.71.246.48,PodIP:10.42.2.51,StartTime:2019-08-22 02:57:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.330: INFO: Pod "nginx-deployment-55fb7cb77f-n249t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-n249t,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-n249t,UID:e72f9d95-6d60-4ad5-9607-2e601b7ee52f,ResourceVersion:21573,Generation:0,CreationTimestamp:2019-08-22 02:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.2.50/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c56e0 0xc0029c56e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c5750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c5770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  }],Message:,Reason:,HostIP:167.71.246.48,PodIP:10.42.2.50,StartTime:2019-08-22 02:57:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.331: INFO: Pod "nginx-deployment-55fb7cb77f-v6xnc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-v6xnc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-v6xnc,UID:f3cf9b62-94a4-4ebb-b018-f8521552e134,ResourceVersion:21643,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c5860 0xc0029c5861}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c58d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c58f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:,StartTime:2019-08-22 02:57:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.331: INFO: Pod "nginx-deployment-55fb7cb77f-vhrtv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-vhrtv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-vhrtv,UID:76b34388-18a1-41bc-87d0-51fba62a54bd,ResourceVersion:21605,Generation:0,CreationTimestamp:2019-08-22 02:57:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.0.60/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c59e0 0xc0029c59e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c5a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c5a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:14 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:10.42.0.60,StartTime:2019-08-22 02:57:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.331: INFO: Pod "nginx-deployment-55fb7cb77f-xmk8z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xmk8z,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-55fb7cb77f-xmk8z,UID:61e5ff6d-eb63-4f81-933d-59143957652c,ResourceVersion:21601,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 4e11b6e0-3a8d-4a98-86b4-0ea7f01affb6 0xc0029c5b60 0xc0029c5b61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c5bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c5bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.332: INFO: Pod "nginx-deployment-7b8c6f4498-27s86" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-27s86,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-27s86,UID:419b830c-f396-42ed-a2cb-242f7f770ec6,ResourceVersion:21556,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc0029c5c70 0xc0029c5c71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c5cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c5cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.332: INFO: Pod "nginx-deployment-7b8c6f4498-2qhkl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2qhkl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-2qhkl,UID:7747daef-541f-4eaa-a7ae-e2fe76cb42e8,ResourceVersion:21547,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc0029c5d70 0xc0029c5d71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c5dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c5df0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.332: INFO: Pod "nginx-deployment-7b8c6f4498-6764f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6764f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-6764f,UID:dd7218c9-c3fc-46f4-91b7-e6bf4d93ef98,ResourceVersion:21456,Generation:0,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.1.203/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc0029c5ea0 0xc0029c5ea1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c5f00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c5f20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:10.42.1.203,StartTime:2019-08-22 02:57:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-22 02:57:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6d32bdb426e0ef0c6ff31d05e92d04f1788bfc2aa066b9c142aca2b82c8f9897}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.332: INFO: Pod "nginx-deployment-7b8c6f4498-6hc75" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6hc75,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-6hc75,UID:ff3241a2-dcc1-4f87-9479-03d9e596e00f,ResourceVersion:21642,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc0029c5ff0 0xc0029c5ff1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000682130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0006821b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:,StartTime:2019-08-22 02:57:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.333: INFO: Pod "nginx-deployment-7b8c6f4498-6thdw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6thdw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-6thdw,UID:7e85e806-e78e-4c43-b4a0-65e017993d37,ResourceVersion:21429,Generation:0,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.0.59/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000682477 0xc000682478}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000682660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0006826a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:10.42.0.59,StartTime:2019-08-22 02:57:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-22 02:57:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://26c3ab6327d3c9317ff36e75661686e6dbc8154dd2af93d3774b192f00598038}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.333: INFO: Pod "nginx-deployment-7b8c6f4498-7m4nx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7m4nx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-7m4nx,UID:eb119509-172f-4509-830a-c2bf8585bce6,ResourceVersion:21546,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000682d90 0xc000682d91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000682f30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000682f80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:167.71.246.48,PodIP:,StartTime:2019-08-22 02:57:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.333: INFO: Pod "nginx-deployment-7b8c6f4498-82fkb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-82fkb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-82fkb,UID:a38b377a-462b-49fc-b1e4-b3a6b2bf58e0,ResourceVersion:21644,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc0006832c7 0xc0006832c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0006834e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000683550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:,StartTime:2019-08-22 02:57:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.333: INFO: Pod "nginx-deployment-7b8c6f4498-97sft" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-97sft,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-97sft,UID:46f97719-6ab3-4b1a-a46c-1961450cf774,ResourceVersion:21647,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.0.61/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000683707 0xc000683708}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0006838c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000683920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:,StartTime:2019-08-22 02:57:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.334: INFO: Pod "nginx-deployment-7b8c6f4498-9kqhk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9kqhk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-9kqhk,UID:63ed31ef-ed80-44a2-b70d-fd827aac0dbd,ResourceVersion:21611,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000683cf7 0xc000683cf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000683f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000683fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:,StartTime:2019-08-22 02:57:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.334: INFO: Pod "nginx-deployment-7b8c6f4498-djcgr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-djcgr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-djcgr,UID:528636e8-d394-46e9-97a5-581c89716a4d,ResourceVersion:21439,Generation:0,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.2.48/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd40b7 0xc000dd40b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd4120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd4140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  }],Message:,Reason:,HostIP:167.71.246.48,PodIP:10.42.2.48,StartTime:2019-08-22 02:57:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-22 02:57:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6ccb330ae99a499538e95b74a34507f773812ae3f8700a9aae105f5912071856}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.334: INFO: Pod "nginx-deployment-7b8c6f4498-lt5lr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lt5lr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-lt5lr,UID:2f20a0e0-7748-49da-aa1c-ac2a502f5c84,ResourceVersion:21426,Generation:0,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.0.58/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd4220 0xc000dd4221}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd4290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd42b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:10.42.0.58,StartTime:2019-08-22 02:57:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-22 02:57:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0d0a80fc45ec1a987188b119b788b47243d0f5db55a44956d376fceb84ade908}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.334: INFO: Pod "nginx-deployment-7b8c6f4498-m6j5t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m6j5t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-m6j5t,UID:34f014a9-7e5f-4ff1-85e2-db50a12ee8ad,ResourceVersion:21596,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd4380 0xc000dd4381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd43e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd4400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:,StartTime:2019-08-22 02:57:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.334: INFO: Pod "nginx-deployment-7b8c6f4498-pwc2x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pwc2x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-pwc2x,UID:5c4904e3-5620-4336-a0fc-8a408cd6485a,ResourceVersion:21449,Generation:0,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.1.205/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd44d7 0xc000dd44d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd4550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd4570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:10.42.1.205,StartTime:2019-08-22 02:57:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-22 02:57:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ef915d3331dfd9e270ea65bbe0d195e0a2bfccaacdcbf79edcd22fb85d48fea1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.335: INFO: Pod "nginx-deployment-7b8c6f4498-qdznh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qdznh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-qdznh,UID:9e1e4da9-a844-434e-a5d8-e35c63f84a3a,ResourceVersion:21603,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd4640 0xc000dd4641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd46a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd46c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.335: INFO: Pod "nginx-deployment-7b8c6f4498-rd4wm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rd4wm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-rd4wm,UID:a1b12a3a-dd9e-4b9c-a138-af8244dd484a,ResourceVersion:21641,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd4740 0xc000dd4741}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd47a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd47c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:,StartTime:2019-08-22 02:57:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.335: INFO: Pod "nginx-deployment-7b8c6f4498-svp7b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-svp7b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-svp7b,UID:66485cca-4ee7-4dc5-913c-d4bcf110de5b,ResourceVersion:21423,Generation:0,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.0.57/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd4897 0xc000dd4898}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd4910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd4930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  }],Message:,Reason:,HostIP:167.71.96.182,PodIP:10.42.0.57,StartTime:2019-08-22 02:57:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-22 02:57:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5eb42407d35aa9c6bd66659bef73204c1a8c6be05c9fc07ef67f6af573f809ac}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.335: INFO: Pod "nginx-deployment-7b8c6f4498-vn842" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vn842,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-vn842,UID:9aac7705-386f-4896-91ba-f2de91cf04a4,ResourceVersion:21436,Generation:0,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.2.47/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd4a10 0xc000dd4a11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd4a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd4aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  }],Message:,Reason:,HostIP:167.71.246.48,PodIP:10.42.2.47,StartTime:2019-08-22 02:57:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-22 02:57:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b450e3ced0bee3cb74da58047b6c04807621e89f64f74219a8809a827007c86e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.336: INFO: Pod "nginx-deployment-7b8c6f4498-vrm92" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vrm92,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-vrm92,UID:c7210bf5-3450-4ce0-9f18-77d65a319459,ResourceVersion:21599,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd4b80 0xc000dd4b81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd4be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd4c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.336: INFO: Pod "nginx-deployment-7b8c6f4498-zbbhj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zbbhj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-zbbhj,UID:04368ad3-8310-4e93-9c51-72aef35ac814,ResourceVersion:21595,Generation:0,CreationTimestamp:2019-08-22 02:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd4c90 0xc000dd4c91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd4cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd4d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 22 02:57:18.336: INFO: Pod "nginx-deployment-7b8c6f4498-zxmr6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zxmr6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7303,SelfLink:/api/v1/namespaces/deployment-7303/pods/nginx-deployment-7b8c6f4498-zxmr6,UID:79d39dbc-df62-44e6-b7e2-b0f786fbc635,ResourceVersion:21453,Generation:0,CreationTimestamp:2019-08-22 02:57:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.1.206/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 089ee0be-5c2b-4ff4-8b8a-e1d2f92a526a 0xc000dd4da0 0xc000dd4da1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xblbf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xblbf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xblbf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd4e00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd4e20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 02:57:10 +0000 UTC  }],Message:,Reason:,HostIP:165.22.32.172,PodIP:10.42.1.206,StartTime:2019-08-22 02:57:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-22 02:57:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1eae62244fc3d3f528462e191a8d840acd194559b19b5ea65ee68cc16177ef8b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:57:18.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7303" for this suite.
Aug 22 02:57:26.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:57:26.638: INFO: namespace deployment-7303 deletion completed in 8.296235953s

• [SLOW TEST:16.782 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:57:26.639: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1087, will wait for the garbage collector to delete the pods
Aug 22 02:57:28.775: INFO: Deleting Job.batch foo took: 8.577978ms
Aug 22 02:57:28.876: INFO: Terminating Job.batch foo pods took: 100.318241ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:58:08.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1087" for this suite.
Aug 22 02:58:14.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:58:14.250: INFO: namespace job-1087 deletion completed in 6.167173863s

• [SLOW TEST:47.611 seconds]
[sig-apps] Job
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:58:14.253: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Aug 22 02:58:14.357: INFO: Waiting up to 5m0s for pod "var-expansion-16f3dbf7-64af-490c-b3fa-902bd818e03d" in namespace "var-expansion-9696" to be "success or failure"
Aug 22 02:58:14.360: INFO: Pod "var-expansion-16f3dbf7-64af-490c-b3fa-902bd818e03d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316532ms
Aug 22 02:58:16.365: INFO: Pod "var-expansion-16f3dbf7-64af-490c-b3fa-902bd818e03d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007705396s
STEP: Saw pod success
Aug 22 02:58:16.365: INFO: Pod "var-expansion-16f3dbf7-64af-490c-b3fa-902bd818e03d" satisfied condition "success or failure"
Aug 22 02:58:16.374: INFO: Trying to get logs from node melsayed-conformance3 pod var-expansion-16f3dbf7-64af-490c-b3fa-902bd818e03d container dapi-container: <nil>
STEP: delete the pod
Aug 22 02:58:16.412: INFO: Waiting for pod var-expansion-16f3dbf7-64af-490c-b3fa-902bd818e03d to disappear
Aug 22 02:58:16.417: INFO: Pod var-expansion-16f3dbf7-64af-490c-b3fa-902bd818e03d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:58:16.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9696" for this suite.
Aug 22 02:58:22.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:58:22.521: INFO: namespace var-expansion-9696 deletion completed in 6.101084021s

• [SLOW TEST:8.269 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:58:22.522: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 22 02:58:22.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-4625'
Aug 22 02:58:22.891: INFO: stderr: ""
Aug 22 02:58:22.891: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 02:58:22.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4625'
Aug 22 02:58:22.993: INFO: stderr: ""
Aug 22 02:58:22.993: INFO: stdout: "update-demo-nautilus-8jmq2 update-demo-nautilus-t5qrq "
Aug 22 02:58:22.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-8jmq2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:23.081: INFO: stderr: ""
Aug 22 02:58:23.081: INFO: stdout: ""
Aug 22 02:58:23.081: INFO: update-demo-nautilus-8jmq2 is created but not running
Aug 22 02:58:28.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4625'
Aug 22 02:58:28.169: INFO: stderr: ""
Aug 22 02:58:28.169: INFO: stdout: "update-demo-nautilus-8jmq2 update-demo-nautilus-t5qrq "
Aug 22 02:58:28.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-8jmq2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:28.252: INFO: stderr: ""
Aug 22 02:58:28.252: INFO: stdout: "true"
Aug 22 02:58:28.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-8jmq2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:28.341: INFO: stderr: ""
Aug 22 02:58:28.341: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 22 02:58:28.341: INFO: validating pod update-demo-nautilus-8jmq2
Aug 22 02:58:28.355: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 02:58:28.355: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 02:58:28.355: INFO: update-demo-nautilus-8jmq2 is verified up and running
Aug 22 02:58:28.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-t5qrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:28.438: INFO: stderr: ""
Aug 22 02:58:28.438: INFO: stdout: "true"
Aug 22 02:58:28.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-t5qrq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:28.522: INFO: stderr: ""
Aug 22 02:58:28.522: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 22 02:58:28.522: INFO: validating pod update-demo-nautilus-t5qrq
Aug 22 02:58:28.529: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 02:58:28.529: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 02:58:28.530: INFO: update-demo-nautilus-t5qrq is verified up and running
STEP: scaling down the replication controller
Aug 22 02:58:28.532: INFO: scanned /root for discovery docs: <nil>
Aug 22 02:58:28.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4625'
Aug 22 02:58:29.653: INFO: stderr: ""
Aug 22 02:58:29.653: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 02:58:29.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4625'
Aug 22 02:58:29.764: INFO: stderr: ""
Aug 22 02:58:29.764: INFO: stdout: "update-demo-nautilus-8jmq2 update-demo-nautilus-t5qrq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 22 02:58:34.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4625'
Aug 22 02:58:34.859: INFO: stderr: ""
Aug 22 02:58:34.859: INFO: stdout: "update-demo-nautilus-t5qrq "
Aug 22 02:58:34.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-t5qrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:34.962: INFO: stderr: ""
Aug 22 02:58:34.962: INFO: stdout: "true"
Aug 22 02:58:34.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-t5qrq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:35.047: INFO: stderr: ""
Aug 22 02:58:35.047: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 22 02:58:35.047: INFO: validating pod update-demo-nautilus-t5qrq
Aug 22 02:58:35.052: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 02:58:35.052: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 02:58:35.052: INFO: update-demo-nautilus-t5qrq is verified up and running
STEP: scaling up the replication controller
Aug 22 02:58:35.054: INFO: scanned /root for discovery docs: <nil>
Aug 22 02:58:35.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4625'
Aug 22 02:58:36.188: INFO: stderr: ""
Aug 22 02:58:36.188: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 02:58:36.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4625'
Aug 22 02:58:36.302: INFO: stderr: ""
Aug 22 02:58:36.302: INFO: stdout: "update-demo-nautilus-5j77w update-demo-nautilus-t5qrq "
Aug 22 02:58:36.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-5j77w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:36.409: INFO: stderr: ""
Aug 22 02:58:36.409: INFO: stdout: ""
Aug 22 02:58:36.409: INFO: update-demo-nautilus-5j77w is created but not running
Aug 22 02:58:41.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4625'
Aug 22 02:58:41.498: INFO: stderr: ""
Aug 22 02:58:41.498: INFO: stdout: "update-demo-nautilus-5j77w update-demo-nautilus-t5qrq "
Aug 22 02:58:41.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-5j77w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:41.599: INFO: stderr: ""
Aug 22 02:58:41.599: INFO: stdout: "true"
Aug 22 02:58:41.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-5j77w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:41.678: INFO: stderr: ""
Aug 22 02:58:41.678: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 22 02:58:41.678: INFO: validating pod update-demo-nautilus-5j77w
Aug 22 02:58:41.686: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 02:58:41.686: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 02:58:41.686: INFO: update-demo-nautilus-5j77w is verified up and running
Aug 22 02:58:41.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-t5qrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:41.781: INFO: stderr: ""
Aug 22 02:58:41.781: INFO: stdout: "true"
Aug 22 02:58:41.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-t5qrq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4625'
Aug 22 02:58:41.865: INFO: stderr: ""
Aug 22 02:58:41.865: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 22 02:58:41.865: INFO: validating pod update-demo-nautilus-t5qrq
Aug 22 02:58:41.869: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 02:58:41.869: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 02:58:41.869: INFO: update-demo-nautilus-t5qrq is verified up and running
STEP: using delete to clean up resources
Aug 22 02:58:41.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 delete --grace-period=0 --force -f - --namespace=kubectl-4625'
Aug 22 02:58:41.963: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 22 02:58:41.963: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 22 02:58:41.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4625'
Aug 22 02:58:42.074: INFO: stderr: "No resources found.\n"
Aug 22 02:58:42.074: INFO: stdout: ""
Aug 22 02:58:42.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -l name=update-demo --namespace=kubectl-4625 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 22 02:58:42.174: INFO: stderr: ""
Aug 22 02:58:42.174: INFO: stdout: "update-demo-nautilus-5j77w\nupdate-demo-nautilus-t5qrq\n"
Aug 22 02:58:42.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4625'
Aug 22 02:58:42.801: INFO: stderr: "No resources found.\n"
Aug 22 02:58:42.801: INFO: stdout: ""
Aug 22 02:58:42.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -l name=update-demo --namespace=kubectl-4625 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 22 02:58:42.913: INFO: stderr: ""
Aug 22 02:58:42.913: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:58:42.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4625" for this suite.
Aug 22 02:59:04.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 02:59:05.066: INFO: namespace kubectl-4625 deletion completed in 22.146915176s

• [SLOW TEST:42.544 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 02:59:05.067: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 22 02:59:05.101: INFO: PodSpec: initContainers in spec.initContainers
Aug 22 02:59:49.012: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bee69639-4fca-4266-bd64-663e1519c1f9", GenerateName:"", Namespace:"init-container-5064", SelfLink:"/api/v1/namespaces/init-container-5064/pods/pod-init-bee69639-4fca-4266-bd64-663e1519c1f9", UID:"cbaf5b0f-b3e5-4159-ae93-61495672c9ee", ResourceVersion:"22337", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63702039545, loc:(*time.Location)(0x7ec7a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"101460427"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.42.1.219/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xqrwx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002757c00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xqrwx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xqrwx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xqrwx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002ee4848), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"melsayed-conformance3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002de6300), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ee48d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ee48f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002ee48f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002ee48fc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039545, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039545, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039545, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702039545, loc:(*time.Location)(0x7ec7a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"165.22.32.172", PodIP:"10.42.1.219", StartTime:(*v1.Time)(0xc00295f820), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00346bc70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00346bce0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://3def78962c4acc50f07f2d80b676587b1b78e3480f039c7d57d5134167fb04b0"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00295f860), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00295f840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 02:59:49.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5064" for this suite.
Aug 22 03:00:11.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:00:11.127: INFO: namespace init-container-5064 deletion completed in 22.109938885s

• [SLOW TEST:66.060 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:00:11.133: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-972d077b-1f8d-4924-8dca-ce1a516d3c2b
STEP: Creating a pod to test consume secrets
Aug 22 03:00:11.391: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-aabf1055-f875-47c7-8c81-a936008a96cc" in namespace "projected-196" to be "success or failure"
Aug 22 03:00:11.394: INFO: Pod "pod-projected-secrets-aabf1055-f875-47c7-8c81-a936008a96cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985295ms
Aug 22 03:00:13.474: INFO: Pod "pod-projected-secrets-aabf1055-f875-47c7-8c81-a936008a96cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082216846s
Aug 22 03:00:15.515: INFO: Pod "pod-projected-secrets-aabf1055-f875-47c7-8c81-a936008a96cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.123217889s
STEP: Saw pod success
Aug 22 03:00:15.515: INFO: Pod "pod-projected-secrets-aabf1055-f875-47c7-8c81-a936008a96cc" satisfied condition "success or failure"
Aug 22 03:00:15.518: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-secrets-aabf1055-f875-47c7-8c81-a936008a96cc container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 22 03:00:15.567: INFO: Waiting for pod pod-projected-secrets-aabf1055-f875-47c7-8c81-a936008a96cc to disappear
Aug 22 03:00:15.599: INFO: Pod pod-projected-secrets-aabf1055-f875-47c7-8c81-a936008a96cc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:00:15.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-196" for this suite.
Aug 22 03:00:21.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:00:21.729: INFO: namespace projected-196 deletion completed in 6.122987336s

• [SLOW TEST:10.596 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:00:21.730: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 03:00:21.778: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2203369e-6f42-4d6f-964c-2fd170900a90" in namespace "downward-api-7515" to be "success or failure"
Aug 22 03:00:21.786: INFO: Pod "downwardapi-volume-2203369e-6f42-4d6f-964c-2fd170900a90": Phase="Pending", Reason="", readiness=false. Elapsed: 8.320673ms
Aug 22 03:00:23.790: INFO: Pod "downwardapi-volume-2203369e-6f42-4d6f-964c-2fd170900a90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012211032s
Aug 22 03:00:25.794: INFO: Pod "downwardapi-volume-2203369e-6f42-4d6f-964c-2fd170900a90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016323774s
STEP: Saw pod success
Aug 22 03:00:25.794: INFO: Pod "downwardapi-volume-2203369e-6f42-4d6f-964c-2fd170900a90" satisfied condition "success or failure"
Aug 22 03:00:25.798: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-2203369e-6f42-4d6f-964c-2fd170900a90 container client-container: <nil>
STEP: delete the pod
Aug 22 03:00:25.828: INFO: Waiting for pod downwardapi-volume-2203369e-6f42-4d6f-964c-2fd170900a90 to disappear
Aug 22 03:00:25.834: INFO: Pod downwardapi-volume-2203369e-6f42-4d6f-964c-2fd170900a90 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:00:25.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7515" for this suite.
Aug 22 03:00:31.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:00:31.968: INFO: namespace downward-api-7515 deletion completed in 6.130070709s

• [SLOW TEST:10.238 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:00:31.970: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:00:32.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5017" for this suite.
Aug 22 03:00:54.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:00:54.147: INFO: namespace pods-5017 deletion completed in 22.123669159s

• [SLOW TEST:22.177 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:00:54.148: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-576ca778-2b89-4970-99f4-6735b77719f9
STEP: Creating a pod to test consume secrets
Aug 22 03:00:54.196: INFO: Waiting up to 5m0s for pod "pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7" in namespace "secrets-9981" to be "success or failure"
Aug 22 03:00:54.204: INFO: Pod "pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.741249ms
Aug 22 03:00:56.209: INFO: Pod "pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01331953s
Aug 22 03:00:58.213: INFO: Pod "pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017012268s
Aug 22 03:01:00.217: INFO: Pod "pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020920297s
Aug 22 03:01:02.221: INFO: Pod "pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025416883s
Aug 22 03:01:04.231: INFO: Pod "pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035632737s
STEP: Saw pod success
Aug 22 03:01:04.231: INFO: Pod "pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7" satisfied condition "success or failure"
Aug 22 03:01:04.235: INFO: Trying to get logs from node melsayed-conformance3 pod pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7 container secret-env-test: <nil>
STEP: delete the pod
Aug 22 03:01:04.256: INFO: Waiting for pod pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7 to disappear
Aug 22 03:01:04.262: INFO: Pod pod-secrets-cddc6dbc-f2ff-4fa1-85c3-6f49d98369f7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:01:04.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9981" for this suite.
Aug 22 03:01:10.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:01:10.429: INFO: namespace secrets-9981 deletion completed in 6.164322119s

• [SLOW TEST:16.282 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:01:10.430: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 03:01:10.467: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4bdbe58-1850-4750-b426-df380a65443c" in namespace "projected-5514" to be "success or failure"
Aug 22 03:01:10.477: INFO: Pod "downwardapi-volume-d4bdbe58-1850-4750-b426-df380a65443c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.120051ms
Aug 22 03:01:12.482: INFO: Pod "downwardapi-volume-d4bdbe58-1850-4750-b426-df380a65443c": Phase="Running", Reason="", readiness=true. Elapsed: 2.015503961s
Aug 22 03:01:14.487: INFO: Pod "downwardapi-volume-d4bdbe58-1850-4750-b426-df380a65443c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019587156s
STEP: Saw pod success
Aug 22 03:01:14.487: INFO: Pod "downwardapi-volume-d4bdbe58-1850-4750-b426-df380a65443c" satisfied condition "success or failure"
Aug 22 03:01:14.489: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-d4bdbe58-1850-4750-b426-df380a65443c container client-container: <nil>
STEP: delete the pod
Aug 22 03:01:14.513: INFO: Waiting for pod downwardapi-volume-d4bdbe58-1850-4750-b426-df380a65443c to disappear
Aug 22 03:01:14.516: INFO: Pod downwardapi-volume-d4bdbe58-1850-4750-b426-df380a65443c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:01:14.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5514" for this suite.
Aug 22 03:01:20.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:01:20.616: INFO: namespace projected-5514 deletion completed in 6.096477086s

• [SLOW TEST:10.186 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:01:20.616: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 22 03:01:27.188: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4653 pod-service-account-ba7d3f6d-d863-48d5-9c1b-b4a79905e7d8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 22 03:01:27.475: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4653 pod-service-account-ba7d3f6d-d863-48d5-9c1b-b4a79905e7d8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 22 03:01:27.774: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4653 pod-service-account-ba7d3f6d-d863-48d5-9c1b-b4a79905e7d8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:01:28.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4653" for this suite.
Aug 22 03:01:34.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:01:34.241: INFO: namespace svcaccounts-4653 deletion completed in 6.118315178s

• [SLOW TEST:13.625 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:01:34.242: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-3351
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3351
STEP: Deleting pre-stop pod
Aug 22 03:01:45.380: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:01:45.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3351" for this suite.
Aug 22 03:02:23.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:02:23.551: INFO: namespace prestop-3351 deletion completed in 38.147033677s

• [SLOW TEST:49.310 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:02:23.551: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-9bd4c5f9-f17c-4d59-a8ab-2c55ff2ca037
STEP: Creating a pod to test consume configMaps
Aug 22 03:02:23.586: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d8dbd647-41a0-4f60-b89b-718365b4ad66" in namespace "projected-2750" to be "success or failure"
Aug 22 03:02:23.592: INFO: Pod "pod-projected-configmaps-d8dbd647-41a0-4f60-b89b-718365b4ad66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.353917ms
Aug 22 03:02:25.596: INFO: Pod "pod-projected-configmaps-d8dbd647-41a0-4f60-b89b-718365b4ad66": Phase="Running", Reason="", readiness=true. Elapsed: 2.009710222s
Aug 22 03:02:27.601: INFO: Pod "pod-projected-configmaps-d8dbd647-41a0-4f60-b89b-718365b4ad66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015289648s
STEP: Saw pod success
Aug 22 03:02:27.602: INFO: Pod "pod-projected-configmaps-d8dbd647-41a0-4f60-b89b-718365b4ad66" satisfied condition "success or failure"
Aug 22 03:02:27.605: INFO: Trying to get logs from node melsayed-conformance3 pod pod-projected-configmaps-d8dbd647-41a0-4f60-b89b-718365b4ad66 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 22 03:02:27.622: INFO: Waiting for pod pod-projected-configmaps-d8dbd647-41a0-4f60-b89b-718365b4ad66 to disappear
Aug 22 03:02:27.627: INFO: Pod pod-projected-configmaps-d8dbd647-41a0-4f60-b89b-718365b4ad66 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:02:27.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2750" for this suite.
Aug 22 03:02:33.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:02:33.768: INFO: namespace projected-2750 deletion completed in 6.132621089s

• [SLOW TEST:10.217 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:02:33.770: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Aug 22 03:02:33.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 create -f - --namespace=kubectl-6009'
Aug 22 03:02:34.067: INFO: stderr: ""
Aug 22 03:02:34.067: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 03:02:34.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6009'
Aug 22 03:02:34.185: INFO: stderr: ""
Aug 22 03:02:34.185: INFO: stdout: "update-demo-nautilus-q29rl update-demo-nautilus-rgbtj "
Aug 22 03:02:34.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-q29rl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6009'
Aug 22 03:02:34.296: INFO: stderr: ""
Aug 22 03:02:34.296: INFO: stdout: ""
Aug 22 03:02:34.296: INFO: update-demo-nautilus-q29rl is created but not running
Aug 22 03:02:39.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6009'
Aug 22 03:02:39.396: INFO: stderr: ""
Aug 22 03:02:39.396: INFO: stdout: "update-demo-nautilus-q29rl update-demo-nautilus-rgbtj "
Aug 22 03:02:39.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-q29rl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6009'
Aug 22 03:02:39.481: INFO: stderr: ""
Aug 22 03:02:39.481: INFO: stdout: "true"
Aug 22 03:02:39.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-q29rl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6009'
Aug 22 03:02:39.587: INFO: stderr: ""
Aug 22 03:02:39.587: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 22 03:02:39.587: INFO: validating pod update-demo-nautilus-q29rl
Aug 22 03:02:39.594: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 03:02:39.594: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 03:02:39.594: INFO: update-demo-nautilus-q29rl is verified up and running
Aug 22 03:02:39.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-rgbtj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6009'
Aug 22 03:02:39.709: INFO: stderr: ""
Aug 22 03:02:39.709: INFO: stdout: "true"
Aug 22 03:02:39.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-nautilus-rgbtj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6009'
Aug 22 03:02:39.797: INFO: stderr: ""
Aug 22 03:02:39.797: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 22 03:02:39.797: INFO: validating pod update-demo-nautilus-rgbtj
Aug 22 03:02:39.805: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 22 03:02:39.805: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 22 03:02:39.805: INFO: update-demo-nautilus-rgbtj is verified up and running
STEP: rolling-update to new replication controller
Aug 22 03:02:39.808: INFO: scanned /root for discovery docs: <nil>
Aug 22 03:02:39.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-6009'
Aug 22 03:03:02.387: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 22 03:03:02.387: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 22 03:03:02.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6009'
Aug 22 03:03:02.482: INFO: stderr: ""
Aug 22 03:03:02.482: INFO: stdout: "update-demo-kitten-lccnj update-demo-kitten-wvw74 "
Aug 22 03:03:02.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-kitten-lccnj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6009'
Aug 22 03:03:02.606: INFO: stderr: ""
Aug 22 03:03:02.606: INFO: stdout: "true"
Aug 22 03:03:02.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-kitten-lccnj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6009'
Aug 22 03:03:02.695: INFO: stderr: ""
Aug 22 03:03:02.695: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 22 03:03:02.695: INFO: validating pod update-demo-kitten-lccnj
Aug 22 03:03:02.703: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 22 03:03:02.703: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 22 03:03:02.703: INFO: update-demo-kitten-lccnj is verified up and running
Aug 22 03:03:02.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-kitten-wvw74 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6009'
Aug 22 03:03:02.789: INFO: stderr: ""
Aug 22 03:03:02.789: INFO: stdout: "true"
Aug 22 03:03:02.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-089099426 get pods update-demo-kitten-wvw74 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6009'
Aug 22 03:03:02.877: INFO: stderr: ""
Aug 22 03:03:02.877: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 22 03:03:02.877: INFO: validating pod update-demo-kitten-wvw74
Aug 22 03:03:02.885: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 22 03:03:02.885: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 22 03:03:02.885: INFO: update-demo-kitten-wvw74 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:03:02.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6009" for this suite.
Aug 22 03:03:24.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:03:25.028: INFO: namespace kubectl-6009 deletion completed in 22.137633663s

• [SLOW TEST:51.258 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:03:25.028: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 22 03:03:25.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e41bfb8-aaa6-4374-9adc-b0b7ebeffaf8" in namespace "downward-api-7123" to be "success or failure"
Aug 22 03:03:25.079: INFO: Pod "downwardapi-volume-5e41bfb8-aaa6-4374-9adc-b0b7ebeffaf8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.962915ms
Aug 22 03:03:27.083: INFO: Pod "downwardapi-volume-5e41bfb8-aaa6-4374-9adc-b0b7ebeffaf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010033705s
STEP: Saw pod success
Aug 22 03:03:27.083: INFO: Pod "downwardapi-volume-5e41bfb8-aaa6-4374-9adc-b0b7ebeffaf8" satisfied condition "success or failure"
Aug 22 03:03:27.086: INFO: Trying to get logs from node melsayed-conformance3 pod downwardapi-volume-5e41bfb8-aaa6-4374-9adc-b0b7ebeffaf8 container client-container: <nil>
STEP: delete the pod
Aug 22 03:03:27.115: INFO: Waiting for pod downwardapi-volume-5e41bfb8-aaa6-4374-9adc-b0b7ebeffaf8 to disappear
Aug 22 03:03:27.131: INFO: Pod downwardapi-volume-5e41bfb8-aaa6-4374-9adc-b0b7ebeffaf8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:03:27.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7123" for this suite.
Aug 22 03:03:33.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:03:33.247: INFO: namespace downward-api-7123 deletion completed in 6.111362292s

• [SLOW TEST:8.218 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:03:33.247: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-qcmb
STEP: Creating a pod to test atomic-volume-subpath
Aug 22 03:03:33.303: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qcmb" in namespace "subpath-6830" to be "success or failure"
Aug 22 03:03:33.309: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.935384ms
Aug 22 03:03:35.312: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009580965s
Aug 22 03:03:37.317: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 4.013863524s
Aug 22 03:03:39.321: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 6.017668082s
Aug 22 03:03:41.324: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 8.021182965s
Aug 22 03:03:43.330: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 10.027084356s
Aug 22 03:03:45.333: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 12.030166062s
Aug 22 03:03:47.337: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 14.034187282s
Aug 22 03:03:49.341: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 16.038149084s
Aug 22 03:03:51.345: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 18.042403346s
Aug 22 03:03:53.349: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 20.046185287s
Aug 22 03:03:55.353: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Running", Reason="", readiness=true. Elapsed: 22.049996283s
Aug 22 03:03:57.360: INFO: Pod "pod-subpath-test-configmap-qcmb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.056684202s
STEP: Saw pod success
Aug 22 03:03:57.360: INFO: Pod "pod-subpath-test-configmap-qcmb" satisfied condition "success or failure"
Aug 22 03:03:57.365: INFO: Trying to get logs from node melsayed-conformance3 pod pod-subpath-test-configmap-qcmb container test-container-subpath-configmap-qcmb: <nil>
STEP: delete the pod
Aug 22 03:03:57.391: INFO: Waiting for pod pod-subpath-test-configmap-qcmb to disappear
Aug 22 03:03:57.397: INFO: Pod pod-subpath-test-configmap-qcmb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qcmb
Aug 22 03:03:57.397: INFO: Deleting pod "pod-subpath-test-configmap-qcmb" in namespace "subpath-6830"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:03:57.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6830" for this suite.
Aug 22 03:04:03.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:04:03.525: INFO: namespace subpath-6830 deletion completed in 6.121977149s

• [SLOW TEST:30.278 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:04:03.526: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:04:05.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3437" for this suite.
Aug 22 03:04:51.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:04:51.722: INFO: namespace kubelet-test-3437 deletion completed in 46.130430581s

• [SLOW TEST:48.196 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:04:51.723: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-cbfd8f1d-de3a-4171-a77e-d7a4985f2199 in namespace container-probe-6720
Aug 22 03:04:57.923: INFO: Started pod busybox-cbfd8f1d-de3a-4171-a77e-d7a4985f2199 in namespace container-probe-6720
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 03:04:57.927: INFO: Initial restart count of pod busybox-cbfd8f1d-de3a-4171-a77e-d7a4985f2199 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:08:58.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6720" for this suite.
Aug 22 03:09:04.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:09:04.721: INFO: namespace container-probe-6720 deletion completed in 6.112177058s

• [SLOW TEST:252.999 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:09:04.722: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-c59e023a-7e9d-48fe-9da5-f50caf87257d in namespace container-probe-7520
Aug 22 03:09:06.767: INFO: Started pod test-webserver-c59e023a-7e9d-48fe-9da5-f50caf87257d in namespace container-probe-7520
STEP: checking the pod's current state and verifying that restartCount is present
Aug 22 03:09:06.770: INFO: Initial restart count of pod test-webserver-c59e023a-7e9d-48fe-9da5-f50caf87257d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:13:07.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7520" for this suite.
Aug 22 03:13:13.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:13:13.518: INFO: namespace container-probe-7520 deletion completed in 6.138659251s

• [SLOW TEST:248.796 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:13:13.525: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 22 03:13:13.563: INFO: Waiting up to 5m0s for pod "pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa" in namespace "emptydir-243" to be "success or failure"
Aug 22 03:13:13.565: INFO: Pod "pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.454693ms
Aug 22 03:13:15.569: INFO: Pod "pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005652038s
Aug 22 03:13:17.572: INFO: Pod "pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009589087s
Aug 22 03:13:19.580: INFO: Pod "pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016820158s
Aug 22 03:13:21.583: INFO: Pod "pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020048977s
STEP: Saw pod success
Aug 22 03:13:21.583: INFO: Pod "pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa" satisfied condition "success or failure"
Aug 22 03:13:21.586: INFO: Trying to get logs from node melsayed-conformance3 pod pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa container test-container: <nil>
STEP: delete the pod
Aug 22 03:13:21.620: INFO: Waiting for pod pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa to disappear
Aug 22 03:13:21.637: INFO: Pod pod-f8e530ca-fe6f-4e28-8f59-ca70411b89aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:13:21.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-243" for this suite.
Aug 22 03:13:27.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:13:27.785: INFO: namespace emptydir-243 deletion completed in 6.144452877s

• [SLOW TEST:14.260 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:13:27.790: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 03:13:27.851: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4b8dc673-6e5a-498f-825d-798e25f83e4b", Controller:(*bool)(0xc0031de986), BlockOwnerDeletion:(*bool)(0xc0031de987)}}
Aug 22 03:13:27.860: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c2ddfd8e-e919-47df-96f9-d3ccad440a86", Controller:(*bool)(0xc002cc9cb6), BlockOwnerDeletion:(*bool)(0xc002cc9cb7)}}
Aug 22 03:13:27.873: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e64a0ba6-565e-4d04-8412-35fd67499e07", Controller:(*bool)(0xc002cc9e66), BlockOwnerDeletion:(*bool)(0xc002cc9e67)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:13:32.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9196" for this suite.
Aug 22 03:13:38.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:13:39.005: INFO: namespace gc-9196 deletion completed in 6.112771806s

• [SLOW TEST:11.215 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:13:39.006: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 22 03:13:45.096: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 03:13:45.101: INFO: Pod pod-with-prestop-http-hook still exists
Aug 22 03:13:47.102: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 03:13:47.107: INFO: Pod pod-with-prestop-http-hook still exists
Aug 22 03:13:49.102: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 03:13:49.119: INFO: Pod pod-with-prestop-http-hook still exists
Aug 22 03:13:51.102: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 03:13:51.105: INFO: Pod pod-with-prestop-http-hook still exists
Aug 22 03:13:53.102: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 03:13:53.106: INFO: Pod pod-with-prestop-http-hook still exists
Aug 22 03:13:55.102: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 03:13:55.105: INFO: Pod pod-with-prestop-http-hook still exists
Aug 22 03:13:57.102: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 22 03:13:57.105: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:13:57.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-174" for this suite.
Aug 22 03:14:19.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:14:19.224: INFO: namespace container-lifecycle-hook-174 deletion completed in 22.104878831s

• [SLOW TEST:40.218 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:14:19.226: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:14:25.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7498" for this suite.
Aug 22 03:14:31.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:14:31.478: INFO: namespace namespaces-7498 deletion completed in 6.1237512s
STEP: Destroying namespace "nsdeletetest-1594" for this suite.
Aug 22 03:14:31.483: INFO: Namespace nsdeletetest-1594 was already deleted
STEP: Destroying namespace "nsdeletetest-3787" for this suite.
Aug 22 03:14:37.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:14:37.586: INFO: namespace nsdeletetest-3787 deletion completed in 6.103307789s

• [SLOW TEST:18.361 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:14:37.590: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 22 03:14:37.625: INFO: Waiting up to 5m0s for pod "downward-api-3acf43a7-a0b8-414c-80cb-ee1966605181" in namespace "downward-api-7006" to be "success or failure"
Aug 22 03:14:37.629: INFO: Pod "downward-api-3acf43a7-a0b8-414c-80cb-ee1966605181": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657529ms
Aug 22 03:14:39.635: INFO: Pod "downward-api-3acf43a7-a0b8-414c-80cb-ee1966605181": Phase="Running", Reason="", readiness=true. Elapsed: 2.009837854s
Aug 22 03:14:41.640: INFO: Pod "downward-api-3acf43a7-a0b8-414c-80cb-ee1966605181": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014938823s
STEP: Saw pod success
Aug 22 03:14:41.640: INFO: Pod "downward-api-3acf43a7-a0b8-414c-80cb-ee1966605181" satisfied condition "success or failure"
Aug 22 03:14:41.644: INFO: Trying to get logs from node melsayed-conformance3 pod downward-api-3acf43a7-a0b8-414c-80cb-ee1966605181 container dapi-container: <nil>
STEP: delete the pod
Aug 22 03:14:41.694: INFO: Waiting for pod downward-api-3acf43a7-a0b8-414c-80cb-ee1966605181 to disappear
Aug 22 03:14:41.700: INFO: Pod downward-api-3acf43a7-a0b8-414c-80cb-ee1966605181 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:14:41.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7006" for this suite.
Aug 22 03:14:47.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:14:47.798: INFO: namespace downward-api-7006 deletion completed in 6.092456552s

• [SLOW TEST:10.208 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:14:47.798: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 22 03:14:47.835: INFO: Waiting up to 5m0s for pod "pod-53f33562-6c4a-42f2-85df-97e2655e188d" in namespace "emptydir-3322" to be "success or failure"
Aug 22 03:14:47.851: INFO: Pod "pod-53f33562-6c4a-42f2-85df-97e2655e188d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.104585ms
Aug 22 03:14:49.854: INFO: Pod "pod-53f33562-6c4a-42f2-85df-97e2655e188d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018780891s
STEP: Saw pod success
Aug 22 03:14:49.854: INFO: Pod "pod-53f33562-6c4a-42f2-85df-97e2655e188d" satisfied condition "success or failure"
Aug 22 03:14:49.857: INFO: Trying to get logs from node melsayed-conformance3 pod pod-53f33562-6c4a-42f2-85df-97e2655e188d container test-container: <nil>
STEP: delete the pod
Aug 22 03:14:49.876: INFO: Waiting for pod pod-53f33562-6c4a-42f2-85df-97e2655e188d to disappear
Aug 22 03:14:49.882: INFO: Pod pod-53f33562-6c4a-42f2-85df-97e2655e188d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:14:49.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3322" for this suite.
Aug 22 03:14:55.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:14:56.018: INFO: namespace emptydir-3322 deletion completed in 6.132133089s

• [SLOW TEST:8.220 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 22 03:14:56.019: INFO: >>> kubeConfig: /tmp/kubeconfig-089099426
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 22 03:14:56.114: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 22 03:15:01.118: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 22 03:15:01.118: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 22 03:15:05.163: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-2975,SelfLink:/apis/apps/v1/namespaces/deployment-2975/deployments/test-cleanup-deployment,UID:7accfd88-1050-4e22-85fc-d0308716d969,ResourceVersion:24642,Generation:1,CreationTimestamp:2019-08-22 03:15:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-22 03:15:01 +0000 UTC 2019-08-22 03:15:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-22 03:15:03 +0000 UTC 2019-08-22 03:15:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 22 03:15:05.167: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-2975,SelfLink:/apis/apps/v1/namespaces/deployment-2975/replicasets/test-cleanup-deployment-55bbcbc84c,UID:85a03f6c-68f2-441e-b68f-3c91c126d1cf,ResourceVersion:24631,Generation:1,CreationTimestamp:2019-08-22 03:15:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 7accfd88-1050-4e22-85fc-d0308716d969 0xc00386b107 0xc00386b108}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 22 03:15:05.171: INFO: Pod "test-cleanup-deployment-55bbcbc84c-28z67" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-28z67,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-2975,SelfLink:/api/v1/namespaces/deployment-2975/pods/test-cleanup-deployment-55bbcbc84c-28z67,UID:f9a64288-3157-487c-8c78-d9e06f76bf45,ResourceVersion:24630,Generation:0,CreationTimestamp:2019-08-22 03:15:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.2.57/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 85a03f6c-68f2-441e-b68f-3c91c126d1cf 0xc00359d607 0xc00359d608}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ktxm2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ktxm2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-ktxm2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:melsayed-conformance2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00359d670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00359d690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 03:15:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 03:15:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 03:15:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-22 03:15:01 +0000 UTC  }],Message:,Reason:,HostIP:167.71.246.48,PodIP:10.42.2.57,StartTime:2019-08-22 03:15:01 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-22 03:15:02 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c046501919fec735b7fb969c5c6393c392d75dc41ceec9747abde3a16e976d75}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 22 03:15:05.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2975" for this suite.
Aug 22 03:15:11.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 22 03:15:11.281: INFO: namespace deployment-2975 deletion completed in 6.106976406s

• [SLOW TEST:15.263 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SAug 22 03:15:11.282: INFO: Running AfterSuite actions on all nodes
Aug 22 03:15:11.284: INFO: Running AfterSuite actions on node 1
Aug 22 03:15:11.284: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5906.719 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h38m29.469868944s
Test Suite Passed
