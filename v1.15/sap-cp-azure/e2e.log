Conformance test: not doing test setup.
I0808 15:04:10.813505    4191 e2e.go:241] Starting e2e run "8d84d478-a6d5-4d99-b43b-9107959478b4" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1565276649 - Will randomize all specs
Will run 212 of 4413 specs

Aug  8 15:04:46.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:04:46.027: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug  8 15:04:46.152: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug  8 15:04:46.254: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug  8 15:04:46.254: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Aug  8 15:04:46.254: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug  8 15:04:46.280: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug  8 15:04:46.280: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug  8 15:04:46.280: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Aug  8 15:04:46.280: INFO: e2e test version: v1.15.2
Aug  8 15:04:46.298: INFO: kube-apiserver version: v1.15.2
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:04:46.299: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
Aug  8 15:04:46.414: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Aug  8 15:04:46.476: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug  8 15:04:54.848: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  8 15:04:54.868: INFO: Pod pod-with-prestop-http-hook still exists
Aug  8 15:04:56.869: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  8 15:04:56.889: INFO: Pod pod-with-prestop-http-hook still exists
Aug  8 15:04:58.869: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  8 15:04:58.889: INFO: Pod pod-with-prestop-http-hook still exists
Aug  8 15:05:00.869: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  8 15:05:00.889: INFO: Pod pod-with-prestop-http-hook still exists
Aug  8 15:05:02.869: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  8 15:05:02.889: INFO: Pod pod-with-prestop-http-hook still exists
Aug  8 15:05:04.869: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug  8 15:05:04.889: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:05:05.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7997" for this suite.
Aug  8 15:05:27.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:05:27.880: INFO: namespace container-lifecycle-hook-7997 deletion completed in 22.796867124s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:05:27.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug  8 15:05:28.171: INFO: Waiting up to 5m0s for pod "pod-53b21d76-d9a8-4004-a0a1-54186f2ed73b" in namespace "emptydir-5285" to be "success or failure"
Aug  8 15:05:28.241: INFO: Pod "pod-53b21d76-d9a8-4004-a0a1-54186f2ed73b": Phase="Pending", Reason="", readiness=false. Elapsed: 69.312393ms
Aug  8 15:05:30.261: INFO: Pod "pod-53b21d76-d9a8-4004-a0a1-54186f2ed73b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089337482s
Aug  8 15:05:32.281: INFO: Pod "pod-53b21d76-d9a8-4004-a0a1-54186f2ed73b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.109893141s
STEP: Saw pod success
Aug  8 15:05:32.281: INFO: Pod "pod-53b21d76-d9a8-4004-a0a1-54186f2ed73b" satisfied condition "success or failure"
Aug  8 15:05:32.301: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-53b21d76-d9a8-4004-a0a1-54186f2ed73b container test-container: <nil>
STEP: delete the pod
Aug  8 15:05:32.354: INFO: Waiting for pod pod-53b21d76-d9a8-4004-a0a1-54186f2ed73b to disappear
Aug  8 15:05:32.373: INFO: Pod pod-53b21d76-d9a8-4004-a0a1-54186f2ed73b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:05:32.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5285" for this suite.
Aug  8 15:05:38.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:05:39.212: INFO: namespace emptydir-5285 deletion completed in 6.81925863s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:05:39.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-b123cb6d-570b-4b1c-8de1-425c6691b160
STEP: Creating a pod to test consume configMaps
Aug  8 15:05:39.500: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e87255a-cb85-498f-b2ce-da3715939141" in namespace "configmap-4743" to be "success or failure"
Aug  8 15:05:39.519: INFO: Pod "pod-configmaps-2e87255a-cb85-498f-b2ce-da3715939141": Phase="Pending", Reason="", readiness=false. Elapsed: 19.74199ms
Aug  8 15:05:41.540: INFO: Pod "pod-configmaps-2e87255a-cb85-498f-b2ce-da3715939141": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040119239s
Aug  8 15:05:43.560: INFO: Pod "pod-configmaps-2e87255a-cb85-498f-b2ce-da3715939141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060527934s
STEP: Saw pod success
Aug  8 15:05:43.560: INFO: Pod "pod-configmaps-2e87255a-cb85-498f-b2ce-da3715939141" satisfied condition "success or failure"
Aug  8 15:05:43.579: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-2e87255a-cb85-498f-b2ce-da3715939141 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 15:05:43.633: INFO: Waiting for pod pod-configmaps-2e87255a-cb85-498f-b2ce-da3715939141 to disappear
Aug  8 15:05:43.652: INFO: Pod pod-configmaps-2e87255a-cb85-498f-b2ce-da3715939141 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:05:43.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4743" for this suite.
Aug  8 15:05:49.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:05:50.477: INFO: namespace configmap-4743 deletion completed in 6.805654303s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:05:50.479: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0808 15:06:00.874513    4191 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug  8 15:06:00.874: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:06:00.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7955" for this suite.
Aug  8 15:06:06.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:06:07.713: INFO: namespace gc-7955 deletion completed in 6.817173843s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:06:07.714: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug  8 15:06:07.977: INFO: Waiting up to 5m0s for pod "pod-269dca32-af15-47d5-b488-c58e05784f83" in namespace "emptydir-2102" to be "success or failure"
Aug  8 15:06:07.996: INFO: Pod "pod-269dca32-af15-47d5-b488-c58e05784f83": Phase="Pending", Reason="", readiness=false. Elapsed: 19.441502ms
Aug  8 15:06:10.016: INFO: Pod "pod-269dca32-af15-47d5-b488-c58e05784f83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039650189s
STEP: Saw pod success
Aug  8 15:06:10.017: INFO: Pod "pod-269dca32-af15-47d5-b488-c58e05784f83" satisfied condition "success or failure"
Aug  8 15:06:10.036: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-269dca32-af15-47d5-b488-c58e05784f83 container test-container: <nil>
STEP: delete the pod
Aug  8 15:06:10.105: INFO: Waiting for pod pod-269dca32-af15-47d5-b488-c58e05784f83 to disappear
Aug  8 15:06:10.125: INFO: Pod pod-269dca32-af15-47d5-b488-c58e05784f83 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:06:10.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2102" for this suite.
Aug  8 15:06:16.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:06:17.054: INFO: namespace emptydir-2102 deletion completed in 6.908884714s
•SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:06:17.054: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug  8 15:06:17.464: INFO: Waiting up to 5m0s for pod "downward-api-885439e2-c345-47c6-b4ec-3ee2232ebc47" in namespace "downward-api-4586" to be "success or failure"
Aug  8 15:06:17.483: INFO: Pod "downward-api-885439e2-c345-47c6-b4ec-3ee2232ebc47": Phase="Pending", Reason="", readiness=false. Elapsed: 19.148498ms
Aug  8 15:06:19.503: INFO: Pod "downward-api-885439e2-c345-47c6-b4ec-3ee2232ebc47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039269392s
Aug  8 15:06:21.523: INFO: Pod "downward-api-885439e2-c345-47c6-b4ec-3ee2232ebc47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059355005s
Aug  8 15:06:23.543: INFO: Pod "downward-api-885439e2-c345-47c6-b4ec-3ee2232ebc47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079455733s
STEP: Saw pod success
Aug  8 15:06:23.543: INFO: Pod "downward-api-885439e2-c345-47c6-b4ec-3ee2232ebc47" satisfied condition "success or failure"
Aug  8 15:06:23.562: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downward-api-885439e2-c345-47c6-b4ec-3ee2232ebc47 container dapi-container: <nil>
STEP: delete the pod
Aug  8 15:06:23.636: INFO: Waiting for pod downward-api-885439e2-c345-47c6-b4ec-3ee2232ebc47 to disappear
Aug  8 15:06:23.655: INFO: Pod downward-api-885439e2-c345-47c6-b4ec-3ee2232ebc47 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:06:23.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4586" for this suite.
Aug  8 15:06:29.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:06:30.456: INFO: namespace downward-api-4586 deletion completed in 6.780607317s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:06:30.456: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-11703976-9440-445b-81e7-7d54b8df4a7e
STEP: Creating a pod to test consume secrets
Aug  8 15:06:30.795: INFO: Waiting up to 5m0s for pod "pod-secrets-72835eaa-8775-42cf-aaf4-92206ec4182f" in namespace "secrets-8119" to be "success or failure"
Aug  8 15:06:30.815: INFO: Pod "pod-secrets-72835eaa-8775-42cf-aaf4-92206ec4182f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.354329ms
Aug  8 15:06:32.835: INFO: Pod "pod-secrets-72835eaa-8775-42cf-aaf4-92206ec4182f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039316856s
STEP: Saw pod success
Aug  8 15:06:32.835: INFO: Pod "pod-secrets-72835eaa-8775-42cf-aaf4-92206ec4182f" satisfied condition "success or failure"
Aug  8 15:06:32.854: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-secrets-72835eaa-8775-42cf-aaf4-92206ec4182f container secret-env-test: <nil>
STEP: delete the pod
Aug  8 15:06:32.910: INFO: Waiting for pod pod-secrets-72835eaa-8775-42cf-aaf4-92206ec4182f to disappear
Aug  8 15:06:32.930: INFO: Pod pod-secrets-72835eaa-8775-42cf-aaf4-92206ec4182f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:06:32.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8119" for this suite.
Aug  8 15:06:39.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:06:39.755: INFO: namespace secrets-8119 deletion completed in 6.805044211s
•SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:06:39.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7309
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7309.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7309.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug  8 15:07:00.714: INFO: DNS probes using dns-7309/dns-test-836bd3ae-0e62-4ea8-a23d-0afe09e3705c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:07:00.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7309" for this suite.
Aug  8 15:07:06.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:07:07.576: INFO: namespace dns-7309 deletion completed in 6.810090918s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:07:07.577: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug  8 15:07:07.956: INFO: Waiting up to 5m0s for pod "pod-05bc0ecc-31d1-4ff3-b348-3663b47a9e25" in namespace "emptydir-1587" to be "success or failure"
Aug  8 15:07:07.975: INFO: Pod "pod-05bc0ecc-31d1-4ff3-b348-3663b47a9e25": Phase="Pending", Reason="", readiness=false. Elapsed: 19.248778ms
Aug  8 15:07:09.995: INFO: Pod "pod-05bc0ecc-31d1-4ff3-b348-3663b47a9e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039589431s
Aug  8 15:07:12.015: INFO: Pod "pod-05bc0ecc-31d1-4ff3-b348-3663b47a9e25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05967169s
STEP: Saw pod success
Aug  8 15:07:12.016: INFO: Pod "pod-05bc0ecc-31d1-4ff3-b348-3663b47a9e25" satisfied condition "success or failure"
Aug  8 15:07:12.035: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-05bc0ecc-31d1-4ff3-b348-3663b47a9e25 container test-container: <nil>
STEP: delete the pod
Aug  8 15:07:12.092: INFO: Waiting for pod pod-05bc0ecc-31d1-4ff3-b348-3663b47a9e25 to disappear
Aug  8 15:07:12.112: INFO: Pod pod-05bc0ecc-31d1-4ff3-b348-3663b47a9e25 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:07:12.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1587" for this suite.
Aug  8 15:07:18.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:07:18.923: INFO: namespace emptydir-1587 deletion completed in 6.791264742s
•SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:07:18.923: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:07:19.287: INFO: (0) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.139589ms)
Aug  8 15:07:19.330: INFO: (1) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 43.297117ms)
Aug  8 15:07:19.354: INFO: (2) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.241741ms)
Aug  8 15:07:19.377: INFO: (3) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 22.240529ms)
Aug  8 15:07:19.398: INFO: (4) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.593282ms)
Aug  8 15:07:19.420: INFO: (5) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.806244ms)
Aug  8 15:07:19.442: INFO: (6) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 22.268248ms)
Aug  8 15:07:19.464: INFO: (7) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.462251ms)
Aug  8 15:07:19.486: INFO: (8) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.597573ms)
Aug  8 15:07:19.507: INFO: (9) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.392101ms)
Aug  8 15:07:19.530: INFO: (10) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 22.749673ms)
Aug  8 15:07:19.552: INFO: (11) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.911584ms)
Aug  8 15:07:19.574: INFO: (12) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.842236ms)
Aug  8 15:07:19.595: INFO: (13) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.306906ms)
Aug  8 15:07:19.617: INFO: (14) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 22.049093ms)
Aug  8 15:07:19.638: INFO: (15) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 20.983106ms)
Aug  8 15:07:19.660: INFO: (16) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.35466ms)
Aug  8 15:07:19.682: INFO: (17) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.589649ms)
Aug  8 15:07:19.704: INFO: (18) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.953309ms)
Aug  8 15:07:19.727: INFO: (19) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 23.126752ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:07:19.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7096" for this suite.
Aug  8 15:07:25.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:07:26.550: INFO: namespace proxy-7096 deletion completed in 6.8015669s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:07:26.551: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9732
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f7b3129d-914a-4627-ac9e-4f24c185ceaa
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f7b3129d-914a-4627-ac9e-4f24c185ceaa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:09:00.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9732" for this suite.
Aug  8 15:09:24.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:09:25.191: INFO: namespace projected-9732 deletion completed in 24.779749838s
•SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:09:25.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug  8 15:10:05.620: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0808 15:10:05.620119    4191 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:10:05.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4324" for this suite.
Aug  8 15:10:11.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:10:12.446: INFO: namespace gc-4324 deletion completed in 6.806932058s
•SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:10:12.447: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug  8 15:10:12.780: INFO: Waiting up to 5m0s for pod "pod-8c78f25f-c8d2-4936-b6f5-2c3269099ccb" in namespace "emptydir-4451" to be "success or failure"
Aug  8 15:10:12.799: INFO: Pod "pod-8c78f25f-c8d2-4936-b6f5-2c3269099ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.120773ms
Aug  8 15:10:14.819: INFO: Pod "pod-8c78f25f-c8d2-4936-b6f5-2c3269099ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039014171s
Aug  8 15:10:16.840: INFO: Pod "pod-8c78f25f-c8d2-4936-b6f5-2c3269099ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059606125s
STEP: Saw pod success
Aug  8 15:10:16.840: INFO: Pod "pod-8c78f25f-c8d2-4936-b6f5-2c3269099ccb" satisfied condition "success or failure"
Aug  8 15:10:16.860: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-8c78f25f-c8d2-4936-b6f5-2c3269099ccb container test-container: <nil>
STEP: delete the pod
Aug  8 15:10:16.918: INFO: Waiting for pod pod-8c78f25f-c8d2-4936-b6f5-2c3269099ccb to disappear
Aug  8 15:10:16.937: INFO: Pod pod-8c78f25f-c8d2-4936-b6f5-2c3269099ccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:10:16.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4451" for this suite.
Aug  8 15:10:23.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:10:23.747: INFO: namespace emptydir-4451 deletion completed in 6.789711556s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:10:23.747: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-177
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-513d057a-98a5-4a47-b4fe-cad05c2c529f
STEP: Creating a pod to test consume secrets
Aug  8 15:10:24.090: INFO: Waiting up to 5m0s for pod "pod-secrets-4fdd335c-d1b2-4bc0-9a7c-fdc410523653" in namespace "secrets-177" to be "success or failure"
Aug  8 15:10:24.109: INFO: Pod "pod-secrets-4fdd335c-d1b2-4bc0-9a7c-fdc410523653": Phase="Pending", Reason="", readiness=false. Elapsed: 19.059252ms
Aug  8 15:10:26.129: INFO: Pod "pod-secrets-4fdd335c-d1b2-4bc0-9a7c-fdc410523653": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038897798s
Aug  8 15:10:28.149: INFO: Pod "pod-secrets-4fdd335c-d1b2-4bc0-9a7c-fdc410523653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058920018s
STEP: Saw pod success
Aug  8 15:10:28.149: INFO: Pod "pod-secrets-4fdd335c-d1b2-4bc0-9a7c-fdc410523653" satisfied condition "success or failure"
Aug  8 15:10:28.168: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-secrets-4fdd335c-d1b2-4bc0-9a7c-fdc410523653 container secret-volume-test: <nil>
STEP: delete the pod
Aug  8 15:10:28.224: INFO: Waiting for pod pod-secrets-4fdd335c-d1b2-4bc0-9a7c-fdc410523653 to disappear
Aug  8 15:10:28.246: INFO: Pod pod-secrets-4fdd335c-d1b2-4bc0-9a7c-fdc410523653 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:10:28.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-177" for this suite.
Aug  8 15:10:34.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:10:35.087: INFO: namespace secrets-177 deletion completed in 6.820942452s
•SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:10:35.087: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-42266630-66ce-4a42-bb7a-4656ffdfda1d in namespace container-probe-1331
Aug  8 15:10:37.420: INFO: Started pod busybox-42266630-66ce-4a42-bb7a-4656ffdfda1d in namespace container-probe-1331
STEP: checking the pod's current state and verifying that restartCount is present
Aug  8 15:10:37.442: INFO: Initial restart count of pod busybox-42266630-66ce-4a42-bb7a-4656ffdfda1d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:14:37.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1331" for this suite.
Aug  8 15:14:43.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:14:44.757: INFO: namespace container-probe-1331 deletion completed in 6.828246238s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:14:44.758: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0808 15:14:51.197322    4191 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug  8 15:14:51.197: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:14:51.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5058" for this suite.
Aug  8 15:14:57.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:14:58.026: INFO: namespace gc-5058 deletion completed in 6.806623453s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:14:58.026: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-ff1f0195-9bb4-400b-a592-b6b8272593da
STEP: Creating a pod to test consume secrets
Aug  8 15:14:58.389: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2ccb9cb6-408d-41c9-93de-98b1418a5cd8" in namespace "projected-6862" to be "success or failure"
Aug  8 15:14:58.409: INFO: Pod "pod-projected-secrets-2ccb9cb6-408d-41c9-93de-98b1418a5cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.343971ms
Aug  8 15:15:00.429: INFO: Pod "pod-projected-secrets-2ccb9cb6-408d-41c9-93de-98b1418a5cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039825405s
Aug  8 15:15:02.450: INFO: Pod "pod-projected-secrets-2ccb9cb6-408d-41c9-93de-98b1418a5cd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060516639s
STEP: Saw pod success
Aug  8 15:15:02.450: INFO: Pod "pod-projected-secrets-2ccb9cb6-408d-41c9-93de-98b1418a5cd8" satisfied condition "success or failure"
Aug  8 15:15:02.469: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-secrets-2ccb9cb6-408d-41c9-93de-98b1418a5cd8 container secret-volume-test: <nil>
STEP: delete the pod
Aug  8 15:15:02.523: INFO: Waiting for pod pod-projected-secrets-2ccb9cb6-408d-41c9-93de-98b1418a5cd8 to disappear
Aug  8 15:15:02.542: INFO: Pod pod-projected-secrets-2ccb9cb6-408d-41c9-93de-98b1418a5cd8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:15:02.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6862" for this suite.
Aug  8 15:15:08.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:15:09.412: INFO: namespace projected-6862 deletion completed in 6.849179252s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:15:09.413: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7925
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-8ebdb61e-5ccc-44e9-afa6-e84d4aec9629
STEP: Creating secret with name s-test-opt-upd-be277b3c-9028-44e2-93de-beb132e15f20
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8ebdb61e-5ccc-44e9-afa6-e84d4aec9629
STEP: Updating secret s-test-opt-upd-be277b3c-9028-44e2-93de-beb132e15f20
STEP: Creating secret with name s-test-opt-create-da9abf75-fcc9-444b-93f6-854db9b341a6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:16:39.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7925" for this suite.
Aug  8 15:17:01.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:17:02.271: INFO: namespace secrets-7925 deletion completed in 22.822791979s
•
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:17:02.271: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug  8 15:17:02.628: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-a,UID:0af8f9b5-8eef-47e0-bbef-01930be83a5c,ResourceVersion:5284,Generation:0,CreationTimestamp:2019-08-08 15:17:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  8 15:17:02.628: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-a,UID:0af8f9b5-8eef-47e0-bbef-01930be83a5c,ResourceVersion:5284,Generation:0,CreationTimestamp:2019-08-08 15:17:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug  8 15:17:12.670: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-a,UID:0af8f9b5-8eef-47e0-bbef-01930be83a5c,ResourceVersion:5308,Generation:0,CreationTimestamp:2019-08-08 15:17:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug  8 15:17:12.670: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-a,UID:0af8f9b5-8eef-47e0-bbef-01930be83a5c,ResourceVersion:5308,Generation:0,CreationTimestamp:2019-08-08 15:17:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug  8 15:17:22.710: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-a,UID:0af8f9b5-8eef-47e0-bbef-01930be83a5c,ResourceVersion:5331,Generation:0,CreationTimestamp:2019-08-08 15:17:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  8 15:17:22.710: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-a,UID:0af8f9b5-8eef-47e0-bbef-01930be83a5c,ResourceVersion:5331,Generation:0,CreationTimestamp:2019-08-08 15:17:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug  8 15:17:32.733: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-a,UID:0af8f9b5-8eef-47e0-bbef-01930be83a5c,ResourceVersion:5354,Generation:0,CreationTimestamp:2019-08-08 15:17:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  8 15:17:32.734: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-a,UID:0af8f9b5-8eef-47e0-bbef-01930be83a5c,ResourceVersion:5354,Generation:0,CreationTimestamp:2019-08-08 15:17:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug  8 15:17:42.756: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-b,UID:ca715252-7f34-433f-b548-8794391310d0,ResourceVersion:5405,Generation:0,CreationTimestamp:2019-08-08 15:17:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  8 15:17:42.756: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-b,UID:ca715252-7f34-433f-b548-8794391310d0,ResourceVersion:5405,Generation:0,CreationTimestamp:2019-08-08 15:17:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug  8 15:17:52.780: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-b,UID:ca715252-7f34-433f-b548-8794391310d0,ResourceVersion:5428,Generation:0,CreationTimestamp:2019-08-08 15:17:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  8 15:17:52.780: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6895,SelfLink:/api/v1/namespaces/watch-6895/configmaps/e2e-watch-test-configmap-b,UID:ca715252-7f34-433f-b548-8794391310d0,ResourceVersion:5428,Generation:0,CreationTimestamp:2019-08-08 15:17:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:18:02.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6895" for this suite.
Aug  8 15:18:08.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:18:09.598: INFO: namespace watch-6895 deletion completed in 6.796232862s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:18:09.599: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 15:18:09.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94604ec3-2bc4-4f12-b53a-aae06390edd8" in namespace "downward-api-3189" to be "success or failure"
Aug  8 15:18:09.895: INFO: Pod "downwardapi-volume-94604ec3-2bc4-4f12-b53a-aae06390edd8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.059131ms
Aug  8 15:18:11.915: INFO: Pod "downwardapi-volume-94604ec3-2bc4-4f12-b53a-aae06390edd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039524457s
Aug  8 15:18:13.936: INFO: Pod "downwardapi-volume-94604ec3-2bc4-4f12-b53a-aae06390edd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060758472s
STEP: Saw pod success
Aug  8 15:18:13.937: INFO: Pod "downwardapi-volume-94604ec3-2bc4-4f12-b53a-aae06390edd8" satisfied condition "success or failure"
Aug  8 15:18:13.956: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-94604ec3-2bc4-4f12-b53a-aae06390edd8 container client-container: <nil>
STEP: delete the pod
Aug  8 15:18:14.010: INFO: Waiting for pod downwardapi-volume-94604ec3-2bc4-4f12-b53a-aae06390edd8 to disappear
Aug  8 15:18:14.029: INFO: Pod downwardapi-volume-94604ec3-2bc4-4f12-b53a-aae06390edd8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:18:14.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3189" for this suite.
Aug  8 15:18:20.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:18:20.850: INFO: namespace downward-api-3189 deletion completed in 6.78904693s
•SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:18:20.851: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  8 15:18:21.152: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4071'
Aug  8 15:18:21.577: INFO: stderr: ""
Aug  8 15:18:21.577: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Aug  8 15:18:21.597: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-4071'
Aug  8 15:18:33.911: INFO: stderr: ""
Aug  8 15:18:33.911: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:18:33.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4071" for this suite.
Aug  8 15:18:39.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:18:40.748: INFO: namespace kubectl-4071 deletion completed in 6.816229517s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:18:40.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9452
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:18:41.150: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:18:41.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9452" for this suite.
Aug  8 15:18:47.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:18:48.664: INFO: namespace custom-resource-definition-9452 deletion completed in 6.804678171s
•SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:18:48.665: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  8 15:18:48.955: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6834'
Aug  8 15:18:49.120: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  8 15:18:49.120: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Aug  8 15:18:49.146: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-6834'
Aug  8 15:18:49.323: INFO: stderr: ""
Aug  8 15:18:49.323: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:18:49.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6834" for this suite.
Aug  8 15:18:55.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:18:56.158: INFO: namespace kubectl-6834 deletion completed in 6.815497651s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:18:56.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug  8 15:18:56.485: INFO: Waiting up to 5m0s for pod "pod-fead3004-59f9-4c51-a69e-03dab4212991" in namespace "emptydir-4898" to be "success or failure"
Aug  8 15:18:56.504: INFO: Pod "pod-fead3004-59f9-4c51-a69e-03dab4212991": Phase="Pending", Reason="", readiness=false. Elapsed: 18.654241ms
Aug  8 15:18:58.524: INFO: Pod "pod-fead3004-59f9-4c51-a69e-03dab4212991": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038677083s
Aug  8 15:19:00.544: INFO: Pod "pod-fead3004-59f9-4c51-a69e-03dab4212991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059198243s
STEP: Saw pod success
Aug  8 15:19:00.544: INFO: Pod "pod-fead3004-59f9-4c51-a69e-03dab4212991" satisfied condition "success or failure"
Aug  8 15:19:00.563: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-fead3004-59f9-4c51-a69e-03dab4212991 container test-container: <nil>
STEP: delete the pod
Aug  8 15:19:00.622: INFO: Waiting for pod pod-fead3004-59f9-4c51-a69e-03dab4212991 to disappear
Aug  8 15:19:00.641: INFO: Pod pod-fead3004-59f9-4c51-a69e-03dab4212991 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:19:00.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4898" for this suite.
Aug  8 15:19:06.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:19:07.452: INFO: namespace emptydir-4898 deletion completed in 6.790163908s
•
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:19:07.453: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 15:19:07.783: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bab7dbd6-cbef-4618-bb15-61b326670028" in namespace "projected-8665" to be "success or failure"
Aug  8 15:19:07.802: INFO: Pod "downwardapi-volume-bab7dbd6-cbef-4618-bb15-61b326670028": Phase="Pending", Reason="", readiness=false. Elapsed: 19.259362ms
Aug  8 15:19:09.822: INFO: Pod "downwardapi-volume-bab7dbd6-cbef-4618-bb15-61b326670028": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039532419s
Aug  8 15:19:11.842: INFO: Pod "downwardapi-volume-bab7dbd6-cbef-4618-bb15-61b326670028": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05974456s
STEP: Saw pod success
Aug  8 15:19:11.843: INFO: Pod "downwardapi-volume-bab7dbd6-cbef-4618-bb15-61b326670028" satisfied condition "success or failure"
Aug  8 15:19:11.862: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-bab7dbd6-cbef-4618-bb15-61b326670028 container client-container: <nil>
STEP: delete the pod
Aug  8 15:19:11.915: INFO: Waiting for pod downwardapi-volume-bab7dbd6-cbef-4618-bb15-61b326670028 to disappear
Aug  8 15:19:11.934: INFO: Pod downwardapi-volume-bab7dbd6-cbef-4618-bb15-61b326670028 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:19:11.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8665" for this suite.
Aug  8 15:19:18.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:19:18.748: INFO: namespace projected-8665 deletion completed in 6.794475629s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:19:18.749: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-533
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Aug  8 15:19:19.081: INFO: Waiting up to 5m0s for pod "client-containers-e19297d9-1ee8-4694-bdad-83317b253e1a" in namespace "containers-533" to be "success or failure"
Aug  8 15:19:19.101: INFO: Pod "client-containers-e19297d9-1ee8-4694-bdad-83317b253e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.229522ms
Aug  8 15:19:21.153: INFO: Pod "client-containers-e19297d9-1ee8-4694-bdad-83317b253e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072034232s
Aug  8 15:19:23.175: INFO: Pod "client-containers-e19297d9-1ee8-4694-bdad-83317b253e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093947118s
Aug  8 15:19:25.195: INFO: Pod "client-containers-e19297d9-1ee8-4694-bdad-83317b253e1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.114106114s
STEP: Saw pod success
Aug  8 15:19:25.196: INFO: Pod "client-containers-e19297d9-1ee8-4694-bdad-83317b253e1a" satisfied condition "success or failure"
Aug  8 15:19:25.215: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod client-containers-e19297d9-1ee8-4694-bdad-83317b253e1a container test-container: <nil>
STEP: delete the pod
Aug  8 15:19:25.268: INFO: Waiting for pod client-containers-e19297d9-1ee8-4694-bdad-83317b253e1a to disappear
Aug  8 15:19:25.287: INFO: Pod client-containers-e19297d9-1ee8-4694-bdad-83317b253e1a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:19:25.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-533" for this suite.
Aug  8 15:19:31.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:19:32.118: INFO: namespace containers-533 deletion completed in 6.81039206s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:19:32.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4654
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Aug  8 15:19:32.469: INFO: Waiting up to 5m0s for pod "pod-878f24f1-1e25-4c4e-8788-0034b6023800" in namespace "emptydir-4654" to be "success or failure"
Aug  8 15:19:32.491: INFO: Pod "pod-878f24f1-1e25-4c4e-8788-0034b6023800": Phase="Pending", Reason="", readiness=false. Elapsed: 21.644648ms
Aug  8 15:19:34.511: INFO: Pod "pod-878f24f1-1e25-4c4e-8788-0034b6023800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041885182s
Aug  8 15:19:36.531: INFO: Pod "pod-878f24f1-1e25-4c4e-8788-0034b6023800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061986781s
STEP: Saw pod success
Aug  8 15:19:36.531: INFO: Pod "pod-878f24f1-1e25-4c4e-8788-0034b6023800" satisfied condition "success or failure"
Aug  8 15:19:36.550: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-878f24f1-1e25-4c4e-8788-0034b6023800 container test-container: <nil>
STEP: delete the pod
Aug  8 15:19:36.605: INFO: Waiting for pod pod-878f24f1-1e25-4c4e-8788-0034b6023800 to disappear
Aug  8 15:19:36.624: INFO: Pod pod-878f24f1-1e25-4c4e-8788-0034b6023800 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:19:36.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4654" for this suite.
Aug  8 15:19:42.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:19:43.458: INFO: namespace emptydir-4654 deletion completed in 6.813315036s
•SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:19:43.458: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3017
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:19:49.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3017" for this suite.
Aug  8 15:19:55.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:19:56.163: INFO: namespace watch-3017 deletion completed in 6.885485673s
•SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:19:56.164: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9531
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:19:56.549: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug  8 15:19:56.590: INFO: Number of nodes with available pods: 0
Aug  8 15:19:56.590: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug  8 15:19:56.679: INFO: Number of nodes with available pods: 0
Aug  8 15:19:56.679: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:19:57.699: INFO: Number of nodes with available pods: 0
Aug  8 15:19:57.699: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:19:58.700: INFO: Number of nodes with available pods: 0
Aug  8 15:19:58.700: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:19:59.699: INFO: Number of nodes with available pods: 1
Aug  8 15:19:59.700: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug  8 15:19:59.779: INFO: Number of nodes with available pods: 0
Aug  8 15:19:59.779: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug  8 15:19:59.821: INFO: Number of nodes with available pods: 0
Aug  8 15:19:59.821: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:00.841: INFO: Number of nodes with available pods: 0
Aug  8 15:20:00.841: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:01.841: INFO: Number of nodes with available pods: 0
Aug  8 15:20:01.841: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:02.847: INFO: Number of nodes with available pods: 0
Aug  8 15:20:02.847: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:03.841: INFO: Number of nodes with available pods: 0
Aug  8 15:20:03.841: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:04.841: INFO: Number of nodes with available pods: 0
Aug  8 15:20:04.841: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:05.841: INFO: Number of nodes with available pods: 0
Aug  8 15:20:05.842: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:06.849: INFO: Number of nodes with available pods: 0
Aug  8 15:20:06.849: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:07.841: INFO: Number of nodes with available pods: 0
Aug  8 15:20:07.841: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:08.842: INFO: Number of nodes with available pods: 0
Aug  8 15:20:08.842: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 15:20:09.841: INFO: Number of nodes with available pods: 1
Aug  8 15:20:09.841: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9531, will wait for the garbage collector to delete the pods
Aug  8 15:20:09.972: INFO: Deleting DaemonSet.extensions daemon-set took: 22.141674ms
Aug  8 15:20:10.072: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.472732ms
Aug  8 15:20:17.192: INFO: Number of nodes with available pods: 0
Aug  8 15:20:17.192: INFO: Number of running nodes: 0, number of available pods: 0
Aug  8 15:20:17.213: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9531/daemonsets","resourceVersion":"6130"},"items":null}

Aug  8 15:20:17.232: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9531/pods","resourceVersion":"6131"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:20:17.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9531" for this suite.
Aug  8 15:20:23.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:20:24.116: INFO: namespace daemonsets-9531 deletion completed in 6.785376383s
•SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:20:24.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug  8 15:20:24.478: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5209,SelfLink:/api/v1/namespaces/watch-5209/configmaps/e2e-watch-test-label-changed,UID:d1d5a938-7546-4ee3-9dbb-cd7b8f6a5a18,ResourceVersion:6161,Generation:0,CreationTimestamp:2019-08-08 15:20:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  8 15:20:24.478: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5209,SelfLink:/api/v1/namespaces/watch-5209/configmaps/e2e-watch-test-label-changed,UID:d1d5a938-7546-4ee3-9dbb-cd7b8f6a5a18,ResourceVersion:6162,Generation:0,CreationTimestamp:2019-08-08 15:20:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug  8 15:20:24.478: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5209,SelfLink:/api/v1/namespaces/watch-5209/configmaps/e2e-watch-test-label-changed,UID:d1d5a938-7546-4ee3-9dbb-cd7b8f6a5a18,ResourceVersion:6163,Generation:0,CreationTimestamp:2019-08-08 15:20:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug  8 15:20:34.620: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5209,SelfLink:/api/v1/namespaces/watch-5209/configmaps/e2e-watch-test-label-changed,UID:d1d5a938-7546-4ee3-9dbb-cd7b8f6a5a18,ResourceVersion:6187,Generation:0,CreationTimestamp:2019-08-08 15:20:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  8 15:20:34.620: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5209,SelfLink:/api/v1/namespaces/watch-5209/configmaps/e2e-watch-test-label-changed,UID:d1d5a938-7546-4ee3-9dbb-cd7b8f6a5a18,ResourceVersion:6188,Generation:0,CreationTimestamp:2019-08-08 15:20:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug  8 15:20:34.620: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5209,SelfLink:/api/v1/namespaces/watch-5209/configmaps/e2e-watch-test-label-changed,UID:d1d5a938-7546-4ee3-9dbb-cd7b8f6a5a18,ResourceVersion:6189,Generation:0,CreationTimestamp:2019-08-08 15:20:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:20:34.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5209" for this suite.
Aug  8 15:20:40.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:20:41.455: INFO: namespace watch-5209 deletion completed in 6.814575591s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:20:41.455: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Aug  8 15:20:41.754: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6285 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug  8 15:20:45.869: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug  8 15:20:45.869: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:20:47.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6285" for this suite.
Aug  8 15:20:55.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:20:56.748: INFO: namespace kubectl-6285 deletion completed in 8.819214694s
•SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:20:56.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug  8 15:21:01.724: INFO: Successfully updated pod "annotationupdated20ee2b7-d110-46ea-81de-e2a982a9f6a3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:21:03.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5786" for this suite.
Aug  8 15:21:25.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:21:26.622: INFO: namespace downward-api-5786 deletion completed in 22.824233915s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:21:26.623: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2041
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug  8 15:21:26.979: INFO: Waiting up to 5m0s for pod "pod-ce06ac57-2cc4-4638-8f4d-8b96c1ed52a2" in namespace "emptydir-2041" to be "success or failure"
Aug  8 15:21:27.008: INFO: Pod "pod-ce06ac57-2cc4-4638-8f4d-8b96c1ed52a2": Phase="Pending", Reason="", readiness=false. Elapsed: 29.15242ms
Aug  8 15:21:29.028: INFO: Pod "pod-ce06ac57-2cc4-4638-8f4d-8b96c1ed52a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049579604s
Aug  8 15:21:31.048: INFO: Pod "pod-ce06ac57-2cc4-4638-8f4d-8b96c1ed52a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069763047s
STEP: Saw pod success
Aug  8 15:21:31.049: INFO: Pod "pod-ce06ac57-2cc4-4638-8f4d-8b96c1ed52a2" satisfied condition "success or failure"
Aug  8 15:21:31.068: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-ce06ac57-2cc4-4638-8f4d-8b96c1ed52a2 container test-container: <nil>
STEP: delete the pod
Aug  8 15:21:31.126: INFO: Waiting for pod pod-ce06ac57-2cc4-4638-8f4d-8b96c1ed52a2 to disappear
Aug  8 15:21:31.145: INFO: Pod pod-ce06ac57-2cc4-4638-8f4d-8b96c1ed52a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:21:31.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2041" for this suite.
Aug  8 15:21:37.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:21:37.976: INFO: namespace emptydir-2041 deletion completed in 6.811449525s
•SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:21:37.976: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Aug  8 15:21:38.247: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Aug  8 15:21:38.423: INFO: stderr: ""
Aug  8 15:21:38.423: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:21:38.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4070" for this suite.
Aug  8 15:21:44.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:21:45.240: INFO: namespace kubectl-4070 deletion completed in 6.793719234s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:21:45.240: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-06a663d8-bbb6-4501-8fa7-71d87a5ec77e
STEP: Creating a pod to test consume configMaps
Aug  8 15:21:45.595: INFO: Waiting up to 5m0s for pod "pod-configmaps-ddc470df-0d44-4721-9122-30939ba3069a" in namespace "configmap-2843" to be "success or failure"
Aug  8 15:21:45.614: INFO: Pod "pod-configmaps-ddc470df-0d44-4721-9122-30939ba3069a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.156359ms
Aug  8 15:21:47.634: INFO: Pod "pod-configmaps-ddc470df-0d44-4721-9122-30939ba3069a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039532987s
Aug  8 15:21:49.659: INFO: Pod "pod-configmaps-ddc470df-0d44-4721-9122-30939ba3069a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063948073s
STEP: Saw pod success
Aug  8 15:21:49.659: INFO: Pod "pod-configmaps-ddc470df-0d44-4721-9122-30939ba3069a" satisfied condition "success or failure"
Aug  8 15:21:49.678: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-ddc470df-0d44-4721-9122-30939ba3069a container configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 15:21:49.741: INFO: Waiting for pod pod-configmaps-ddc470df-0d44-4721-9122-30939ba3069a to disappear
Aug  8 15:21:49.760: INFO: Pod pod-configmaps-ddc470df-0d44-4721-9122-30939ba3069a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:21:49.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2843" for this suite.
Aug  8 15:21:55.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:21:56.605: INFO: namespace configmap-2843 deletion completed in 6.824585389s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:21:56.605: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4318
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug  8 15:21:59.980: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:22:00.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4318" for this suite.
Aug  8 15:22:06.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:22:06.854: INFO: namespace container-runtime-4318 deletion completed in 6.802069471s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:22:06.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1358
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug  8 15:22:11.315: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:22:11.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1358" for this suite.
Aug  8 15:22:33.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:22:34.210: INFO: namespace replicaset-1358 deletion completed in 22.812051204s
•SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:22:34.210: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1808
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 15:22:34.478: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc656473-7e9b-4488-9e80-9ce9ad9e0ae3" in namespace "projected-1808" to be "success or failure"
Aug  8 15:22:34.498: INFO: Pod "downwardapi-volume-cc656473-7e9b-4488-9e80-9ce9ad9e0ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 19.451656ms
Aug  8 15:22:36.518: INFO: Pod "downwardapi-volume-cc656473-7e9b-4488-9e80-9ce9ad9e0ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03988705s
Aug  8 15:22:38.538: INFO: Pod "downwardapi-volume-cc656473-7e9b-4488-9e80-9ce9ad9e0ae3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05984469s
STEP: Saw pod success
Aug  8 15:22:38.538: INFO: Pod "downwardapi-volume-cc656473-7e9b-4488-9e80-9ce9ad9e0ae3" satisfied condition "success or failure"
Aug  8 15:22:38.558: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-cc656473-7e9b-4488-9e80-9ce9ad9e0ae3 container client-container: <nil>
STEP: delete the pod
Aug  8 15:22:38.623: INFO: Waiting for pod downwardapi-volume-cc656473-7e9b-4488-9e80-9ce9ad9e0ae3 to disappear
Aug  8 15:22:38.643: INFO: Pod downwardapi-volume-cc656473-7e9b-4488-9e80-9ce9ad9e0ae3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:22:38.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1808" for this suite.
Aug  8 15:22:44.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:22:45.473: INFO: namespace projected-1808 deletion completed in 6.810377087s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:22:45.474: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug  8 15:22:45.754: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug  8 15:22:45.794: INFO: Waiting for terminating namespaces to be deleted...
Aug  8 15:22:45.814: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr before test
Aug  8 15:22:45.860: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-695fb4fdcd-cknxk from kube-system started at 2019-08-08 14:54:31 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.860: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Aug  8 15:22:45.861: INFO: kube-proxy-vvfrb from kube-system started at 2019-08-08 14:54:09 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container kube-proxy ready: true, restart count 0
Aug  8 15:22:45.861: INFO: addons-kubernetes-dashboard-5c8d9945bc-spkwl from kube-system started at 2019-08-08 14:54:29 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug  8 15:22:45.861: INFO: coredns-85cc454dd8-2tknm from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container coredns ready: true, restart count 0
Aug  8 15:22:45.861: INFO: calico-node-6w86k from kube-system started at 2019-08-08 14:54:10 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container calico-node ready: true, restart count 0
Aug  8 15:22:45.861: INFO: metrics-server-566847b67f-f2cmb from kube-system started at 2019-08-08 14:54:29 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container metrics-server ready: true, restart count 0
Aug  8 15:22:45.861: INFO: coredns-85cc454dd8-cqd76 from kube-system started at 2019-08-08 14:54:29 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container coredns ready: true, restart count 0
Aug  8 15:22:45.861: INFO: vpn-shoot-89d5dc9c8-84rcr from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container vpn-shoot ready: true, restart count 0
Aug  8 15:22:45.861: INFO: calico-kube-controllers-5f4b46ffb5-zt29p from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug  8 15:22:45.861: INFO: node-exporter-s4q7v from kube-system started at 2019-08-08 14:54:09 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container node-exporter ready: true, restart count 0
Aug  8 15:22:45.861: INFO: addons-nginx-ingress-controller-6496d947df-hzpqv from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug  8 15:22:45.861: INFO: blackbox-exporter-954dd954b-6rgqr from kube-system started at 2019-08-08 14:54:09 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.861: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug  8 15:22:45.861: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk before test
Aug  8 15:22:45.895: INFO: node-exporter-lzqzm from kube-system started at 2019-08-08 14:54:36 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.895: INFO: 	Container node-exporter ready: true, restart count 0
Aug  8 15:22:45.895: INFO: kube-proxy-dxmfq from kube-system started at 2019-08-08 14:54:36 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.895: INFO: 	Container kube-proxy ready: true, restart count 0
Aug  8 15:22:45.895: INFO: calico-node-84fr2 from kube-system started at 2019-08-08 14:54:36 +0000 UTC (1 container statuses recorded)
Aug  8 15:22:45.895: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c4d71eb1-55ad-4bbb-92cc-272947e25b6c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c4d71eb1-55ad-4bbb-92cc-272947e25b6c off the node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c4d71eb1-55ad-4bbb-92cc-272947e25b6c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:22:56.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1383" for this suite.
Aug  8 15:23:06.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:23:07.039: INFO: namespace sched-pred-1383 deletion completed in 10.829566903s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:23:07.040: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug  8 15:23:07.372: INFO: Waiting up to 5m0s for pod "downward-api-0daf2f4d-d0f9-4100-8631-455a3f468973" in namespace "downward-api-4355" to be "success or failure"
Aug  8 15:23:07.392: INFO: Pod "downward-api-0daf2f4d-d0f9-4100-8631-455a3f468973": Phase="Pending", Reason="", readiness=false. Elapsed: 19.098345ms
Aug  8 15:23:09.413: INFO: Pod "downward-api-0daf2f4d-d0f9-4100-8631-455a3f468973": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040107193s
Aug  8 15:23:11.433: INFO: Pod "downward-api-0daf2f4d-d0f9-4100-8631-455a3f468973": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060621741s
STEP: Saw pod success
Aug  8 15:23:11.433: INFO: Pod "downward-api-0daf2f4d-d0f9-4100-8631-455a3f468973" satisfied condition "success or failure"
Aug  8 15:23:11.453: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downward-api-0daf2f4d-d0f9-4100-8631-455a3f468973 container dapi-container: <nil>
STEP: delete the pod
Aug  8 15:23:11.504: INFO: Waiting for pod downward-api-0daf2f4d-d0f9-4100-8631-455a3f468973 to disappear
Aug  8 15:23:11.523: INFO: Pod downward-api-0daf2f4d-d0f9-4100-8631-455a3f468973 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:23:11.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4355" for this suite.
Aug  8 15:23:17.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:23:18.396: INFO: namespace downward-api-4355 deletion completed in 6.852007602s
•
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:23:18.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-ngpmm in namespace proxy-5826
I0808 15:23:18.705794    4191 runners.go:180] Created replication controller with name: proxy-service-ngpmm, namespace: proxy-5826, replica count: 1
I0808 15:23:19.756492    4191 runners.go:180] proxy-service-ngpmm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0808 15:23:20.760736    4191 runners.go:180] proxy-service-ngpmm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0808 15:23:21.761231    4191 runners.go:180] proxy-service-ngpmm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0808 15:23:22.761503    4191 runners.go:180] proxy-service-ngpmm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0808 15:23:23.761881    4191 runners.go:180] proxy-service-ngpmm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0808 15:23:24.762145    4191 runners.go:180] proxy-service-ngpmm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0808 15:23:25.762479    4191 runners.go:180] proxy-service-ngpmm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug  8 15:23:25.782: INFO: setup took 7.133962734s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug  8 15:23:25.847: INFO: (0) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 65.410166ms)
Aug  8 15:23:25.848: INFO: (0) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 65.914698ms)
Aug  8 15:23:25.848: INFO: (0) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 65.84142ms)
Aug  8 15:23:25.848: INFO: (0) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 65.802446ms)
Aug  8 15:23:25.849: INFO: (0) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 67.47924ms)
Aug  8 15:23:25.861: INFO: (0) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 79.3928ms)
Aug  8 15:23:25.861: INFO: (0) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 79.43524ms)
Aug  8 15:23:25.864: INFO: (0) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 82.289072ms)
Aug  8 15:23:25.864: INFO: (0) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 82.29577ms)
Aug  8 15:23:25.864: INFO: (0) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 82.439982ms)
Aug  8 15:23:25.864: INFO: (0) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 82.420404ms)
Aug  8 15:23:25.866: INFO: (0) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 83.871004ms)
Aug  8 15:23:25.866: INFO: (0) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 84.12877ms)
Aug  8 15:23:25.869: INFO: (0) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 86.985539ms)
Aug  8 15:23:25.869: INFO: (0) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 86.908986ms)
Aug  8 15:23:25.887: INFO: (0) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 105.348612ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 21.949419ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 21.99964ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 22.104285ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 22.281303ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.104209ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 22.793842ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 22.898642ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 22.960541ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 23.072462ms)
Aug  8 15:23:25.910: INFO: (1) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 23.141897ms)
Aug  8 15:23:25.911: INFO: (1) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 23.002781ms)
Aug  8 15:23:25.911: INFO: (1) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 23.378912ms)
Aug  8 15:23:25.911: INFO: (1) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 23.805719ms)
Aug  8 15:23:25.911: INFO: (1) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 23.715806ms)
Aug  8 15:23:25.912: INFO: (1) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 24.56746ms)
Aug  8 15:23:25.912: INFO: (1) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 24.563675ms)
Aug  8 15:23:25.934: INFO: (2) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 21.342718ms)
Aug  8 15:23:25.934: INFO: (2) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 21.42458ms)
Aug  8 15:23:25.934: INFO: (2) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 21.543847ms)
Aug  8 15:23:25.934: INFO: (2) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 21.4796ms)
Aug  8 15:23:25.934: INFO: (2) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 21.85298ms)
Aug  8 15:23:25.934: INFO: (2) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 21.834761ms)
Aug  8 15:23:25.934: INFO: (2) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 21.406556ms)
Aug  8 15:23:25.934: INFO: (2) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 21.486987ms)
Aug  8 15:23:25.934: INFO: (2) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 21.466677ms)
Aug  8 15:23:25.936: INFO: (2) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 23.692245ms)
Aug  8 15:23:25.936: INFO: (2) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 23.699389ms)
Aug  8 15:23:25.937: INFO: (2) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 24.953432ms)
Aug  8 15:23:25.937: INFO: (2) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 24.986367ms)
Aug  8 15:23:25.937: INFO: (2) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 24.973912ms)
Aug  8 15:23:25.941: INFO: (2) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 28.397323ms)
Aug  8 15:23:25.941: INFO: (2) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 28.745747ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.69423ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 22.895873ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 22.814412ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.864733ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 22.925727ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 23.066035ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 23.123519ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.979983ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 22.92451ms)
Aug  8 15:23:25.964: INFO: (3) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.91449ms)
Aug  8 15:23:25.966: INFO: (3) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 24.48107ms)
Aug  8 15:23:25.966: INFO: (3) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 24.427065ms)
Aug  8 15:23:25.970: INFO: (3) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 28.575204ms)
Aug  8 15:23:25.970: INFO: (3) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 28.540658ms)
Aug  8 15:23:25.970: INFO: (3) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 28.535837ms)
Aug  8 15:23:25.970: INFO: (3) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 28.866699ms)
Aug  8 15:23:25.992: INFO: (4) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 21.727726ms)
Aug  8 15:23:25.992: INFO: (4) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 21.96743ms)
Aug  8 15:23:25.992: INFO: (4) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 21.824232ms)
Aug  8 15:23:25.992: INFO: (4) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.037639ms)
Aug  8 15:23:25.992: INFO: (4) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 21.877881ms)
Aug  8 15:23:25.992: INFO: (4) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 21.936668ms)
Aug  8 15:23:25.993: INFO: (4) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 22.512599ms)
Aug  8 15:23:25.994: INFO: (4) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 23.126333ms)
Aug  8 15:23:25.998: INFO: (4) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 27.772057ms)
Aug  8 15:23:25.998: INFO: (4) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 27.740822ms)
Aug  8 15:23:25.998: INFO: (4) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 27.860076ms)
Aug  8 15:23:25.998: INFO: (4) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 27.894919ms)
Aug  8 15:23:25.998: INFO: (4) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 27.775473ms)
Aug  8 15:23:25.998: INFO: (4) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 27.773053ms)
Aug  8 15:23:25.999: INFO: (4) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 28.512491ms)
Aug  8 15:23:26.036: INFO: (4) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 65.733384ms)
Aug  8 15:23:26.059: INFO: (5) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 22.478312ms)
Aug  8 15:23:26.059: INFO: (5) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.406947ms)
Aug  8 15:23:26.059: INFO: (5) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.442071ms)
Aug  8 15:23:26.059: INFO: (5) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 22.752287ms)
Aug  8 15:23:26.059: INFO: (5) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.483096ms)
Aug  8 15:23:26.060: INFO: (5) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 23.100905ms)
Aug  8 15:23:26.060: INFO: (5) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 23.061867ms)
Aug  8 15:23:26.060: INFO: (5) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.886972ms)
Aug  8 15:23:26.060: INFO: (5) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 23.008105ms)
Aug  8 15:23:26.060: INFO: (5) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 22.975612ms)
Aug  8 15:23:26.060: INFO: (5) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 23.056115ms)
Aug  8 15:23:26.061: INFO: (5) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 23.962902ms)
Aug  8 15:23:26.061: INFO: (5) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 24.010234ms)
Aug  8 15:23:26.061: INFO: (5) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 24.026008ms)
Aug  8 15:23:26.062: INFO: (5) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 24.89383ms)
Aug  8 15:23:26.104: INFO: (5) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 67.224689ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 21.924724ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.029333ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 22.275046ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 21.809521ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.007588ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 21.912145ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 22.908768ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 22.248749ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 22.179673ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 22.889724ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 22.731103ms)
Aug  8 15:23:26.127: INFO: (6) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 23.243116ms)
Aug  8 15:23:26.129: INFO: (6) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 24.369976ms)
Aug  8 15:23:26.129: INFO: (6) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 24.690422ms)
Aug  8 15:23:26.129: INFO: (6) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 24.041286ms)
Aug  8 15:23:26.130: INFO: (6) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 24.734179ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 22.320002ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.816217ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 22.276415ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.817656ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 22.351022ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.77379ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 22.64274ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.301351ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 23.257093ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 23.074351ms)
Aug  8 15:23:26.153: INFO: (7) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 22.73924ms)
Aug  8 15:23:26.154: INFO: (7) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 22.738662ms)
Aug  8 15:23:26.155: INFO: (7) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 24.355887ms)
Aug  8 15:23:26.155: INFO: (7) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 24.092122ms)
Aug  8 15:23:26.156: INFO: (7) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 25.233878ms)
Aug  8 15:23:26.156: INFO: (7) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 25.590292ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 21.971302ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.126684ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 21.959678ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 22.433198ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 21.840556ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.608295ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 21.852783ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.253848ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.183065ms)
Aug  8 15:23:26.179: INFO: (8) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 21.939616ms)
Aug  8 15:23:26.181: INFO: (8) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 23.354406ms)
Aug  8 15:23:26.181: INFO: (8) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 23.559598ms)
Aug  8 15:23:26.181: INFO: (8) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 24.005304ms)
Aug  8 15:23:26.181: INFO: (8) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 23.34506ms)
Aug  8 15:23:26.181: INFO: (8) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 24.70915ms)
Aug  8 15:23:26.182: INFO: (8) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 25.840926ms)
Aug  8 15:23:26.205: INFO: (9) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 21.499197ms)
Aug  8 15:23:26.205: INFO: (9) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 21.967171ms)
Aug  8 15:23:26.205: INFO: (9) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 22.00347ms)
Aug  8 15:23:26.205: INFO: (9) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 21.884515ms)
Aug  8 15:23:26.205: INFO: (9) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 22.185867ms)
Aug  8 15:23:26.205: INFO: (9) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 22.079828ms)
Aug  8 15:23:26.206: INFO: (9) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.747811ms)
Aug  8 15:23:26.206: INFO: (9) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 23.113046ms)
Aug  8 15:23:26.206: INFO: (9) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.783759ms)
Aug  8 15:23:26.206: INFO: (9) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.778414ms)
Aug  8 15:23:26.207: INFO: (9) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 24.179339ms)
Aug  8 15:23:26.208: INFO: (9) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 24.779177ms)
Aug  8 15:23:26.208: INFO: (9) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 24.393189ms)
Aug  8 15:23:26.208: INFO: (9) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 24.909216ms)
Aug  8 15:23:26.208: INFO: (9) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 24.841627ms)
Aug  8 15:23:26.208: INFO: (9) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 24.62813ms)
Aug  8 15:23:26.231: INFO: (10) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.085528ms)
Aug  8 15:23:26.231: INFO: (10) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 22.353761ms)
Aug  8 15:23:26.231: INFO: (10) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 21.996894ms)
Aug  8 15:23:26.231: INFO: (10) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 22.450889ms)
Aug  8 15:23:26.231: INFO: (10) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 22.186399ms)
Aug  8 15:23:26.231: INFO: (10) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 22.214039ms)
Aug  8 15:23:26.231: INFO: (10) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.322258ms)
Aug  8 15:23:26.231: INFO: (10) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.494004ms)
Aug  8 15:23:26.231: INFO: (10) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.62542ms)
Aug  8 15:23:26.233: INFO: (10) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 24.432811ms)
Aug  8 15:23:26.233: INFO: (10) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 24.392457ms)
Aug  8 15:23:26.233: INFO: (10) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 24.77608ms)
Aug  8 15:23:26.233: INFO: (10) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 25.455615ms)
Aug  8 15:23:26.234: INFO: (10) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 25.770841ms)
Aug  8 15:23:26.234: INFO: (10) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 26.129463ms)
Aug  8 15:23:26.234: INFO: (10) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 25.921972ms)
Aug  8 15:23:26.257: INFO: (11) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 21.997993ms)
Aug  8 15:23:26.257: INFO: (11) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 22.130325ms)
Aug  8 15:23:26.257: INFO: (11) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 22.2645ms)
Aug  8 15:23:26.257: INFO: (11) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.326087ms)
Aug  8 15:23:26.257: INFO: (11) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.137648ms)
Aug  8 15:23:26.257: INFO: (11) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.639444ms)
Aug  8 15:23:26.260: INFO: (11) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 25.381653ms)
Aug  8 15:23:26.260: INFO: (11) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 25.633716ms)
Aug  8 15:23:26.260: INFO: (11) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 25.290857ms)
Aug  8 15:23:26.261: INFO: (11) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 25.426142ms)
Aug  8 15:23:26.261: INFO: (11) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 25.956405ms)
Aug  8 15:23:26.261: INFO: (11) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 26.073471ms)
Aug  8 15:23:26.262: INFO: (11) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 27.227311ms)
Aug  8 15:23:26.262: INFO: (11) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 26.976665ms)
Aug  8 15:23:26.262: INFO: (11) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 26.97467ms)
Aug  8 15:23:26.262: INFO: (11) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 27.437202ms)
Aug  8 15:23:26.286: INFO: (12) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 23.22643ms)
Aug  8 15:23:26.286: INFO: (12) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 22.706011ms)
Aug  8 15:23:26.286: INFO: (12) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 23.214282ms)
Aug  8 15:23:26.286: INFO: (12) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 23.175741ms)
Aug  8 15:23:26.287: INFO: (12) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 23.524616ms)
Aug  8 15:23:26.287: INFO: (12) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 23.873447ms)
Aug  8 15:23:26.287: INFO: (12) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 23.63749ms)
Aug  8 15:23:26.287: INFO: (12) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 24.225136ms)
Aug  8 15:23:26.287: INFO: (12) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 23.47667ms)
Aug  8 15:23:26.291: INFO: (12) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 27.926914ms)
Aug  8 15:23:26.291: INFO: (12) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 27.909105ms)
Aug  8 15:23:26.291: INFO: (12) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 27.727779ms)
Aug  8 15:23:26.292: INFO: (12) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 28.674729ms)
Aug  8 15:23:26.295: INFO: (12) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 31.994059ms)
Aug  8 15:23:26.295: INFO: (12) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 31.334708ms)
Aug  8 15:23:26.295: INFO: (12) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 31.796175ms)
Aug  8 15:23:26.318: INFO: (13) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.370386ms)
Aug  8 15:23:26.318: INFO: (13) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.72442ms)
Aug  8 15:23:26.318: INFO: (13) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 23.661629ms)
Aug  8 15:23:26.318: INFO: (13) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 23.145724ms)
Aug  8 15:23:26.319: INFO: (13) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 22.883962ms)
Aug  8 15:23:26.319: INFO: (13) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 23.449173ms)
Aug  8 15:23:26.319: INFO: (13) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 23.620225ms)
Aug  8 15:23:26.319: INFO: (13) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 23.712949ms)
Aug  8 15:23:26.319: INFO: (13) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 23.557148ms)
Aug  8 15:23:26.319: INFO: (13) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 23.815774ms)
Aug  8 15:23:26.319: INFO: (13) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 24.142042ms)
Aug  8 15:23:26.320: INFO: (13) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 24.337051ms)
Aug  8 15:23:26.321: INFO: (13) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 25.075462ms)
Aug  8 15:23:26.322: INFO: (13) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 26.58388ms)
Aug  8 15:23:26.322: INFO: (13) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 26.275924ms)
Aug  8 15:23:26.322: INFO: (13) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 26.736851ms)
Aug  8 15:23:26.347: INFO: (14) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 24.103346ms)
Aug  8 15:23:26.347: INFO: (14) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 23.897529ms)
Aug  8 15:23:26.347: INFO: (14) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 24.619421ms)
Aug  8 15:23:26.347: INFO: (14) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 24.912533ms)
Aug  8 15:23:26.347: INFO: (14) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 23.980681ms)
Aug  8 15:23:26.347: INFO: (14) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 24.514357ms)
Aug  8 15:23:26.347: INFO: (14) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 24.399502ms)
Aug  8 15:23:26.347: INFO: (14) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 24.332157ms)
Aug  8 15:23:26.347: INFO: (14) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 25.038001ms)
Aug  8 15:23:26.349: INFO: (14) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 26.742681ms)
Aug  8 15:23:26.349: INFO: (14) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 26.307817ms)
Aug  8 15:23:26.349: INFO: (14) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 26.864714ms)
Aug  8 15:23:26.349: INFO: (14) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 26.800818ms)
Aug  8 15:23:26.349: INFO: (14) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 26.74875ms)
Aug  8 15:23:26.349: INFO: (14) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 27.073707ms)
Aug  8 15:23:26.351: INFO: (14) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 28.613896ms)
Aug  8 15:23:26.373: INFO: (15) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 21.77112ms)
Aug  8 15:23:26.373: INFO: (15) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 21.854954ms)
Aug  8 15:23:26.373: INFO: (15) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 22.032766ms)
Aug  8 15:23:26.373: INFO: (15) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 22.116721ms)
Aug  8 15:23:26.373: INFO: (15) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 22.079223ms)
Aug  8 15:23:26.374: INFO: (15) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.166676ms)
Aug  8 15:23:26.374: INFO: (15) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.1448ms)
Aug  8 15:23:26.374: INFO: (15) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 22.363999ms)
Aug  8 15:23:26.374: INFO: (15) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 22.142365ms)
Aug  8 15:23:26.374: INFO: (15) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 22.264744ms)
Aug  8 15:23:26.375: INFO: (15) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 23.86508ms)
Aug  8 15:23:26.375: INFO: (15) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 24.047065ms)
Aug  8 15:23:26.375: INFO: (15) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 23.977258ms)
Aug  8 15:23:26.375: INFO: (15) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 24.056619ms)
Aug  8 15:23:26.377: INFO: (15) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 25.734357ms)
Aug  8 15:23:26.377: INFO: (15) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 25.690923ms)
Aug  8 15:23:26.399: INFO: (16) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 21.841789ms)
Aug  8 15:23:26.399: INFO: (16) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 21.773974ms)
Aug  8 15:23:26.399: INFO: (16) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 21.806606ms)
Aug  8 15:23:26.399: INFO: (16) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 21.737304ms)
Aug  8 15:23:26.399: INFO: (16) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 21.797815ms)
Aug  8 15:23:26.399: INFO: (16) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 21.923923ms)
Aug  8 15:23:26.400: INFO: (16) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 22.670039ms)
Aug  8 15:23:26.400: INFO: (16) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 22.676889ms)
Aug  8 15:23:26.400: INFO: (16) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 23.001186ms)
Aug  8 15:23:26.402: INFO: (16) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 24.211041ms)
Aug  8 15:23:26.402: INFO: (16) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 24.259724ms)
Aug  8 15:23:26.402: INFO: (16) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 24.265966ms)
Aug  8 15:23:26.402: INFO: (16) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 24.538367ms)
Aug  8 15:23:26.402: INFO: (16) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 24.573955ms)
Aug  8 15:23:26.402: INFO: (16) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 24.624818ms)
Aug  8 15:23:26.404: INFO: (16) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 26.592986ms)
Aug  8 15:23:26.428: INFO: (17) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 23.856301ms)
Aug  8 15:23:26.428: INFO: (17) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 23.854023ms)
Aug  8 15:23:26.428: INFO: (17) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 23.871214ms)
Aug  8 15:23:26.428: INFO: (17) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 23.889908ms)
Aug  8 15:23:26.428: INFO: (17) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 23.858894ms)
Aug  8 15:23:26.429: INFO: (17) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 24.456741ms)
Aug  8 15:23:26.429: INFO: (17) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 24.519999ms)
Aug  8 15:23:26.429: INFO: (17) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 24.755738ms)
Aug  8 15:23:26.429: INFO: (17) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 24.606037ms)
Aug  8 15:23:26.429: INFO: (17) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 24.663287ms)
Aug  8 15:23:26.432: INFO: (17) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 27.80946ms)
Aug  8 15:23:26.432: INFO: (17) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 27.874262ms)
Aug  8 15:23:26.432: INFO: (17) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 27.944961ms)
Aug  8 15:23:26.432: INFO: (17) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 28.385997ms)
Aug  8 15:23:26.434: INFO: (17) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 29.292085ms)
Aug  8 15:23:26.434: INFO: (17) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 30.308654ms)
Aug  8 15:23:26.460: INFO: (18) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 25.176183ms)
Aug  8 15:23:26.460: INFO: (18) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 25.879691ms)
Aug  8 15:23:26.460: INFO: (18) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 25.741101ms)
Aug  8 15:23:26.460: INFO: (18) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 25.723319ms)
Aug  8 15:23:26.460: INFO: (18) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 25.734801ms)
Aug  8 15:23:26.460: INFO: (18) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 25.803629ms)
Aug  8 15:23:26.461: INFO: (18) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 25.848908ms)
Aug  8 15:23:26.461: INFO: (18) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 25.801472ms)
Aug  8 15:23:26.461: INFO: (18) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 26.233527ms)
Aug  8 15:23:26.461: INFO: (18) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 26.211086ms)
Aug  8 15:23:26.461: INFO: (18) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 26.210436ms)
Aug  8 15:23:26.461: INFO: (18) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 26.271546ms)
Aug  8 15:23:26.505: INFO: (18) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 70.50708ms)
Aug  8 15:23:26.505: INFO: (18) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 70.645469ms)
Aug  8 15:23:26.505: INFO: (18) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 70.618777ms)
Aug  8 15:23:26.505: INFO: (18) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 70.6357ms)
Aug  8 15:23:26.529: INFO: (19) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname2/proxy/: tls qux (200; 23.450201ms)
Aug  8 15:23:26.529: INFO: (19) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">test<... (200; 23.413149ms)
Aug  8 15:23:26.529: INFO: (19) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:1080/proxy/rewriteme">... (200; 23.688079ms)
Aug  8 15:23:26.529: INFO: (19) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:443/proxy/tlsrewritem... (200; 23.648212ms)
Aug  8 15:23:26.529: INFO: (19) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:462/proxy/: tls qux (200; 23.582729ms)
Aug  8 15:23:26.529: INFO: (19) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 23.752705ms)
Aug  8 15:23:26.530: INFO: (19) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:162/proxy/: bar (200; 24.564016ms)
Aug  8 15:23:26.530: INFO: (19) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/: <a href="/api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm/proxy/rewriteme">test</a> (200; 24.601627ms)
Aug  8 15:23:26.530: INFO: (19) /api/v1/namespaces/proxy-5826/pods/https:proxy-service-ngpmm-6gxfm:460/proxy/: tls baz (200; 24.661733ms)
Aug  8 15:23:26.530: INFO: (19) /api/v1/namespaces/proxy-5826/pods/http:proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 24.710969ms)
Aug  8 15:23:26.530: INFO: (19) /api/v1/namespaces/proxy-5826/services/https:proxy-service-ngpmm:tlsportname1/proxy/: tls baz (200; 24.69754ms)
Aug  8 15:23:26.530: INFO: (19) /api/v1/namespaces/proxy-5826/pods/proxy-service-ngpmm-6gxfm:160/proxy/: foo (200; 24.667838ms)
Aug  8 15:23:26.531: INFO: (19) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname2/proxy/: bar (200; 25.655666ms)
Aug  8 15:23:26.531: INFO: (19) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname2/proxy/: bar (200; 25.817066ms)
Aug  8 15:23:26.574: INFO: (19) /api/v1/namespaces/proxy-5826/services/http:proxy-service-ngpmm:portname1/proxy/: foo (200; 68.810533ms)
Aug  8 15:23:26.574: INFO: (19) /api/v1/namespaces/proxy-5826/services/proxy-service-ngpmm:portname1/proxy/: foo (200; 68.783732ms)
STEP: deleting ReplicationController proxy-service-ngpmm in namespace proxy-5826, will wait for the garbage collector to delete the pods
Aug  8 15:23:26.668: INFO: Deleting ReplicationController proxy-service-ngpmm took: 23.344578ms
Aug  8 15:23:26.768: INFO: Terminating ReplicationController proxy-service-ngpmm pods took: 100.403598ms
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:23:30.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5826" for this suite.
Aug  8 15:23:36.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:23:37.023: INFO: namespace proxy-5826 deletion completed in 6.833935254s
•SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:23:37.023: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4822
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug  8 15:23:49.416: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:49.416: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:49.888: INFO: Exec stderr: ""
Aug  8 15:23:49.888: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:49.888: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:50.386: INFO: Exec stderr: ""
Aug  8 15:23:50.386: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:50.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:50.858: INFO: Exec stderr: ""
Aug  8 15:23:50.858: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:50.858: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:51.325: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug  8 15:23:51.325: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:51.325: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:51.805: INFO: Exec stderr: ""
Aug  8 15:23:51.805: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:51.805: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:52.409: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug  8 15:23:52.409: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:52.409: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:52.877: INFO: Exec stderr: ""
Aug  8 15:23:52.877: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:52.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:53.435: INFO: Exec stderr: ""
Aug  8 15:23:53.435: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:53.435: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:53.882: INFO: Exec stderr: ""
Aug  8 15:23:53.882: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4822 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:23:53.882: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:23:54.470: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:23:54.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4822" for this suite.
Aug  8 15:24:48.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:24:49.327: INFO: namespace e2e-kubelet-etc-hosts-4822 deletion completed in 54.835621467s
•SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:24:49.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8068
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug  8 15:24:49.737: INFO: Found 0 stateful pods, waiting for 3
Aug  8 15:24:59.759: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 15:24:59.759: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 15:24:59.759: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug  8 15:24:59.868: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug  8 15:24:59.975: INFO: Updating stateful set ss2
Aug  8 15:25:00.014: INFO: Waiting for Pod statefulset-8068/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug  8 15:25:10.053: INFO: Waiting for Pod statefulset-8068/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Aug  8 15:25:20.141: INFO: Found 2 stateful pods, waiting for 3
Aug  8 15:25:30.163: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 15:25:30.163: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 15:25:30.163: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug  8 15:25:30.258: INFO: Updating stateful set ss2
Aug  8 15:25:30.299: INFO: Waiting for Pod statefulset-8068/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug  8 15:25:40.390: INFO: Updating stateful set ss2
Aug  8 15:25:40.429: INFO: Waiting for StatefulSet statefulset-8068/ss2 to complete update
Aug  8 15:25:40.429: INFO: Waiting for Pod statefulset-8068/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug  8 15:25:50.469: INFO: Waiting for StatefulSet statefulset-8068/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug  8 15:26:00.470: INFO: Deleting all statefulset in ns statefulset-8068
Aug  8 15:26:00.489: INFO: Scaling statefulset ss2 to 0
Aug  8 15:26:30.572: INFO: Waiting for statefulset status.replicas updated to 0
Aug  8 15:26:30.591: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:26:30.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8068" for this suite.
Aug  8 15:26:36.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:26:37.474: INFO: namespace statefulset-8068 deletion completed in 6.801552116s
•
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:26:37.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:26:41.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2571" for this suite.
Aug  8 15:27:21.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:27:22.673: INFO: namespace kubelet-test-2571 deletion completed in 40.794584772s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:27:22.674: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1103
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-62711f85-d364-4124-b09e-a66c99937770
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:27:27.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1103" for this suite.
Aug  8 15:27:51.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:27:52.019: INFO: namespace configmap-1103 deletion completed in 24.808756257s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:27:52.020: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5221
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug  8 15:27:52.375: INFO: Waiting up to 5m0s for pod "pod-8d05d960-6747-4b2c-96eb-b6badb7dfc22" in namespace "emptydir-5221" to be "success or failure"
Aug  8 15:27:52.394: INFO: Pod "pod-8d05d960-6747-4b2c-96eb-b6badb7dfc22": Phase="Pending", Reason="", readiness=false. Elapsed: 19.159963ms
Aug  8 15:27:54.414: INFO: Pod "pod-8d05d960-6747-4b2c-96eb-b6badb7dfc22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039790444s
Aug  8 15:27:56.435: INFO: Pod "pod-8d05d960-6747-4b2c-96eb-b6badb7dfc22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060629445s
STEP: Saw pod success
Aug  8 15:27:56.435: INFO: Pod "pod-8d05d960-6747-4b2c-96eb-b6badb7dfc22" satisfied condition "success or failure"
Aug  8 15:27:56.455: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-8d05d960-6747-4b2c-96eb-b6badb7dfc22 container test-container: <nil>
STEP: delete the pod
Aug  8 15:27:56.531: INFO: Waiting for pod pod-8d05d960-6747-4b2c-96eb-b6badb7dfc22 to disappear
Aug  8 15:27:56.550: INFO: Pod pod-8d05d960-6747-4b2c-96eb-b6badb7dfc22 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:27:56.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5221" for this suite.
Aug  8 15:28:02.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:28:03.364: INFO: namespace emptydir-5221 deletion completed in 6.793925317s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:28:03.364: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug  8 15:28:08.306: INFO: Successfully updated pod "labelsupdate5b69e560-ef8c-4ae1-afef-a6b09337acc5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:28:10.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-266" for this suite.
Aug  8 15:28:32.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:28:33.219: INFO: namespace downward-api-266 deletion completed in 22.828359609s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:28:33.220: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-05388215-3d54-4afd-b5f3-4b394c4e10b3
STEP: Creating a pod to test consume secrets
Aug  8 15:28:33.605: INFO: Waiting up to 5m0s for pod "pod-secrets-b424ccdb-6069-4286-a293-7fae30337fd0" in namespace "secrets-5589" to be "success or failure"
Aug  8 15:28:33.625: INFO: Pod "pod-secrets-b424ccdb-6069-4286-a293-7fae30337fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.206333ms
Aug  8 15:28:35.645: INFO: Pod "pod-secrets-b424ccdb-6069-4286-a293-7fae30337fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040834214s
Aug  8 15:28:37.666: INFO: Pod "pod-secrets-b424ccdb-6069-4286-a293-7fae30337fd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061363092s
STEP: Saw pod success
Aug  8 15:28:37.666: INFO: Pod "pod-secrets-b424ccdb-6069-4286-a293-7fae30337fd0" satisfied condition "success or failure"
Aug  8 15:28:37.685: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-secrets-b424ccdb-6069-4286-a293-7fae30337fd0 container secret-volume-test: <nil>
STEP: delete the pod
Aug  8 15:28:37.738: INFO: Waiting for pod pod-secrets-b424ccdb-6069-4286-a293-7fae30337fd0 to disappear
Aug  8 15:28:37.757: INFO: Pod pod-secrets-b424ccdb-6069-4286-a293-7fae30337fd0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:28:37.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5589" for this suite.
Aug  8 15:28:43.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:28:44.590: INFO: namespace secrets-5589 deletion completed in 6.812732169s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:28:44.591: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9795
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9795
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9795
STEP: Creating statefulset with conflicting port in namespace statefulset-9795
STEP: Waiting until pod test-pod will start running in namespace statefulset-9795
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9795
Aug  8 15:28:50.996: INFO: Observed stateful pod in namespace: statefulset-9795, name: ss-0, uid: 7232c77d-1c39-4aa0-9229-dabe3c8412fa, status phase: Pending. Waiting for statefulset controller to delete.
Aug  8 15:28:57.108: INFO: Observed stateful pod in namespace: statefulset-9795, name: ss-0, uid: 7232c77d-1c39-4aa0-9229-dabe3c8412fa, status phase: Failed. Waiting for statefulset controller to delete.
Aug  8 15:28:57.112: INFO: Observed stateful pod in namespace: statefulset-9795, name: ss-0, uid: 7232c77d-1c39-4aa0-9229-dabe3c8412fa, status phase: Failed. Waiting for statefulset controller to delete.
Aug  8 15:28:57.115: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9795
STEP: Removing pod with conflicting port in namespace statefulset-9795
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9795 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug  8 15:29:03.231: INFO: Deleting all statefulset in ns statefulset-9795
Aug  8 15:29:03.251: INFO: Scaling statefulset ss to 0
Aug  8 15:29:23.332: INFO: Waiting for statefulset status.replicas updated to 0
Aug  8 15:29:23.353: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:29:23.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9795" for this suite.
Aug  8 15:29:29.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:29:30.290: INFO: namespace statefulset-9795 deletion completed in 6.855652067s
•SSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:29:30.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug  8 15:29:36.649: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-d88d2c97-9bae-4e15-b23d-09f96369c529,GenerateName:,Namespace:events-1394,SelfLink:/api/v1/namespaces/events-1394/pods/send-events-d88d2c97-9bae-4e15-b23d-09f96369c529,UID:4c5edfd6-3fa4-4924-9dd8-b9b9efdee830,ResourceVersion:8307,Generation:0,CreationTimestamp:2019-08-08 15:29:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 547502093,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.56/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-s6bs5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s6bs5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-s6bs5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003946540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0039465f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:29:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:29:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:29:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:29:30 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.56,StartTime:2019-08-08 15:29:30 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-08 15:29:34 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://066ce505fb358f4b36493fe8e9f4dcf6a391163226448f233d86968fea54e75f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug  8 15:29:38.671: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug  8 15:29:40.691: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:29:40.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1394" for this suite.
Aug  8 15:30:28.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:30:29.588: INFO: namespace events-1394 deletion completed in 48.843959615s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:30:29.588: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:30:29.845: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug  8 15:30:29.885: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug  8 15:30:33.929: INFO: Creating deployment "test-rolling-update-deployment"
Aug  8 15:30:33.950: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug  8 15:30:34.000: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Aug  8 15:30:36.043: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug  8 15:30:36.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700875033, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700875033, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700875033, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700875033, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 15:30:38.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700875033, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700875033, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700875033, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700875033, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 15:30:40.083: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug  8 15:30:40.145: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-3559,SelfLink:/apis/apps/v1/namespaces/deployment-3559/deployments/test-rolling-update-deployment,UID:9b2ba45f-18a5-4ba5-9f2a-a57e76162112,ResourceVersion:8522,Generation:1,CreationTimestamp:2019-08-08 15:30:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-08 15:30:33 +0000 UTC 2019-08-08 15:30:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-08 15:30:38 +0000 UTC 2019-08-08 15:30:33 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug  8 15:30:40.171: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-3559,SelfLink:/apis/apps/v1/namespaces/deployment-3559/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:433e16b3-1baf-4cfd-b791-05369c1e13e9,ResourceVersion:8515,Generation:1,CreationTimestamp:2019-08-08 15:30:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9b2ba45f-18a5-4ba5-9f2a-a57e76162112 0xc0011175f7 0xc0011175f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug  8 15:30:40.171: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug  8 15:30:40.172: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-3559,SelfLink:/apis/apps/v1/namespaces/deployment-3559/replicasets/test-rolling-update-controller,UID:63f70479-e749-41a9-87d3-ab74527f3039,ResourceVersion:8521,Generation:2,CreationTimestamp:2019-08-08 15:30:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9b2ba45f-18a5-4ba5-9f2a-a57e76162112 0xc00111751f 0xc001117530}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  8 15:30:40.192: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-hbl95" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-hbl95,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-3559,SelfLink:/api/v1/namespaces/deployment-3559/pods/test-rolling-update-deployment-79f6b9d75c-hbl95,UID:576425d2-d4cc-474a-a019-5763eac0ec15,ResourceVersion:8514,Generation:0,CreationTimestamp:2019-08-08 15:30:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.58/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 433e16b3-1baf-4cfd-b791-05369c1e13e9 0xc001117ed7 0xc001117ed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ps9pm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ps9pm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-ps9pm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001117f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001117f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:30:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:30:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:30:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:30:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.58,StartTime:2019-08-08 15:30:33 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-08 15:30:38 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://770b8c398d64f534383bb36086e74bb2ca649b650d9ab075e4691747c32c1de9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:30:40.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3559" for this suite.
Aug  8 15:30:46.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:30:47.035: INFO: namespace deployment-3559 deletion completed in 6.823065006s
•SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:30:47.035: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:30:47.394: INFO: (0) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.016691ms)
Aug  8 15:30:47.438: INFO: (1) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 44.03902ms)
Aug  8 15:30:47.460: INFO: (2) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.421974ms)
Aug  8 15:30:47.481: INFO: (3) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.412162ms)
Aug  8 15:30:47.503: INFO: (4) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.260092ms)
Aug  8 15:30:47.524: INFO: (5) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.477961ms)
Aug  8 15:30:47.546: INFO: (6) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.39326ms)
Aug  8 15:30:47.570: INFO: (7) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.337336ms)
Aug  8 15:30:47.593: INFO: (8) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 23.021665ms)
Aug  8 15:30:47.616: INFO: (9) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 23.036074ms)
Aug  8 15:30:47.638: INFO: (10) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.241902ms)
Aug  8 15:30:47.659: INFO: (11) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.868653ms)
Aug  8 15:30:47.681: INFO: (12) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.564248ms)
Aug  8 15:30:47.703: INFO: (13) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.449661ms)
Aug  8 15:30:47.732: INFO: (14) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 28.813793ms)
Aug  8 15:30:47.753: INFO: (15) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.049701ms)
Aug  8 15:30:47.775: INFO: (16) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 22.650601ms)
Aug  8 15:30:47.796: INFO: (17) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.023546ms)
Aug  8 15:30:47.818: INFO: (18) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.121348ms)
Aug  8 15:30:47.840: INFO: (19) /api/v1/nodes/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.84626ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:30:47.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3661" for this suite.
Aug  8 15:30:53.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:30:54.695: INFO: namespace proxy-3661 deletion completed in 6.835698167s
•SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:30:54.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug  8 15:30:59.102: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:30:59.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2387" for this suite.
Aug  8 15:31:05.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:31:05.993: INFO: namespace container-runtime-2387 deletion completed in 6.823807966s
•SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:31:05.993: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug  8 15:31:36.518: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:31:36.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0808 15:31:36.518139    4191 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7789" for this suite.
Aug  8 15:31:42.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:31:43.350: INFO: namespace gc-7789 deletion completed in 6.812091698s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:31:43.350: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-b1af6dff-4551-4d57-ac4d-2e2f6367b72b
STEP: Creating a pod to test consume configMaps
Aug  8 15:31:43.700: INFO: Waiting up to 5m0s for pod "pod-configmaps-edbd4d36-f0de-469f-94cd-d0617c8c6296" in namespace "configmap-3378" to be "success or failure"
Aug  8 15:31:43.720: INFO: Pod "pod-configmaps-edbd4d36-f0de-469f-94cd-d0617c8c6296": Phase="Pending", Reason="", readiness=false. Elapsed: 19.25538ms
Aug  8 15:31:45.740: INFO: Pod "pod-configmaps-edbd4d36-f0de-469f-94cd-d0617c8c6296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039737511s
Aug  8 15:31:47.761: INFO: Pod "pod-configmaps-edbd4d36-f0de-469f-94cd-d0617c8c6296": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060708038s
STEP: Saw pod success
Aug  8 15:31:47.761: INFO: Pod "pod-configmaps-edbd4d36-f0de-469f-94cd-d0617c8c6296" satisfied condition "success or failure"
Aug  8 15:31:47.781: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-edbd4d36-f0de-469f-94cd-d0617c8c6296 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 15:31:47.971: INFO: Waiting for pod pod-configmaps-edbd4d36-f0de-469f-94cd-d0617c8c6296 to disappear
Aug  8 15:31:47.990: INFO: Pod pod-configmaps-edbd4d36-f0de-469f-94cd-d0617c8c6296 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:31:47.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3378" for this suite.
Aug  8 15:31:54.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:31:54.840: INFO: namespace configmap-3378 deletion completed in 6.830483373s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:31:54.840: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2455
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2455
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug  8 15:31:55.146: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug  8 15:32:17.499: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.62 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2455 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:32:17.499: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:32:18.980: INFO: Found all expected endpoints: [netserver-0]
Aug  8 15:32:18.999: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.27 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2455 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:32:18.999: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:32:20.453: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:32:20.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2455" for this suite.
Aug  8 15:32:42.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:32:43.273: INFO: namespace pod-network-test-2455 deletion completed in 22.798634931s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:32:43.273: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6695
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6695
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6695
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6695
Aug  8 15:32:43.647: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug  8 15:32:53.669: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug  8 15:32:53.688: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6695 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 15:32:54.682: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 15:32:54.683: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 15:32:54.683: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  8 15:32:54.703: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug  8 15:33:04.725: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug  8 15:33:04.725: INFO: Waiting for statefulset status.replicas updated to 0
Aug  8 15:33:04.804: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999557s
Aug  8 15:33:05.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980149918s
Aug  8 15:33:06.845: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.959106021s
Aug  8 15:33:07.867: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.938552375s
Aug  8 15:33:08.887: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.917440313s
Aug  8 15:33:09.908: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.896866862s
Aug  8 15:33:10.929: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.876323466s
Aug  8 15:33:11.949: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.855414401s
Aug  8 15:33:12.970: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.834876668s
Aug  8 15:33:13.990: INFO: Verifying statefulset ss doesn't scale past 1 for another 814.490529ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6695
Aug  8 15:33:15.011: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6695 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:33:15.622: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug  8 15:33:15.623: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  8 15:33:15.623: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  8 15:33:15.643: INFO: Found 1 stateful pods, waiting for 3
Aug  8 15:33:25.664: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 15:33:25.664: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 15:33:25.664: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug  8 15:33:25.703: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6695 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 15:33:26.348: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 15:33:26.348: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 15:33:26.348: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  8 15:33:26.348: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6695 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 15:33:27.038: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 15:33:27.038: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 15:33:27.038: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  8 15:33:27.039: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6695 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 15:33:27.705: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 15:33:27.705: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 15:33:27.705: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  8 15:33:27.705: INFO: Waiting for statefulset status.replicas updated to 0
Aug  8 15:33:27.725: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug  8 15:33:37.766: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug  8 15:33:37.766: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug  8 15:33:37.766: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug  8 15:33:37.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999916s
Aug  8 15:33:38.847: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979501745s
Aug  8 15:33:39.868: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.95797683s
Aug  8 15:33:40.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.937107834s
Aug  8 15:33:41.909: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.916106301s
Aug  8 15:33:42.930: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.895649034s
Aug  8 15:33:43.951: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.875089978s
Aug  8 15:33:44.971: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.854327794s
Aug  8 15:33:45.991: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.834171982s
Aug  8 15:33:47.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 813.958642ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6695
Aug  8 15:33:48.033: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6695 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:33:48.690: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug  8 15:33:48.690: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  8 15:33:48.690: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  8 15:33:48.690: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6695 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:33:49.366: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug  8 15:33:49.366: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  8 15:33:49.366: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  8 15:33:49.366: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6695 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:33:49.990: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug  8 15:33:49.991: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  8 15:33:49.991: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  8 15:33:49.991: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug  8 15:34:30.201: INFO: Deleting all statefulset in ns statefulset-6695
Aug  8 15:34:30.220: INFO: Scaling statefulset ss to 0
Aug  8 15:34:30.281: INFO: Waiting for statefulset status.replicas updated to 0
Aug  8 15:34:30.300: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:34:30.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6695" for this suite.
Aug  8 15:34:36.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:34:37.202: INFO: namespace statefulset-6695 deletion completed in 6.803796892s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:34:37.203: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-decd30af-dfea-4149-a179-16966ca448c8
STEP: Creating a pod to test consume configMaps
Aug  8 15:34:37.494: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48200935-f60e-44b2-a919-b98fd18404b5" in namespace "projected-2320" to be "success or failure"
Aug  8 15:34:37.514: INFO: Pod "pod-projected-configmaps-48200935-f60e-44b2-a919-b98fd18404b5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.245911ms
Aug  8 15:34:39.534: INFO: Pod "pod-projected-configmaps-48200935-f60e-44b2-a919-b98fd18404b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040042073s
Aug  8 15:34:41.555: INFO: Pod "pod-projected-configmaps-48200935-f60e-44b2-a919-b98fd18404b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060643944s
STEP: Saw pod success
Aug  8 15:34:41.555: INFO: Pod "pod-projected-configmaps-48200935-f60e-44b2-a919-b98fd18404b5" satisfied condition "success or failure"
Aug  8 15:34:41.575: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-configmaps-48200935-f60e-44b2-a919-b98fd18404b5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 15:34:41.628: INFO: Waiting for pod pod-projected-configmaps-48200935-f60e-44b2-a919-b98fd18404b5 to disappear
Aug  8 15:34:41.647: INFO: Pod pod-projected-configmaps-48200935-f60e-44b2-a919-b98fd18404b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:34:41.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2320" for this suite.
Aug  8 15:34:47.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:34:48.487: INFO: namespace projected-2320 deletion completed in 6.803407621s
•SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:34:48.488: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7648
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Aug  8 15:34:49.545: INFO: created pod pod-service-account-defaultsa
Aug  8 15:34:49.545: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug  8 15:34:49.566: INFO: created pod pod-service-account-mountsa
Aug  8 15:34:49.566: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug  8 15:34:49.588: INFO: created pod pod-service-account-nomountsa
Aug  8 15:34:49.588: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug  8 15:34:49.608: INFO: created pod pod-service-account-defaultsa-mountspec
Aug  8 15:34:49.608: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug  8 15:34:49.634: INFO: created pod pod-service-account-mountsa-mountspec
Aug  8 15:34:49.634: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug  8 15:34:49.654: INFO: created pod pod-service-account-nomountsa-mountspec
Aug  8 15:34:49.654: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug  8 15:34:49.675: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug  8 15:34:49.675: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug  8 15:34:49.695: INFO: created pod pod-service-account-mountsa-nomountspec
Aug  8 15:34:49.695: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug  8 15:34:49.716: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug  8 15:34:49.716: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:34:49.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7648" for this suite.
Aug  8 15:35:31.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:35:32.560: INFO: namespace svcaccounts-7648 deletion completed in 42.795114536s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:35:32.560: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:35:32.851: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Aug  8 15:35:33.011: INFO: stderr: ""
Aug  8 15:35:33.011: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:23:26Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:15:22Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:35:33.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6541" for this suite.
Aug  8 15:35:39.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:35:39.873: INFO: namespace kubectl-6541 deletion completed in 6.837915406s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:35:39.874: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-9ccc075b-10e2-4c73-a65a-6f1604bc8821
STEP: Creating a pod to test consume secrets
Aug  8 15:35:40.203: INFO: Waiting up to 5m0s for pod "pod-secrets-2b95abc8-234d-4e41-a8c6-fa06d35e24c0" in namespace "secrets-6954" to be "success or failure"
Aug  8 15:35:40.223: INFO: Pod "pod-secrets-2b95abc8-234d-4e41-a8c6-fa06d35e24c0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.417166ms
Aug  8 15:35:42.245: INFO: Pod "pod-secrets-2b95abc8-234d-4e41-a8c6-fa06d35e24c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041103378s
Aug  8 15:35:44.265: INFO: Pod "pod-secrets-2b95abc8-234d-4e41-a8c6-fa06d35e24c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061709656s
STEP: Saw pod success
Aug  8 15:35:44.265: INFO: Pod "pod-secrets-2b95abc8-234d-4e41-a8c6-fa06d35e24c0" satisfied condition "success or failure"
Aug  8 15:35:44.285: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-secrets-2b95abc8-234d-4e41-a8c6-fa06d35e24c0 container secret-volume-test: <nil>
STEP: delete the pod
Aug  8 15:35:44.345: INFO: Waiting for pod pod-secrets-2b95abc8-234d-4e41-a8c6-fa06d35e24c0 to disappear
Aug  8 15:35:44.364: INFO: Pod pod-secrets-2b95abc8-234d-4e41-a8c6-fa06d35e24c0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:35:44.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6954" for this suite.
Aug  8 15:35:50.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:35:51.241: INFO: namespace secrets-6954 deletion completed in 6.839775233s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:35:51.241: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Aug  8 15:35:51.543: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8405'
Aug  8 15:35:51.885: INFO: stderr: ""
Aug  8 15:35:51.885: INFO: stdout: "pod/pause created\n"
Aug  8 15:35:51.885: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug  8 15:35:51.885: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8405" to be "running and ready"
Aug  8 15:35:51.904: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.995875ms
Aug  8 15:35:53.925: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039727188s
Aug  8 15:35:55.945: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.05992171s
Aug  8 15:35:55.945: INFO: Pod "pause" satisfied condition "running and ready"
Aug  8 15:35:55.945: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Aug  8 15:35:55.945: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-8405'
Aug  8 15:35:56.124: INFO: stderr: ""
Aug  8 15:35:56.124: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug  8 15:35:56.124: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-8405'
Aug  8 15:35:56.303: INFO: stderr: ""
Aug  8 15:35:56.303: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug  8 15:35:56.303: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-8405'
Aug  8 15:35:56.500: INFO: stderr: ""
Aug  8 15:35:56.500: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug  8 15:35:56.500: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-8405'
Aug  8 15:35:56.677: INFO: stderr: ""
Aug  8 15:35:56.677: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Aug  8 15:35:56.677: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-8405'
Aug  8 15:35:56.863: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 15:35:56.863: INFO: stdout: "pod \"pause\" force deleted\n"
Aug  8 15:35:56.863: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-8405'
Aug  8 15:35:57.056: INFO: stderr: "No resources found.\n"
Aug  8 15:35:57.056: INFO: stdout: ""
Aug  8 15:35:57.056: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-8405 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug  8 15:35:57.214: INFO: stderr: ""
Aug  8 15:35:57.214: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:35:57.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8405" for this suite.
Aug  8 15:36:03.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:36:04.046: INFO: namespace kubectl-8405 deletion completed in 6.795144437s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:36:04.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:36:04.349: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5410'
Aug  8 15:36:04.651: INFO: stderr: ""
Aug  8 15:36:04.652: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug  8 15:36:04.652: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5410'
Aug  8 15:36:04.970: INFO: stderr: ""
Aug  8 15:36:04.970: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug  8 15:36:05.991: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 15:36:05.991: INFO: Found 0 / 1
Aug  8 15:36:06.991: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 15:36:06.991: INFO: Found 0 / 1
Aug  8 15:36:07.991: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 15:36:07.991: INFO: Found 1 / 1
Aug  8 15:36:07.991: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug  8 15:36:08.011: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 15:36:08.011: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug  8 15:36:08.011: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-7nhzn --namespace=kubectl-5410'
Aug  8 15:36:08.221: INFO: stderr: ""
Aug  8 15:36:08.221: INFO: stdout: "Name:           redis-master-7nhzn\nNamespace:      kubectl-5410\nPriority:       0\nNode:           shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk/10.250.0.5\nStart Time:     Thu, 08 Aug 2019 15:36:04 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 100.96.1.73/32\n                kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             100.96.1.73\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://1ee27f169b9daa04294718b19968230a1634070a853224d2c38e31c24dd8c6fe\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 08 Aug 2019 15:36:07 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-q4772 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-q4772:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-q4772\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                      Message\n  ----    ------     ----  ----                                                      -------\n  Normal  Scheduled  4s    default-scheduler                                         Successfully assigned kubectl-5410/redis-master-7nhzn to shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk\n  Normal  Pulled     2s    kubelet, shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Created container redis-master\n  Normal  Started    1s    kubelet, shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Started container redis-master\n"
Aug  8 15:36:08.221: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-5410'
Aug  8 15:36:08.444: INFO: stderr: ""
Aug  8 15:36:08.444: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5410\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-7nhzn\n"
Aug  8 15:36:08.444: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-5410'
Aug  8 15:36:08.657: INFO: stderr: ""
Aug  8 15:36:08.657: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5410\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.66.55.89\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.73:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug  8 15:36:08.695: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr'
Aug  8 15:36:08.949: INFO: stderr: ""
Aug  8 15:36:08.950: INFO: stdout: "Name:               shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2s_v3\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=westeurope\n                    failure-domain.beta.kubernetes.io/zone=0\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/role=node\n                    worker.garden.sapcloud.io/group=cpu-worker\n                    worker.gardener.cloud/pool=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.108.8.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 08 Aug 2019 14:54:09 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 08 Aug 2019 14:55:48 +0000   Thu, 08 Aug 2019 14:55:48 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Thu, 08 Aug 2019 15:36:03 +0000   Thu, 08 Aug 2019 14:54:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 08 Aug 2019 15:36:03 +0000   Thu, 08 Aug 2019 14:54:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 08 Aug 2019 15:36:03 +0000   Thu, 08 Aug 2019 14:54:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 08 Aug 2019 15:36:03 +0000   Thu, 08 Aug 2019 14:54:29 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  Hostname:    shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr\n  InternalIP:  10.250.0.4\nCapacity:\n attachable-volumes-azure-disk:  4\n cpu:                            2\n ephemeral-storage:              33136428Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         8145236Ki\n pods:                           110\nAllocatable:\n attachable-volumes-azure-disk:  4\n cpu:                            1920m\n ephemeral-storage:              32235117134\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         6849943751\n pods:                           110\nSystem Info:\n Machine ID:                 1ea48a7ac8224581ac789292b7ded3c0\n System UUID:                4c1ec150-a6be-ef40-baa8-fe12ead0d1eb\n Boot ID:                    719d6cd7-93ac-4bb5-aeb9-9852b3ab3da2\n Kernel Version:             4.19.56-coreos-r1\n OS Image:                   Container Linux by CoreOS 2135.6.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.2\n Kube-Proxy Version:         v1.15.2\nPodCIDR:                     100.96.0.0/24\nProviderID:                  azure:///subscriptions//resourceGroups/shoot--it--tm-lf7ue/providers/Microsoft.Compute/virtualMachines/shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr\nNon-terminated Pods:         (12 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-kubernetes-dashboard-5c8d9945bc-spkwl                       50m (2%)      100m (5%)   50Mi (0%)        256Mi (3%)     44m\n  kube-system                addons-nginx-ingress-controller-6496d947df-hzpqv                   100m (5%)     2 (104%)    100Mi (1%)       800Mi (12%)    44m\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-695fb4fdcd-cknxk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  kube-system                blackbox-exporter-954dd954b-6rgqr                                  5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)      44m\n  kube-system                calico-kube-controllers-5f4b46ffb5-zt29p                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  kube-system                calico-node-6w86k                                                  100m (5%)     500m (26%)  100Mi (1%)       700Mi (10%)    41m\n  kube-system                coredns-85cc454dd8-2tknm                                           50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)     43m\n  kube-system                coredns-85cc454dd8-cqd76                                           50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)     44m\n  kube-system                kube-proxy-vvfrb                                                   20m (1%)      0 (0%)      64Mi (0%)        0 (0%)         41m\n  kube-system                metrics-server-566847b67f-f2cmb                                    20m (1%)      80m (4%)    100Mi (1%)       400Mi (6%)     44m\n  kube-system                node-exporter-s4q7v                                                5m (0%)       25m (1%)    10Mi (0%)        100Mi (1%)     41m\n  kube-system                vpn-shoot-89d5dc9c8-84rcr                                          100m (5%)     1 (52%)     100Mi (1%)       1000Mi (15%)   44m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            500m (26%)  3915m (203%)\n  memory                         559Mi (8%)  3491Mi (53%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:\n  Type    Reason                   Age                From                                                         Message\n  ----    ------                   ----               ----                                                         -------\n  Normal  NodeHasSufficientMemory  41m (x8 over 42m)  kubelet, shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr     Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    41m (x8 over 42m)  kubelet, shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr     Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr status is now: NodeHasNoDiskPressure\n  Normal  Starting                 41m                kube-proxy, shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Starting kube-proxy.\n"
Aug  8 15:36:08.950: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-5410'
Aug  8 15:36:09.164: INFO: stderr: ""
Aug  8 15:36:09.164: INFO: stdout: "Name:         kubectl-5410\nLabels:       e2e-framework=kubectl\n              e2e-run=8d84d478-a6d5-4d99-b43b-9107959478b4\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:36:09.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5410" for this suite.
Aug  8 15:36:31.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:36:31.998: INFO: namespace kubectl-5410 deletion completed in 22.787199173s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:36:31.998: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-294
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:36:32.251: INFO: Creating deployment "nginx-deployment"
Aug  8 15:36:32.272: INFO: Waiting for observed generation 1
Aug  8 15:36:32.292: INFO: Waiting for all required pods to come up
Aug  8 15:36:32.315: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug  8 15:36:44.373: INFO: Waiting for deployment "nginx-deployment" to complete
Aug  8 15:36:44.412: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug  8 15:36:44.453: INFO: Updating deployment nginx-deployment
Aug  8 15:36:44.453: INFO: Waiting for observed generation 2
Aug  8 15:36:46.506: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug  8 15:36:46.526: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug  8 15:36:46.546: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug  8 15:36:46.604: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug  8 15:36:46.605: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug  8 15:36:46.624: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug  8 15:36:46.662: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug  8 15:36:46.662: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug  8 15:36:46.701: INFO: Updating deployment nginx-deployment
Aug  8 15:36:46.701: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug  8 15:36:46.757: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug  8 15:36:48.797: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug  8 15:36:48.836: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-294,SelfLink:/apis/apps/v1/namespaces/deployment-294/deployments/nginx-deployment,UID:2d91ca57-bf20-4a53-9d8f-ec1f48a6756c,ResourceVersion:10126,Generation:3,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-08-08 15:36:46 +0000 UTC 2019-08-08 15:36:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-08 15:36:46 +0000 UTC 2019-08-08 15:36:32 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug  8 15:36:48.857: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-294,SelfLink:/apis/apps/v1/namespaces/deployment-294/replicasets/nginx-deployment-55fb7cb77f,UID:97754e40-c1bf-432d-a22b-ccb7ba7d3ad0,ResourceVersion:10123,Generation:3,CreationTimestamp:2019-08-08 15:36:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 2d91ca57-bf20-4a53-9d8f-ec1f48a6756c 0xc002b71a07 0xc002b71a08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  8 15:36:48.857: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug  8 15:36:48.857: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-294,SelfLink:/apis/apps/v1/namespaces/deployment-294/replicasets/nginx-deployment-7b8c6f4498,UID:94dacdec-5add-4c85-b2ad-00b74247feb2,ResourceVersion:10124,Generation:3,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 2d91ca57-bf20-4a53-9d8f-ec1f48a6756c 0xc002b71ad7 0xc002b71ad8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug  8 15:36:48.915: INFO: Pod "nginx-deployment-55fb7cb77f-2m4mn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2m4mn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-2m4mn,UID:cf16e956-d59a-47f2-b6ad-dc98d839900f,ResourceVersion:10129,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e4aa7 0xc0010e4aa8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e4b10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e4b30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.915: INFO: Pod "nginx-deployment-55fb7cb77f-2qq5t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2qq5t,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-2qq5t,UID:d9168fb6-00b2-4d47-ba4f-1fb1c6fbe91b,ResourceVersion:10093,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e4c00 0xc0010e4c01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e4c70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e4c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.915: INFO: Pod "nginx-deployment-55fb7cb77f-58hgp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-58hgp,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-58hgp,UID:857e9042-e138-436a-9232-b8fda901f735,ResourceVersion:10055,Generation:0,CreationTimestamp:2019-08-08 15:36:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e4d10 0xc0010e4d11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e4d80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e4da0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-08-08 15:36:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.915: INFO: Pod "nginx-deployment-55fb7cb77f-cb98f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cb98f,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-cb98f,UID:763c0262-333d-41e4-b740-a4213611df09,ResourceVersion:10139,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e4e70 0xc0010e4e71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e4ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e4f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-08-08 15:36:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.916: INFO: Pod "nginx-deployment-55fb7cb77f-dlqff" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dlqff,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-dlqff,UID:f0f99da1-42a1-423b-ad74-ea7f6c0283ee,ResourceVersion:10131,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e4fd0 0xc0010e4fd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e5060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.916: INFO: Pod "nginx-deployment-55fb7cb77f-hr7vs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hr7vs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-hr7vs,UID:3c7c0c4b-ada7-4d67-832d-ec288ebd63b6,ResourceVersion:10097,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e5130 0xc0010e5131}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e51a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e51c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.916: INFO: Pod "nginx-deployment-55fb7cb77f-jwd2m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jwd2m,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-jwd2m,UID:65f3e0bb-6a2c-4daf-8a1b-d991825266d5,ResourceVersion:10122,Generation:0,CreationTimestamp:2019-08-08 15:36:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.35/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e52a0 0xc0010e52a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e5330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.916: INFO: Pod "nginx-deployment-55fb7cb77f-l7v8n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-l7v8n,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-l7v8n,UID:8ecd6b2d-72fa-42d7-8264-abe41dc0fe06,ResourceVersion:10142,Generation:0,CreationTimestamp:2019-08-08 15:36:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.34/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e5410 0xc0010e5411}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e54a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.34,StartTime:2019-08-08 15:36:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.916: INFO: Pod "nginx-deployment-55fb7cb77f-m7mvr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-m7mvr,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-m7mvr,UID:975c0c30-508f-464f-81e4-512080fc71e5,ResourceVersion:10054,Generation:0,CreationTimestamp:2019-08-08 15:36:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e5590 0xc0010e5591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e5740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-08-08 15:36:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.916: INFO: Pod "nginx-deployment-55fb7cb77f-q8xd9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-q8xd9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-q8xd9,UID:e2341603-12a8-4e8c-b2fa-9e567c1aa59a,ResourceVersion:10067,Generation:0,CreationTimestamp:2019-08-08 15:36:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e5850 0xc0010e5851}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e5920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-08-08 15:36:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.916: INFO: Pod "nginx-deployment-55fb7cb77f-sc9bb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-sc9bb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-sc9bb,UID:166cba38-0be9-4e22-b5c2-1ce15430dbe4,ResourceVersion:10130,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e59f0 0xc0010e59f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5a60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e5a80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.916: INFO: Pod "nginx-deployment-55fb7cb77f-t944q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-t944q,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-t944q,UID:0fdd4894-9c7a-4d1d-8d31-9616a233ad97,ResourceVersion:10111,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e5b50 0xc0010e5b51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e5be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.916: INFO: Pod "nginx-deployment-55fb7cb77f-z7j6m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-z7j6m,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-55fb7cb77f-z7j6m,UID:5243c00b-77a7-482e-90d8-d120fa6f9f4e,ResourceVersion:10119,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 97754e40-c1bf-432d-a22b-ccb7ba7d3ad0 0xc0010e5c60 0xc0010e5c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e5cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.917: INFO: Pod "nginx-deployment-7b8c6f4498-65ll9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-65ll9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-65ll9,UID:e470c82b-c43b-4c13-894f-f6776dfa02d0,ResourceVersion:10001,Generation:0,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.31/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc0010e5d80 0xc0010e5d81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e5e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.31,StartTime:2019-08-08 15:36:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-08 15:36:37 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9010a9be39b50b29f2e7523ffc316e04635ecc02ed1d972e73551210cf4bc221}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.917: INFO: Pod "nginx-deployment-7b8c6f4498-67td6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-67td6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-67td6,UID:61421473-be60-4e00-aa4c-ccc705be08a4,ResourceVersion:10019,Generation:0,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.75/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc0010e5ee0 0xc0010e5ee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0010e5f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0010e5f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.75,StartTime:2019-08-08 15:36:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-08 15:36:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5788935a38db80e6eeff36cb72c545b7324f15974e68e06f33eff901124beb12}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.917: INFO: Pod "nginx-deployment-7b8c6f4498-8gsr9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8gsr9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-8gsr9,UID:9dfcc187-3e40-4aad-90d3-9e6a7fdbf557,ResourceVersion:10094,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9e030 0xc000a9e031}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9e090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9e0b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.917: INFO: Pod "nginx-deployment-7b8c6f4498-9klxn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9klxn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-9klxn,UID:c738a754-0a8d-4950-b559-8e6703d4c60f,ResourceVersion:10115,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9e130 0xc000a9e131}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9e190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9e1b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.917: INFO: Pod "nginx-deployment-7b8c6f4498-b9dlv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-b9dlv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-b9dlv,UID:84866ad0-5e2e-4292-ab08-fdb800e20dc0,ResourceVersion:10110,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9e230 0xc000a9e231}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9e290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9e2b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.917: INFO: Pod "nginx-deployment-7b8c6f4498-cmtgx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cmtgx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-cmtgx,UID:9132e982-4ead-49c7-8bbe-9cc71137151f,ResourceVersion:10113,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9e330 0xc000a9e331}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9e390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9e3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.917: INFO: Pod "nginx-deployment-7b8c6f4498-d978v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d978v,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-d978v,UID:3237e5bc-3c55-42e3-b0fe-9810c84e2fce,ResourceVersion:9995,Generation:0,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.32/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9e440 0xc000a9e441}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9e4a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9e4c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.32,StartTime:2019-08-08 15:36:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-08 15:36:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2d3b2307c78c857efddf1516e8ce09b4f6b3dedcbd8410d5352939c784bae96b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.917: INFO: Pod "nginx-deployment-7b8c6f4498-dbnnc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dbnnc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-dbnnc,UID:d20516c9-7ba1-4b1f-a4be-f7a87f857022,ResourceVersion:10128,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9e590 0xc000a9e591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9e5f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9e610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.918: INFO: Pod "nginx-deployment-7b8c6f4498-dqkgh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dqkgh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-dqkgh,UID:e6f40f6b-9921-491d-8dbf-6ce4a22c9f18,ResourceVersion:10140,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9e6d0 0xc000a9e6d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9e730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9e750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-08-08 15:36:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.918: INFO: Pod "nginx-deployment-7b8c6f4498-g8w6m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-g8w6m,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-g8w6m,UID:c875ff06-b79c-4144-a882-363b0a438d22,ResourceVersion:10120,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9e810 0xc000a9e811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9ea70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9ea90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.918: INFO: Pod "nginx-deployment-7b8c6f4498-k62gq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-k62gq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-k62gq,UID:2682ddc5-97ea-4411-b7fc-929162f98099,ResourceVersion:10010,Generation:0,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.77/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9eb80 0xc000a9eb81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9ebe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9ec00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.77,StartTime:2019-08-08 15:36:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-08 15:36:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a128f2908d3f665521bb0c9956122162bda953d1ca61e0f1e7b02abc39351c21}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.918: INFO: Pod "nginx-deployment-7b8c6f4498-ncws5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ncws5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-ncws5,UID:7234e894-7b2f-43b1-acaa-1cc2d9f3c4d6,ResourceVersion:10013,Generation:0,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.74/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9ece0 0xc000a9ece1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9ed40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9ed60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.74,StartTime:2019-08-08 15:36:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-08 15:36:40 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e2a0df641544d0a569b9a92c3bb3d70fbe58825052e6a2bfb35ee6a9536055dd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.918: INFO: Pod "nginx-deployment-7b8c6f4498-qb6sq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qb6sq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-qb6sq,UID:5b95c42d-e73e-4fd4-a0c3-686105c8c6a0,ResourceVersion:10132,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9ee30 0xc000a9ee31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9ee90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9eeb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.918: INFO: Pod "nginx-deployment-7b8c6f4498-qxjsl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qxjsl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-qxjsl,UID:820a8449-d9eb-46c6-bdc7-050adfd4f4fa,ResourceVersion:9998,Generation:0,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.30/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9ef80 0xc000a9ef81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9efe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9f000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.30,StartTime:2019-08-08 15:36:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-08 15:36:37 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7fde8d8277b13b043019f233238283e7577d3376da645971f1d50a2920defbc2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.918: INFO: Pod "nginx-deployment-7b8c6f4498-t4z84" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t4z84,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-t4z84,UID:de54c1ef-6b71-4cda-ae53-036627f9ea57,ResourceVersion:10127,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9f0d0 0xc000a9f0d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9f130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9f150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.918: INFO: Pod "nginx-deployment-7b8c6f4498-t5t2t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t5t2t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-t5t2t,UID:d17ff4d2-d4f6-47eb-97ec-ad608594028b,ResourceVersion:10133,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9f210 0xc000a9f211}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9f270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9f290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.918: INFO: Pod "nginx-deployment-7b8c6f4498-tshbk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tshbk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-tshbk,UID:5a7329df-3dc8-413f-a723-ed6a8bdca4c2,ResourceVersion:9992,Generation:0,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.33/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9f360 0xc000a9f361}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9f3c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9f3e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.33,StartTime:2019-08-08 15:36:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-08 15:36:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b25d44bf310613744f4af998fa17363f88fe8509a7bdd38dc03fb48e8f865723}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.919: INFO: Pod "nginx-deployment-7b8c6f4498-tx8rd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tx8rd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-tx8rd,UID:2d2b1def-d33e-4601-a11c-094f14ee70cc,ResourceVersion:10121,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9f4b0 0xc000a9f4b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9f510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9f530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-08-08 15:36:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.919: INFO: Pod "nginx-deployment-7b8c6f4498-whw95" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-whw95,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-whw95,UID:c3715362-39e3-4621-86c3-c196bba25a24,ResourceVersion:10016,Generation:0,CreationTimestamp:2019-08-08 15:36:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.76/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9f600 0xc000a9f601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9f660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9f680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.76,StartTime:2019-08-08 15:36:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-08 15:36:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e59e58c241b1c2328095510a53ae79b96bd364090b8c9e738a2cbb3cd0e9ad0c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug  8 15:36:48.919: INFO: Pod "nginx-deployment-7b8c6f4498-xhd22" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xhd22,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-294,SelfLink:/api/v1/namespaces/deployment-294/pods/nginx-deployment-7b8c6f4498-xhd22,UID:3166e837-d067-411a-8c91-b6f4cf7b2f65,ResourceVersion:10136,Generation:0,CreationTimestamp:2019-08-08 15:36:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94dacdec-5add-4c85-b2ad-00b74247feb2 0xc000a9f750 0xc000a9f751}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zpnl4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zpnl4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zpnl4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a9f7b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a9f7d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:36:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-08-08 15:36:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:36:48.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-294" for this suite.
Aug  8 15:36:57.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:36:57.737: INFO: namespace deployment-294 deletion completed in 8.798022357s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:36:57.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-8d79bfd4-4064-4b8a-b70c-54618588806a
STEP: Creating a pod to test consume configMaps
Aug  8 15:36:58.097: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26" in namespace "projected-368" to be "success or failure"
Aug  8 15:36:58.116: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26": Phase="Pending", Reason="", readiness=false. Elapsed: 19.358957ms
Aug  8 15:37:00.139: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042423581s
Aug  8 15:37:02.160: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063016106s
Aug  8 15:37:04.180: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083841863s
Aug  8 15:37:06.200: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26": Phase="Pending", Reason="", readiness=false. Elapsed: 8.103728503s
Aug  8 15:37:08.220: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26": Phase="Pending", Reason="", readiness=false. Elapsed: 10.123899553s
Aug  8 15:37:10.241: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26": Phase="Pending", Reason="", readiness=false. Elapsed: 12.144391349s
Aug  8 15:37:12.261: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26": Phase="Pending", Reason="", readiness=false. Elapsed: 14.164743518s
Aug  8 15:37:14.282: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.185199855s
STEP: Saw pod success
Aug  8 15:37:14.282: INFO: Pod "pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26" satisfied condition "success or failure"
Aug  8 15:37:14.301: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 15:37:14.355: INFO: Waiting for pod pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26 to disappear
Aug  8 15:37:14.375: INFO: Pod pod-projected-configmaps-847fc0df-a0c9-45cc-82e5-75eba9cb5f26 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:37:14.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-368" for this suite.
Aug  8 15:37:20.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:37:21.240: INFO: namespace projected-368 deletion completed in 6.827895244s
•SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:37:21.241: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  8 15:37:21.551: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9805'
Aug  8 15:37:21.751: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  8 15:37:21.751: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Aug  8 15:37:21.790: INFO: scanned /root for discovery docs: <nil>
Aug  8 15:37:21.790: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9805'
Aug  8 15:37:35.150: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug  8 15:37:35.150: INFO: stdout: "Created e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4\nScaling up e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug  8 15:37:35.150: INFO: stdout: "Created e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4\nScaling up e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug  8 15:37:35.150: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9805'
Aug  8 15:37:35.314: INFO: stderr: ""
Aug  8 15:37:35.314: INFO: stdout: "e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4-7pzj4 "
Aug  8 15:37:35.314: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4-7pzj4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9805'
Aug  8 15:37:35.470: INFO: stderr: ""
Aug  8 15:37:35.470: INFO: stdout: "true"
Aug  8 15:37:35.470: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4-7pzj4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9805'
Aug  8 15:37:35.621: INFO: stderr: ""
Aug  8 15:37:35.621: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug  8 15:37:35.621: INFO: e2e-test-nginx-rc-e0223114cfca307c04b0ae9391fcc2d4-7pzj4 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Aug  8 15:37:35.622: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-9805'
Aug  8 15:37:35.796: INFO: stderr: ""
Aug  8 15:37:35.796: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:37:35.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9805" for this suite.
Aug  8 15:37:59.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:38:00.638: INFO: namespace kubectl-9805 deletion completed in 24.804707468s
•
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:38:00.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9048
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-9048
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9048 to expose endpoints map[]
Aug  8 15:38:01.006: INFO: successfully validated that service endpoint-test2 in namespace services-9048 exposes endpoints map[] (19.329407ms elapsed)
STEP: Creating pod pod1 in namespace services-9048
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9048 to expose endpoints map[pod1:[80]]
Aug  8 15:38:05.233: INFO: successfully validated that service endpoint-test2 in namespace services-9048 exposes endpoints map[pod1:[80]] (4.197818175s elapsed)
STEP: Creating pod pod2 in namespace services-9048
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9048 to expose endpoints map[pod1:[80] pod2:[80]]
Aug  8 15:38:09.572: INFO: successfully validated that service endpoint-test2 in namespace services-9048 exposes endpoints map[pod1:[80] pod2:[80]] (4.317409544s elapsed)
STEP: Deleting pod pod1 in namespace services-9048
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9048 to expose endpoints map[pod2:[80]]
Aug  8 15:38:09.631: INFO: successfully validated that service endpoint-test2 in namespace services-9048 exposes endpoints map[pod2:[80]] (38.716859ms elapsed)
STEP: Deleting pod pod2 in namespace services-9048
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9048 to expose endpoints map[]
Aug  8 15:38:09.671: INFO: successfully validated that service endpoint-test2 in namespace services-9048 exposes endpoints map[] (19.110007ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:38:09.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9048" for this suite.
Aug  8 15:38:31.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:38:32.547: INFO: namespace services-9048 deletion completed in 22.795022304s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:38:32.548: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 15:38:32.871: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25d49b40-a6a9-4618-8c07-f1046fac7514" in namespace "downward-api-7647" to be "success or failure"
Aug  8 15:38:32.891: INFO: Pod "downwardapi-volume-25d49b40-a6a9-4618-8c07-f1046fac7514": Phase="Pending", Reason="", readiness=false. Elapsed: 19.151543ms
Aug  8 15:38:34.911: INFO: Pod "downwardapi-volume-25d49b40-a6a9-4618-8c07-f1046fac7514": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039251501s
Aug  8 15:38:36.931: INFO: Pod "downwardapi-volume-25d49b40-a6a9-4618-8c07-f1046fac7514": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059456523s
STEP: Saw pod success
Aug  8 15:38:36.931: INFO: Pod "downwardapi-volume-25d49b40-a6a9-4618-8c07-f1046fac7514" satisfied condition "success or failure"
Aug  8 15:38:36.951: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-25d49b40-a6a9-4618-8c07-f1046fac7514 container client-container: <nil>
STEP: delete the pod
Aug  8 15:38:37.008: INFO: Waiting for pod downwardapi-volume-25d49b40-a6a9-4618-8c07-f1046fac7514 to disappear
Aug  8 15:38:37.027: INFO: Pod downwardapi-volume-25d49b40-a6a9-4618-8c07-f1046fac7514 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:38:37.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7647" for this suite.
Aug  8 15:38:43.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:38:43.888: INFO: namespace downward-api-7647 deletion completed in 6.813062304s
•SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:38:43.888: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-f4a741ec-b8c4-4500-ad93-deb5d2416374
STEP: Creating a pod to test consume secrets
Aug  8 15:38:44.196: INFO: Waiting up to 5m0s for pod "pod-secrets-772b20d7-ae2a-460e-be1a-a2c957606e31" in namespace "secrets-7233" to be "success or failure"
Aug  8 15:38:44.216: INFO: Pod "pod-secrets-772b20d7-ae2a-460e-be1a-a2c957606e31": Phase="Pending", Reason="", readiness=false. Elapsed: 19.220647ms
Aug  8 15:38:46.242: INFO: Pod "pod-secrets-772b20d7-ae2a-460e-be1a-a2c957606e31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045473228s
Aug  8 15:38:48.263: INFO: Pod "pod-secrets-772b20d7-ae2a-460e-be1a-a2c957606e31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066161275s
STEP: Saw pod success
Aug  8 15:38:48.263: INFO: Pod "pod-secrets-772b20d7-ae2a-460e-be1a-a2c957606e31" satisfied condition "success or failure"
Aug  8 15:38:48.282: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-secrets-772b20d7-ae2a-460e-be1a-a2c957606e31 container secret-volume-test: <nil>
STEP: delete the pod
Aug  8 15:38:48.333: INFO: Waiting for pod pod-secrets-772b20d7-ae2a-460e-be1a-a2c957606e31 to disappear
Aug  8 15:38:48.351: INFO: Pod pod-secrets-772b20d7-ae2a-460e-be1a-a2c957606e31 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:38:48.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7233" for this suite.
Aug  8 15:38:54.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:38:55.196: INFO: namespace secrets-7233 deletion completed in 6.80738412s
•SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:38:55.197: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2595
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7944
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5184
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:39:02.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2595" for this suite.
Aug  8 15:39:08.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:39:08.912: INFO: namespace namespaces-2595 deletion completed in 6.819033724s
STEP: Destroying namespace "nsdeletetest-7944" for this suite.
Aug  8 15:39:08.931: INFO: Namespace nsdeletetest-7944 was already deleted
STEP: Destroying namespace "nsdeletetest-5184" for this suite.
Aug  8 15:39:14.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:39:15.764: INFO: namespace nsdeletetest-5184 deletion completed in 6.83313295s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:39:15.765: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6978
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-87b96a09-45db-48ee-9967-d584c2746935
STEP: Creating a pod to test consume configMaps
Aug  8 15:39:16.096: INFO: Waiting up to 5m0s for pod "pod-configmaps-798d4a19-2e2e-4c75-99d5-0f9d8f5b679a" in namespace "configmap-6978" to be "success or failure"
Aug  8 15:39:16.115: INFO: Pod "pod-configmaps-798d4a19-2e2e-4c75-99d5-0f9d8f5b679a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.074758ms
Aug  8 15:39:18.137: INFO: Pod "pod-configmaps-798d4a19-2e2e-4c75-99d5-0f9d8f5b679a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040703326s
Aug  8 15:39:20.158: INFO: Pod "pod-configmaps-798d4a19-2e2e-4c75-99d5-0f9d8f5b679a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061459687s
STEP: Saw pod success
Aug  8 15:39:20.158: INFO: Pod "pod-configmaps-798d4a19-2e2e-4c75-99d5-0f9d8f5b679a" satisfied condition "success or failure"
Aug  8 15:39:20.177: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-798d4a19-2e2e-4c75-99d5-0f9d8f5b679a container configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 15:39:20.462: INFO: Waiting for pod pod-configmaps-798d4a19-2e2e-4c75-99d5-0f9d8f5b679a to disappear
Aug  8 15:39:20.487: INFO: Pod pod-configmaps-798d4a19-2e2e-4c75-99d5-0f9d8f5b679a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:39:20.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6978" for this suite.
Aug  8 15:39:26.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:39:27.329: INFO: namespace configmap-6978 deletion completed in 6.804793s
•SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:39:27.329: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 15:39:27.671: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f0bbcea-e543-4610-b176-496867a59400" in namespace "projected-9242" to be "success or failure"
Aug  8 15:39:27.690: INFO: Pod "downwardapi-volume-1f0bbcea-e543-4610-b176-496867a59400": Phase="Pending", Reason="", readiness=false. Elapsed: 19.054027ms
Aug  8 15:39:29.711: INFO: Pod "downwardapi-volume-1f0bbcea-e543-4610-b176-496867a59400": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039760121s
Aug  8 15:39:31.731: INFO: Pod "downwardapi-volume-1f0bbcea-e543-4610-b176-496867a59400": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059898869s
STEP: Saw pod success
Aug  8 15:39:31.731: INFO: Pod "downwardapi-volume-1f0bbcea-e543-4610-b176-496867a59400" satisfied condition "success or failure"
Aug  8 15:39:31.750: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-1f0bbcea-e543-4610-b176-496867a59400 container client-container: <nil>
STEP: delete the pod
Aug  8 15:39:31.803: INFO: Waiting for pod downwardapi-volume-1f0bbcea-e543-4610-b176-496867a59400 to disappear
Aug  8 15:39:31.822: INFO: Pod downwardapi-volume-1f0bbcea-e543-4610-b176-496867a59400 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:39:31.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9242" for this suite.
Aug  8 15:39:37.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:39:38.650: INFO: namespace projected-9242 deletion completed in 6.790987039s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:39:38.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3837
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug  8 15:39:49.279: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0808 15:39:49.279283    4191 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug  8 15:39:49.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3837" for this suite.
Aug  8 15:39:55.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:39:56.119: INFO: namespace gc-3837 deletion completed in 6.820200947s
•SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:39:56.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:39:56.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6357" for this suite.
Aug  8 15:40:20.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:40:21.444: INFO: namespace pods-6357 deletion completed in 24.807619309s
•SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:40:21.444: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug  8 15:40:29.966: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:29.986: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:31.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:32.006: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:33.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:34.007: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:35.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:36.006: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:37.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:38.006: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:39.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:40.007: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:41.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:42.007: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:43.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:44.007: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:45.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:46.007: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:47.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:48.007: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:49.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:50.006: INFO: Pod pod-with-poststart-exec-hook still exists
Aug  8 15:40:51.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug  8 15:40:52.007: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:40:52.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3723" for this suite.
Aug  8 15:41:14.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:41:14.847: INFO: namespace container-lifecycle-hook-3723 deletion completed in 22.802201322s
•S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:41:14.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug  8 15:41:15.177: INFO: Waiting up to 5m0s for pod "pod-c699d0f3-69d7-4248-8296-ddfc027e3ad4" in namespace "emptydir-4485" to be "success or failure"
Aug  8 15:41:15.203: INFO: Pod "pod-c699d0f3-69d7-4248-8296-ddfc027e3ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 25.635948ms
Aug  8 15:41:17.223: INFO: Pod "pod-c699d0f3-69d7-4248-8296-ddfc027e3ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046380938s
Aug  8 15:41:19.244: INFO: Pod "pod-c699d0f3-69d7-4248-8296-ddfc027e3ad4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066883373s
STEP: Saw pod success
Aug  8 15:41:19.244: INFO: Pod "pod-c699d0f3-69d7-4248-8296-ddfc027e3ad4" satisfied condition "success or failure"
Aug  8 15:41:19.263: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-c699d0f3-69d7-4248-8296-ddfc027e3ad4 container test-container: <nil>
STEP: delete the pod
Aug  8 15:41:19.324: INFO: Waiting for pod pod-c699d0f3-69d7-4248-8296-ddfc027e3ad4 to disappear
Aug  8 15:41:19.343: INFO: Pod pod-c699d0f3-69d7-4248-8296-ddfc027e3ad4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:41:19.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4485" for this suite.
Aug  8 15:41:25.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:41:26.193: INFO: namespace emptydir-4485 deletion completed in 6.812718657s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:41:26.193: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-ac36f36f-7e54-420b-a94f-eb9a7f39759d
STEP: Creating a pod to test consume secrets
Aug  8 15:41:26.497: INFO: Waiting up to 5m0s for pod "pod-secrets-cb182690-d836-432d-9a8a-b65fb9abe41b" in namespace "secrets-4566" to be "success or failure"
Aug  8 15:41:26.517: INFO: Pod "pod-secrets-cb182690-d836-432d-9a8a-b65fb9abe41b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.522508ms
Aug  8 15:41:28.537: INFO: Pod "pod-secrets-cb182690-d836-432d-9a8a-b65fb9abe41b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039944913s
Aug  8 15:41:30.558: INFO: Pod "pod-secrets-cb182690-d836-432d-9a8a-b65fb9abe41b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060692477s
STEP: Saw pod success
Aug  8 15:41:30.558: INFO: Pod "pod-secrets-cb182690-d836-432d-9a8a-b65fb9abe41b" satisfied condition "success or failure"
Aug  8 15:41:30.577: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-secrets-cb182690-d836-432d-9a8a-b65fb9abe41b container secret-volume-test: <nil>
STEP: delete the pod
Aug  8 15:41:30.820: INFO: Waiting for pod pod-secrets-cb182690-d836-432d-9a8a-b65fb9abe41b to disappear
Aug  8 15:41:30.839: INFO: Pod pod-secrets-cb182690-d836-432d-9a8a-b65fb9abe41b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:41:30.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4566" for this suite.
Aug  8 15:41:36.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:41:37.693: INFO: namespace secrets-4566 deletion completed in 6.817170397s
•SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:41:37.693: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1987
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-ef3fc1e7-fdcb-4327-a5d9-86f0b8dd1b24
STEP: Creating secret with name s-test-opt-upd-0f1a6e21-6df6-4a47-8061-16a0afce2c30
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ef3fc1e7-fdcb-4327-a5d9-86f0b8dd1b24
STEP: Updating secret s-test-opt-upd-0f1a6e21-6df6-4a47-8061-16a0afce2c30
STEP: Creating secret with name s-test-opt-create-0b8011c7-0ded-4736-aa45-0a0214937614
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:42:59.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1987" for this suite.
Aug  8 15:43:23.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:43:24.442: INFO: namespace projected-1987 deletion completed in 24.821137347s
•SSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:43:24.442: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7182
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-d655648b-9ed3-4583-ad43-a70f199ee172
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:43:24.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7182" for this suite.
Aug  8 15:43:30.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:43:31.587: INFO: namespace configmap-7182 deletion completed in 6.795207578s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:43:31.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 15:43:31.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-851ebbfc-3c78-4770-ba10-3c7628f60c24" in namespace "projected-6342" to be "success or failure"
Aug  8 15:43:31.895: INFO: Pod "downwardapi-volume-851ebbfc-3c78-4770-ba10-3c7628f60c24": Phase="Pending", Reason="", readiness=false. Elapsed: 19.360404ms
Aug  8 15:43:33.916: INFO: Pod "downwardapi-volume-851ebbfc-3c78-4770-ba10-3c7628f60c24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040208037s
Aug  8 15:43:35.937: INFO: Pod "downwardapi-volume-851ebbfc-3c78-4770-ba10-3c7628f60c24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061604266s
STEP: Saw pod success
Aug  8 15:43:35.937: INFO: Pod "downwardapi-volume-851ebbfc-3c78-4770-ba10-3c7628f60c24" satisfied condition "success or failure"
Aug  8 15:43:35.957: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-851ebbfc-3c78-4770-ba10-3c7628f60c24 container client-container: <nil>
STEP: delete the pod
Aug  8 15:43:36.013: INFO: Waiting for pod downwardapi-volume-851ebbfc-3c78-4770-ba10-3c7628f60c24 to disappear
Aug  8 15:43:36.032: INFO: Pod downwardapi-volume-851ebbfc-3c78-4770-ba10-3c7628f60c24 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:43:36.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6342" for this suite.
Aug  8 15:43:42.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:43:42.873: INFO: namespace projected-6342 deletion completed in 6.803924299s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:43:42.873: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug  8 15:43:47.805: INFO: Successfully updated pod "labelsupdate38f2ec21-b0e1-4340-aed3-fb4fa2f91813"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:43:49.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5059" for this suite.
Aug  8 15:44:11.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:44:12.691: INFO: namespace projected-5059 deletion completed in 22.794224148s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:44:12.691: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4521
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f6c2244f-a2c3-45cd-81b4-d036d3e2ad3b
STEP: Creating a pod to test consume configMaps
Aug  8 15:44:12.991: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7a93ca6-e4b8-4000-9743-84cc2ec13f1d" in namespace "projected-4521" to be "success or failure"
Aug  8 15:44:13.011: INFO: Pod "pod-projected-configmaps-d7a93ca6-e4b8-4000-9743-84cc2ec13f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.754335ms
Aug  8 15:44:15.032: INFO: Pod "pod-projected-configmaps-d7a93ca6-e4b8-4000-9743-84cc2ec13f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040801825s
Aug  8 15:44:17.052: INFO: Pod "pod-projected-configmaps-d7a93ca6-e4b8-4000-9743-84cc2ec13f1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061573624s
STEP: Saw pod success
Aug  8 15:44:17.053: INFO: Pod "pod-projected-configmaps-d7a93ca6-e4b8-4000-9743-84cc2ec13f1d" satisfied condition "success or failure"
Aug  8 15:44:17.072: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-configmaps-d7a93ca6-e4b8-4000-9743-84cc2ec13f1d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 15:44:17.177: INFO: Waiting for pod pod-projected-configmaps-d7a93ca6-e4b8-4000-9743-84cc2ec13f1d to disappear
Aug  8 15:44:17.196: INFO: Pod pod-projected-configmaps-d7a93ca6-e4b8-4000-9743-84cc2ec13f1d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:44:17.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4521" for this suite.
Aug  8 15:44:23.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:44:24.071: INFO: namespace projected-4521 deletion completed in 6.838474946s
•SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:44:24.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2130
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-76f7ad05-3d02-46bc-8cae-1e8224d3deb4
STEP: Creating a pod to test consume secrets
Aug  8 15:44:24.395: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97127cfd-0fa1-4fff-b7de-7e3ec138e7be" in namespace "projected-2130" to be "success or failure"
Aug  8 15:44:24.414: INFO: Pod "pod-projected-secrets-97127cfd-0fa1-4fff-b7de-7e3ec138e7be": Phase="Pending", Reason="", readiness=false. Elapsed: 18.991459ms
Aug  8 15:44:26.434: INFO: Pod "pod-projected-secrets-97127cfd-0fa1-4fff-b7de-7e3ec138e7be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039418655s
Aug  8 15:44:28.455: INFO: Pod "pod-projected-secrets-97127cfd-0fa1-4fff-b7de-7e3ec138e7be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059851789s
STEP: Saw pod success
Aug  8 15:44:28.455: INFO: Pod "pod-projected-secrets-97127cfd-0fa1-4fff-b7de-7e3ec138e7be" satisfied condition "success or failure"
Aug  8 15:44:28.474: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-secrets-97127cfd-0fa1-4fff-b7de-7e3ec138e7be container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  8 15:44:28.541: INFO: Waiting for pod pod-projected-secrets-97127cfd-0fa1-4fff-b7de-7e3ec138e7be to disappear
Aug  8 15:44:28.560: INFO: Pod pod-projected-secrets-97127cfd-0fa1-4fff-b7de-7e3ec138e7be no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:44:28.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2130" for this suite.
Aug  8 15:44:34.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:44:35.412: INFO: namespace projected-2130 deletion completed in 6.816310358s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:44:35.413: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:44:39.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-951" for this suite.
Aug  8 15:45:27.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:45:28.602: INFO: namespace kubelet-test-951 deletion completed in 48.797945395s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:45:28.602: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug  8 15:45:31.981: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:45:32.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4609" for this suite.
Aug  8 15:45:38.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:45:38.875: INFO: namespace container-runtime-4609 deletion completed in 6.812216493s
•SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:45:38.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1457
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug  8 15:45:40.215: INFO: Pod name wrapped-volume-race-c9a8f8ea-4539-46cd-a561-be64a19eecb6: Found 3 pods out of 5
Aug  8 15:45:45.257: INFO: Pod name wrapped-volume-race-c9a8f8ea-4539-46cd-a561-be64a19eecb6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c9a8f8ea-4539-46cd-a561-be64a19eecb6 in namespace emptydir-wrapper-1457, will wait for the garbage collector to delete the pods
Aug  8 15:45:57.469: INFO: Deleting ReplicationController wrapped-volume-race-c9a8f8ea-4539-46cd-a561-be64a19eecb6 took: 22.17036ms
Aug  8 15:45:57.969: INFO: Terminating ReplicationController wrapped-volume-race-c9a8f8ea-4539-46cd-a561-be64a19eecb6 pods took: 500.373285ms
STEP: Creating RC which spawns configmap-volume pods
Aug  8 15:46:37.437: INFO: Pod name wrapped-volume-race-2b30a26d-176e-42f9-8c10-f99e6afad6ed: Found 3 pods out of 5
Aug  8 15:46:42.481: INFO: Pod name wrapped-volume-race-2b30a26d-176e-42f9-8c10-f99e6afad6ed: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2b30a26d-176e-42f9-8c10-f99e6afad6ed in namespace emptydir-wrapper-1457, will wait for the garbage collector to delete the pods
Aug  8 15:46:50.694: INFO: Deleting ReplicationController wrapped-volume-race-2b30a26d-176e-42f9-8c10-f99e6afad6ed took: 21.699065ms
Aug  8 15:46:51.302: INFO: Terminating ReplicationController wrapped-volume-race-2b30a26d-176e-42f9-8c10-f99e6afad6ed pods took: 608.383319ms
STEP: Creating RC which spawns configmap-volume pods
Aug  8 15:47:37.370: INFO: Pod name wrapped-volume-race-f3cfd1b7-b652-4893-8b9c-9951d7884445: Found 0 pods out of 5
Aug  8 15:47:42.410: INFO: Pod name wrapped-volume-race-f3cfd1b7-b652-4893-8b9c-9951d7884445: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f3cfd1b7-b652-4893-8b9c-9951d7884445 in namespace emptydir-wrapper-1457, will wait for the garbage collector to delete the pods
Aug  8 15:47:50.623: INFO: Deleting ReplicationController wrapped-volume-race-f3cfd1b7-b652-4893-8b9c-9951d7884445 took: 24.054726ms
Aug  8 15:47:51.023: INFO: Terminating ReplicationController wrapped-volume-race-f3cfd1b7-b652-4893-8b9c-9951d7884445 pods took: 400.301911ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:48:38.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1457" for this suite.
Aug  8 15:48:46.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:48:47.343: INFO: namespace emptydir-wrapper-1457 deletion completed in 8.794234032s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:48:47.344: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug  8 15:48:47.752: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:48:53.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8840" for this suite.
Aug  8 15:48:59.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:49:00.202: INFO: namespace init-container-8840 deletion completed in 6.820794158s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:49:00.202: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8767
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-1213d8fd-3b0d-46c9-8eae-ad051b25fb1a
STEP: Creating a pod to test consume configMaps
Aug  8 15:49:00.493: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b5bc68f4-8ee3-4e32-b727-0dce7d0f0fe0" in namespace "projected-8767" to be "success or failure"
Aug  8 15:49:00.514: INFO: Pod "pod-projected-configmaps-b5bc68f4-8ee3-4e32-b727-0dce7d0f0fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.816925ms
Aug  8 15:49:02.534: INFO: Pod "pod-projected-configmaps-b5bc68f4-8ee3-4e32-b727-0dce7d0f0fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041160362s
Aug  8 15:49:04.555: INFO: Pod "pod-projected-configmaps-b5bc68f4-8ee3-4e32-b727-0dce7d0f0fe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061771732s
STEP: Saw pod success
Aug  8 15:49:04.555: INFO: Pod "pod-projected-configmaps-b5bc68f4-8ee3-4e32-b727-0dce7d0f0fe0" satisfied condition "success or failure"
Aug  8 15:49:04.575: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-configmaps-b5bc68f4-8ee3-4e32-b727-0dce7d0f0fe0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 15:49:04.643: INFO: Waiting for pod pod-projected-configmaps-b5bc68f4-8ee3-4e32-b727-0dce7d0f0fe0 to disappear
Aug  8 15:49:04.663: INFO: Pod pod-projected-configmaps-b5bc68f4-8ee3-4e32-b727-0dce7d0f0fe0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:49:04.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8767" for this suite.
Aug  8 15:49:10.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:49:11.509: INFO: namespace projected-8767 deletion completed in 6.80961338s
•SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:49:11.509: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-0eb564c5-c9d8-4d6f-9ce7-9ae81147a673
STEP: Creating a pod to test consume secrets
Aug  8 15:49:11.795: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7e22012a-aa67-4ccc-8b97-0fa82380d78b" in namespace "projected-2139" to be "success or failure"
Aug  8 15:49:11.814: INFO: Pod "pod-projected-secrets-7e22012a-aa67-4ccc-8b97-0fa82380d78b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.875697ms
Aug  8 15:49:13.834: INFO: Pod "pod-projected-secrets-7e22012a-aa67-4ccc-8b97-0fa82380d78b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039165491s
Aug  8 15:49:15.855: INFO: Pod "pod-projected-secrets-7e22012a-aa67-4ccc-8b97-0fa82380d78b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059695553s
STEP: Saw pod success
Aug  8 15:49:15.855: INFO: Pod "pod-projected-secrets-7e22012a-aa67-4ccc-8b97-0fa82380d78b" satisfied condition "success or failure"
Aug  8 15:49:15.874: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-secrets-7e22012a-aa67-4ccc-8b97-0fa82380d78b container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  8 15:49:15.925: INFO: Waiting for pod pod-projected-secrets-7e22012a-aa67-4ccc-8b97-0fa82380d78b to disappear
Aug  8 15:49:15.945: INFO: Pod pod-projected-secrets-7e22012a-aa67-4ccc-8b97-0fa82380d78b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:49:15.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2139" for this suite.
Aug  8 15:49:22.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:49:22.770: INFO: namespace projected-2139 deletion completed in 6.787088975s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:49:22.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug  8 15:49:23.137: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8400'
Aug  8 15:49:23.894: INFO: stderr: ""
Aug  8 15:49:23.894: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug  8 15:49:24.914: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 15:49:24.914: INFO: Found 0 / 1
Aug  8 15:49:25.914: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 15:49:25.914: INFO: Found 0 / 1
Aug  8 15:49:26.914: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 15:49:26.914: INFO: Found 1 / 1
Aug  8 15:49:26.914: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug  8 15:49:26.934: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 15:49:26.934: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug  8 15:49:26.934: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-xvh57 --namespace=kubectl-8400 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug  8 15:49:27.108: INFO: stderr: ""
Aug  8 15:49:27.108: INFO: stdout: "pod/redis-master-xvh57 patched\n"
STEP: checking annotations
Aug  8 15:49:27.128: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 15:49:27.128: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:49:27.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8400" for this suite.
Aug  8 15:49:49.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:49:50.023: INFO: namespace kubectl-8400 deletion completed in 22.857373631s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:49:50.024: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:49:54.482: INFO: Waiting up to 5m0s for pod "client-envvars-69555b56-c6c6-4073-b502-83270bdc6d69" in namespace "pods-1268" to be "success or failure"
Aug  8 15:49:54.507: INFO: Pod "client-envvars-69555b56-c6c6-4073-b502-83270bdc6d69": Phase="Pending", Reason="", readiness=false. Elapsed: 24.33264ms
Aug  8 15:49:56.527: INFO: Pod "client-envvars-69555b56-c6c6-4073-b502-83270bdc6d69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044526057s
Aug  8 15:49:58.547: INFO: Pod "client-envvars-69555b56-c6c6-4073-b502-83270bdc6d69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065005205s
STEP: Saw pod success
Aug  8 15:49:58.547: INFO: Pod "client-envvars-69555b56-c6c6-4073-b502-83270bdc6d69" satisfied condition "success or failure"
Aug  8 15:49:58.567: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod client-envvars-69555b56-c6c6-4073-b502-83270bdc6d69 container env3cont: <nil>
STEP: delete the pod
Aug  8 15:49:58.619: INFO: Waiting for pod client-envvars-69555b56-c6c6-4073-b502-83270bdc6d69 to disappear
Aug  8 15:49:58.641: INFO: Pod client-envvars-69555b56-c6c6-4073-b502-83270bdc6d69 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:49:58.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1268" for this suite.
Aug  8 15:50:46.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:50:47.476: INFO: namespace pods-1268 deletion completed in 48.791540915s
•SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:50:47.476: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-jp8g
STEP: Creating a pod to test atomic-volume-subpath
Aug  8 15:50:47.807: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-jp8g" in namespace "subpath-4916" to be "success or failure"
Aug  8 15:50:47.826: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Pending", Reason="", readiness=false. Elapsed: 19.300088ms
Aug  8 15:50:49.846: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039398554s
Aug  8 15:50:51.866: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 4.059296267s
Aug  8 15:50:53.886: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 6.0796203s
Aug  8 15:50:55.906: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 8.099735265s
Aug  8 15:50:57.927: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 10.120715993s
Aug  8 15:50:59.948: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 12.140996459s
Aug  8 15:51:01.968: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 14.161078049s
Aug  8 15:51:03.988: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 16.18105455s
Aug  8 15:51:06.007: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 18.200800526s
Aug  8 15:51:08.028: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 20.22094479s
Aug  8 15:51:10.048: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Running", Reason="", readiness=true. Elapsed: 22.241748143s
Aug  8 15:51:12.068: INFO: Pod "pod-subpath-test-projected-jp8g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.261634619s
STEP: Saw pod success
Aug  8 15:51:12.068: INFO: Pod "pod-subpath-test-projected-jp8g" satisfied condition "success or failure"
Aug  8 15:51:12.091: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-subpath-test-projected-jp8g container test-container-subpath-projected-jp8g: <nil>
STEP: delete the pod
Aug  8 15:51:12.147: INFO: Waiting for pod pod-subpath-test-projected-jp8g to disappear
Aug  8 15:51:12.172: INFO: Pod pod-subpath-test-projected-jp8g no longer exists
STEP: Deleting pod pod-subpath-test-projected-jp8g
Aug  8 15:51:12.172: INFO: Deleting pod "pod-subpath-test-projected-jp8g" in namespace "subpath-4916"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:51:12.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4916" for this suite.
Aug  8 15:51:18.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:51:19.005: INFO: namespace subpath-4916 deletion completed in 6.776286613s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:51:19.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-5cee284f-7325-4586-8c97-9c569ac53f7c
STEP: Creating a pod to test consume configMaps
Aug  8 15:51:19.296: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-91689dce-2b75-4eab-b474-6bb29c01d7d2" in namespace "projected-9085" to be "success or failure"
Aug  8 15:51:19.315: INFO: Pod "pod-projected-configmaps-91689dce-2b75-4eab-b474-6bb29c01d7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.929818ms
Aug  8 15:51:21.336: INFO: Pod "pod-projected-configmaps-91689dce-2b75-4eab-b474-6bb29c01d7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039459463s
Aug  8 15:51:23.357: INFO: Pod "pod-projected-configmaps-91689dce-2b75-4eab-b474-6bb29c01d7d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060005397s
STEP: Saw pod success
Aug  8 15:51:23.357: INFO: Pod "pod-projected-configmaps-91689dce-2b75-4eab-b474-6bb29c01d7d2" satisfied condition "success or failure"
Aug  8 15:51:23.376: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-configmaps-91689dce-2b75-4eab-b474-6bb29c01d7d2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 15:51:23.431: INFO: Waiting for pod pod-projected-configmaps-91689dce-2b75-4eab-b474-6bb29c01d7d2 to disappear
Aug  8 15:51:23.451: INFO: Pod pod-projected-configmaps-91689dce-2b75-4eab-b474-6bb29c01d7d2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:51:23.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9085" for this suite.
Aug  8 15:51:29.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:51:30.297: INFO: namespace projected-9085 deletion completed in 6.809639841s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:51:30.297: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9192
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9192
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug  8 15:51:30.553: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug  8 15:51:58.909: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.116:8080/dial?request=hostName&protocol=http&host=100.96.0.59&port=8080&tries=1'] Namespace:pod-network-test-9192 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:51:58.909: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:51:59.427: INFO: Waiting for endpoints: map[]
Aug  8 15:51:59.447: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.116:8080/dial?request=hostName&protocol=http&host=100.96.1.115&port=8080&tries=1'] Namespace:pod-network-test-9192 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:51:59.447: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:51:59.909: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:51:59.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9192" for this suite.
Aug  8 15:52:22.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:52:22.748: INFO: namespace pod-network-test-9192 deletion completed in 22.799961029s
•SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:52:22.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-82
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-2917
STEP: Creating secret with name secret-test-7e2d7e87-e40f-4405-ad52-5bf3e815d622
STEP: Creating a pod to test consume secrets
Aug  8 15:52:23.393: INFO: Waiting up to 5m0s for pod "pod-secrets-0037d154-aeb3-4e73-89a6-389564895951" in namespace "secrets-82" to be "success or failure"
Aug  8 15:52:23.412: INFO: Pod "pod-secrets-0037d154-aeb3-4e73-89a6-389564895951": Phase="Pending", Reason="", readiness=false. Elapsed: 18.843409ms
Aug  8 15:52:25.432: INFO: Pod "pod-secrets-0037d154-aeb3-4e73-89a6-389564895951": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039202359s
Aug  8 15:52:27.452: INFO: Pod "pod-secrets-0037d154-aeb3-4e73-89a6-389564895951": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059577347s
STEP: Saw pod success
Aug  8 15:52:27.452: INFO: Pod "pod-secrets-0037d154-aeb3-4e73-89a6-389564895951" satisfied condition "success or failure"
Aug  8 15:52:27.472: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-secrets-0037d154-aeb3-4e73-89a6-389564895951 container secret-volume-test: <nil>
STEP: delete the pod
Aug  8 15:52:27.534: INFO: Waiting for pod pod-secrets-0037d154-aeb3-4e73-89a6-389564895951 to disappear
Aug  8 15:52:27.553: INFO: Pod pod-secrets-0037d154-aeb3-4e73-89a6-389564895951 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:52:27.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-82" for this suite.
Aug  8 15:52:33.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:52:34.417: INFO: namespace secrets-82 deletion completed in 6.826824711s
STEP: Destroying namespace "secret-namespace-2917" for this suite.
Aug  8 15:52:40.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:52:41.264: INFO: namespace secret-namespace-2917 deletion completed in 6.84696783s
•SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:52:41.264: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 15:52:41.586: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug  8 15:52:45.627: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug  8 15:52:49.803: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-7361,SelfLink:/apis/apps/v1/namespaces/deployment-7361/deployments/test-cleanup-deployment,UID:40c903e7-f615-46bb-888d-4cf9fcf0ca08,ResourceVersion:13933,Generation:1,CreationTimestamp:2019-08-08 15:52:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-08 15:52:45 +0000 UTC 2019-08-08 15:52:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-08 15:52:49 +0000 UTC 2019-08-08 15:52:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug  8 15:52:49.823: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-7361,SelfLink:/apis/apps/v1/namespaces/deployment-7361/replicasets/test-cleanup-deployment-55bbcbc84c,UID:2555dc99-cb93-4a0a-b5d1-f43a0f1ad54e,ResourceVersion:13926,Generation:1,CreationTimestamp:2019-08-08 15:52:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 40c903e7-f615-46bb-888d-4cf9fcf0ca08 0xc003bdbcd7 0xc003bdbcd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug  8 15:52:49.843: INFO: Pod "test-cleanup-deployment-55bbcbc84c-drjfb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-drjfb,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-7361,SelfLink:/api/v1/namespaces/deployment-7361/pods/test-cleanup-deployment-55bbcbc84c-drjfb,UID:928ceb72-b377-4e91-8f3d-b04739dbb739,ResourceVersion:13925,Generation:0,CreationTimestamp:2019-08-08 15:52:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.119/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 2555dc99-cb93-4a0a-b5d1-f43a0f1ad54e 0xc002b142e7 0xc002b142e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5hc9n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5hc9n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-5hc9n true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b14350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b14370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:52:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:52:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:52:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:52:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.119,StartTime:2019-08-08 15:52:45 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-08 15:52:48 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://8990a6cab8145f271cb5a139fb32366e9ad195043e2bc3905b13a80d5b24febe}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:52:49.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7361" for this suite.
Aug  8 15:52:55.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:52:56.905: INFO: namespace deployment-7361 deletion completed in 7.025377916s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:52:56.906: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6539
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug  8 15:52:57.250: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug  8 15:53:25.593: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.60:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6539 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:53:25.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:53:26.085: INFO: Found all expected endpoints: [netserver-0]
Aug  8 15:53:26.104: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.120:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6539 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 15:53:26.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 15:53:26.562: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:53:26.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6539" for this suite.
Aug  8 15:53:48.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:53:49.742: INFO: namespace pod-network-test-6539 deletion completed in 23.143846962s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:53:49.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 15:53:50.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4cd2f89-7649-47eb-af13-d5cabb1dd55d" in namespace "downward-api-6002" to be "success or failure"
Aug  8 15:53:50.092: INFO: Pod "downwardapi-volume-a4cd2f89-7649-47eb-af13-d5cabb1dd55d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.112313ms
Aug  8 15:53:52.112: INFO: Pod "downwardapi-volume-a4cd2f89-7649-47eb-af13-d5cabb1dd55d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039158637s
Aug  8 15:53:54.133: INFO: Pod "downwardapi-volume-a4cd2f89-7649-47eb-af13-d5cabb1dd55d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059341491s
STEP: Saw pod success
Aug  8 15:53:54.133: INFO: Pod "downwardapi-volume-a4cd2f89-7649-47eb-af13-d5cabb1dd55d" satisfied condition "success or failure"
Aug  8 15:53:54.152: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-a4cd2f89-7649-47eb-af13-d5cabb1dd55d container client-container: <nil>
STEP: delete the pod
Aug  8 15:53:54.414: INFO: Waiting for pod downwardapi-volume-a4cd2f89-7649-47eb-af13-d5cabb1dd55d to disappear
Aug  8 15:53:54.433: INFO: Pod downwardapi-volume-a4cd2f89-7649-47eb-af13-d5cabb1dd55d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 15:53:54.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6002" for this suite.
Aug  8 15:54:00.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 15:54:01.302: INFO: namespace downward-api-6002 deletion completed in 6.832880042s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 15:54:01.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4924
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4924
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-4924
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4924
Aug  8 15:54:01.620: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug  8 15:54:11.642: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug  8 15:54:11.661: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 15:54:12.357: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 15:54:12.357: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 15:54:12.357: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  8 15:54:12.378: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug  8 15:54:22.399: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug  8 15:54:22.399: INFO: Waiting for statefulset status.replicas updated to 0
Aug  8 15:54:22.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999275s
Aug  8 15:54:23.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.972077141s
Aug  8 15:54:24.530: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.951363914s
Aug  8 15:54:25.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.930816369s
Aug  8 15:54:26.572: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.909561474s
Aug  8 15:54:27.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.888951542s
Aug  8 15:54:28.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.869030097s
Aug  8 15:54:29.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.848891669s
Aug  8 15:54:30.653: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.828808968s
Aug  8 15:54:31.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 808.235106ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4924
Aug  8 15:54:32.693: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:54:33.317: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug  8 15:54:33.317: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  8 15:54:33.317: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  8 15:54:33.317: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:54:33.980: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug  8 15:54:33.981: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  8 15:54:33.981: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  8 15:54:33.981: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:54:34.595: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug  8 15:54:34.595: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  8 15:54:34.595: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  8 15:54:34.615: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 15:54:34.615: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 15:54:34.615: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug  8 15:54:34.635: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 15:54:35.306: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 15:54:35.306: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 15:54:35.306: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  8 15:54:35.306: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 15:54:35.962: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 15:54:35.962: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 15:54:35.962: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  8 15:54:35.962: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 15:54:36.701: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 15:54:36.701: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 15:54:36.701: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  8 15:54:36.701: INFO: Waiting for statefulset status.replicas updated to 0
Aug  8 15:54:36.720: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug  8 15:54:46.761: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug  8 15:54:46.761: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug  8 15:54:46.761: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug  8 15:54:46.824: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:46.824: INFO: ss-0  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:01 +0000 UTC  }]
Aug  8 15:54:46.824: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:46.824: INFO: ss-2  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:46.824: INFO: 
Aug  8 15:54:46.824: INFO: StatefulSet ss has not reached scale 0, at 3
Aug  8 15:54:47.845: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:47.845: INFO: ss-0  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:01 +0000 UTC  }]
Aug  8 15:54:47.845: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:47.845: INFO: ss-2  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:47.845: INFO: 
Aug  8 15:54:47.845: INFO: StatefulSet ss has not reached scale 0, at 3
Aug  8 15:54:48.878: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:48.878: INFO: ss-0  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:01 +0000 UTC  }]
Aug  8 15:54:48.878: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:48.878: INFO: ss-2  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:48.878: INFO: 
Aug  8 15:54:48.878: INFO: StatefulSet ss has not reached scale 0, at 3
Aug  8 15:54:49.898: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:49.898: INFO: ss-0  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:01 +0000 UTC  }]
Aug  8 15:54:49.898: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:49.898: INFO: ss-2  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:49.898: INFO: 
Aug  8 15:54:49.898: INFO: StatefulSet ss has not reached scale 0, at 3
Aug  8 15:54:50.918: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:50.918: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:50.918: INFO: ss-2  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:50.918: INFO: 
Aug  8 15:54:50.918: INFO: StatefulSet ss has not reached scale 0, at 2
Aug  8 15:54:51.938: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:51.938: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:51.938: INFO: ss-2  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:51.938: INFO: 
Aug  8 15:54:51.939: INFO: StatefulSet ss has not reached scale 0, at 2
Aug  8 15:54:52.959: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:52.959: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:52.959: INFO: ss-2  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:52.959: INFO: 
Aug  8 15:54:52.959: INFO: StatefulSet ss has not reached scale 0, at 2
Aug  8 15:54:53.979: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:53.979: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:53.979: INFO: 
Aug  8 15:54:53.979: INFO: StatefulSet ss has not reached scale 0, at 1
Aug  8 15:54:54.999: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:54.999: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:54.999: INFO: 
Aug  8 15:54:54.999: INFO: StatefulSet ss has not reached scale 0, at 1
Aug  8 15:54:56.021: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Aug  8 15:54:56.021: INFO: ss-1  shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 15:54:22 +0000 UTC  }]
Aug  8 15:54:56.021: INFO: 
Aug  8 15:54:56.021: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4924
Aug  8 15:54:57.041: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:54:57.229: INFO: rc: 1
Aug  8 15:54:57.229: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00398f650 exit status 1 <nil> <nil> true [0xc00272e618 0xc00272e630 0xc00272e648] [0xc00272e618 0xc00272e630 0xc00272e648] [0xc00272e628 0xc00272e640] [0x9d17b0 0x9d17b0] 0xc000c01b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:55:07.230: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:55:07.399: INFO: rc: 1
Aug  8 15:55:07.399: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00398fbf0 exit status 1 <nil> <nil> true [0xc00272e650 0xc00272e668 0xc00272e680] [0xc00272e650 0xc00272e668 0xc00272e680] [0xc00272e660 0xc00272e678] [0x9d17b0 0x9d17b0] 0xc000c01e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:55:17.400: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:55:17.564: INFO: rc: 1
Aug  8 15:55:17.565: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0025f1770 exit status 1 <nil> <nil> true [0xc002516508 0xc002516520 0xc002516538] [0xc002516508 0xc002516520 0xc002516538] [0xc002516518 0xc002516530] [0x9d17b0 0x9d17b0] 0xc002a14f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:55:27.565: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:55:27.731: INFO: rc: 1
Aug  8 15:55:27.731: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00091ed20 exit status 1 <nil> <nil> true [0xc0003336f8 0xc000333778 0xc0003337c0] [0xc0003336f8 0xc000333778 0xc0003337c0] [0xc000333750 0xc0003337b0] [0x9d17b0 0x9d17b0] 0xc003886a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:55:37.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:55:37.906: INFO: rc: 1
Aug  8 15:55:37.906: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0025f1d40 exit status 1 <nil> <nil> true [0xc002516540 0xc002516560 0xc002516578] [0xc002516540 0xc002516560 0xc002516578] [0xc002516558 0xc002516570] [0x9d17b0 0x9d17b0] 0xc002a15800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:55:47.907: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:55:48.073: INFO: rc: 1
Aug  8 15:55:48.073: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003ac4540 exit status 1 <nil> <nil> true [0xc00065ecf0 0xc00065ee38 0xc00065f548] [0xc00065ecf0 0xc00065ee38 0xc00065f548] [0xc00065ee18 0xc00065f4b8] [0x9d17b0 0x9d17b0] 0xc002618900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:55:58.074: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:55:58.246: INFO: rc: 1
Aug  8 15:55:58.246: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003834570 exit status 1 <nil> <nil> true [0xc000010060 0xc0006944a8 0xc000694598] [0xc000010060 0xc0006944a8 0xc000694598] [0xc0006943c0 0xc000694580] [0x9d17b0 0x9d17b0] 0xc002377260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:56:08.247: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:56:08.418: INFO: rc: 1
Aug  8 15:56:08.419: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003834ae0 exit status 1 <nil> <nil> true [0xc000694678 0xc000694ea0 0xc0006950a0] [0xc000694678 0xc000694ea0 0xc0006950a0] [0xc000694d58 0xc000694ff8] [0x9d17b0 0x9d17b0] 0xc001c21860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:56:18.419: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:56:18.627: INFO: rc: 1
Aug  8 15:56:18.627: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038350e0 exit status 1 <nil> <nil> true [0xc0006951b8 0xc0006955c0 0xc0006957f8] [0xc0006951b8 0xc0006955c0 0xc0006957f8] [0xc0006954f8 0xc000695758] [0x9d17b0 0x9d17b0] 0xc0020701e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:56:28.628: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:56:28.809: INFO: rc: 1
Aug  8 15:56:28.809: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002b38540 exit status 1 <nil> <nil> true [0xc000334248 0xc000334e50 0xc000335170] [0xc000334248 0xc000334e50 0xc000335170] [0xc000334818 0xc000335150] [0x9d17b0 0x9d17b0] 0xc001fbe480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:56:38.809: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:56:38.994: INFO: rc: 1
Aug  8 15:56:38.994: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003835680 exit status 1 <nil> <nil> true [0xc000695840 0xc000695a70 0xc000695bb8] [0xc000695840 0xc000695a70 0xc000695bb8] [0xc0006959b8 0xc000695b50] [0x9d17b0 0x9d17b0] 0xc001d86600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:56:48.995: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:56:49.166: INFO: rc: 1
Aug  8 15:56:49.167: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0010b0600 exit status 1 <nil> <nil> true [0xc0006b87f0 0xc0006b88e0 0xc0006b8a08] [0xc0006b87f0 0xc0006b88e0 0xc0006b8a08] [0xc0006b88d8 0xc0006b89e0] [0x9d17b0 0x9d17b0] 0xc00194e480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:56:59.167: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:56:59.350: INFO: rc: 1
Aug  8 15:56:59.350: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003ac4b10 exit status 1 <nil> <nil> true [0xc00065f7e8 0xc00065fa10 0xc00065fad0] [0xc00065f7e8 0xc00065fa10 0xc00065fad0] [0xc00065f970 0xc00065fac8] [0x9d17b0 0x9d17b0] 0xc00171c000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:57:09.350: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:57:09.532: INFO: rc: 1
Aug  8 15:57:09.532: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002b38ae0 exit status 1 <nil> <nil> true [0xc0003351b0 0xc0003353d8 0xc000335540] [0xc0003351b0 0xc0003353d8 0xc000335540] [0xc0003353b8 0xc000335498] [0x9d17b0 0x9d17b0] 0xc001710060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:57:19.532: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:57:19.709: INFO: rc: 1
Aug  8 15:57:19.709: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003ac50e0 exit status 1 <nil> <nil> true [0xc00065fbe8 0xc00065fd90 0xc00065ffe0] [0xc00065fbe8 0xc00065fd90 0xc00065ffe0] [0xc00065fcf8 0xc00065ffc0] [0x9d17b0 0x9d17b0] 0xc002113da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:57:29.709: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:57:29.892: INFO: rc: 1
Aug  8 15:57:29.892: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002b39230 exit status 1 <nil> <nil> true [0xc000335580 0xc000335740 0xc000335810] [0xc000335580 0xc000335740 0xc000335810] [0xc0003356e8 0xc000335808] [0x9d17b0 0x9d17b0] 0xc001db12c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:57:39.892: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:57:40.065: INFO: rc: 1
Aug  8 15:57:40.065: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003ac5680 exit status 1 <nil> <nil> true [0xc00065fff8 0xc00272e010 0xc00272e028] [0xc00065fff8 0xc00272e010 0xc00272e028] [0xc00272e008 0xc00272e020] [0x9d17b0 0x9d17b0] 0xc002569200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:57:50.066: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:57:50.246: INFO: rc: 1
Aug  8 15:57:50.246: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002b38570 exit status 1 <nil> <nil> true [0xc0000105a0 0xc00065edc0 0xc00065f0a8] [0xc0000105a0 0xc00065edc0 0xc00065f0a8] [0xc00065ecf0 0xc00065ee38] [0x9d17b0 0x9d17b0] 0xc001ad2b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:58:00.246: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:58:00.407: INFO: rc: 1
Aug  8 15:58:00.407: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0010b0570 exit status 1 <nil> <nil> true [0xc000334248 0xc000334e50 0xc000335170] [0xc000334248 0xc000334e50 0xc000335170] [0xc000334818 0xc000335150] [0x9d17b0 0x9d17b0] 0xc001dcfb60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:58:10.408: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:58:10.565: INFO: rc: 1
Aug  8 15:58:10.565: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0038345a0 exit status 1 <nil> <nil> true [0xc0006b87f0 0xc0006b88e0 0xc0006b8a08] [0xc0006b87f0 0xc0006b88e0 0xc0006b8a08] [0xc0006b88d8 0xc0006b89e0] [0x9d17b0 0x9d17b0] 0xc000c04000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:58:20.565: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:58:20.737: INFO: rc: 1
Aug  8 15:58:20.737: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002b38b40 exit status 1 <nil> <nil> true [0xc00065f4b8 0xc00065f8f0 0xc00065fab0] [0xc00065f4b8 0xc00065f8f0 0xc00065fab0] [0xc00065f7e8 0xc00065fa10] [0x9d17b0 0x9d17b0] 0xc001aa5ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:58:30.737: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:58:30.938: INFO: rc: 1
Aug  8 15:58:30.938: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003ac4570 exit status 1 <nil> <nil> true [0xc0006943c0 0xc000694580 0xc0006947e8] [0xc0006943c0 0xc000694580 0xc0006947e8] [0xc000694520 0xc000694678] [0x9d17b0 0x9d17b0] 0xc0019197a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:58:40.939: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:58:41.106: INFO: rc: 1
Aug  8 15:58:41.106: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003ac4b40 exit status 1 <nil> <nil> true [0xc000694d58 0xc000694ff8 0xc000695390] [0xc000694d58 0xc000694ff8 0xc000695390] [0xc000694f10 0xc0006951b8] [0x9d17b0 0x9d17b0] 0xc0021d2300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:58:51.106: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:58:51.310: INFO: rc: 1
Aug  8 15:58:51.310: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003ac5140 exit status 1 <nil> <nil> true [0xc0006954f8 0xc000695758 0xc000695858] [0xc0006954f8 0xc000695758 0xc000695858] [0xc0006956b0 0xc000695840] [0x9d17b0 0x9d17b0] 0xc001c21860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:59:01.310: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:59:01.524: INFO: rc: 1
Aug  8 15:59:01.524: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003ac5740 exit status 1 <nil> <nil> true [0xc0006959b8 0xc000695b50 0xc000695c78] [0xc0006959b8 0xc000695b50 0xc000695c78] [0xc000695af8 0xc000695c30] [0x9d17b0 0x9d17b0] 0xc002377260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:59:11.524: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:59:11.701: INFO: rc: 1
Aug  8 15:59:11.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002b39290 exit status 1 <nil> <nil> true [0xc00065fac8 0xc00065fc68 0xc00065fef8] [0xc00065fac8 0xc00065fc68 0xc00065fef8] [0xc00065fbe8 0xc00065fd90] [0x9d17b0 0x9d17b0] 0xc0026190e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:59:21.701: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:59:21.882: INFO: rc: 1
Aug  8 15:59:21.882: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003ac5ce0 exit status 1 <nil> <nil> true [0xc000695ca8 0xc000695d90 0xc000695ee8] [0xc000695ca8 0xc000695d90 0xc000695ee8] [0xc000695cf0 0xc000695e30] [0x9d17b0 0x9d17b0] 0xc001db0060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:59:31.882: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:59:32.295: INFO: rc: 1
Aug  8 15:59:32.295: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003834bd0 exit status 1 <nil> <nil> true [0xc0006b8a20 0xc0006b8ac8 0xc0006b8ba0] [0xc0006b8a20 0xc0006b8ac8 0xc0006b8ba0] [0xc0006b8a60 0xc0006b8b58] [0x9d17b0 0x9d17b0] 0xc002568840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:59:42.296: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:59:42.475: INFO: rc: 1
Aug  8 15:59:42.475: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002b39830 exit status 1 <nil> <nil> true [0xc00065ffc0 0xc00272e000 0xc00272e018] [0xc00065ffc0 0xc00272e000 0xc00272e018] [0xc00065fff8 0xc00272e010] [0x9d17b0 0x9d17b0] 0xc0028ee840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 15:59:52.476: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 15:59:52.648: INFO: rc: 1
Aug  8 15:59:52.648: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003834540 exit status 1 <nil> <nil> true [0xc00065ecf0 0xc00065ee38 0xc00065f548] [0xc00065ecf0 0xc00065ee38 0xc00065f548] [0xc00065ee18 0xc00065f4b8] [0x9d17b0 0x9d17b0] 0xc00232f380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug  8 16:00:02.649: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4924 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 16:00:02.823: INFO: rc: 1
Aug  8 16:00:02.823: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Aug  8 16:00:02.823: INFO: Scaling statefulset ss to 0
Aug  8 16:00:02.882: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug  8 16:00:02.902: INFO: Deleting all statefulset in ns statefulset-4924
Aug  8 16:00:02.924: INFO: Scaling statefulset ss to 0
Aug  8 16:00:02.982: INFO: Waiting for statefulset status.replicas updated to 0
Aug  8 16:00:03.001: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:00:03.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4924" for this suite.
Aug  8 16:00:09.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:00:09.885: INFO: namespace statefulset-4924 deletion completed in 6.788109335s

• [SLOW TEST:368.582 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:00:09.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 16:00:34.314: INFO: Container started at 2019-08-08 16:00:12 +0000 UTC, pod became ready at 2019-08-08 16:00:33 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:00:34.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6904" for this suite.
Aug  8 16:00:56.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:00:57.162: INFO: namespace container-probe-6904 deletion completed in 22.811160873s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:00:57.163: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-165bb00d-9102-4c4c-985f-8f27173e5a17
STEP: Creating a pod to test consume configMaps
Aug  8 16:00:57.492: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f535a965-f0ff-4007-ab1f-f10f422fc572" in namespace "projected-5507" to be "success or failure"
Aug  8 16:00:57.510: INFO: Pod "pod-projected-configmaps-f535a965-f0ff-4007-ab1f-f10f422fc572": Phase="Pending", Reason="", readiness=false. Elapsed: 18.76913ms
Aug  8 16:00:59.530: INFO: Pod "pod-projected-configmaps-f535a965-f0ff-4007-ab1f-f10f422fc572": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038600936s
Aug  8 16:01:01.551: INFO: Pod "pod-projected-configmaps-f535a965-f0ff-4007-ab1f-f10f422fc572": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059524238s
STEP: Saw pod success
Aug  8 16:01:01.551: INFO: Pod "pod-projected-configmaps-f535a965-f0ff-4007-ab1f-f10f422fc572" satisfied condition "success or failure"
Aug  8 16:01:01.571: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-configmaps-f535a965-f0ff-4007-ab1f-f10f422fc572 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 16:01:01.630: INFO: Waiting for pod pod-projected-configmaps-f535a965-f0ff-4007-ab1f-f10f422fc572 to disappear
Aug  8 16:01:01.652: INFO: Pod pod-projected-configmaps-f535a965-f0ff-4007-ab1f-f10f422fc572 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:01:01.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5507" for this suite.
Aug  8 16:01:07.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:01:08.504: INFO: namespace projected-5507 deletion completed in 6.815118864s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:01:08.504: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8915
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug  8 16:01:08.756: INFO: namespace kubectl-8915
Aug  8 16:01:08.756: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8915'
Aug  8 16:01:09.058: INFO: stderr: ""
Aug  8 16:01:09.058: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug  8 16:01:10.077: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 16:01:10.077: INFO: Found 0 / 1
Aug  8 16:01:11.078: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 16:01:11.078: INFO: Found 0 / 1
Aug  8 16:01:12.077: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 16:01:12.077: INFO: Found 1 / 1
Aug  8 16:01:12.077: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug  8 16:01:12.097: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 16:01:12.097: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug  8 16:01:12.097: INFO: wait on redis-master startup in kubectl-8915 
Aug  8 16:01:12.097: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-m46gc redis-master --namespace=kubectl-8915'
Aug  8 16:01:12.436: INFO: stderr: ""
Aug  8 16:01:12.436: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Aug 16:01:11.622 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Aug 16:01:11.622 # Server started, Redis version 3.2.12\n1:M 08 Aug 16:01:11.622 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Aug 16:01:11.622 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug  8 16:01:12.436: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8915'
Aug  8 16:01:12.661: INFO: stderr: ""
Aug  8 16:01:12.661: INFO: stdout: "service/rm2 exposed\n"
Aug  8 16:01:12.680: INFO: Service rm2 in namespace kubectl-8915 found.
STEP: exposing service
Aug  8 16:01:14.718: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8915'
Aug  8 16:01:14.926: INFO: stderr: ""
Aug  8 16:01:14.926: INFO: stdout: "service/rm3 exposed\n"
Aug  8 16:01:14.947: INFO: Service rm3 in namespace kubectl-8915 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:01:16.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8915" for this suite.
Aug  8 16:01:39.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:01:39.810: INFO: namespace kubectl-8915 deletion completed in 22.787003335s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:01:39.811: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:01:40.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50e1bd60-a522-4cae-843f-2c59981a132b" in namespace "downward-api-7967" to be "success or failure"
Aug  8 16:01:40.092: INFO: Pod "downwardapi-volume-50e1bd60-a522-4cae-843f-2c59981a132b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.953776ms
Aug  8 16:01:42.111: INFO: Pod "downwardapi-volume-50e1bd60-a522-4cae-843f-2c59981a132b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038854506s
Aug  8 16:01:44.131: INFO: Pod "downwardapi-volume-50e1bd60-a522-4cae-843f-2c59981a132b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05890314s
STEP: Saw pod success
Aug  8 16:01:44.132: INFO: Pod "downwardapi-volume-50e1bd60-a522-4cae-843f-2c59981a132b" satisfied condition "success or failure"
Aug  8 16:01:44.151: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-50e1bd60-a522-4cae-843f-2c59981a132b container client-container: <nil>
STEP: delete the pod
Aug  8 16:01:44.206: INFO: Waiting for pod downwardapi-volume-50e1bd60-a522-4cae-843f-2c59981a132b to disappear
Aug  8 16:01:44.227: INFO: Pod downwardapi-volume-50e1bd60-a522-4cae-843f-2c59981a132b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:01:44.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7967" for this suite.
Aug  8 16:01:50.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:01:51.049: INFO: namespace downward-api-7967 deletion completed in 6.78558599s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:01:51.049: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug  8 16:01:51.373: INFO: Waiting up to 5m0s for pod "pod-83962bf5-eb42-4988-a235-89f2f141e8d6" in namespace "emptydir-342" to be "success or failure"
Aug  8 16:01:51.392: INFO: Pod "pod-83962bf5-eb42-4988-a235-89f2f141e8d6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.809568ms
Aug  8 16:01:53.411: INFO: Pod "pod-83962bf5-eb42-4988-a235-89f2f141e8d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03868723s
Aug  8 16:01:55.432: INFO: Pod "pod-83962bf5-eb42-4988-a235-89f2f141e8d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059490736s
STEP: Saw pod success
Aug  8 16:01:55.432: INFO: Pod "pod-83962bf5-eb42-4988-a235-89f2f141e8d6" satisfied condition "success or failure"
Aug  8 16:01:55.452: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-83962bf5-eb42-4988-a235-89f2f141e8d6 container test-container: <nil>
STEP: delete the pod
Aug  8 16:01:55.515: INFO: Waiting for pod pod-83962bf5-eb42-4988-a235-89f2f141e8d6 to disappear
Aug  8 16:01:55.534: INFO: Pod pod-83962bf5-eb42-4988-a235-89f2f141e8d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:01:55.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-342" for this suite.
Aug  8 16:02:01.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:02:02.373: INFO: namespace emptydir-342 deletion completed in 6.802763967s
•SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:02:02.373: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-62bfaa2d-4a18-4bf8-a0a5-d7d6dbea1ad9
STEP: Creating a pod to test consume secrets
Aug  8 16:02:02.706: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4c329cb9-dac3-4126-9690-6291515ef97a" in namespace "projected-730" to be "success or failure"
Aug  8 16:02:02.725: INFO: Pod "pod-projected-secrets-4c329cb9-dac3-4126-9690-6291515ef97a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.072751ms
Aug  8 16:02:04.746: INFO: Pod "pod-projected-secrets-4c329cb9-dac3-4126-9690-6291515ef97a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039358985s
Aug  8 16:02:06.766: INFO: Pod "pod-projected-secrets-4c329cb9-dac3-4126-9690-6291515ef97a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059911523s
STEP: Saw pod success
Aug  8 16:02:06.766: INFO: Pod "pod-projected-secrets-4c329cb9-dac3-4126-9690-6291515ef97a" satisfied condition "success or failure"
Aug  8 16:02:06.785: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-secrets-4c329cb9-dac3-4126-9690-6291515ef97a container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  8 16:02:06.836: INFO: Waiting for pod pod-projected-secrets-4c329cb9-dac3-4126-9690-6291515ef97a to disappear
Aug  8 16:02:06.856: INFO: Pod pod-projected-secrets-4c329cb9-dac3-4126-9690-6291515ef97a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:02:06.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-730" for this suite.
Aug  8 16:02:12.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:02:13.687: INFO: namespace projected-730 deletion completed in 6.794910628s
•
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:02:13.687: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:02:13.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b93d7a6a-249a-4061-a63a-02429fcb46e0" in namespace "projected-9066" to be "success or failure"
Aug  8 16:02:14.006: INFO: Pod "downwardapi-volume-b93d7a6a-249a-4061-a63a-02429fcb46e0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.497661ms
Aug  8 16:02:16.026: INFO: Pod "downwardapi-volume-b93d7a6a-249a-4061-a63a-02429fcb46e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050593577s
Aug  8 16:02:18.047: INFO: Pod "downwardapi-volume-b93d7a6a-249a-4061-a63a-02429fcb46e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071502002s
STEP: Saw pod success
Aug  8 16:02:18.047: INFO: Pod "downwardapi-volume-b93d7a6a-249a-4061-a63a-02429fcb46e0" satisfied condition "success or failure"
Aug  8 16:02:18.066: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-b93d7a6a-249a-4061-a63a-02429fcb46e0 container client-container: <nil>
STEP: delete the pod
Aug  8 16:02:18.139: INFO: Waiting for pod downwardapi-volume-b93d7a6a-249a-4061-a63a-02429fcb46e0 to disappear
Aug  8 16:02:18.158: INFO: Pod downwardapi-volume-b93d7a6a-249a-4061-a63a-02429fcb46e0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:02:18.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9066" for this suite.
Aug  8 16:02:24.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:02:25.010: INFO: namespace projected-9066 deletion completed in 6.814930107s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:02:25.010: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8887
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug  8 16:02:25.250: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:02:31.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8887" for this suite.
Aug  8 16:02:37.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:02:37.993: INFO: namespace init-container-8887 deletion completed in 6.811834189s
•SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:02:37.993: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Aug  8 16:02:42.354: INFO: Pod pod-hostip-6f91418a-f7c6-4b94-a6b7-4e6e9518cfa0 has hostIP: 10.250.0.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:02:42.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-478" for this suite.
Aug  8 16:03:04.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:03:05.209: INFO: namespace pods-478 deletion completed in 22.818018015s
•SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:03:05.209: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Aug  8 16:03:05.471: INFO: Waiting up to 5m0s for pod "var-expansion-3b264b6a-7dce-436a-9063-9ba1150ee9a7" in namespace "var-expansion-5424" to be "success or failure"
Aug  8 16:03:05.489: INFO: Pod "var-expansion-3b264b6a-7dce-436a-9063-9ba1150ee9a7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.833834ms
Aug  8 16:03:07.509: INFO: Pod "var-expansion-3b264b6a-7dce-436a-9063-9ba1150ee9a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038603795s
Aug  8 16:03:09.529: INFO: Pod "var-expansion-3b264b6a-7dce-436a-9063-9ba1150ee9a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058687079s
STEP: Saw pod success
Aug  8 16:03:09.529: INFO: Pod "var-expansion-3b264b6a-7dce-436a-9063-9ba1150ee9a7" satisfied condition "success or failure"
Aug  8 16:03:09.548: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod var-expansion-3b264b6a-7dce-436a-9063-9ba1150ee9a7 container dapi-container: <nil>
STEP: delete the pod
Aug  8 16:03:09.606: INFO: Waiting for pod var-expansion-3b264b6a-7dce-436a-9063-9ba1150ee9a7 to disappear
Aug  8 16:03:09.626: INFO: Pod var-expansion-3b264b6a-7dce-436a-9063-9ba1150ee9a7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:03:09.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5424" for this suite.
Aug  8 16:03:15.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:03:16.443: INFO: namespace var-expansion-5424 deletion completed in 6.781540156s
•SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:03:16.443: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5301/configmap-test-d84e1e81-4d59-4ece-8363-3bc86eb609b9
STEP: Creating a pod to test consume configMaps
Aug  8 16:03:16.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac296e08-a06f-4872-ba93-f10f7f17c295" in namespace "configmap-5301" to be "success or failure"
Aug  8 16:03:16.827: INFO: Pod "pod-configmaps-ac296e08-a06f-4872-ba93-f10f7f17c295": Phase="Pending", Reason="", readiness=false. Elapsed: 27.904845ms
Aug  8 16:03:18.848: INFO: Pod "pod-configmaps-ac296e08-a06f-4872-ba93-f10f7f17c295": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048255123s
Aug  8 16:03:20.868: INFO: Pod "pod-configmaps-ac296e08-a06f-4872-ba93-f10f7f17c295": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068186642s
STEP: Saw pod success
Aug  8 16:03:20.868: INFO: Pod "pod-configmaps-ac296e08-a06f-4872-ba93-f10f7f17c295" satisfied condition "success or failure"
Aug  8 16:03:20.887: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-ac296e08-a06f-4872-ba93-f10f7f17c295 container env-test: <nil>
STEP: delete the pod
Aug  8 16:03:20.945: INFO: Waiting for pod pod-configmaps-ac296e08-a06f-4872-ba93-f10f7f17c295 to disappear
Aug  8 16:03:20.964: INFO: Pod pod-configmaps-ac296e08-a06f-4872-ba93-f10f7f17c295 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:03:20.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5301" for this suite.
Aug  8 16:03:27.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:03:27.803: INFO: namespace configmap-5301 deletion completed in 6.801379002s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:03:27.804: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2276
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5184
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:03:54.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5374" for this suite.
Aug  8 16:04:00.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:04:01.634: INFO: namespace namespaces-5374 deletion completed in 6.838263288s
STEP: Destroying namespace "nsdeletetest-2276" for this suite.
Aug  8 16:04:01.655: INFO: Namespace nsdeletetest-2276 was already deleted
STEP: Destroying namespace "nsdeletetest-5184" for this suite.
Aug  8 16:04:07.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:04:08.498: INFO: namespace nsdeletetest-5184 deletion completed in 6.843240468s
•
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:04:08.498: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Aug  8 16:04:08.783: INFO: Waiting up to 5m0s for pod "var-expansion-95598df3-a265-4d0a-80cd-42dff94761fb" in namespace "var-expansion-7679" to be "success or failure"
Aug  8 16:04:08.802: INFO: Pod "var-expansion-95598df3-a265-4d0a-80cd-42dff94761fb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.950873ms
Aug  8 16:04:10.822: INFO: Pod "var-expansion-95598df3-a265-4d0a-80cd-42dff94761fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038869346s
Aug  8 16:04:12.843: INFO: Pod "var-expansion-95598df3-a265-4d0a-80cd-42dff94761fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059428988s
STEP: Saw pod success
Aug  8 16:04:12.843: INFO: Pod "var-expansion-95598df3-a265-4d0a-80cd-42dff94761fb" satisfied condition "success or failure"
Aug  8 16:04:12.862: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod var-expansion-95598df3-a265-4d0a-80cd-42dff94761fb container dapi-container: <nil>
STEP: delete the pod
Aug  8 16:04:12.914: INFO: Waiting for pod var-expansion-95598df3-a265-4d0a-80cd-42dff94761fb to disappear
Aug  8 16:04:12.933: INFO: Pod var-expansion-95598df3-a265-4d0a-80cd-42dff94761fb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:04:12.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7679" for this suite.
Aug  8 16:04:19.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:04:19.759: INFO: namespace var-expansion-7679 deletion completed in 6.789164133s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:04:19.760: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  8 16:04:20.045: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7391'
Aug  8 16:04:20.218: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  8 16:04:20.218: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Aug  8 16:04:20.245: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-7391'
Aug  8 16:04:20.413: INFO: stderr: ""
Aug  8 16:04:20.413: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:04:20.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7391" for this suite.
Aug  8 16:04:26.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:04:27.237: INFO: namespace kubectl-7391 deletion completed in 6.80254278s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:04:27.237: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9355
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-f6ed917a-2855-4ca6-9599-b53c7e617230
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f6ed917a-2855-4ca6-9599-b53c7e617230
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:05:34.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9355" for this suite.
Aug  8 16:05:56.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:05:57.642: INFO: namespace configmap-9355 deletion completed in 22.843144941s
•SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:05:57.643: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-22w2
STEP: Creating a pod to test atomic-volume-subpath
Aug  8 16:05:58.007: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-22w2" in namespace "subpath-2810" to be "success or failure"
Aug  8 16:05:58.026: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.041336ms
Aug  8 16:06:00.047: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039418136s
Aug  8 16:06:02.074: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 4.067224823s
Aug  8 16:06:04.094: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 6.087163737s
Aug  8 16:06:06.114: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 8.10720209s
Aug  8 16:06:08.135: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 10.128271456s
Aug  8 16:06:10.156: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 12.148940059s
Aug  8 16:06:12.176: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 14.168997916s
Aug  8 16:06:14.211: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 16.203286301s
Aug  8 16:06:16.231: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 18.223411464s
Aug  8 16:06:18.251: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 20.243778728s
Aug  8 16:06:20.271: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Running", Reason="", readiness=true. Elapsed: 22.264150499s
Aug  8 16:06:22.291: INFO: Pod "pod-subpath-test-configmap-22w2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.284192023s
STEP: Saw pod success
Aug  8 16:06:22.291: INFO: Pod "pod-subpath-test-configmap-22w2" satisfied condition "success or failure"
Aug  8 16:06:22.311: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-subpath-test-configmap-22w2 container test-container-subpath-configmap-22w2: <nil>
STEP: delete the pod
Aug  8 16:06:22.459: INFO: Waiting for pod pod-subpath-test-configmap-22w2 to disappear
Aug  8 16:06:22.477: INFO: Pod pod-subpath-test-configmap-22w2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-22w2
Aug  8 16:06:22.477: INFO: Deleting pod "pod-subpath-test-configmap-22w2" in namespace "subpath-2810"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:06:22.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2810" for this suite.
Aug  8 16:06:28.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:06:29.355: INFO: namespace subpath-2810 deletion completed in 6.814392263s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:06:29.357: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:06:29.679: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d2b554a-9958-4bbd-921f-5ae4656280aa" in namespace "downward-api-4822" to be "success or failure"
Aug  8 16:06:29.698: INFO: Pod "downwardapi-volume-3d2b554a-9958-4bbd-921f-5ae4656280aa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.998661ms
Aug  8 16:06:31.718: INFO: Pod "downwardapi-volume-3d2b554a-9958-4bbd-921f-5ae4656280aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03876589s
Aug  8 16:06:33.738: INFO: Pod "downwardapi-volume-3d2b554a-9958-4bbd-921f-5ae4656280aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05898674s
STEP: Saw pod success
Aug  8 16:06:33.738: INFO: Pod "downwardapi-volume-3d2b554a-9958-4bbd-921f-5ae4656280aa" satisfied condition "success or failure"
Aug  8 16:06:33.757: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-3d2b554a-9958-4bbd-921f-5ae4656280aa container client-container: <nil>
STEP: delete the pod
Aug  8 16:06:33.817: INFO: Waiting for pod downwardapi-volume-3d2b554a-9958-4bbd-921f-5ae4656280aa to disappear
Aug  8 16:06:33.838: INFO: Pod downwardapi-volume-3d2b554a-9958-4bbd-921f-5ae4656280aa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:06:33.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4822" for this suite.
Aug  8 16:06:39.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:06:40.657: INFO: namespace downward-api-4822 deletion completed in 6.782144428s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:06:40.658: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-148
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug  8 16:06:40.975: INFO: Waiting up to 5m0s for pod "pod-5ed43745-a9b3-48f1-b92d-0858e229c853" in namespace "emptydir-148" to be "success or failure"
Aug  8 16:06:40.994: INFO: Pod "pod-5ed43745-a9b3-48f1-b92d-0858e229c853": Phase="Pending", Reason="", readiness=false. Elapsed: 18.946874ms
Aug  8 16:06:43.014: INFO: Pod "pod-5ed43745-a9b3-48f1-b92d-0858e229c853": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039005429s
Aug  8 16:06:45.035: INFO: Pod "pod-5ed43745-a9b3-48f1-b92d-0858e229c853": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059363006s
STEP: Saw pod success
Aug  8 16:06:45.035: INFO: Pod "pod-5ed43745-a9b3-48f1-b92d-0858e229c853" satisfied condition "success or failure"
Aug  8 16:06:45.055: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-5ed43745-a9b3-48f1-b92d-0858e229c853 container test-container: <nil>
STEP: delete the pod
Aug  8 16:06:45.107: INFO: Waiting for pod pod-5ed43745-a9b3-48f1-b92d-0858e229c853 to disappear
Aug  8 16:06:45.126: INFO: Pod pod-5ed43745-a9b3-48f1-b92d-0858e229c853 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:06:45.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-148" for this suite.
Aug  8 16:06:51.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:06:51.972: INFO: namespace emptydir-148 deletion completed in 6.809799432s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:06:51.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6419
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-f6323ec3-e16a-4914-8256-29196c2245ea
Aug  8 16:06:52.292: INFO: Pod name my-hostname-basic-f6323ec3-e16a-4914-8256-29196c2245ea: Found 0 pods out of 1
Aug  8 16:06:57.312: INFO: Pod name my-hostname-basic-f6323ec3-e16a-4914-8256-29196c2245ea: Found 1 pods out of 1
Aug  8 16:06:57.312: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f6323ec3-e16a-4914-8256-29196c2245ea" are running
Aug  8 16:06:57.331: INFO: Pod "my-hostname-basic-f6323ec3-e16a-4914-8256-29196c2245ea-86z8z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-08 16:06:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-08 16:06:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-08 16:06:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-08 16:06:52 +0000 UTC Reason: Message:}])
Aug  8 16:06:57.331: INFO: Trying to dial the pod
Aug  8 16:07:02.479: INFO: Controller my-hostname-basic-f6323ec3-e16a-4914-8256-29196c2245ea: Got expected result from replica 1 [my-hostname-basic-f6323ec3-e16a-4914-8256-29196c2245ea-86z8z]: "my-hostname-basic-f6323ec3-e16a-4914-8256-29196c2245ea-86z8z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:07:02.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6419" for this suite.
Aug  8 16:07:08.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:07:09.326: INFO: namespace replication-controller-6419 deletion completed in 6.809536949s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:07:09.326: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5766/configmap-test-f37acc84-8622-4a4a-b38a-8989a29d8b9a
STEP: Creating a pod to test consume configMaps
Aug  8 16:07:09.698: INFO: Waiting up to 5m0s for pod "pod-configmaps-23565471-b0b7-45b8-a801-d32ad894aa19" in namespace "configmap-5766" to be "success or failure"
Aug  8 16:07:09.719: INFO: Pod "pod-configmaps-23565471-b0b7-45b8-a801-d32ad894aa19": Phase="Pending", Reason="", readiness=false. Elapsed: 20.994664ms
Aug  8 16:07:11.739: INFO: Pod "pod-configmaps-23565471-b0b7-45b8-a801-d32ad894aa19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041110793s
Aug  8 16:07:13.760: INFO: Pod "pod-configmaps-23565471-b0b7-45b8-a801-d32ad894aa19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061479433s
Aug  8 16:07:15.780: INFO: Pod "pod-configmaps-23565471-b0b7-45b8-a801-d32ad894aa19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081711645s
STEP: Saw pod success
Aug  8 16:07:15.780: INFO: Pod "pod-configmaps-23565471-b0b7-45b8-a801-d32ad894aa19" satisfied condition "success or failure"
Aug  8 16:07:15.799: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-23565471-b0b7-45b8-a801-d32ad894aa19 container env-test: <nil>
STEP: delete the pod
Aug  8 16:07:15.849: INFO: Waiting for pod pod-configmaps-23565471-b0b7-45b8-a801-d32ad894aa19 to disappear
Aug  8 16:07:15.868: INFO: Pod pod-configmaps-23565471-b0b7-45b8-a801-d32ad894aa19 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:07:15.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5766" for this suite.
Aug  8 16:07:21.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:07:22.727: INFO: namespace configmap-5766 deletion completed in 6.822360314s
•SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:07:22.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug  8 16:07:23.068: INFO: Waiting up to 5m0s for pod "pod-1f45ee5a-79d4-453f-9f5c-023b02e3ae73" in namespace "emptydir-2995" to be "success or failure"
Aug  8 16:07:23.087: INFO: Pod "pod-1f45ee5a-79d4-453f-9f5c-023b02e3ae73": Phase="Pending", Reason="", readiness=false. Elapsed: 19.106155ms
Aug  8 16:07:25.107: INFO: Pod "pod-1f45ee5a-79d4-453f-9f5c-023b02e3ae73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039190229s
Aug  8 16:07:27.127: INFO: Pod "pod-1f45ee5a-79d4-453f-9f5c-023b02e3ae73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05928163s
STEP: Saw pod success
Aug  8 16:07:27.127: INFO: Pod "pod-1f45ee5a-79d4-453f-9f5c-023b02e3ae73" satisfied condition "success or failure"
Aug  8 16:07:27.146: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-1f45ee5a-79d4-453f-9f5c-023b02e3ae73 container test-container: <nil>
STEP: delete the pod
Aug  8 16:07:27.201: INFO: Waiting for pod pod-1f45ee5a-79d4-453f-9f5c-023b02e3ae73 to disappear
Aug  8 16:07:27.221: INFO: Pod pod-1f45ee5a-79d4-453f-9f5c-023b02e3ae73 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:07:27.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2995" for this suite.
Aug  8 16:07:33.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:07:34.059: INFO: namespace emptydir-2995 deletion completed in 6.801981569s
•SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:07:34.059: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-1b66118d-e91e-428a-9779-f1371f796701 in namespace container-probe-1928
Aug  8 16:07:38.409: INFO: Started pod test-webserver-1b66118d-e91e-428a-9779-f1371f796701 in namespace container-probe-1928
STEP: checking the pod's current state and verifying that restartCount is present
Aug  8 16:07:38.431: INFO: Initial restart count of pod test-webserver-1b66118d-e91e-428a-9779-f1371f796701 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:11:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1928" for this suite.
Aug  8 16:11:44.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:11:45.739: INFO: namespace container-probe-1928 deletion completed in 6.81991831s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:11:45.739: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-b4fw
STEP: Creating a pod to test atomic-volume-subpath
Aug  8 16:11:46.117: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-b4fw" in namespace "subpath-3780" to be "success or failure"
Aug  8 16:11:46.136: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Pending", Reason="", readiness=false. Elapsed: 19.292335ms
Aug  8 16:11:48.156: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039113002s
Aug  8 16:11:50.179: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 4.062034134s
Aug  8 16:11:52.199: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 6.081873201s
Aug  8 16:11:54.219: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 8.102496353s
Aug  8 16:11:56.240: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 10.122738597s
Aug  8 16:11:58.260: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 12.143201348s
Aug  8 16:12:00.280: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 14.162849885s
Aug  8 16:12:02.300: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 16.182906213s
Aug  8 16:12:04.320: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 18.203088168s
Aug  8 16:12:06.340: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 20.223009286s
Aug  8 16:12:08.360: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Running", Reason="", readiness=true. Elapsed: 22.243635519s
Aug  8 16:12:10.381: INFO: Pod "pod-subpath-test-downwardapi-b4fw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.263835229s
STEP: Saw pod success
Aug  8 16:12:10.381: INFO: Pod "pod-subpath-test-downwardapi-b4fw" satisfied condition "success or failure"
Aug  8 16:12:10.400: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-subpath-test-downwardapi-b4fw container test-container-subpath-downwardapi-b4fw: <nil>
STEP: delete the pod
Aug  8 16:12:10.559: INFO: Waiting for pod pod-subpath-test-downwardapi-b4fw to disappear
Aug  8 16:12:10.578: INFO: Pod pod-subpath-test-downwardapi-b4fw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-b4fw
Aug  8 16:12:10.578: INFO: Deleting pod "pod-subpath-test-downwardapi-b4fw" in namespace "subpath-3780"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:12:10.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3780" for this suite.
Aug  8 16:12:16.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:12:17.446: INFO: namespace subpath-3780 deletion completed in 6.813675847s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:12:17.447: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug  8 16:12:17.792: INFO: Pod name pod-release: Found 0 pods out of 1
Aug  8 16:12:22.811: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:12:22.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6285" for this suite.
Aug  8 16:12:28.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:12:29.694: INFO: namespace replication-controller-6285 deletion completed in 6.781194476s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:12:29.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2333
I0808 16:12:29.969872    4191 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2333, replica count: 1
I0808 16:12:31.020380    4191 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0808 16:12:32.020623    4191 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0808 16:12:33.020899    4191 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0808 16:12:34.021129    4191 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug  8 16:12:34.155: INFO: Created: latency-svc-kxgmb
Aug  8 16:12:34.159: INFO: Got endpoints: latency-svc-kxgmb [37.891571ms]
Aug  8 16:12:34.192: INFO: Created: latency-svc-x54bw
Aug  8 16:12:34.209: INFO: Got endpoints: latency-svc-x54bw [49.825284ms]
Aug  8 16:12:34.211: INFO: Created: latency-svc-qbz8f
Aug  8 16:12:34.214: INFO: Got endpoints: latency-svc-qbz8f [54.930736ms]
Aug  8 16:12:34.228: INFO: Created: latency-svc-twbdr
Aug  8 16:12:34.247: INFO: Got endpoints: latency-svc-twbdr [87.991251ms]
Aug  8 16:12:34.248: INFO: Created: latency-svc-hzdk9
Aug  8 16:12:34.287: INFO: Got endpoints: latency-svc-hzdk9 [127.823629ms]
Aug  8 16:12:34.300: INFO: Created: latency-svc-2hhsf
Aug  8 16:12:34.309: INFO: Got endpoints: latency-svc-2hhsf [149.685962ms]
Aug  8 16:12:34.384: INFO: Created: latency-svc-v9rdm
Aug  8 16:12:34.386: INFO: Got endpoints: latency-svc-v9rdm [226.970622ms]
Aug  8 16:12:34.402: INFO: Created: latency-svc-j7vg6
Aug  8 16:12:34.404: INFO: Got endpoints: latency-svc-j7vg6 [244.611297ms]
Aug  8 16:12:34.420: INFO: Created: latency-svc-sgglt
Aug  8 16:12:34.422: INFO: Got endpoints: latency-svc-sgglt [263.163414ms]
Aug  8 16:12:34.445: INFO: Created: latency-svc-dbsrg
Aug  8 16:12:34.448: INFO: Got endpoints: latency-svc-dbsrg [288.847195ms]
Aug  8 16:12:34.458: INFO: Created: latency-svc-tqckd
Aug  8 16:12:34.461: INFO: Got endpoints: latency-svc-tqckd [301.576415ms]
Aug  8 16:12:34.497: INFO: Created: latency-svc-g8sq7
Aug  8 16:12:34.500: INFO: Got endpoints: latency-svc-g8sq7 [340.377089ms]
Aug  8 16:12:34.544: INFO: Created: latency-svc-kdc45
Aug  8 16:12:34.546: INFO: Got endpoints: latency-svc-kdc45 [386.704992ms]
Aug  8 16:12:34.558: INFO: Created: latency-svc-b6xx7
Aug  8 16:12:34.562: INFO: Got endpoints: latency-svc-b6xx7 [402.208776ms]
Aug  8 16:12:34.571: INFO: Created: latency-svc-s9pbh
Aug  8 16:12:34.592: INFO: Got endpoints: latency-svc-s9pbh [432.379166ms]
Aug  8 16:12:34.642: INFO: Created: latency-svc-q7c5k
Aug  8 16:12:34.645: INFO: Got endpoints: latency-svc-q7c5k [485.473021ms]
Aug  8 16:12:34.661: INFO: Created: latency-svc-p4w9t
Aug  8 16:12:34.664: INFO: Got endpoints: latency-svc-p4w9t [455.009323ms]
Aug  8 16:12:34.674: INFO: Created: latency-svc-l7ccd
Aug  8 16:12:34.678: INFO: Got endpoints: latency-svc-l7ccd [463.565309ms]
Aug  8 16:12:34.690: INFO: Created: latency-svc-xvxqf
Aug  8 16:12:34.700: INFO: Got endpoints: latency-svc-xvxqf [452.758434ms]
Aug  8 16:12:34.710: INFO: Created: latency-svc-k72s5
Aug  8 16:12:34.713: INFO: Got endpoints: latency-svc-k72s5 [425.590423ms]
Aug  8 16:12:34.756: INFO: Created: latency-svc-8tp5l
Aug  8 16:12:34.757: INFO: Got endpoints: latency-svc-8tp5l [447.516728ms]
Aug  8 16:12:34.793: INFO: Created: latency-svc-kwg47
Aug  8 16:12:34.796: INFO: Got endpoints: latency-svc-kwg47 [409.345777ms]
Aug  8 16:12:34.809: INFO: Created: latency-svc-8wdm4
Aug  8 16:12:34.812: INFO: Got endpoints: latency-svc-8wdm4 [407.904345ms]
Aug  8 16:12:34.822: INFO: Created: latency-svc-xmhnl
Aug  8 16:12:34.836: INFO: Got endpoints: latency-svc-xmhnl [413.290605ms]
Aug  8 16:12:34.846: INFO: Created: latency-svc-tfgsf
Aug  8 16:12:34.891: INFO: Created: latency-svc-mqx4w
Aug  8 16:12:34.892: INFO: Got endpoints: latency-svc-tfgsf [443.276236ms]
Aug  8 16:12:34.904: INFO: Created: latency-svc-kbb86
Aug  8 16:12:34.904: INFO: Got endpoints: latency-svc-mqx4w [442.734329ms]
Aug  8 16:12:34.913: INFO: Got endpoints: latency-svc-kbb86 [413.185204ms]
Aug  8 16:12:34.923: INFO: Created: latency-svc-qfsqq
Aug  8 16:12:34.936: INFO: Got endpoints: latency-svc-qfsqq [389.822739ms]
Aug  8 16:12:34.949: INFO: Created: latency-svc-47224
Aug  8 16:12:34.952: INFO: Got endpoints: latency-svc-47224 [390.497185ms]
Aug  8 16:12:34.963: INFO: Created: latency-svc-q4p8k
Aug  8 16:12:34.966: INFO: Got endpoints: latency-svc-q4p8k [374.347681ms]
Aug  8 16:12:34.990: INFO: Created: latency-svc-xpgt7
Aug  8 16:12:35.003: INFO: Got endpoints: latency-svc-xpgt7 [358.225859ms]
Aug  8 16:12:35.004: INFO: Created: latency-svc-gld7n
Aug  8 16:12:35.046: INFO: Got endpoints: latency-svc-gld7n [381.67608ms]
Aug  8 16:12:35.055: INFO: Created: latency-svc-z4k5n
Aug  8 16:12:35.057: INFO: Got endpoints: latency-svc-z4k5n [379.680279ms]
Aug  8 16:12:35.072: INFO: Created: latency-svc-27skf
Aug  8 16:12:35.075: INFO: Got endpoints: latency-svc-27skf [374.719167ms]
Aug  8 16:12:35.093: INFO: Created: latency-svc-sxs64
Aug  8 16:12:35.097: INFO: Got endpoints: latency-svc-sxs64 [384.008195ms]
Aug  8 16:12:35.146: INFO: Created: latency-svc-dkg2v
Aug  8 16:12:35.149: INFO: Got endpoints: latency-svc-dkg2v [392.291589ms]
Aug  8 16:12:35.167: INFO: Created: latency-svc-hk4n2
Aug  8 16:12:35.170: INFO: Got endpoints: latency-svc-hk4n2 [373.874828ms]
Aug  8 16:12:35.181: INFO: Created: latency-svc-6qd9l
Aug  8 16:12:35.186: INFO: Got endpoints: latency-svc-6qd9l [373.962165ms]
Aug  8 16:12:35.195: INFO: Created: latency-svc-67cg9
Aug  8 16:12:35.198: INFO: Got endpoints: latency-svc-67cg9 [362.029363ms]
Aug  8 16:12:35.208: INFO: Created: latency-svc-psv4s
Aug  8 16:12:35.218: INFO: Got endpoints: latency-svc-psv4s [326.420711ms]
Aug  8 16:12:35.239: INFO: Created: latency-svc-xr7tf
Aug  8 16:12:35.242: INFO: Got endpoints: latency-svc-xr7tf [338.240581ms]
Aug  8 16:12:35.296: INFO: Created: latency-svc-dm4bs
Aug  8 16:12:35.301: INFO: Got endpoints: latency-svc-dm4bs [388.245652ms]
Aug  8 16:12:35.311: INFO: Created: latency-svc-jdqbr
Aug  8 16:12:35.313: INFO: Got endpoints: latency-svc-jdqbr [377.041453ms]
Aug  8 16:12:35.344: INFO: Created: latency-svc-dsfgv
Aug  8 16:12:35.348: INFO: Got endpoints: latency-svc-dsfgv [395.708037ms]
Aug  8 16:12:35.357: INFO: Created: latency-svc-nr57p
Aug  8 16:12:35.360: INFO: Got endpoints: latency-svc-nr57p [394.048062ms]
Aug  8 16:12:35.371: INFO: Created: latency-svc-ccwlx
Aug  8 16:12:35.375: INFO: Got endpoints: latency-svc-ccwlx [371.440271ms]
Aug  8 16:12:35.393: INFO: Created: latency-svc-bw8sc
Aug  8 16:12:35.436: INFO: Got endpoints: latency-svc-bw8sc [390.211211ms]
Aug  8 16:12:35.437: INFO: Created: latency-svc-7vh4c
Aug  8 16:12:35.441: INFO: Got endpoints: latency-svc-7vh4c [383.89949ms]
Aug  8 16:12:35.454: INFO: Created: latency-svc-4c5wx
Aug  8 16:12:35.468: INFO: Got endpoints: latency-svc-4c5wx [392.90881ms]
Aug  8 16:12:35.477: INFO: Created: latency-svc-qb6t6
Aug  8 16:12:35.486: INFO: Got endpoints: latency-svc-qb6t6 [388.846646ms]
Aug  8 16:12:35.496: INFO: Created: latency-svc-2b4gn
Aug  8 16:12:35.498: INFO: Got endpoints: latency-svc-2b4gn [349.37388ms]
Aug  8 16:12:35.511: INFO: Created: latency-svc-7bwh8
Aug  8 16:12:35.514: INFO: Got endpoints: latency-svc-7bwh8 [344.353048ms]
Aug  8 16:12:35.550: INFO: Created: latency-svc-hm6bg
Aug  8 16:12:35.589: INFO: Created: latency-svc-kbqpd
Aug  8 16:12:35.589: INFO: Got endpoints: latency-svc-hm6bg [402.860407ms]
Aug  8 16:12:35.592: INFO: Got endpoints: latency-svc-kbqpd [393.747485ms]
Aug  8 16:12:35.607: INFO: Created: latency-svc-7dbqz
Aug  8 16:12:35.622: INFO: Created: latency-svc-t62lh
Aug  8 16:12:35.622: INFO: Got endpoints: latency-svc-7dbqz [403.50019ms]
Aug  8 16:12:35.626: INFO: Got endpoints: latency-svc-t62lh [384.116774ms]
Aug  8 16:12:35.645: INFO: Created: latency-svc-8dnkh
Aug  8 16:12:35.687: INFO: Got endpoints: latency-svc-8dnkh [385.852737ms]
Aug  8 16:12:35.688: INFO: Created: latency-svc-hfp5f
Aug  8 16:12:35.691: INFO: Got endpoints: latency-svc-hfp5f [378.247602ms]
Aug  8 16:12:35.701: INFO: Created: latency-svc-d8zk8
Aug  8 16:12:35.704: INFO: Got endpoints: latency-svc-d8zk8 [356.031508ms]
Aug  8 16:12:35.721: INFO: Created: latency-svc-4644b
Aug  8 16:12:35.725: INFO: Got endpoints: latency-svc-4644b [364.436844ms]
Aug  8 16:12:35.741: INFO: Created: latency-svc-8rbkc
Aug  8 16:12:35.744: INFO: Got endpoints: latency-svc-8rbkc [369.09999ms]
Aug  8 16:12:35.758: INFO: Created: latency-svc-x2dcp
Aug  8 16:12:35.760: INFO: Got endpoints: latency-svc-x2dcp [324.412924ms]
Aug  8 16:12:35.773: INFO: Created: latency-svc-zphd4
Aug  8 16:12:35.803: INFO: Created: latency-svc-wg44k
Aug  8 16:12:35.839: INFO: Got endpoints: latency-svc-zphd4 [397.645895ms]
Aug  8 16:12:35.840: INFO: Created: latency-svc-9dkrk
Aug  8 16:12:35.857: INFO: Created: latency-svc-zgtd4
Aug  8 16:12:35.859: INFO: Got endpoints: latency-svc-wg44k [390.917521ms]
Aug  8 16:12:35.872: INFO: Created: latency-svc-zfsxc
Aug  8 16:12:35.900: INFO: Created: latency-svc-wkqpd
Aug  8 16:12:35.943: INFO: Got endpoints: latency-svc-9dkrk [457.330649ms]
Aug  8 16:12:35.944: INFO: Created: latency-svc-4jptv
Aug  8 16:12:35.961: INFO: Got endpoints: latency-svc-zgtd4 [463.20135ms]
Aug  8 16:12:35.962: INFO: Created: latency-svc-nfgrn
Aug  8 16:12:35.973: INFO: Created: latency-svc-l8b2m
Aug  8 16:12:36.003: INFO: Created: latency-svc-rb8vb
Aug  8 16:12:36.017: INFO: Created: latency-svc-4jhzj
Aug  8 16:12:36.018: INFO: Got endpoints: latency-svc-zfsxc [503.485582ms]
Aug  8 16:12:36.039: INFO: Created: latency-svc-jmmqf
Aug  8 16:12:36.086: INFO: Created: latency-svc-7p7cw
Aug  8 16:12:36.086: INFO: Got endpoints: latency-svc-wkqpd [497.337903ms]
Aug  8 16:12:36.104: INFO: Created: latency-svc-zjxjc
Aug  8 16:12:36.117: INFO: Got endpoints: latency-svc-4jptv [524.820405ms]
Aug  8 16:12:36.117: INFO: Created: latency-svc-4bsks
Aug  8 16:12:36.146: INFO: Created: latency-svc-zpgk7
Aug  8 16:12:36.161: INFO: Got endpoints: latency-svc-nfgrn [539.170016ms]
Aug  8 16:12:36.161: INFO: Created: latency-svc-pmtt6
Aug  8 16:12:36.186: INFO: Created: latency-svc-gkjbn
Aug  8 16:12:36.202: INFO: Created: latency-svc-c8n2b
Aug  8 16:12:36.236: INFO: Got endpoints: latency-svc-l8b2m [609.801846ms]
Aug  8 16:12:36.249: INFO: Created: latency-svc-qzgc8
Aug  8 16:12:36.262: INFO: Created: latency-svc-9ls4l
Aug  8 16:12:36.262: INFO: Got endpoints: latency-svc-rb8vb [574.489525ms]
Aug  8 16:12:36.292: INFO: Created: latency-svc-l6cv9
Aug  8 16:12:36.347: INFO: Created: latency-svc-n7rs4
Aug  8 16:12:36.347: INFO: Got endpoints: latency-svc-4jhzj [655.883029ms]
Aug  8 16:12:36.361: INFO: Got endpoints: latency-svc-jmmqf [657.066018ms]
Aug  8 16:12:36.361: INFO: Created: latency-svc-wk2ss
Aug  8 16:12:36.392: INFO: Created: latency-svc-kbpdj
Aug  8 16:12:36.404: INFO: Created: latency-svc-t2scb
Aug  8 16:12:36.416: INFO: Got endpoints: latency-svc-7p7cw [691.924442ms]
Aug  8 16:12:36.417: INFO: Created: latency-svc-z9pqb
Aug  8 16:12:36.489: INFO: Got endpoints: latency-svc-zjxjc [745.137439ms]
Aug  8 16:12:36.489: INFO: Created: latency-svc-dmb6p
Aug  8 16:12:36.508: INFO: Got endpoints: latency-svc-4bsks [747.407831ms]
Aug  8 16:12:36.524: INFO: Created: latency-svc-zd7fn
Aug  8 16:12:36.546: INFO: Created: latency-svc-5zdkx
Aug  8 16:12:36.559: INFO: Got endpoints: latency-svc-zpgk7 [719.503824ms]
Aug  8 16:12:36.603: INFO: Created: latency-svc-hhs72
Aug  8 16:12:36.608: INFO: Got endpoints: latency-svc-pmtt6 [748.862215ms]
Aug  8 16:12:36.649: INFO: Created: latency-svc-zfjmp
Aug  8 16:12:36.658: INFO: Got endpoints: latency-svc-gkjbn [715.028577ms]
Aug  8 16:12:36.691: INFO: Created: latency-svc-l4zbr
Aug  8 16:12:36.736: INFO: Got endpoints: latency-svc-c8n2b [774.222056ms]
Aug  8 16:12:36.763: INFO: Got endpoints: latency-svc-qzgc8 [745.862046ms]
Aug  8 16:12:36.775: INFO: Created: latency-svc-vdg8p
Aug  8 16:12:36.798: INFO: Created: latency-svc-xb2tp
Aug  8 16:12:36.817: INFO: Got endpoints: latency-svc-9ls4l [731.177229ms]
Aug  8 16:12:36.865: INFO: Created: latency-svc-qfnh4
Aug  8 16:12:36.865: INFO: Got endpoints: latency-svc-l6cv9 [748.410831ms]
Aug  8 16:12:36.901: INFO: Created: latency-svc-xcmlc
Aug  8 16:12:36.908: INFO: Got endpoints: latency-svc-n7rs4 [747.306245ms]
Aug  8 16:12:36.941: INFO: Created: latency-svc-vfvdk
Aug  8 16:12:36.967: INFO: Got endpoints: latency-svc-wk2ss [730.654658ms]
Aug  8 16:12:37.000: INFO: Created: latency-svc-k8bwn
Aug  8 16:12:37.008: INFO: Got endpoints: latency-svc-kbpdj [745.952844ms]
Aug  8 16:12:37.040: INFO: Created: latency-svc-6jv6p
Aug  8 16:12:37.059: INFO: Got endpoints: latency-svc-t2scb [711.624883ms]
Aug  8 16:12:37.096: INFO: Created: latency-svc-l78qc
Aug  8 16:12:37.108: INFO: Got endpoints: latency-svc-z9pqb [746.685913ms]
Aug  8 16:12:37.142: INFO: Created: latency-svc-gftpp
Aug  8 16:12:37.159: INFO: Got endpoints: latency-svc-dmb6p [742.281053ms]
Aug  8 16:12:37.196: INFO: Created: latency-svc-qkb9f
Aug  8 16:12:37.208: INFO: Got endpoints: latency-svc-zd7fn [718.719318ms]
Aug  8 16:12:37.240: INFO: Created: latency-svc-qgcw2
Aug  8 16:12:37.258: INFO: Got endpoints: latency-svc-5zdkx [750.051754ms]
Aug  8 16:12:37.290: INFO: Created: latency-svc-r6x6t
Aug  8 16:12:37.339: INFO: Got endpoints: latency-svc-hhs72 [780.187554ms]
Aug  8 16:12:37.359: INFO: Got endpoints: latency-svc-zfjmp [750.743328ms]
Aug  8 16:12:37.375: INFO: Created: latency-svc-g7p7h
Aug  8 16:12:37.392: INFO: Created: latency-svc-lb64z
Aug  8 16:12:37.408: INFO: Got endpoints: latency-svc-l4zbr [749.749373ms]
Aug  8 16:12:37.459: INFO: Created: latency-svc-8hqzw
Aug  8 16:12:37.459: INFO: Got endpoints: latency-svc-vdg8p [723.471962ms]
Aug  8 16:12:37.498: INFO: Created: latency-svc-l5f4b
Aug  8 16:12:37.508: INFO: Got endpoints: latency-svc-xb2tp [744.887607ms]
Aug  8 16:12:37.543: INFO: Created: latency-svc-gnb5l
Aug  8 16:12:37.582: INFO: Got endpoints: latency-svc-qfnh4 [764.516331ms]
Aug  8 16:12:37.617: INFO: Got endpoints: latency-svc-xcmlc [751.876642ms]
Aug  8 16:12:37.617: INFO: Created: latency-svc-bf75d
Aug  8 16:12:37.657: INFO: Created: latency-svc-446n9
Aug  8 16:12:37.660: INFO: Got endpoints: latency-svc-vfvdk [752.216167ms]
Aug  8 16:12:37.701: INFO: Created: latency-svc-b7l8l
Aug  8 16:12:37.708: INFO: Got endpoints: latency-svc-k8bwn [741.454878ms]
Aug  8 16:12:37.743: INFO: Created: latency-svc-qh59r
Aug  8 16:12:37.759: INFO: Got endpoints: latency-svc-6jv6p [750.618958ms]
Aug  8 16:12:37.789: INFO: Created: latency-svc-wb4rd
Aug  8 16:12:37.817: INFO: Got endpoints: latency-svc-l78qc [758.372855ms]
Aug  8 16:12:37.849: INFO: Created: latency-svc-8rw62
Aug  8 16:12:37.858: INFO: Got endpoints: latency-svc-gftpp [750.1348ms]
Aug  8 16:12:37.900: INFO: Created: latency-svc-z89qw
Aug  8 16:12:37.908: INFO: Got endpoints: latency-svc-qkb9f [749.375652ms]
Aug  8 16:12:37.956: INFO: Created: latency-svc-2bz7n
Aug  8 16:12:37.958: INFO: Got endpoints: latency-svc-qgcw2 [750.549793ms]
Aug  8 16:12:37.992: INFO: Created: latency-svc-2sx6c
Aug  8 16:12:38.008: INFO: Got endpoints: latency-svc-r6x6t [749.98951ms]
Aug  8 16:12:38.040: INFO: Created: latency-svc-cs97j
Aug  8 16:12:38.062: INFO: Got endpoints: latency-svc-g7p7h [722.688433ms]
Aug  8 16:12:38.115: INFO: Got endpoints: latency-svc-lb64z [756.154593ms]
Aug  8 16:12:38.115: INFO: Created: latency-svc-sbh55
Aug  8 16:12:38.147: INFO: Created: latency-svc-9hsxd
Aug  8 16:12:38.158: INFO: Got endpoints: latency-svc-8hqzw [750.288548ms]
Aug  8 16:12:38.202: INFO: Created: latency-svc-n8cng
Aug  8 16:12:38.211: INFO: Got endpoints: latency-svc-l5f4b [751.449524ms]
Aug  8 16:12:38.242: INFO: Created: latency-svc-52tmq
Aug  8 16:12:38.258: INFO: Got endpoints: latency-svc-gnb5l [749.41088ms]
Aug  8 16:12:38.487: INFO: Got endpoints: latency-svc-446n9 [869.836352ms]
Aug  8 16:12:38.487: INFO: Got endpoints: latency-svc-bf75d [904.853087ms]
Aug  8 16:12:38.488: INFO: Got endpoints: latency-svc-b7l8l [827.862655ms]
Aug  8 16:12:38.489: INFO: Created: latency-svc-kgrsk
Aug  8 16:12:38.496: INFO: Got endpoints: latency-svc-qh59r [787.927712ms]
Aug  8 16:12:38.508: INFO: Got endpoints: latency-svc-wb4rd [749.363637ms]
Aug  8 16:12:38.545: INFO: Created: latency-svc-4slb7
Aug  8 16:12:38.559: INFO: Created: latency-svc-qkk47
Aug  8 16:12:38.561: INFO: Got endpoints: latency-svc-8rw62 [743.197764ms]
Aug  8 16:12:38.575: INFO: Created: latency-svc-pqk64
Aug  8 16:12:38.601: INFO: Created: latency-svc-f4ct9
Aug  8 16:12:38.612: INFO: Got endpoints: latency-svc-z89qw [753.784868ms]
Aug  8 16:12:38.622: INFO: Created: latency-svc-rfp6x
Aug  8 16:12:38.690: INFO: Got endpoints: latency-svc-2bz7n [781.153965ms]
Aug  8 16:12:38.690: INFO: Created: latency-svc-m4xqq
Aug  8 16:12:38.702: INFO: Created: latency-svc-6f7vp
Aug  8 16:12:38.708: INFO: Got endpoints: latency-svc-2sx6c [749.116356ms]
Aug  8 16:12:38.725: INFO: Created: latency-svc-bgkqz
Aug  8 16:12:38.745: INFO: Created: latency-svc-hvtbf
Aug  8 16:12:38.758: INFO: Got endpoints: latency-svc-cs97j [750.175997ms]
Aug  8 16:12:38.811: INFO: Got endpoints: latency-svc-sbh55 [749.302799ms]
Aug  8 16:12:38.811: INFO: Created: latency-svc-hcwfj
Aug  8 16:12:38.851: INFO: Created: latency-svc-z95lw
Aug  8 16:12:38.858: INFO: Got endpoints: latency-svc-9hsxd [743.367977ms]
Aug  8 16:12:38.893: INFO: Created: latency-svc-cqgbl
Aug  8 16:12:38.908: INFO: Got endpoints: latency-svc-n8cng [749.442327ms]
Aug  8 16:12:38.942: INFO: Created: latency-svc-ckqw7
Aug  8 16:12:38.958: INFO: Got endpoints: latency-svc-52tmq [747.573181ms]
Aug  8 16:12:38.990: INFO: Created: latency-svc-6nfl2
Aug  8 16:12:39.008: INFO: Got endpoints: latency-svc-kgrsk [749.866276ms]
Aug  8 16:12:39.044: INFO: Created: latency-svc-dmnhk
Aug  8 16:12:39.058: INFO: Got endpoints: latency-svc-4slb7 [571.509857ms]
Aug  8 16:12:39.090: INFO: Created: latency-svc-mcqwk
Aug  8 16:12:39.108: INFO: Got endpoints: latency-svc-qkk47 [621.362252ms]
Aug  8 16:12:39.140: INFO: Created: latency-svc-zq4kh
Aug  8 16:12:39.186: INFO: Got endpoints: latency-svc-pqk64 [697.152524ms]
Aug  8 16:12:39.219: INFO: Created: latency-svc-dscgj
Aug  8 16:12:39.219: INFO: Got endpoints: latency-svc-f4ct9 [722.938247ms]
Aug  8 16:12:39.255: INFO: Created: latency-svc-l7wd8
Aug  8 16:12:39.258: INFO: Got endpoints: latency-svc-rfp6x [749.919928ms]
Aug  8 16:12:39.302: INFO: Created: latency-svc-9z5r8
Aug  8 16:12:39.308: INFO: Got endpoints: latency-svc-m4xqq [747.332311ms]
Aug  8 16:12:39.343: INFO: Created: latency-svc-mxnkz
Aug  8 16:12:39.358: INFO: Got endpoints: latency-svc-6f7vp [746.411206ms]
Aug  8 16:12:39.391: INFO: Created: latency-svc-h65gz
Aug  8 16:12:39.417: INFO: Got endpoints: latency-svc-bgkqz [727.165896ms]
Aug  8 16:12:39.449: INFO: Created: latency-svc-sjb59
Aug  8 16:12:39.458: INFO: Got endpoints: latency-svc-hvtbf [750.083387ms]
Aug  8 16:12:39.492: INFO: Created: latency-svc-48pv2
Aug  8 16:12:39.508: INFO: Got endpoints: latency-svc-hcwfj [749.475417ms]
Aug  8 16:12:39.551: INFO: Created: latency-svc-fsz6q
Aug  8 16:12:39.558: INFO: Got endpoints: latency-svc-z95lw [747.081515ms]
Aug  8 16:12:39.590: INFO: Created: latency-svc-lkvm8
Aug  8 16:12:39.608: INFO: Got endpoints: latency-svc-cqgbl [750.134851ms]
Aug  8 16:12:39.657: INFO: Created: latency-svc-rp8qc
Aug  8 16:12:39.659: INFO: Got endpoints: latency-svc-ckqw7 [750.69843ms]
Aug  8 16:12:39.694: INFO: Created: latency-svc-v7mpx
Aug  8 16:12:39.708: INFO: Got endpoints: latency-svc-6nfl2 [749.756635ms]
Aug  8 16:12:39.740: INFO: Created: latency-svc-zk872
Aug  8 16:12:39.770: INFO: Got endpoints: latency-svc-dmnhk [761.893876ms]
Aug  8 16:12:39.802: INFO: Created: latency-svc-wls87
Aug  8 16:12:39.811: INFO: Got endpoints: latency-svc-mcqwk [752.193211ms]
Aug  8 16:12:39.843: INFO: Created: latency-svc-rfjgk
Aug  8 16:12:39.858: INFO: Got endpoints: latency-svc-zq4kh [749.969809ms]
Aug  8 16:12:39.899: INFO: Created: latency-svc-wdrvq
Aug  8 16:12:39.908: INFO: Got endpoints: latency-svc-dscgj [722.295369ms]
Aug  8 16:12:39.940: INFO: Created: latency-svc-gh9ch
Aug  8 16:12:39.959: INFO: Got endpoints: latency-svc-l7wd8 [739.513624ms]
Aug  8 16:12:40.006: INFO: Created: latency-svc-8cj6t
Aug  8 16:12:40.008: INFO: Got endpoints: latency-svc-9z5r8 [749.746307ms]
Aug  8 16:12:40.044: INFO: Created: latency-svc-z55g2
Aug  8 16:12:40.058: INFO: Got endpoints: latency-svc-mxnkz [750.323025ms]
Aug  8 16:12:40.090: INFO: Created: latency-svc-hjnlr
Aug  8 16:12:40.128: INFO: Got endpoints: latency-svc-h65gz [769.209508ms]
Aug  8 16:12:40.164: INFO: Created: latency-svc-k8rkd
Aug  8 16:12:40.165: INFO: Got endpoints: latency-svc-sjb59 [747.693879ms]
Aug  8 16:12:40.197: INFO: Created: latency-svc-cmhvm
Aug  8 16:12:40.209: INFO: Got endpoints: latency-svc-48pv2 [750.811536ms]
Aug  8 16:12:40.255: INFO: Created: latency-svc-j4xwb
Aug  8 16:12:40.258: INFO: Got endpoints: latency-svc-fsz6q [749.972556ms]
Aug  8 16:12:40.290: INFO: Created: latency-svc-vrtsj
Aug  8 16:12:40.308: INFO: Got endpoints: latency-svc-lkvm8 [750.170881ms]
Aug  8 16:12:40.340: INFO: Created: latency-svc-9dtxs
Aug  8 16:12:40.359: INFO: Got endpoints: latency-svc-rp8qc [750.086761ms]
Aug  8 16:12:40.393: INFO: Created: latency-svc-6krpm
Aug  8 16:12:40.408: INFO: Got endpoints: latency-svc-v7mpx [749.07512ms]
Aug  8 16:12:40.442: INFO: Created: latency-svc-kt6jp
Aug  8 16:12:40.468: INFO: Got endpoints: latency-svc-zk872 [760.149587ms]
Aug  8 16:12:40.500: INFO: Created: latency-svc-bbgwt
Aug  8 16:12:40.508: INFO: Got endpoints: latency-svc-wls87 [738.159136ms]
Aug  8 16:12:40.540: INFO: Created: latency-svc-hwwrr
Aug  8 16:12:40.558: INFO: Got endpoints: latency-svc-rfjgk [747.51654ms]
Aug  8 16:12:40.598: INFO: Created: latency-svc-sbdb9
Aug  8 16:12:40.608: INFO: Got endpoints: latency-svc-wdrvq [749.609962ms]
Aug  8 16:12:40.644: INFO: Created: latency-svc-w8jjg
Aug  8 16:12:40.658: INFO: Got endpoints: latency-svc-gh9ch [750.436168ms]
Aug  8 16:12:40.706: INFO: Created: latency-svc-qfgqq
Aug  8 16:12:40.708: INFO: Got endpoints: latency-svc-8cj6t [749.084397ms]
Aug  8 16:12:40.743: INFO: Created: latency-svc-hqmjq
Aug  8 16:12:40.765: INFO: Got endpoints: latency-svc-z55g2 [756.724359ms]
Aug  8 16:12:40.798: INFO: Created: latency-svc-zr2jn
Aug  8 16:12:40.823: INFO: Got endpoints: latency-svc-hjnlr [764.019276ms]
Aug  8 16:12:40.856: INFO: Created: latency-svc-6qhml
Aug  8 16:12:40.859: INFO: Got endpoints: latency-svc-k8rkd [730.89033ms]
Aug  8 16:12:40.893: INFO: Created: latency-svc-c24bm
Aug  8 16:12:40.908: INFO: Got endpoints: latency-svc-cmhvm [743.454297ms]
Aug  8 16:12:40.958: INFO: Created: latency-svc-gp5mk
Aug  8 16:12:40.961: INFO: Got endpoints: latency-svc-j4xwb [752.74461ms]
Aug  8 16:12:40.997: INFO: Created: latency-svc-rtnf4
Aug  8 16:12:41.008: INFO: Got endpoints: latency-svc-vrtsj [749.952972ms]
Aug  8 16:12:41.042: INFO: Created: latency-svc-wjzcs
Aug  8 16:12:41.066: INFO: Got endpoints: latency-svc-9dtxs [757.355433ms]
Aug  8 16:12:41.100: INFO: Created: latency-svc-vzcnf
Aug  8 16:12:41.108: INFO: Got endpoints: latency-svc-6krpm [748.972392ms]
Aug  8 16:12:41.140: INFO: Created: latency-svc-l6j6m
Aug  8 16:12:41.158: INFO: Got endpoints: latency-svc-kt6jp [750.228721ms]
Aug  8 16:12:41.199: INFO: Created: latency-svc-544s2
Aug  8 16:12:41.208: INFO: Got endpoints: latency-svc-bbgwt [739.915704ms]
Aug  8 16:12:41.243: INFO: Created: latency-svc-xmkmk
Aug  8 16:12:41.258: INFO: Got endpoints: latency-svc-hwwrr [750.427634ms]
Aug  8 16:12:41.311: INFO: Got endpoints: latency-svc-sbdb9 [753.033191ms]
Aug  8 16:12:41.311: INFO: Created: latency-svc-gvvl5
Aug  8 16:12:41.354: INFO: Created: latency-svc-fg2h9
Aug  8 16:12:41.358: INFO: Got endpoints: latency-svc-w8jjg [750.376175ms]
Aug  8 16:12:41.390: INFO: Created: latency-svc-5bh7g
Aug  8 16:12:41.408: INFO: Got endpoints: latency-svc-qfgqq [749.686989ms]
Aug  8 16:12:41.441: INFO: Created: latency-svc-9qv59
Aug  8 16:12:41.458: INFO: Got endpoints: latency-svc-hqmjq [750.562082ms]
Aug  8 16:12:41.497: INFO: Created: latency-svc-jd882
Aug  8 16:12:41.508: INFO: Got endpoints: latency-svc-zr2jn [743.351674ms]
Aug  8 16:12:41.547: INFO: Created: latency-svc-8cnhc
Aug  8 16:12:41.558: INFO: Got endpoints: latency-svc-6qhml [735.234083ms]
Aug  8 16:12:41.590: INFO: Created: latency-svc-tkjpt
Aug  8 16:12:41.608: INFO: Got endpoints: latency-svc-c24bm [749.319713ms]
Aug  8 16:12:41.642: INFO: Created: latency-svc-c49qc
Aug  8 16:12:41.665: INFO: Got endpoints: latency-svc-gp5mk [756.746486ms]
Aug  8 16:12:41.697: INFO: Created: latency-svc-tmzs9
Aug  8 16:12:41.708: INFO: Got endpoints: latency-svc-rtnf4 [746.28345ms]
Aug  8 16:12:41.740: INFO: Created: latency-svc-jbpw5
Aug  8 16:12:41.758: INFO: Got endpoints: latency-svc-wjzcs [750.192859ms]
Aug  8 16:12:41.808: INFO: Created: latency-svc-59964
Aug  8 16:12:41.810: INFO: Got endpoints: latency-svc-vzcnf [744.580804ms]
Aug  8 16:12:41.846: INFO: Created: latency-svc-4ptpk
Aug  8 16:12:41.858: INFO: Got endpoints: latency-svc-l6j6m [750.582333ms]
Aug  8 16:12:41.891: INFO: Created: latency-svc-sr8pv
Aug  8 16:12:41.910: INFO: Got endpoints: latency-svc-544s2 [752.263021ms]
Aug  8 16:12:41.952: INFO: Created: latency-svc-knm4q
Aug  8 16:12:41.958: INFO: Got endpoints: latency-svc-xmkmk [749.192076ms]
Aug  8 16:12:42.001: INFO: Created: latency-svc-mqqqs
Aug  8 16:12:42.008: INFO: Got endpoints: latency-svc-gvvl5 [749.230758ms]
Aug  8 16:12:42.059: INFO: Got endpoints: latency-svc-fg2h9 [747.626881ms]
Aug  8 16:12:42.108: INFO: Got endpoints: latency-svc-5bh7g [749.861305ms]
Aug  8 16:12:42.159: INFO: Got endpoints: latency-svc-9qv59 [750.454987ms]
Aug  8 16:12:42.208: INFO: Got endpoints: latency-svc-jd882 [749.701485ms]
Aug  8 16:12:42.258: INFO: Got endpoints: latency-svc-8cnhc [750.396325ms]
Aug  8 16:12:42.308: INFO: Got endpoints: latency-svc-tkjpt [749.982722ms]
Aug  8 16:12:42.358: INFO: Got endpoints: latency-svc-c49qc [750.169963ms]
Aug  8 16:12:42.408: INFO: Got endpoints: latency-svc-tmzs9 [743.033863ms]
Aug  8 16:12:42.458: INFO: Got endpoints: latency-svc-jbpw5 [750.014409ms]
Aug  8 16:12:42.508: INFO: Got endpoints: latency-svc-59964 [749.94536ms]
Aug  8 16:12:42.558: INFO: Got endpoints: latency-svc-4ptpk [747.374288ms]
Aug  8 16:12:42.608: INFO: Got endpoints: latency-svc-sr8pv [749.569484ms]
Aug  8 16:12:42.658: INFO: Got endpoints: latency-svc-knm4q [748.131974ms]
Aug  8 16:12:42.710: INFO: Got endpoints: latency-svc-mqqqs [752.759926ms]
Aug  8 16:12:42.711: INFO: Latencies: [49.825284ms 54.930736ms 87.991251ms 127.823629ms 149.685962ms 226.970622ms 244.611297ms 263.163414ms 288.847195ms 301.576415ms 324.412924ms 326.420711ms 338.240581ms 340.377089ms 344.353048ms 349.37388ms 356.031508ms 358.225859ms 362.029363ms 364.436844ms 369.09999ms 371.440271ms 373.874828ms 373.962165ms 374.347681ms 374.719167ms 377.041453ms 378.247602ms 379.680279ms 381.67608ms 383.89949ms 384.008195ms 384.116774ms 385.852737ms 386.704992ms 388.245652ms 388.846646ms 389.822739ms 390.211211ms 390.497185ms 390.917521ms 392.291589ms 392.90881ms 393.747485ms 394.048062ms 395.708037ms 397.645895ms 402.208776ms 402.860407ms 403.50019ms 407.904345ms 409.345777ms 413.185204ms 413.290605ms 425.590423ms 432.379166ms 442.734329ms 443.276236ms 447.516728ms 452.758434ms 455.009323ms 457.330649ms 463.20135ms 463.565309ms 485.473021ms 497.337903ms 503.485582ms 524.820405ms 539.170016ms 571.509857ms 574.489525ms 609.801846ms 621.362252ms 655.883029ms 657.066018ms 691.924442ms 697.152524ms 711.624883ms 715.028577ms 718.719318ms 719.503824ms 722.295369ms 722.688433ms 722.938247ms 723.471962ms 727.165896ms 730.654658ms 730.89033ms 731.177229ms 735.234083ms 738.159136ms 739.513624ms 739.915704ms 741.454878ms 742.281053ms 743.033863ms 743.197764ms 743.351674ms 743.367977ms 743.454297ms 744.580804ms 744.887607ms 745.137439ms 745.862046ms 745.952844ms 746.28345ms 746.411206ms 746.685913ms 747.081515ms 747.306245ms 747.332311ms 747.374288ms 747.407831ms 747.51654ms 747.573181ms 747.626881ms 747.693879ms 748.131974ms 748.410831ms 748.862215ms 748.972392ms 749.07512ms 749.084397ms 749.116356ms 749.192076ms 749.230758ms 749.302799ms 749.319713ms 749.363637ms 749.375652ms 749.41088ms 749.442327ms 749.475417ms 749.569484ms 749.609962ms 749.686989ms 749.701485ms 749.746307ms 749.749373ms 749.756635ms 749.861305ms 749.866276ms 749.919928ms 749.94536ms 749.952972ms 749.969809ms 749.972556ms 749.982722ms 749.98951ms 750.014409ms 750.051754ms 750.083387ms 750.086761ms 750.1348ms 750.134851ms 750.169963ms 750.170881ms 750.175997ms 750.192859ms 750.228721ms 750.288548ms 750.323025ms 750.376175ms 750.396325ms 750.427634ms 750.436168ms 750.454987ms 750.549793ms 750.562082ms 750.582333ms 750.618958ms 750.69843ms 750.743328ms 750.811536ms 751.449524ms 751.876642ms 752.193211ms 752.216167ms 752.263021ms 752.74461ms 752.759926ms 753.033191ms 753.784868ms 756.154593ms 756.724359ms 756.746486ms 757.355433ms 758.372855ms 760.149587ms 761.893876ms 764.019276ms 764.516331ms 769.209508ms 774.222056ms 780.187554ms 781.153965ms 787.927712ms 827.862655ms 869.836352ms 904.853087ms]
Aug  8 16:12:42.711: INFO: 50 %ile: 744.580804ms
Aug  8 16:12:42.711: INFO: 90 %ile: 752.759926ms
Aug  8 16:12:42.711: INFO: 99 %ile: 869.836352ms
Aug  8 16:12:42.711: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:12:42.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2333" for this suite.
Aug  8 16:12:56.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:12:57.526: INFO: namespace svc-latency-2333 deletion completed in 14.795383979s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:12:57.527: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-3baacaba-02a9-45af-92f9-511b2a3850da
STEP: Creating a pod to test consume configMaps
Aug  8 16:12:57.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdfe9336-4537-4313-ba25-f1f74d9467f0" in namespace "configmap-9271" to be "success or failure"
Aug  8 16:12:57.915: INFO: Pod "pod-configmaps-bdfe9336-4537-4313-ba25-f1f74d9467f0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.219248ms
Aug  8 16:12:59.934: INFO: Pod "pod-configmaps-bdfe9336-4537-4313-ba25-f1f74d9467f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038641239s
Aug  8 16:13:01.954: INFO: Pod "pod-configmaps-bdfe9336-4537-4313-ba25-f1f74d9467f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05857827s
STEP: Saw pod success
Aug  8 16:13:01.954: INFO: Pod "pod-configmaps-bdfe9336-4537-4313-ba25-f1f74d9467f0" satisfied condition "success or failure"
Aug  8 16:13:01.973: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-bdfe9336-4537-4313-ba25-f1f74d9467f0 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 16:13:02.030: INFO: Waiting for pod pod-configmaps-bdfe9336-4537-4313-ba25-f1f74d9467f0 to disappear
Aug  8 16:13:02.053: INFO: Pod pod-configmaps-bdfe9336-4537-4313-ba25-f1f74d9467f0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:13:02.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9271" for this suite.
Aug  8 16:13:08.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:13:08.883: INFO: namespace configmap-9271 deletion completed in 6.792545122s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:13:08.883: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug  8 16:13:09.152: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:13:14.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4400" for this suite.
Aug  8 16:13:36.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:13:37.690: INFO: namespace init-container-4400 deletion completed in 22.780703786s
•SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:13:37.690: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2987
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug  8 16:13:37.956: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug  8 16:14:06.315: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.154:8080/dial?request=hostName&protocol=udp&host=100.96.0.62&port=8081&tries=1'] Namespace:pod-network-test-2987 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 16:14:06.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 16:14:06.817: INFO: Waiting for endpoints: map[]
Aug  8 16:14:06.837: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.154:8080/dial?request=hostName&protocol=udp&host=100.96.1.153&port=8081&tries=1'] Namespace:pod-network-test-2987 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug  8 16:14:06.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug  8 16:14:07.314: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:14:07.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2987" for this suite.
Aug  8 16:14:31.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:14:32.143: INFO: namespace pod-network-test-2987 deletion completed in 24.791962374s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:14:32.143: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5167.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5167.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5167.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5167.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5167.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5167.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5167.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5167.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5167.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5167.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 64.128.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.128.64_udp@PTR;check="$$(dig +tcp +noall +answer +search 64.128.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.128.64_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5167.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5167.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5167.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5167.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5167.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5167.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5167.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5167.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5167.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5167.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5167.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 64.128.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.128.64_udp@PTR;check="$$(dig +tcp +noall +answer +search 64.128.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.128.64_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug  8 16:14:38.687: INFO: Unable to read wheezy_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:38.730: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:38.751: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:38.772: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:39.220: INFO: Unable to read jessie_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:39.241: INFO: Unable to read jessie_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:39.263: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:39.708: INFO: Lookups using dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466 failed for: [wheezy_udp@dns-test-service.dns-5167.svc.cluster.local wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local jessie_udp@dns-test-service.dns-5167.svc.cluster.local jessie_tcp@dns-test-service.dns-5167.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5167.svc.cluster.local]

Aug  8 16:14:44.731: INFO: Unable to read wheezy_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:44.758: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:45.335: INFO: Unable to read jessie_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:45.357: INFO: Unable to read jessie_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:45.894: INFO: Lookups using dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466 failed for: [wheezy_udp@dns-test-service.dns-5167.svc.cluster.local wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local jessie_udp@dns-test-service.dns-5167.svc.cluster.local jessie_tcp@dns-test-service.dns-5167.svc.cluster.local]

Aug  8 16:14:49.730: INFO: Unable to read wheezy_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:49.776: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:50.353: INFO: Unable to read jessie_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:50.375: INFO: Unable to read jessie_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:50.910: INFO: Lookups using dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466 failed for: [wheezy_udp@dns-test-service.dns-5167.svc.cluster.local wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local jessie_udp@dns-test-service.dns-5167.svc.cluster.local jessie_tcp@dns-test-service.dns-5167.svc.cluster.local]

Aug  8 16:14:54.731: INFO: Unable to read wheezy_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:54.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:55.288: INFO: Unable to read jessie_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:55.310: INFO: Unable to read jessie_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:55.891: INFO: Lookups using dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466 failed for: [wheezy_udp@dns-test-service.dns-5167.svc.cluster.local wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local jessie_udp@dns-test-service.dns-5167.svc.cluster.local jessie_tcp@dns-test-service.dns-5167.svc.cluster.local]

Aug  8 16:14:59.730: INFO: Unable to read wheezy_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:14:59.774: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:15:00.357: INFO: Unable to read jessie_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:15:00.378: INFO: Unable to read jessie_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:15:00.910: INFO: Lookups using dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466 failed for: [wheezy_udp@dns-test-service.dns-5167.svc.cluster.local wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local jessie_udp@dns-test-service.dns-5167.svc.cluster.local jessie_tcp@dns-test-service.dns-5167.svc.cluster.local]

Aug  8 16:15:04.733: INFO: Unable to read wheezy_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:15:04.776: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:15:05.314: INFO: Unable to read jessie_udp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:15:05.336: INFO: Unable to read jessie_tcp@dns-test-service.dns-5167.svc.cluster.local from pod dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466: the server could not find the requested resource (get pods dns-test-82a1cc21-cf3e-414e-9e74-68286a409466)
Aug  8 16:15:05.877: INFO: Lookups using dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466 failed for: [wheezy_udp@dns-test-service.dns-5167.svc.cluster.local wheezy_tcp@dns-test-service.dns-5167.svc.cluster.local jessie_udp@dns-test-service.dns-5167.svc.cluster.local jessie_tcp@dns-test-service.dns-5167.svc.cluster.local]

Aug  8 16:15:10.969: INFO: DNS probes using dns-5167/dns-test-82a1cc21-cf3e-414e-9e74-68286a409466 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:15:11.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5167" for this suite.
Aug  8 16:15:17.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:15:17.941: INFO: namespace dns-5167 deletion completed in 6.833734606s
•S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:15:17.941: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-1497
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1497
STEP: Deleting pre-stop pod
Aug  8 16:15:33.524: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:15:33.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1497" for this suite.
Aug  8 16:16:15.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:16:16.404: INFO: namespace prestop-1497 deletion completed in 42.813255829s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:16:16.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-dc8227d1-38e1-4f2e-b5bf-2817e62941cb
STEP: Creating a pod to test consume secrets
Aug  8 16:16:16.777: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-53d58e3b-c30b-4eb4-b8bd-9e17738b45eb" in namespace "projected-4562" to be "success or failure"
Aug  8 16:16:16.796: INFO: Pod "pod-projected-secrets-53d58e3b-c30b-4eb4-b8bd-9e17738b45eb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.704275ms
Aug  8 16:16:18.817: INFO: Pod "pod-projected-secrets-53d58e3b-c30b-4eb4-b8bd-9e17738b45eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039225128s
Aug  8 16:16:20.837: INFO: Pod "pod-projected-secrets-53d58e3b-c30b-4eb4-b8bd-9e17738b45eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059445615s
STEP: Saw pod success
Aug  8 16:16:20.837: INFO: Pod "pod-projected-secrets-53d58e3b-c30b-4eb4-b8bd-9e17738b45eb" satisfied condition "success or failure"
Aug  8 16:16:20.857: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-secrets-53d58e3b-c30b-4eb4-b8bd-9e17738b45eb container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  8 16:16:20.915: INFO: Waiting for pod pod-projected-secrets-53d58e3b-c30b-4eb4-b8bd-9e17738b45eb to disappear
Aug  8 16:16:20.944: INFO: Pod pod-projected-secrets-53d58e3b-c30b-4eb4-b8bd-9e17738b45eb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:16:20.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4562" for this suite.
Aug  8 16:16:27.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:16:27.769: INFO: namespace projected-4562 deletion completed in 6.788906248s
•S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:16:27.770: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:16:28.072: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f6373c9-9b6c-4260-91f6-cd8f2499881c" in namespace "downward-api-4505" to be "success or failure"
Aug  8 16:16:28.091: INFO: Pod "downwardapi-volume-8f6373c9-9b6c-4260-91f6-cd8f2499881c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.682162ms
Aug  8 16:16:30.112: INFO: Pod "downwardapi-volume-8f6373c9-9b6c-4260-91f6-cd8f2499881c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039294811s
Aug  8 16:16:32.131: INFO: Pod "downwardapi-volume-8f6373c9-9b6c-4260-91f6-cd8f2499881c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059270837s
STEP: Saw pod success
Aug  8 16:16:32.132: INFO: Pod "downwardapi-volume-8f6373c9-9b6c-4260-91f6-cd8f2499881c" satisfied condition "success or failure"
Aug  8 16:16:32.151: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-8f6373c9-9b6c-4260-91f6-cd8f2499881c container client-container: <nil>
STEP: delete the pod
Aug  8 16:16:32.216: INFO: Waiting for pod downwardapi-volume-8f6373c9-9b6c-4260-91f6-cd8f2499881c to disappear
Aug  8 16:16:32.235: INFO: Pod downwardapi-volume-8f6373c9-9b6c-4260-91f6-cd8f2499881c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:16:32.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4505" for this suite.
Aug  8 16:16:38.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:16:39.063: INFO: namespace downward-api-4505 deletion completed in 6.786920386s
•SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:16:39.063: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3945
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug  8 16:16:39.404: INFO: Found 1 stateful pods, waiting for 3
Aug  8 16:16:49.425: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 16:16:49.425: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 16:16:49.425: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug  8 16:16:59.425: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 16:16:59.425: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 16:16:59.425: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug  8 16:16:59.483: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3945 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 16:17:15.439: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 16:17:15.439: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 16:17:15.439: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug  8 16:17:25.570: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug  8 16:17:25.628: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3945 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 16:17:26.283: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug  8 16:17:26.283: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  8 16:17:26.283: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  8 16:17:36.402: INFO: Waiting for StatefulSet statefulset-3945/ss2 to complete update
Aug  8 16:17:36.402: INFO: Waiting for Pod statefulset-3945/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug  8 16:17:36.402: INFO: Waiting for Pod statefulset-3945/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug  8 16:17:46.443: INFO: Waiting for StatefulSet statefulset-3945/ss2 to complete update
Aug  8 16:17:46.443: INFO: Waiting for Pod statefulset-3945/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug  8 16:17:46.443: INFO: Waiting for Pod statefulset-3945/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug  8 16:17:56.442: INFO: Waiting for StatefulSet statefulset-3945/ss2 to complete update
Aug  8 16:17:56.442: INFO: Waiting for Pod statefulset-3945/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug  8 16:18:06.442: INFO: Waiting for StatefulSet statefulset-3945/ss2 to complete update
STEP: Rolling back to a previous revision
Aug  8 16:18:16.442: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3945 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug  8 16:18:17.181: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug  8 16:18:17.181: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug  8 16:18:17.181: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug  8 16:18:17.270: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug  8 16:18:17.331: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3945 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug  8 16:18:17.993: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug  8 16:18:17.993: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug  8 16:18:17.993: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug  8 16:18:28.111: INFO: Waiting for StatefulSet statefulset-3945/ss2 to complete update
Aug  8 16:18:28.112: INFO: Waiting for Pod statefulset-3945/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug  8 16:18:28.112: INFO: Waiting for Pod statefulset-3945/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug  8 16:18:38.151: INFO: Waiting for StatefulSet statefulset-3945/ss2 to complete update
Aug  8 16:18:38.151: INFO: Waiting for Pod statefulset-3945/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug  8 16:18:48.151: INFO: Waiting for StatefulSet statefulset-3945/ss2 to complete update
Aug  8 16:18:48.151: INFO: Waiting for Pod statefulset-3945/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug  8 16:18:58.155: INFO: Waiting for StatefulSet statefulset-3945/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug  8 16:19:08.151: INFO: Deleting all statefulset in ns statefulset-3945
Aug  8 16:19:08.171: INFO: Scaling statefulset ss2 to 0
Aug  8 16:19:28.252: INFO: Waiting for statefulset status.replicas updated to 0
Aug  8 16:19:28.271: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:19:28.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3945" for this suite.
Aug  8 16:19:34.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:19:35.159: INFO: namespace statefulset-3945 deletion completed in 6.786931842s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:19:35.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Aug  8 16:19:35.460: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug  8 16:19:35.460: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5209'
Aug  8 16:19:41.209: INFO: stderr: ""
Aug  8 16:19:41.209: INFO: stdout: "service/redis-slave created\n"
Aug  8 16:19:41.213: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug  8 16:19:41.213: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5209'
Aug  8 16:19:41.959: INFO: stderr: ""
Aug  8 16:19:41.959: INFO: stdout: "service/redis-master created\n"
Aug  8 16:19:42.004: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug  8 16:19:42.005: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5209'
Aug  8 16:19:42.627: INFO: stderr: ""
Aug  8 16:19:42.627: INFO: stdout: "service/frontend created\n"
Aug  8 16:19:42.629: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug  8 16:19:42.629: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5209'
Aug  8 16:19:43.328: INFO: stderr: ""
Aug  8 16:19:43.328: INFO: stdout: "deployment.apps/frontend created\n"
Aug  8 16:19:43.329: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug  8 16:19:43.329: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5209'
Aug  8 16:19:43.960: INFO: stderr: ""
Aug  8 16:19:43.960: INFO: stdout: "deployment.apps/redis-master created\n"
Aug  8 16:19:43.961: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug  8 16:19:43.961: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5209'
Aug  8 16:19:44.583: INFO: stderr: ""
Aug  8 16:19:44.583: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug  8 16:19:44.583: INFO: Waiting for all frontend pods to be Running.
Aug  8 16:20:19.642: INFO: Waiting for frontend to serve content.
Aug  8 16:20:24.730: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Aug  8 16:20:29.836: INFO: Trying to add a new entry to the guestbook.
Aug  8 16:20:29.969: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug  8 16:20:30.017: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5209'
Aug  8 16:20:30.354: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 16:20:30.354: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug  8 16:20:30.355: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5209'
Aug  8 16:20:30.741: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 16:20:30.741: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug  8 16:20:30.741: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5209'
Aug  8 16:20:31.182: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 16:20:31.182: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug  8 16:20:31.182: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5209'
Aug  8 16:20:31.669: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 16:20:31.669: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug  8 16:20:31.669: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5209'
Aug  8 16:20:32.064: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 16:20:32.064: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug  8 16:20:32.064: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5209'
Aug  8 16:20:32.468: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 16:20:32.468: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:20:32.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5209" for this suite.
Aug  8 16:21:12.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:21:13.336: INFO: namespace kubectl-5209 deletion completed in 40.830955974s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:21:13.337: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug  8 16:21:13.811: INFO: Number of nodes with available pods: 0
Aug  8 16:21:13.811: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:21:14.878: INFO: Number of nodes with available pods: 0
Aug  8 16:21:14.878: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:21:15.872: INFO: Number of nodes with available pods: 0
Aug  8 16:21:15.872: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:21:16.879: INFO: Number of nodes with available pods: 0
Aug  8 16:21:16.879: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:21:17.872: INFO: Number of nodes with available pods: 2
Aug  8 16:21:17.872: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug  8 16:21:17.981: INFO: Number of nodes with available pods: 1
Aug  8 16:21:17.981: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:21:19.043: INFO: Number of nodes with available pods: 1
Aug  8 16:21:19.043: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:21:20.037: INFO: Number of nodes with available pods: 1
Aug  8 16:21:20.037: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:21:21.042: INFO: Number of nodes with available pods: 1
Aug  8 16:21:21.042: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:21:22.041: INFO: Number of nodes with available pods: 1
Aug  8 16:21:22.041: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:21:23.051: INFO: Number of nodes with available pods: 2
Aug  8 16:21:23.051: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9749, will wait for the garbage collector to delete the pods
Aug  8 16:21:23.192: INFO: Deleting DaemonSet.extensions daemon-set took: 20.941531ms
Aug  8 16:21:23.292: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.245772ms
Aug  8 16:21:34.011: INFO: Number of nodes with available pods: 0
Aug  8 16:21:34.011: INFO: Number of running nodes: 0, number of available pods: 0
Aug  8 16:21:34.041: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9749/daemonsets","resourceVersion":"21208"},"items":null}

Aug  8 16:21:34.062: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9749/pods","resourceVersion":"21208"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:21:34.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9749" for this suite.
Aug  8 16:21:40.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:21:41.025: INFO: namespace daemonsets-9749 deletion completed in 6.860184124s
•S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:21:41.026: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:21:45.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4512" for this suite.
Aug  8 16:22:37.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:22:38.392: INFO: namespace kubelet-test-4512 deletion completed in 52.806746692s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:22:38.396: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:23:38.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7594" for this suite.
Aug  8 16:24:00.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:24:01.543: INFO: namespace container-probe-7594 deletion completed in 22.805647252s
•SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:24:01.544: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-15
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug  8 16:24:01.851: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Aug  8 16:24:02.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:04.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:06.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:08.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:10.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:12.924: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:14.922: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:16.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:18.922: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:20.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:22.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878242, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:24:26.233: INFO: Waited 1.28954086s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:24:27.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-15" for this suite.
Aug  8 16:24:33.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:24:34.336: INFO: namespace aggregator-15 deletion completed in 6.931808054s
•SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:24:34.336: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:24:34.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1626" for this suite.
Aug  8 16:24:40.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:24:41.500: INFO: namespace services-1626 deletion completed in 6.806846922s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:24:41.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9305
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-d4442d6e-d320-4b4e-a5e3-3c394162f176 in namespace container-probe-9305
Aug  8 16:24:45.814: INFO: Started pod busybox-d4442d6e-d320-4b4e-a5e3-3c394162f176 in namespace container-probe-9305
STEP: checking the pod's current state and verifying that restartCount is present
Aug  8 16:24:45.833: INFO: Initial restart count of pod busybox-d4442d6e-d320-4b4e-a5e3-3c394162f176 is 0
Aug  8 16:25:34.347: INFO: Restart count of pod container-probe-9305/busybox-d4442d6e-d320-4b4e-a5e3-3c394162f176 is now 1 (48.513685109s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:25:34.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9305" for this suite.
Aug  8 16:25:40.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:25:41.207: INFO: namespace container-probe-9305 deletion completed in 6.797332379s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:25:41.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Aug  8 16:25:41.554: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:25:41.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7966" for this suite.
Aug  8 16:25:47.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:25:48.627: INFO: namespace kubectl-7966 deletion completed in 6.785469866s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:25:48.627: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Aug  8 16:25:48.958: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6873'
Aug  8 16:25:49.330: INFO: stderr: ""
Aug  8 16:25:49.331: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Aug  8 16:25:50.351: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 16:25:50.351: INFO: Found 0 / 1
Aug  8 16:25:51.353: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 16:25:51.353: INFO: Found 0 / 1
Aug  8 16:25:52.351: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 16:25:52.351: INFO: Found 0 / 1
Aug  8 16:25:53.351: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 16:25:53.351: INFO: Found 1 / 1
Aug  8 16:25:53.351: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug  8 16:25:53.370: INFO: Selector matched 1 pods for map[app:redis]
Aug  8 16:25:53.370: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug  8 16:25:53.371: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-bq4nf redis-master --namespace=kubectl-6873'
Aug  8 16:25:53.597: INFO: stderr: ""
Aug  8 16:25:53.597: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Aug 16:25:52.417 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Aug 16:25:52.418 # Server started, Redis version 3.2.12\n1:M 08 Aug 16:25:52.418 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Aug 16:25:52.418 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug  8 16:25:53.597: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-bq4nf redis-master --namespace=kubectl-6873 --tail=1'
Aug  8 16:25:53.820: INFO: stderr: ""
Aug  8 16:25:53.820: INFO: stdout: "1:M 08 Aug 16:25:52.418 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug  8 16:25:53.820: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-bq4nf redis-master --namespace=kubectl-6873 --limit-bytes=1'
Aug  8 16:25:54.052: INFO: stderr: ""
Aug  8 16:25:54.052: INFO: stdout: " "
STEP: exposing timestamps
Aug  8 16:25:54.052: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-bq4nf redis-master --namespace=kubectl-6873 --tail=1 --timestamps'
Aug  8 16:25:54.293: INFO: stderr: ""
Aug  8 16:25:54.293: INFO: stdout: "2019-08-08T16:25:52.418412247Z 1:M 08 Aug 16:25:52.418 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug  8 16:25:56.794: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-bq4nf redis-master --namespace=kubectl-6873 --since=1s'
Aug  8 16:25:57.074: INFO: stderr: ""
Aug  8 16:25:57.074: INFO: stdout: ""
Aug  8 16:25:57.074: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-bq4nf redis-master --namespace=kubectl-6873 --since=24h'
Aug  8 16:25:57.309: INFO: stderr: ""
Aug  8 16:25:57.309: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 08 Aug 16:25:52.417 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 08 Aug 16:25:52.418 # Server started, Redis version 3.2.12\n1:M 08 Aug 16:25:52.418 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 08 Aug 16:25:52.418 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Aug  8 16:25:57.309: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-6873'
Aug  8 16:25:57.535: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 16:25:57.535: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug  8 16:25:57.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-6873'
Aug  8 16:25:57.790: INFO: stderr: "No resources found.\n"
Aug  8 16:25:57.790: INFO: stdout: ""
Aug  8 16:25:57.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-6873 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug  8 16:25:57.985: INFO: stderr: ""
Aug  8 16:25:57.985: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:25:57.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6873" for this suite.
Aug  8 16:26:20.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:26:20.814: INFO: namespace kubectl-6873 deletion completed in 22.791285798s
•SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:26:20.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5198
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  8 16:26:21.050: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5198'
Aug  8 16:26:21.313: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  8 16:26:21.313: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug  8 16:26:21.362: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-vj75v]
Aug  8 16:26:21.362: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-vj75v" in namespace "kubectl-5198" to be "running and ready"
Aug  8 16:26:21.381: INFO: Pod "e2e-test-nginx-rc-vj75v": Phase="Pending", Reason="", readiness=false. Elapsed: 18.70436ms
Aug  8 16:26:23.401: INFO: Pod "e2e-test-nginx-rc-vj75v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038537822s
Aug  8 16:26:25.420: INFO: Pod "e2e-test-nginx-rc-vj75v": Phase="Running", Reason="", readiness=true. Elapsed: 4.058066535s
Aug  8 16:26:25.421: INFO: Pod "e2e-test-nginx-rc-vj75v" satisfied condition "running and ready"
Aug  8 16:26:25.421: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-vj75v]
Aug  8 16:26:25.421: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-5198'
Aug  8 16:26:25.678: INFO: stderr: ""
Aug  8 16:26:25.678: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Aug  8 16:26:25.678: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-5198'
Aug  8 16:26:25.912: INFO: stderr: ""
Aug  8 16:26:25.912: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:26:25.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5198" for this suite.
Aug  8 16:26:48.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:26:48.758: INFO: namespace kubectl-5198 deletion completed in 22.808905503s
•SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:26:48.758: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug  8 16:26:49.126: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7359,SelfLink:/api/v1/namespaces/watch-7359/configmaps/e2e-watch-test-watch-closed,UID:2eab5e78-9511-437b-a9c3-c74d564b83c5,ResourceVersion:22259,Generation:0,CreationTimestamp:2019-08-08 16:26:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug  8 16:26:49.127: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7359,SelfLink:/api/v1/namespaces/watch-7359/configmaps/e2e-watch-test-watch-closed,UID:2eab5e78-9511-437b-a9c3-c74d564b83c5,ResourceVersion:22261,Generation:0,CreationTimestamp:2019-08-08 16:26:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug  8 16:26:49.204: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7359,SelfLink:/api/v1/namespaces/watch-7359/configmaps/e2e-watch-test-watch-closed,UID:2eab5e78-9511-437b-a9c3-c74d564b83c5,ResourceVersion:22262,Generation:0,CreationTimestamp:2019-08-08 16:26:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  8 16:26:49.204: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7359,SelfLink:/api/v1/namespaces/watch-7359/configmaps/e2e-watch-test-watch-closed,UID:2eab5e78-9511-437b-a9c3-c74d564b83c5,ResourceVersion:22263,Generation:0,CreationTimestamp:2019-08-08 16:26:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:26:49.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7359" for this suite.
Aug  8 16:26:55.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:26:56.016: INFO: namespace watch-7359 deletion completed in 6.791865578s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:26:56.017: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1308
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-974ffd6e-4c5b-4f1e-b2f9-0e6c6314697d
STEP: Creating configMap with name cm-test-opt-upd-f3de22df-5ce3-43b6-850b-fdae036bc245
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-974ffd6e-4c5b-4f1e-b2f9-0e6c6314697d
STEP: Updating configmap cm-test-opt-upd-f3de22df-5ce3-43b6-850b-fdae036bc245
STEP: Creating configMap with name cm-test-opt-create-40f12392-d6ec-49e2-99b0-8f3aa5412449
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:28:28.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1308" for this suite.
Aug  8 16:28:50.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:28:50.971: INFO: namespace projected-1308 deletion completed in 22.786632805s
•SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:28:50.971: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Aug  8 16:28:51.984: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0808 16:28:51.984148    4191 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:28:51.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-759" for this suite.
Aug  8 16:28:58.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:28:58.798: INFO: namespace gc-759 deletion completed in 6.794864986s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:28:58.799: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6605
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Aug  8 16:28:59.052: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Aug  8 16:28:59.489: INFO: stderr: ""
Aug  8 16:28:59.489: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:28:59.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6605" for this suite.
Aug  8 16:29:05.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:29:06.337: INFO: namespace kubectl-6605 deletion completed in 6.810727996s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:29:06.338: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:29:40.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-327" for this suite.
Aug  8 16:29:46.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:29:47.742: INFO: namespace container-runtime-327 deletion completed in 6.871730131s
•SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:29:47.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4093
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:29:48.075: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2865c18e-7e50-414b-bd1c-b11fbed6dfed" in namespace "downward-api-4093" to be "success or failure"
Aug  8 16:29:48.094: INFO: Pod "downwardapi-volume-2865c18e-7e50-414b-bd1c-b11fbed6dfed": Phase="Pending", Reason="", readiness=false. Elapsed: 19.040815ms
Aug  8 16:29:50.114: INFO: Pod "downwardapi-volume-2865c18e-7e50-414b-bd1c-b11fbed6dfed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039139449s
Aug  8 16:29:52.135: INFO: Pod "downwardapi-volume-2865c18e-7e50-414b-bd1c-b11fbed6dfed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059370856s
STEP: Saw pod success
Aug  8 16:29:52.135: INFO: Pod "downwardapi-volume-2865c18e-7e50-414b-bd1c-b11fbed6dfed" satisfied condition "success or failure"
Aug  8 16:29:52.154: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-2865c18e-7e50-414b-bd1c-b11fbed6dfed container client-container: <nil>
STEP: delete the pod
Aug  8 16:29:52.210: INFO: Waiting for pod downwardapi-volume-2865c18e-7e50-414b-bd1c-b11fbed6dfed to disappear
Aug  8 16:29:52.235: INFO: Pod downwardapi-volume-2865c18e-7e50-414b-bd1c-b11fbed6dfed no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:29:52.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4093" for this suite.
Aug  8 16:29:58.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:29:59.072: INFO: namespace downward-api-4093 deletion completed in 6.798290788s
•SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:29:59.073: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:30:03.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-957" for this suite.
Aug  8 16:30:09.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:30:10.413: INFO: namespace emptydir-wrapper-957 deletion completed in 6.826302054s
•SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:30:10.413: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-944
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug  8 16:30:14.861: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec pod-sharedvolume-cf85642c-a549-49f3-8eb9-87476191c418 -c busybox-main-container --namespace=emptydir-944 -- cat /usr/share/volumeshare/shareddata.txt'
Aug  8 16:30:15.575: INFO: stderr: ""
Aug  8 16:30:15.575: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:30:15.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-944" for this suite.
Aug  8 16:30:21.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:30:22.759: INFO: namespace emptydir-944 deletion completed in 7.146853434s
•
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:30:22.759: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 16:30:23.056: INFO: Creating deployment "test-recreate-deployment"
Aug  8 16:30:23.076: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug  8 16:30:23.119: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug  8 16:30:23.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-6df85df6b9\""}}, CollisionCount:(*int32)(nil)}
Aug  8 16:30:25.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:30:27.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700878623, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:30:29.163: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug  8 16:30:29.202: INFO: Updating deployment test-recreate-deployment
Aug  8 16:30:29.202: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug  8 16:30:29.317: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-5814,SelfLink:/apis/apps/v1/namespaces/deployment-5814/deployments/test-recreate-deployment,UID:ead6b333-5bda-4931-9eed-11279878f68a,ResourceVersion:23064,Generation:2,CreationTimestamp:2019-08-08 16:30:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-08 16:30:29 +0000 UTC 2019-08-08 16:30:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-08 16:30:29 +0000 UTC 2019-08-08 16:30:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug  8 16:30:29.337: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-5814,SelfLink:/apis/apps/v1/namespaces/deployment-5814/replicasets/test-recreate-deployment-5c8c9cc69d,UID:f014d357-ae10-4870-bb6f-c6f945f12462,ResourceVersion:23063,Generation:1,CreationTimestamp:2019-08-08 16:30:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ead6b333-5bda-4931-9eed-11279878f68a 0xc0038d4a57 0xc0038d4a58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  8 16:30:29.337: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug  8 16:30:29.337: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-5814,SelfLink:/apis/apps/v1/namespaces/deployment-5814/replicasets/test-recreate-deployment-6df85df6b9,UID:1b6f6289-5536-44c0-8b5f-3509347a3f20,ResourceVersion:23055,Generation:2,CreationTimestamp:2019-08-08 16:30:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ead6b333-5bda-4931-9eed-11279878f68a 0xc0038d4b27 0xc0038d4b28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  8 16:30:29.357: INFO: Pod "test-recreate-deployment-5c8c9cc69d-47dvl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-47dvl,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-5814,SelfLink:/api/v1/namespaces/deployment-5814/pods/test-recreate-deployment-5c8c9cc69d-47dvl,UID:1ee1e60f-f352-48a6-87f0-b6349d83bae8,ResourceVersion:23061,Generation:0,CreationTimestamp:2019-08-08 16:30:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d f014d357-ae10-4870-bb6f-c6f945f12462 0xc003acaed7 0xc003acaed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rr2mr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rr2mr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rr2mr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003acaf40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003acaf60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 16:30:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 16:30:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-08 16:30:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 16:30:29 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-08-08 16:30:29 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:30:29.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5814" for this suite.
Aug  8 16:30:35.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:30:36.210: INFO: namespace deployment-5814 deletion completed in 6.815664124s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:30:36.211: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2126
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug  8 16:30:36.490: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:30:53.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2126" for this suite.
Aug  8 16:31:00.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:31:00.770: INFO: namespace pods-2126 deletion completed in 6.799989424s
•S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:31:00.770: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 16:31:01.150: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug  8 16:31:01.216: INFO: Number of nodes with available pods: 0
Aug  8 16:31:01.216: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:31:02.274: INFO: Number of nodes with available pods: 0
Aug  8 16:31:02.274: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:31:03.275: INFO: Number of nodes with available pods: 0
Aug  8 16:31:03.275: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:31:04.274: INFO: Number of nodes with available pods: 1
Aug  8 16:31:04.274: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:31:05.274: INFO: Number of nodes with available pods: 2
Aug  8 16:31:05.274: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug  8 16:31:05.413: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:05.413: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:06.464: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:06.464: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:07.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:07.456: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:08.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:08.456: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:09.457: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:09.457: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:09.457: INFO: Pod daemon-set-tzrwl is not available
Aug  8 16:31:10.455: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:10.455: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:10.455: INFO: Pod daemon-set-tzrwl is not available
Aug  8 16:31:11.458: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:11.458: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:11.458: INFO: Pod daemon-set-tzrwl is not available
Aug  8 16:31:12.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:12.457: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:12.457: INFO: Pod daemon-set-tzrwl is not available
Aug  8 16:31:13.457: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:13.457: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:13.457: INFO: Pod daemon-set-tzrwl is not available
Aug  8 16:31:14.457: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:14.457: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:14.457: INFO: Pod daemon-set-tzrwl is not available
Aug  8 16:31:15.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:15.456: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:15.456: INFO: Pod daemon-set-tzrwl is not available
Aug  8 16:31:16.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:16.456: INFO: Wrong image for pod: daemon-set-tzrwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:16.456: INFO: Pod daemon-set-tzrwl is not available
Aug  8 16:31:17.456: INFO: Pod daemon-set-4m9pn is not available
Aug  8 16:31:17.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:18.456: INFO: Pod daemon-set-4m9pn is not available
Aug  8 16:31:18.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:19.456: INFO: Pod daemon-set-4m9pn is not available
Aug  8 16:31:19.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:20.456: INFO: Pod daemon-set-4m9pn is not available
Aug  8 16:31:20.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:21.463: INFO: Pod daemon-set-4m9pn is not available
Aug  8 16:31:21.463: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:22.455: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:23.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:24.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:24.456: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:25.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:25.456: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:26.457: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:26.457: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:27.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:27.456: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:28.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:28.456: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:29.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:29.456: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:30.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:30.456: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:31.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:31.456: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:32.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:32.456: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:33.456: INFO: Wrong image for pod: daemon-set-5zgnm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug  8 16:31:33.456: INFO: Pod daemon-set-5zgnm is not available
Aug  8 16:31:34.456: INFO: Pod daemon-set-xwlgf is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug  8 16:31:34.532: INFO: Number of nodes with available pods: 1
Aug  8 16:31:34.532: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:31:35.590: INFO: Number of nodes with available pods: 1
Aug  8 16:31:35.590: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:31:36.592: INFO: Number of nodes with available pods: 1
Aug  8 16:31:36.592: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:31:37.591: INFO: Number of nodes with available pods: 2
Aug  8 16:31:37.591: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5622, will wait for the garbage collector to delete the pods
Aug  8 16:31:37.781: INFO: Deleting DaemonSet.extensions daemon-set took: 22.569836ms
Aug  8 16:31:37.881: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.211214ms
Aug  8 16:31:54.001: INFO: Number of nodes with available pods: 0
Aug  8 16:31:54.001: INFO: Number of running nodes: 0, number of available pods: 0
Aug  8 16:31:54.020: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5622/daemonsets","resourceVersion":"23378"},"items":null}

Aug  8 16:31:54.039: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5622/pods","resourceVersion":"23378"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:31:54.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5622" for this suite.
Aug  8 16:32:00.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:32:00.954: INFO: namespace daemonsets-5622 deletion completed in 6.819208302s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:32:00.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3236
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-8b9b364d-85c1-4f0b-addb-7433e4abb489
STEP: Creating configMap with name cm-test-opt-upd-39f19878-9af9-4aa2-9675-9933acefc2a9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-8b9b364d-85c1-4f0b-addb-7433e4abb489
STEP: Updating configmap cm-test-opt-upd-39f19878-9af9-4aa2-9675-9933acefc2a9
STEP: Creating configMap with name cm-test-opt-create-9b17bf0c-4a6b-4da3-8517-99b31c8b4796
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:33:35.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3236" for this suite.
Aug  8 16:33:57.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:33:57.917: INFO: namespace configmap-3236 deletion completed in 22.804465844s
•SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:33:57.917: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug  8 16:34:02.294: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:34:02.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5632" for this suite.
Aug  8 16:34:08.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:34:09.187: INFO: namespace container-runtime-5632 deletion completed in 6.808267869s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:34:09.188: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-3e2caca2-d5f3-4ea7-8e8d-bb37306566e0
STEP: Creating a pod to test consume secrets
Aug  8 16:34:09.501: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-44d17ec1-b13a-468a-ab7a-4b3c45bcb6de" in namespace "projected-6787" to be "success or failure"
Aug  8 16:34:09.520: INFO: Pod "pod-projected-secrets-44d17ec1-b13a-468a-ab7a-4b3c45bcb6de": Phase="Pending", Reason="", readiness=false. Elapsed: 19.091267ms
Aug  8 16:34:11.540: INFO: Pod "pod-projected-secrets-44d17ec1-b13a-468a-ab7a-4b3c45bcb6de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039270286s
Aug  8 16:34:13.563: INFO: Pod "pod-projected-secrets-44d17ec1-b13a-468a-ab7a-4b3c45bcb6de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062550708s
STEP: Saw pod success
Aug  8 16:34:13.563: INFO: Pod "pod-projected-secrets-44d17ec1-b13a-468a-ab7a-4b3c45bcb6de" satisfied condition "success or failure"
Aug  8 16:34:13.583: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-secrets-44d17ec1-b13a-468a-ab7a-4b3c45bcb6de container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug  8 16:34:13.634: INFO: Waiting for pod pod-projected-secrets-44d17ec1-b13a-468a-ab7a-4b3c45bcb6de to disappear
Aug  8 16:34:13.659: INFO: Pod pod-projected-secrets-44d17ec1-b13a-468a-ab7a-4b3c45bcb6de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:34:13.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6787" for this suite.
Aug  8 16:34:19.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:34:20.504: INFO: namespace projected-6787 deletion completed in 6.808238528s
•SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:34:20.505: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3847
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug  8 16:34:24.868: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug  8 16:34:35.086: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:34:35.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3847" for this suite.
Aug  8 16:34:41.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:34:41.931: INFO: namespace pods-3847 deletion completed in 6.805343853s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:34:41.931: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-ad5e650d-7e1a-4764-9cf5-a1ac56d57f13
STEP: Creating a pod to test consume configMaps
Aug  8 16:34:42.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48c6fb20-21c7-42ff-9954-e39d93ad5aa0" in namespace "projected-3357" to be "success or failure"
Aug  8 16:34:42.323: INFO: Pod "pod-projected-configmaps-48c6fb20-21c7-42ff-9954-e39d93ad5aa0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.059623ms
Aug  8 16:34:44.343: INFO: Pod "pod-projected-configmaps-48c6fb20-21c7-42ff-9954-e39d93ad5aa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043745463s
Aug  8 16:34:46.364: INFO: Pod "pod-projected-configmaps-48c6fb20-21c7-42ff-9954-e39d93ad5aa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064128641s
STEP: Saw pod success
Aug  8 16:34:46.364: INFO: Pod "pod-projected-configmaps-48c6fb20-21c7-42ff-9954-e39d93ad5aa0" satisfied condition "success or failure"
Aug  8 16:34:46.383: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-projected-configmaps-48c6fb20-21c7-42ff-9954-e39d93ad5aa0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 16:34:46.446: INFO: Waiting for pod pod-projected-configmaps-48c6fb20-21c7-42ff-9954-e39d93ad5aa0 to disappear
Aug  8 16:34:46.465: INFO: Pod pod-projected-configmaps-48c6fb20-21c7-42ff-9954-e39d93ad5aa0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:34:46.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3357" for this suite.
Aug  8 16:34:52.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:34:53.319: INFO: namespace projected-3357 deletion completed in 6.817504836s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:34:53.319: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:34:53.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42db7940-ff8b-4a7e-b2e1-1bb76faad479" in namespace "projected-3118" to be "success or failure"
Aug  8 16:34:53.606: INFO: Pod "downwardapi-volume-42db7940-ff8b-4a7e-b2e1-1bb76faad479": Phase="Pending", Reason="", readiness=false. Elapsed: 24.838936ms
Aug  8 16:34:55.627: INFO: Pod "downwardapi-volume-42db7940-ff8b-4a7e-b2e1-1bb76faad479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045232179s
Aug  8 16:34:57.647: INFO: Pod "downwardapi-volume-42db7940-ff8b-4a7e-b2e1-1bb76faad479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065270293s
STEP: Saw pod success
Aug  8 16:34:57.647: INFO: Pod "downwardapi-volume-42db7940-ff8b-4a7e-b2e1-1bb76faad479" satisfied condition "success or failure"
Aug  8 16:34:57.666: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-42db7940-ff8b-4a7e-b2e1-1bb76faad479 container client-container: <nil>
STEP: delete the pod
Aug  8 16:34:57.725: INFO: Waiting for pod downwardapi-volume-42db7940-ff8b-4a7e-b2e1-1bb76faad479 to disappear
Aug  8 16:34:57.747: INFO: Pod downwardapi-volume-42db7940-ff8b-4a7e-b2e1-1bb76faad479 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:34:57.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3118" for this suite.
Aug  8 16:35:03.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:35:04.571: INFO: namespace projected-3118 deletion completed in 6.786810359s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:35:04.572: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-cr6c
STEP: Creating a pod to test atomic-volume-subpath
Aug  8 16:35:04.925: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cr6c" in namespace "subpath-3992" to be "success or failure"
Aug  8 16:35:04.944: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.897547ms
Aug  8 16:35:06.964: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038813741s
Aug  8 16:35:08.984: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 4.058701447s
Aug  8 16:35:11.004: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 6.079133668s
Aug  8 16:35:13.025: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 8.099598502s
Aug  8 16:35:15.045: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 10.119807559s
Aug  8 16:35:17.065: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 12.13960951s
Aug  8 16:35:19.085: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 14.159622714s
Aug  8 16:35:21.105: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 16.17964987s
Aug  8 16:35:23.125: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 18.199794238s
Aug  8 16:35:25.145: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 20.220190732s
Aug  8 16:35:27.166: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Running", Reason="", readiness=true. Elapsed: 22.24050337s
Aug  8 16:35:29.185: INFO: Pod "pod-subpath-test-secret-cr6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.259962736s
STEP: Saw pod success
Aug  8 16:35:29.185: INFO: Pod "pod-subpath-test-secret-cr6c" satisfied condition "success or failure"
Aug  8 16:35:29.204: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-subpath-test-secret-cr6c container test-container-subpath-secret-cr6c: <nil>
STEP: delete the pod
Aug  8 16:35:29.261: INFO: Waiting for pod pod-subpath-test-secret-cr6c to disappear
Aug  8 16:35:29.281: INFO: Pod pod-subpath-test-secret-cr6c no longer exists
STEP: Deleting pod pod-subpath-test-secret-cr6c
Aug  8 16:35:29.281: INFO: Deleting pod "pod-subpath-test-secret-cr6c" in namespace "subpath-3992"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:35:29.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3992" for this suite.
Aug  8 16:35:35.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:35:36.145: INFO: namespace subpath-3992 deletion completed in 6.807596363s
•
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:35:36.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug  8 16:35:41.179: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7b1ab785-da65-449c-a4dc-78f8aeedc37c"
Aug  8 16:35:41.179: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7b1ab785-da65-449c-a4dc-78f8aeedc37c" in namespace "pods-9624" to be "terminated due to deadline exceeded"
Aug  8 16:35:41.198: INFO: Pod "pod-update-activedeadlineseconds-7b1ab785-da65-449c-a4dc-78f8aeedc37c": Phase="Running", Reason="", readiness=true. Elapsed: 19.02079ms
Aug  8 16:35:43.218: INFO: Pod "pod-update-activedeadlineseconds-7b1ab785-da65-449c-a4dc-78f8aeedc37c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.039217965s
Aug  8 16:35:43.218: INFO: Pod "pod-update-activedeadlineseconds-7b1ab785-da65-449c-a4dc-78f8aeedc37c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:35:43.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9624" for this suite.
Aug  8 16:35:49.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:35:50.090: INFO: namespace pods-9624 deletion completed in 6.834650232s
•SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:35:50.090: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:35:50.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-735680e6-6efd-413a-bed2-cc931b44ef12" in namespace "downward-api-3267" to be "success or failure"
Aug  8 16:35:50.392: INFO: Pod "downwardapi-volume-735680e6-6efd-413a-bed2-cc931b44ef12": Phase="Pending", Reason="", readiness=false. Elapsed: 19.031701ms
Aug  8 16:35:52.411: INFO: Pod "downwardapi-volume-735680e6-6efd-413a-bed2-cc931b44ef12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038794039s
Aug  8 16:35:54.432: INFO: Pod "downwardapi-volume-735680e6-6efd-413a-bed2-cc931b44ef12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05940522s
STEP: Saw pod success
Aug  8 16:35:54.432: INFO: Pod "downwardapi-volume-735680e6-6efd-413a-bed2-cc931b44ef12" satisfied condition "success or failure"
Aug  8 16:35:54.452: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-735680e6-6efd-413a-bed2-cc931b44ef12 container client-container: <nil>
STEP: delete the pod
Aug  8 16:35:54.513: INFO: Waiting for pod downwardapi-volume-735680e6-6efd-413a-bed2-cc931b44ef12 to disappear
Aug  8 16:35:54.532: INFO: Pod downwardapi-volume-735680e6-6efd-413a-bed2-cc931b44ef12 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:35:54.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3267" for this suite.
Aug  8 16:36:00.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:36:01.380: INFO: namespace downward-api-3267 deletion completed in 6.811639989s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:36:01.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Aug  8 16:36:01.672: INFO: Waiting up to 5m0s for pod "client-containers-85ec5960-45b5-4ca2-bfac-e31ce9287576" in namespace "containers-1002" to be "success or failure"
Aug  8 16:36:01.690: INFO: Pod "client-containers-85ec5960-45b5-4ca2-bfac-e31ce9287576": Phase="Pending", Reason="", readiness=false. Elapsed: 18.94425ms
Aug  8 16:36:03.710: INFO: Pod "client-containers-85ec5960-45b5-4ca2-bfac-e31ce9287576": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038690358s
Aug  8 16:36:05.730: INFO: Pod "client-containers-85ec5960-45b5-4ca2-bfac-e31ce9287576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058739587s
STEP: Saw pod success
Aug  8 16:36:05.730: INFO: Pod "client-containers-85ec5960-45b5-4ca2-bfac-e31ce9287576" satisfied condition "success or failure"
Aug  8 16:36:05.750: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod client-containers-85ec5960-45b5-4ca2-bfac-e31ce9287576 container test-container: <nil>
STEP: delete the pod
Aug  8 16:36:05.807: INFO: Waiting for pod client-containers-85ec5960-45b5-4ca2-bfac-e31ce9287576 to disappear
Aug  8 16:36:05.826: INFO: Pod client-containers-85ec5960-45b5-4ca2-bfac-e31ce9287576 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:36:05.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1002" for this suite.
Aug  8 16:36:11.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:36:12.646: INFO: namespace containers-1002 deletion completed in 6.783029032s
•SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:36:12.647: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-db39873d-9fb2-4a54-a607-05c0118da8f0
STEP: Creating a pod to test consume configMaps
Aug  8 16:36:13.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-ccf832b2-4baf-4ed0-9d3c-b47489aa65f5" in namespace "configmap-4964" to be "success or failure"
Aug  8 16:36:13.103: INFO: Pod "pod-configmaps-ccf832b2-4baf-4ed0-9d3c-b47489aa65f5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.797982ms
Aug  8 16:36:15.128: INFO: Pod "pod-configmaps-ccf832b2-4baf-4ed0-9d3c-b47489aa65f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043552062s
Aug  8 16:36:17.148: INFO: Pod "pod-configmaps-ccf832b2-4baf-4ed0-9d3c-b47489aa65f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063558395s
STEP: Saw pod success
Aug  8 16:36:17.148: INFO: Pod "pod-configmaps-ccf832b2-4baf-4ed0-9d3c-b47489aa65f5" satisfied condition "success or failure"
Aug  8 16:36:17.169: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-ccf832b2-4baf-4ed0-9d3c-b47489aa65f5 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 16:36:17.227: INFO: Waiting for pod pod-configmaps-ccf832b2-4baf-4ed0-9d3c-b47489aa65f5 to disappear
Aug  8 16:36:17.246: INFO: Pod pod-configmaps-ccf832b2-4baf-4ed0-9d3c-b47489aa65f5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:36:17.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4964" for this suite.
Aug  8 16:36:23.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:36:24.094: INFO: namespace configmap-4964 deletion completed in 6.810511135s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:36:24.095: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5484
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 16:36:24.472: INFO: Create a RollingUpdate DaemonSet
Aug  8 16:36:24.492: INFO: Check that daemon pods launch on every node of the cluster
Aug  8 16:36:24.532: INFO: Number of nodes with available pods: 0
Aug  8 16:36:24.532: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:36:25.588: INFO: Number of nodes with available pods: 0
Aug  8 16:36:25.588: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:36:26.591: INFO: Number of nodes with available pods: 0
Aug  8 16:36:26.591: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:36:27.595: INFO: Number of nodes with available pods: 2
Aug  8 16:36:27.595: INFO: Number of running nodes: 2, number of available pods: 2
Aug  8 16:36:27.595: INFO: Update the DaemonSet to trigger a rollout
Aug  8 16:36:27.633: INFO: Updating DaemonSet daemon-set
Aug  8 16:36:44.731: INFO: Roll back the DaemonSet before rollout is complete
Aug  8 16:36:44.771: INFO: Updating DaemonSet daemon-set
Aug  8 16:36:44.771: INFO: Make sure DaemonSet rollback is complete
Aug  8 16:36:44.791: INFO: Wrong image for pod: daemon-set-jj52h. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug  8 16:36:44.791: INFO: Pod daemon-set-jj52h is not available
Aug  8 16:36:45.830: INFO: Wrong image for pod: daemon-set-jj52h. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug  8 16:36:45.830: INFO: Pod daemon-set-jj52h is not available
Aug  8 16:36:46.830: INFO: Wrong image for pod: daemon-set-jj52h. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug  8 16:36:46.830: INFO: Pod daemon-set-jj52h is not available
Aug  8 16:36:47.883: INFO: Pod daemon-set-mczrr is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5484, will wait for the garbage collector to delete the pods
Aug  8 16:36:48.179: INFO: Deleting DaemonSet.extensions daemon-set took: 32.121783ms
Aug  8 16:36:48.579: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.295929ms
Aug  8 16:36:57.198: INFO: Number of nodes with available pods: 0
Aug  8 16:36:57.199: INFO: Number of running nodes: 0, number of available pods: 0
Aug  8 16:36:57.225: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5484/daemonsets","resourceVersion":"24473"},"items":null}

Aug  8 16:36:57.244: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5484/pods","resourceVersion":"24473"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:36:57.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5484" for this suite.
Aug  8 16:37:03.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:37:04.138: INFO: namespace daemonsets-5484 deletion completed in 6.799627308s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:37:04.139: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug  8 16:37:04.561: INFO: Waiting up to 5m0s for pod "downward-api-250a6fb1-9b11-4aa5-b8ab-932ba91d20f0" in namespace "downward-api-7868" to be "success or failure"
Aug  8 16:37:04.584: INFO: Pod "downward-api-250a6fb1-9b11-4aa5-b8ab-932ba91d20f0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.151292ms
Aug  8 16:37:06.604: INFO: Pod "downward-api-250a6fb1-9b11-4aa5-b8ab-932ba91d20f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042712547s
Aug  8 16:37:08.624: INFO: Pod "downward-api-250a6fb1-9b11-4aa5-b8ab-932ba91d20f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062533307s
STEP: Saw pod success
Aug  8 16:37:08.624: INFO: Pod "downward-api-250a6fb1-9b11-4aa5-b8ab-932ba91d20f0" satisfied condition "success or failure"
Aug  8 16:37:08.643: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downward-api-250a6fb1-9b11-4aa5-b8ab-932ba91d20f0 container dapi-container: <nil>
STEP: delete the pod
Aug  8 16:37:08.854: INFO: Waiting for pod downward-api-250a6fb1-9b11-4aa5-b8ab-932ba91d20f0 to disappear
Aug  8 16:37:08.873: INFO: Pod downward-api-250a6fb1-9b11-4aa5-b8ab-932ba91d20f0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:37:08.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7868" for this suite.
Aug  8 16:37:14.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:37:15.715: INFO: namespace downward-api-7868 deletion completed in 6.798685176s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:37:15.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7428
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Aug  8 16:37:15.972: INFO: Waiting up to 5m0s for pod "client-containers-ca03c33f-85fc-4aa7-b442-f60998153a48" in namespace "containers-7428" to be "success or failure"
Aug  8 16:37:15.991: INFO: Pod "client-containers-ca03c33f-85fc-4aa7-b442-f60998153a48": Phase="Pending", Reason="", readiness=false. Elapsed: 18.706081ms
Aug  8 16:37:18.011: INFO: Pod "client-containers-ca03c33f-85fc-4aa7-b442-f60998153a48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038411956s
Aug  8 16:37:20.030: INFO: Pod "client-containers-ca03c33f-85fc-4aa7-b442-f60998153a48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058282113s
STEP: Saw pod success
Aug  8 16:37:20.031: INFO: Pod "client-containers-ca03c33f-85fc-4aa7-b442-f60998153a48" satisfied condition "success or failure"
Aug  8 16:37:20.050: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod client-containers-ca03c33f-85fc-4aa7-b442-f60998153a48 container test-container: <nil>
STEP: delete the pod
Aug  8 16:37:20.102: INFO: Waiting for pod client-containers-ca03c33f-85fc-4aa7-b442-f60998153a48 to disappear
Aug  8 16:37:20.122: INFO: Pod client-containers-ca03c33f-85fc-4aa7-b442-f60998153a48 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:37:20.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7428" for this suite.
Aug  8 16:37:26.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:37:26.971: INFO: namespace containers-7428 deletion completed in 6.81267109s
•SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:37:26.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9544.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9544.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9544.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9544.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9544.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9544.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug  8 16:37:33.915: INFO: DNS probes using dns-9544/dns-test-db7cc1d7-d687-4174-bb60-9909dde17009 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:37:33.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9544" for this suite.
Aug  8 16:37:40.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:37:40.768: INFO: namespace dns-9544 deletion completed in 6.784881712s
•SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:37:40.768: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:37:41.164: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52c942a5-8a4b-4d65-ab5b-fb1b5635140b" in namespace "projected-9376" to be "success or failure"
Aug  8 16:37:41.183: INFO: Pod "downwardapi-volume-52c942a5-8a4b-4d65-ab5b-fb1b5635140b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.90046ms
Aug  8 16:37:43.203: INFO: Pod "downwardapi-volume-52c942a5-8a4b-4d65-ab5b-fb1b5635140b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039002794s
Aug  8 16:37:45.224: INFO: Pod "downwardapi-volume-52c942a5-8a4b-4d65-ab5b-fb1b5635140b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05904738s
Aug  8 16:37:47.245: INFO: Pod "downwardapi-volume-52c942a5-8a4b-4d65-ab5b-fb1b5635140b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08033281s
STEP: Saw pod success
Aug  8 16:37:47.245: INFO: Pod "downwardapi-volume-52c942a5-8a4b-4d65-ab5b-fb1b5635140b" satisfied condition "success or failure"
Aug  8 16:37:47.265: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-52c942a5-8a4b-4d65-ab5b-fb1b5635140b container client-container: <nil>
STEP: delete the pod
Aug  8 16:37:47.320: INFO: Waiting for pod downwardapi-volume-52c942a5-8a4b-4d65-ab5b-fb1b5635140b to disappear
Aug  8 16:37:47.343: INFO: Pod downwardapi-volume-52c942a5-8a4b-4d65-ab5b-fb1b5635140b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:37:47.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9376" for this suite.
Aug  8 16:37:53.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:37:54.158: INFO: namespace projected-9376 deletion completed in 6.77800298s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:37:54.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug  8 16:37:59.093: INFO: Successfully updated pod "pod-update-408b5fc6-edb2-4e7a-a47e-522a6e28b801"
STEP: verifying the updated pod is in kubernetes
Aug  8 16:37:59.131: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:37:59.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4428" for this suite.
Aug  8 16:38:21.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:38:21.992: INFO: namespace pods-4428 deletion completed in 22.823062173s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:38:21.992: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2067
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:38:22.488: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b728b9ca-b3ca-4879-9962-2df775d0c213" in namespace "projected-2067" to be "success or failure"
Aug  8 16:38:22.582: INFO: Pod "downwardapi-volume-b728b9ca-b3ca-4879-9962-2df775d0c213": Phase="Pending", Reason="", readiness=false. Elapsed: 94.497255ms
Aug  8 16:38:24.602: INFO: Pod "downwardapi-volume-b728b9ca-b3ca-4879-9962-2df775d0c213": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114417333s
Aug  8 16:38:26.622: INFO: Pod "downwardapi-volume-b728b9ca-b3ca-4879-9962-2df775d0c213": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.134407999s
STEP: Saw pod success
Aug  8 16:38:26.622: INFO: Pod "downwardapi-volume-b728b9ca-b3ca-4879-9962-2df775d0c213" satisfied condition "success or failure"
Aug  8 16:38:26.642: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-b728b9ca-b3ca-4879-9962-2df775d0c213 container client-container: <nil>
STEP: delete the pod
Aug  8 16:38:26.745: INFO: Waiting for pod downwardapi-volume-b728b9ca-b3ca-4879-9962-2df775d0c213 to disappear
Aug  8 16:38:26.764: INFO: Pod downwardapi-volume-b728b9ca-b3ca-4879-9962-2df775d0c213 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:38:26.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2067" for this suite.
Aug  8 16:38:32.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:38:33.594: INFO: namespace projected-2067 deletion completed in 6.793917039s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:38:33.596: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-78929924-641b-455f-8b92-89848f5e3db5
STEP: Creating secret with name secret-projected-all-test-volume-7186f613-a7ff-4dac-b782-7074531e5ff3
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug  8 16:38:33.909: INFO: Waiting up to 5m0s for pod "projected-volume-0d05160d-29c9-43fd-8abd-d74ac39861ef" in namespace "projected-4956" to be "success or failure"
Aug  8 16:38:33.928: INFO: Pod "projected-volume-0d05160d-29c9-43fd-8abd-d74ac39861ef": Phase="Pending", Reason="", readiness=false. Elapsed: 19.021274ms
Aug  8 16:38:35.949: INFO: Pod "projected-volume-0d05160d-29c9-43fd-8abd-d74ac39861ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039068555s
Aug  8 16:38:37.969: INFO: Pod "projected-volume-0d05160d-29c9-43fd-8abd-d74ac39861ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059281549s
STEP: Saw pod success
Aug  8 16:38:37.969: INFO: Pod "projected-volume-0d05160d-29c9-43fd-8abd-d74ac39861ef" satisfied condition "success or failure"
Aug  8 16:38:37.988: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod projected-volume-0d05160d-29c9-43fd-8abd-d74ac39861ef container projected-all-volume-test: <nil>
STEP: delete the pod
Aug  8 16:38:38.040: INFO: Waiting for pod projected-volume-0d05160d-29c9-43fd-8abd-d74ac39861ef to disappear
Aug  8 16:38:38.066: INFO: Pod projected-volume-0d05160d-29c9-43fd-8abd-d74ac39861ef no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:38:38.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4956" for this suite.
Aug  8 16:38:44.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:38:44.900: INFO: namespace projected-4956 deletion completed in 6.796356792s
•
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:38:44.900: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug  8 16:38:53.351: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  8 16:38:53.371: INFO: Pod pod-with-poststart-http-hook still exists
Aug  8 16:38:55.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  8 16:38:55.391: INFO: Pod pod-with-poststart-http-hook still exists
Aug  8 16:38:57.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  8 16:38:57.395: INFO: Pod pod-with-poststart-http-hook still exists
Aug  8 16:38:59.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  8 16:38:59.397: INFO: Pod pod-with-poststart-http-hook still exists
Aug  8 16:39:01.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  8 16:39:01.391: INFO: Pod pod-with-poststart-http-hook still exists
Aug  8 16:39:03.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  8 16:39:03.391: INFO: Pod pod-with-poststart-http-hook still exists
Aug  8 16:39:05.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug  8 16:39:05.390: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:39:05.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9724" for this suite.
Aug  8 16:39:27.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:39:28.236: INFO: namespace container-lifecycle-hook-9724 deletion completed in 22.807717423s
•SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:39:28.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:39:32.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6167" for this suite.
Aug  8 16:39:54.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:39:55.535: INFO: namespace replication-controller-6167 deletion completed in 22.829696734s
•SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:39:55.536: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4299
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-4299
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4299 to expose endpoints map[]
Aug  8 16:39:56.027: INFO: successfully validated that service multi-endpoint-test in namespace services-4299 exposes endpoints map[] (19.088868ms elapsed)
STEP: Creating pod pod1 in namespace services-4299
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4299 to expose endpoints map[pod1:[100]]
Aug  8 16:40:00.255: INFO: successfully validated that service multi-endpoint-test in namespace services-4299 exposes endpoints map[pod1:[100]] (4.205278568s elapsed)
STEP: Creating pod pod2 in namespace services-4299
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4299 to expose endpoints map[pod1:[100] pod2:[101]]
Aug  8 16:40:04.570: INFO: successfully validated that service multi-endpoint-test in namespace services-4299 exposes endpoints map[pod1:[100] pod2:[101]] (4.294346674s elapsed)
STEP: Deleting pod pod1 in namespace services-4299
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4299 to expose endpoints map[pod2:[101]]
Aug  8 16:40:04.629: INFO: successfully validated that service multi-endpoint-test in namespace services-4299 exposes endpoints map[pod2:[101]] (37.49515ms elapsed)
STEP: Deleting pod pod2 in namespace services-4299
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4299 to expose endpoints map[]
Aug  8 16:40:04.676: INFO: successfully validated that service multi-endpoint-test in namespace services-4299 exposes endpoints map[] (26.810592ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:40:04.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4299" for this suite.
Aug  8 16:40:28.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:40:29.568: INFO: namespace services-4299 deletion completed in 24.816778416s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:40:29.569: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug  8 16:40:29.987: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3539,SelfLink:/api/v1/namespaces/watch-3539/configmaps/e2e-watch-test-resource-version,UID:049f98ff-6721-4dd4-b803-7de388fabac2,ResourceVersion:25293,Generation:0,CreationTimestamp:2019-08-08 16:40:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug  8 16:40:29.987: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3539,SelfLink:/api/v1/namespaces/watch-3539/configmaps/e2e-watch-test-resource-version,UID:049f98ff-6721-4dd4-b803-7de388fabac2,ResourceVersion:25294,Generation:0,CreationTimestamp:2019-08-08 16:40:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:40:29.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3539" for this suite.
Aug  8 16:40:36.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:40:36.795: INFO: namespace watch-3539 deletion completed in 6.788753224s
•S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:40:36.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug  8 16:40:37.057: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug  8 16:40:37.097: INFO: Waiting for terminating namespaces to be deleted...
Aug  8 16:40:37.116: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr before test
Aug  8 16:40:37.161: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-695fb4fdcd-cknxk from kube-system started at 2019-08-08 14:54:31 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Aug  8 16:40:37.161: INFO: calico-node-6w86k from kube-system started at 2019-08-08 14:54:10 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container calico-node ready: true, restart count 0
Aug  8 16:40:37.161: INFO: metrics-server-566847b67f-f2cmb from kube-system started at 2019-08-08 14:54:29 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container metrics-server ready: true, restart count 0
Aug  8 16:40:37.161: INFO: coredns-85cc454dd8-cqd76 from kube-system started at 2019-08-08 14:54:29 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container coredns ready: true, restart count 0
Aug  8 16:40:37.161: INFO: node-exporter-s4q7v from kube-system started at 2019-08-08 14:54:09 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container node-exporter ready: true, restart count 0
Aug  8 16:40:37.161: INFO: blackbox-exporter-954dd954b-6rgqr from kube-system started at 2019-08-08 14:54:09 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug  8 16:40:37.161: INFO: kube-proxy-vvfrb from kube-system started at 2019-08-08 14:54:09 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container kube-proxy ready: true, restart count 0
Aug  8 16:40:37.161: INFO: coredns-85cc454dd8-2tknm from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container coredns ready: true, restart count 0
Aug  8 16:40:37.161: INFO: addons-kubernetes-dashboard-5c8d9945bc-spkwl from kube-system started at 2019-08-08 14:54:29 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug  8 16:40:37.161: INFO: calico-kube-controllers-5f4b46ffb5-zt29p from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug  8 16:40:37.161: INFO: vpn-shoot-89d5dc9c8-84rcr from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container vpn-shoot ready: true, restart count 0
Aug  8 16:40:37.161: INFO: addons-nginx-ingress-controller-6496d947df-hzpqv from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.161: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug  8 16:40:37.161: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk before test
Aug  8 16:40:37.190: INFO: calico-node-84fr2 from kube-system started at 2019-08-08 14:54:36 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.190: INFO: 	Container calico-node ready: true, restart count 0
Aug  8 16:40:37.190: INFO: node-exporter-lzqzm from kube-system started at 2019-08-08 14:54:36 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.190: INFO: 	Container node-exporter ready: true, restart count 0
Aug  8 16:40:37.190: INFO: kube-proxy-dxmfq from kube-system started at 2019-08-08 14:54:36 +0000 UTC (1 container statuses recorded)
Aug  8 16:40:37.190: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b900155e9f9c68], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:40:38.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2228" for this suite.
Aug  8 16:40:44.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:40:45.135: INFO: namespace sched-pred-2228 deletion completed in 6.804850051s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:40:45.136: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug  8 16:40:45.460: INFO: PodSpec: initContainers in spec.initContainers
Aug  8 16:41:35.929: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a1480cc4-fffb-4740-932e-1a5c4cbc8681", GenerateName:"", Namespace:"init-container-6393", SelfLink:"/api/v1/namespaces/init-container-6393/pods/pod-init-a1480cc4-fffb-4740-932e-1a5c4cbc8681", UID:"814940b9-00ad-4641-a3f2-440eaa8778f5", ResourceVersion:"25501", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63700879245, loc:(*time.Location)(0x80bfa40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"460075085"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.215/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ggsjt", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0038f8b00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ggsjt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ggsjt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ggsjt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003ecd8a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0037aeae0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ecd920)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ecd940)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003ecd948), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003ecd94c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879245, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879245, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879245, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879245, loc:(*time.Location)(0x80bfa40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.5", PodIP:"100.96.1.215", StartTime:(*v1.Time)(0xc002bf1780), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002346f50)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002346fc0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://ccadb0f6d3957f7255769562988c9cba6bdec52d22f5d6ced56c065362df938d"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002bf17c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002bf17a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:41:35.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6393" for this suite.
Aug  8 16:42:00.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:42:00.794: INFO: namespace init-container-6393 deletion completed in 24.823611676s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:42:00.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Aug  8 16:42:01.080: INFO: Waiting up to 5m0s for pod "client-containers-5bb4ec2b-2e59-43fe-8486-896a077057c3" in namespace "containers-2416" to be "success or failure"
Aug  8 16:42:01.099: INFO: Pod "client-containers-5bb4ec2b-2e59-43fe-8486-896a077057c3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.9004ms
Aug  8 16:42:03.118: INFO: Pod "client-containers-5bb4ec2b-2e59-43fe-8486-896a077057c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038827432s
Aug  8 16:42:05.141: INFO: Pod "client-containers-5bb4ec2b-2e59-43fe-8486-896a077057c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061147843s
STEP: Saw pod success
Aug  8 16:42:05.141: INFO: Pod "client-containers-5bb4ec2b-2e59-43fe-8486-896a077057c3" satisfied condition "success or failure"
Aug  8 16:42:05.161: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod client-containers-5bb4ec2b-2e59-43fe-8486-896a077057c3 container test-container: <nil>
STEP: delete the pod
Aug  8 16:42:05.219: INFO: Waiting for pod client-containers-5bb4ec2b-2e59-43fe-8486-896a077057c3 to disappear
Aug  8 16:42:05.238: INFO: Pod client-containers-5bb4ec2b-2e59-43fe-8486-896a077057c3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:42:05.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2416" for this suite.
Aug  8 16:42:11.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:42:12.078: INFO: namespace containers-2416 deletion completed in 6.802433457s
•SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:42:12.078: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-3b0ba9bc-0980-4d97-a433-5f6debbee184 in namespace container-probe-6662
Aug  8 16:42:18.410: INFO: Started pod liveness-3b0ba9bc-0980-4d97-a433-5f6debbee184 in namespace container-probe-6662
STEP: checking the pod's current state and verifying that restartCount is present
Aug  8 16:42:18.429: INFO: Initial restart count of pod liveness-3b0ba9bc-0980-4d97-a433-5f6debbee184 is 0
Aug  8 16:42:34.614: INFO: Restart count of pod container-probe-6662/liveness-3b0ba9bc-0980-4d97-a433-5f6debbee184 is now 1 (16.185362178s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:42:34.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6662" for this suite.
Aug  8 16:42:40.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:42:41.514: INFO: namespace container-probe-6662 deletion completed in 6.83803068s
•SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:42:41.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:42:41.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e881b0e-512e-4949-80c7-9815108e4891" in namespace "projected-3310" to be "success or failure"
Aug  8 16:42:41.794: INFO: Pod "downwardapi-volume-1e881b0e-512e-4949-80c7-9815108e4891": Phase="Pending", Reason="", readiness=false. Elapsed: 20.624071ms
Aug  8 16:42:43.815: INFO: Pod "downwardapi-volume-1e881b0e-512e-4949-80c7-9815108e4891": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040893493s
Aug  8 16:42:45.835: INFO: Pod "downwardapi-volume-1e881b0e-512e-4949-80c7-9815108e4891": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061121875s
STEP: Saw pod success
Aug  8 16:42:45.835: INFO: Pod "downwardapi-volume-1e881b0e-512e-4949-80c7-9815108e4891" satisfied condition "success or failure"
Aug  8 16:42:45.854: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-1e881b0e-512e-4949-80c7-9815108e4891 container client-container: <nil>
STEP: delete the pod
Aug  8 16:42:45.907: INFO: Waiting for pod downwardapi-volume-1e881b0e-512e-4949-80c7-9815108e4891 to disappear
Aug  8 16:42:45.927: INFO: Pod downwardapi-volume-1e881b0e-512e-4949-80c7-9815108e4891 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:42:45.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3310" for this suite.
Aug  8 16:42:52.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:42:52.748: INFO: namespace projected-3310 deletion completed in 6.776813677s
•S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:42:52.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 16:42:53.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:42:57.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4798" for this suite.
Aug  8 16:43:37.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:43:38.227: INFO: namespace pods-4798 deletion completed in 40.793482088s
•SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:43:38.227: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug  8 16:43:38.547: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug  8 16:43:38.587: INFO: Waiting for terminating namespaces to be deleted...
Aug  8 16:43:38.606: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr before test
Aug  8 16:43:38.654: INFO: node-exporter-s4q7v from kube-system started at 2019-08-08 14:54:09 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container node-exporter ready: true, restart count 0
Aug  8 16:43:38.654: INFO: metrics-server-566847b67f-f2cmb from kube-system started at 2019-08-08 14:54:29 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container metrics-server ready: true, restart count 0
Aug  8 16:43:38.654: INFO: coredns-85cc454dd8-cqd76 from kube-system started at 2019-08-08 14:54:29 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container coredns ready: true, restart count 0
Aug  8 16:43:38.654: INFO: blackbox-exporter-954dd954b-6rgqr from kube-system started at 2019-08-08 14:54:09 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug  8 16:43:38.654: INFO: kube-proxy-vvfrb from kube-system started at 2019-08-08 14:54:09 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container kube-proxy ready: true, restart count 0
Aug  8 16:43:38.654: INFO: addons-kubernetes-dashboard-5c8d9945bc-spkwl from kube-system started at 2019-08-08 14:54:29 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug  8 16:43:38.654: INFO: coredns-85cc454dd8-2tknm from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container coredns ready: true, restart count 0
Aug  8 16:43:38.654: INFO: vpn-shoot-89d5dc9c8-84rcr from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container vpn-shoot ready: true, restart count 0
Aug  8 16:43:38.654: INFO: calico-kube-controllers-5f4b46ffb5-zt29p from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug  8 16:43:38.654: INFO: addons-nginx-ingress-controller-6496d947df-hzpqv from kube-system started at 2019-08-08 14:54:30 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug  8 16:43:38.654: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-695fb4fdcd-cknxk from kube-system started at 2019-08-08 14:54:31 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Aug  8 16:43:38.654: INFO: calico-node-6w86k from kube-system started at 2019-08-08 14:54:10 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.654: INFO: 	Container calico-node ready: true, restart count 0
Aug  8 16:43:38.654: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk before test
Aug  8 16:43:38.690: INFO: calico-node-84fr2 from kube-system started at 2019-08-08 14:54:36 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.690: INFO: 	Container calico-node ready: true, restart count 0
Aug  8 16:43:38.690: INFO: node-exporter-lzqzm from kube-system started at 2019-08-08 14:54:36 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.690: INFO: 	Container node-exporter ready: true, restart count 0
Aug  8 16:43:38.690: INFO: kube-proxy-dxmfq from kube-system started at 2019-08-08 14:54:36 +0000 UTC (1 container statuses recorded)
Aug  8 16:43:38.690: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
STEP: verifying the node has the label node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk
Aug  8 16:43:38.818: INFO: Pod addons-kubernetes-dashboard-5c8d9945bc-spkwl requesting resource cpu=50m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod addons-nginx-ingress-controller-6496d947df-hzpqv requesting resource cpu=100m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-695fb4fdcd-cknxk requesting resource cpu=0m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod blackbox-exporter-954dd954b-6rgqr requesting resource cpu=5m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod calico-kube-controllers-5f4b46ffb5-zt29p requesting resource cpu=0m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod calico-node-6w86k requesting resource cpu=100m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod calico-node-84fr2 requesting resource cpu=100m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk
Aug  8 16:43:38.818: INFO: Pod coredns-85cc454dd8-2tknm requesting resource cpu=50m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod coredns-85cc454dd8-cqd76 requesting resource cpu=50m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod kube-proxy-dxmfq requesting resource cpu=20m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk
Aug  8 16:43:38.818: INFO: Pod kube-proxy-vvfrb requesting resource cpu=20m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod metrics-server-566847b67f-f2cmb requesting resource cpu=20m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod node-exporter-lzqzm requesting resource cpu=5m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk
Aug  8 16:43:38.818: INFO: Pod node-exporter-s4q7v requesting resource cpu=5m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
Aug  8 16:43:38.818: INFO: Pod vpn-shoot-89d5dc9c8-84rcr requesting resource cpu=100m on Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26f1a680-1a4a-4e90-8fba-16f752c3d959.15b9003fa512151b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3160/filler-pod-26f1a680-1a4a-4e90-8fba-16f752c3d959 to shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26f1a680-1a4a-4e90-8fba-16f752c3d959.15b90040040342ef], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26f1a680-1a4a-4e90-8fba-16f752c3d959.15b9004033f6c4e6], Reason = [Created], Message = [Created container filler-pod-26f1a680-1a4a-4e90-8fba-16f752c3d959]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-26f1a680-1a4a-4e90-8fba-16f752c3d959.15b900403ed44a6b], Reason = [Started], Message = [Started container filler-pod-26f1a680-1a4a-4e90-8fba-16f752c3d959]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d7e16fb8-a912-425d-aabe-446d70d5fc93.15b9003fa649af90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3160/filler-pod-d7e16fb8-a912-425d-aabe-446d70d5fc93 to shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d7e16fb8-a912-425d-aabe-446d70d5fc93.15b900400a40eb3f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d7e16fb8-a912-425d-aabe-446d70d5fc93.15b900404071f786], Reason = [Created], Message = [Created container filler-pod-d7e16fb8-a912-425d-aabe-446d70d5fc93]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d7e16fb8-a912-425d-aabe-446d70d5fc93.15b900404a5e320a], Reason = [Started], Message = [Started container filler-pod-d7e16fb8-a912-425d-aabe-446d70d5fc93]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b900409b84beed], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:43:44.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3160" for this suite.
Aug  8 16:43:50.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:43:50.969: INFO: namespace sched-pred-3160 deletion completed in 6.832967082s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:43:50.970: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  8 16:43:51.247: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-3452'
Aug  8 16:43:51.732: INFO: stderr: ""
Aug  8 16:43:51.732: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug  8 16:43:56.782: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-3452 -o json'
Aug  8 16:43:56.958: INFO: stderr: ""
Aug  8 16:43:56.958: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.221/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-08-08T16:43:51Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-3452\",\n        \"resourceVersion\": \"25993\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3452/pods/e2e-test-nginx-pod\",\n        \"uid\": \"737a31a0-be47-4ff5-8930-3f070ed475d6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-wf2fh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-wf2fh\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-wf2fh\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-08T16:43:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-08T16:43:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-08T16:43:55Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-08T16:43:51Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c93ea50e894e5fa83fb10f8e2cfc9799ff4acf8817759dea029d6c45b39531ee\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-08T16:43:54Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.221\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-08T16:43:51Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug  8 16:43:56.958: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-3452'
Aug  8 16:43:57.226: INFO: stderr: ""
Aug  8 16:43:57.226: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Aug  8 16:43:57.277: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-3452'
Aug  8 16:44:03.919: INFO: stderr: ""
Aug  8 16:44:03.919: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:44:03.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3452" for this suite.
Aug  8 16:44:10.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:44:10.746: INFO: namespace kubectl-3452 deletion completed in 6.789484048s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:44:10.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3719
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Aug  8 16:44:11.252: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3719'
Aug  8 16:44:11.624: INFO: stderr: ""
Aug  8 16:44:11.624: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  8 16:44:11.624: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3719'
Aug  8 16:44:11.814: INFO: stderr: ""
Aug  8 16:44:11.814: INFO: stdout: "update-demo-nautilus-nlv5t update-demo-nautilus-qqfdf "
Aug  8 16:44:11.814: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nlv5t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3719'
Aug  8 16:44:12.001: INFO: stderr: ""
Aug  8 16:44:12.001: INFO: stdout: ""
Aug  8 16:44:12.001: INFO: update-demo-nautilus-nlv5t is created but not running
Aug  8 16:44:17.001: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3719'
Aug  8 16:44:17.173: INFO: stderr: ""
Aug  8 16:44:17.173: INFO: stdout: "update-demo-nautilus-nlv5t update-demo-nautilus-qqfdf "
Aug  8 16:44:17.173: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nlv5t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3719'
Aug  8 16:44:17.343: INFO: stderr: ""
Aug  8 16:44:17.343: INFO: stdout: "true"
Aug  8 16:44:17.343: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nlv5t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3719'
Aug  8 16:44:17.490: INFO: stderr: ""
Aug  8 16:44:17.490: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  8 16:44:17.490: INFO: validating pod update-demo-nautilus-nlv5t
Aug  8 16:44:17.595: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  8 16:44:17.595: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  8 16:44:17.595: INFO: update-demo-nautilus-nlv5t is verified up and running
Aug  8 16:44:17.595: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-qqfdf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3719'
Aug  8 16:44:17.761: INFO: stderr: ""
Aug  8 16:44:17.761: INFO: stdout: "true"
Aug  8 16:44:17.761: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-qqfdf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3719'
Aug  8 16:44:17.914: INFO: stderr: ""
Aug  8 16:44:17.914: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  8 16:44:17.914: INFO: validating pod update-demo-nautilus-qqfdf
Aug  8 16:44:18.020: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  8 16:44:18.020: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  8 16:44:18.020: INFO: update-demo-nautilus-qqfdf is verified up and running
STEP: rolling-update to new replication controller
Aug  8 16:44:18.022: INFO: scanned /root for discovery docs: <nil>
Aug  8 16:44:18.022: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3719'
Aug  8 16:44:40.539: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug  8 16:44:40.539: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  8 16:44:40.539: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3719'
Aug  8 16:44:40.729: INFO: stderr: ""
Aug  8 16:44:40.729: INFO: stdout: "update-demo-kitten-6kcqq update-demo-kitten-98bsh update-demo-nautilus-qqfdf "
STEP: Replicas for name=update-demo: expected=2 actual=3
Aug  8 16:44:45.729: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3719'
Aug  8 16:44:45.901: INFO: stderr: ""
Aug  8 16:44:45.901: INFO: stdout: "update-demo-kitten-6kcqq update-demo-kitten-98bsh "
Aug  8 16:44:45.901: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-6kcqq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3719'
Aug  8 16:44:46.085: INFO: stderr: ""
Aug  8 16:44:46.085: INFO: stdout: "true"
Aug  8 16:44:46.085: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-6kcqq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3719'
Aug  8 16:44:46.288: INFO: stderr: ""
Aug  8 16:44:46.288: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug  8 16:44:46.288: INFO: validating pod update-demo-kitten-6kcqq
Aug  8 16:44:46.394: INFO: got data: {
  "image": "kitten.jpg"
}

Aug  8 16:44:46.394: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug  8 16:44:46.394: INFO: update-demo-kitten-6kcqq is verified up and running
Aug  8 16:44:46.394: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-98bsh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3719'
Aug  8 16:44:46.588: INFO: stderr: ""
Aug  8 16:44:46.589: INFO: stdout: "true"
Aug  8 16:44:46.589: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-98bsh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3719'
Aug  8 16:44:46.765: INFO: stderr: ""
Aug  8 16:44:46.765: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug  8 16:44:46.765: INFO: validating pod update-demo-kitten-98bsh
Aug  8 16:44:46.870: INFO: got data: {
  "image": "kitten.jpg"
}

Aug  8 16:44:46.870: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug  8 16:44:46.870: INFO: update-demo-kitten-98bsh is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:44:46.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3719" for this suite.
Aug  8 16:45:10.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:45:11.696: INFO: namespace kubectl-3719 deletion completed in 24.784983171s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:45:11.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-xxh6
STEP: Creating a pod to test atomic-volume-subpath
Aug  8 16:45:12.012: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xxh6" in namespace "subpath-778" to be "success or failure"
Aug  8 16:45:12.032: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.104936ms
Aug  8 16:45:14.052: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039574107s
Aug  8 16:45:16.072: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 4.059672909s
Aug  8 16:45:18.092: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 6.079509647s
Aug  8 16:45:20.114: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 8.101820225s
Aug  8 16:45:22.134: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 10.121439588s
Aug  8 16:45:24.154: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 12.141804153s
Aug  8 16:45:26.174: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 14.161815328s
Aug  8 16:45:28.194: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 16.181630283s
Aug  8 16:45:30.215: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 18.202157528s
Aug  8 16:45:32.238: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 20.225365214s
Aug  8 16:45:34.265: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Running", Reason="", readiness=true. Elapsed: 22.252331313s
Aug  8 16:45:36.285: INFO: Pod "pod-subpath-test-configmap-xxh6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.272739354s
STEP: Saw pod success
Aug  8 16:45:36.285: INFO: Pod "pod-subpath-test-configmap-xxh6" satisfied condition "success or failure"
Aug  8 16:45:36.305: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-subpath-test-configmap-xxh6 container test-container-subpath-configmap-xxh6: <nil>
STEP: delete the pod
Aug  8 16:45:36.462: INFO: Waiting for pod pod-subpath-test-configmap-xxh6 to disappear
Aug  8 16:45:36.481: INFO: Pod pod-subpath-test-configmap-xxh6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xxh6
Aug  8 16:45:36.481: INFO: Deleting pod "pod-subpath-test-configmap-xxh6" in namespace "subpath-778"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:45:36.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-778" for this suite.
Aug  8 16:45:42.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:45:43.324: INFO: namespace subpath-778 deletion completed in 6.786545982s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:45:43.325: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9680
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug  8 16:45:43.667: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75e2928c-9fde-45dc-8467-217b3128b9d4" in namespace "downward-api-9680" to be "success or failure"
Aug  8 16:45:43.686: INFO: Pod "downwardapi-volume-75e2928c-9fde-45dc-8467-217b3128b9d4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.109942ms
Aug  8 16:45:45.706: INFO: Pod "downwardapi-volume-75e2928c-9fde-45dc-8467-217b3128b9d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03975205s
Aug  8 16:45:47.727: INFO: Pod "downwardapi-volume-75e2928c-9fde-45dc-8467-217b3128b9d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059796634s
STEP: Saw pod success
Aug  8 16:45:47.727: INFO: Pod "downwardapi-volume-75e2928c-9fde-45dc-8467-217b3128b9d4" satisfied condition "success or failure"
Aug  8 16:45:47.746: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downwardapi-volume-75e2928c-9fde-45dc-8467-217b3128b9d4 container client-container: <nil>
STEP: delete the pod
Aug  8 16:45:47.975: INFO: Waiting for pod downwardapi-volume-75e2928c-9fde-45dc-8467-217b3128b9d4 to disappear
Aug  8 16:45:47.994: INFO: Pod downwardapi-volume-75e2928c-9fde-45dc-8467-217b3128b9d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:45:47.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9680" for this suite.
Aug  8 16:45:54.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:45:54.846: INFO: namespace downward-api-9680 deletion completed in 6.810990304s
•S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:45:54.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 16:45:55.190: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug  8 16:45:59.231: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug  8 16:46:01.251: INFO: Creating deployment "test-rollover-deployment"
Aug  8 16:46:01.377: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug  8 16:46:03.500: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug  8 16:46:03.540: INFO: Ensure that both replica sets have 1 created replica
Aug  8 16:46:03.579: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug  8 16:46:03.617: INFO: Updating deployment test-rollover-deployment
Aug  8 16:46:03.617: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug  8 16:46:05.659: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug  8 16:46:05.698: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug  8 16:46:05.736: INFO: all replica sets need to contain the pod-template-hash label
Aug  8 16:46:05.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879563, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:46:07.777: INFO: all replica sets need to contain the pod-template-hash label
Aug  8 16:46:07.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879567, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:46:09.776: INFO: all replica sets need to contain the pod-template-hash label
Aug  8 16:46:09.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879567, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:46:11.776: INFO: all replica sets need to contain the pod-template-hash label
Aug  8 16:46:11.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879567, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:46:13.776: INFO: all replica sets need to contain the pod-template-hash label
Aug  8 16:46:13.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879567, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:46:15.777: INFO: all replica sets need to contain the pod-template-hash label
Aug  8 16:46:15.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879567, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700879561, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug  8 16:46:17.776: INFO: 
Aug  8 16:46:17.776: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug  8 16:46:17.833: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-1952,SelfLink:/apis/apps/v1/namespaces/deployment-1952/deployments/test-rollover-deployment,UID:554aef0a-3b85-48d9-89a0-9f067bcc0c55,ResourceVersion:26583,Generation:2,CreationTimestamp:2019-08-08 16:46:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-08 16:46:01 +0000 UTC 2019-08-08 16:46:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-08 16:46:17 +0000 UTC 2019-08-08 16:46:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug  8 16:46:17.853: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-1952,SelfLink:/apis/apps/v1/namespaces/deployment-1952/replicasets/test-rollover-deployment-854595fc44,UID:f106a21c-e866-4f9d-a688-144e01f3c716,ResourceVersion:26576,Generation:2,CreationTimestamp:2019-08-08 16:46:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 554aef0a-3b85-48d9-89a0-9f067bcc0c55 0xc0039462d7 0xc0039462d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug  8 16:46:17.853: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug  8 16:46:17.853: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-1952,SelfLink:/apis/apps/v1/namespaces/deployment-1952/replicasets/test-rollover-controller,UID:137c7fa6-bbf2-474b-83cf-91a173998059,ResourceVersion:26582,Generation:2,CreationTimestamp:2019-08-08 16:45:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 554aef0a-3b85-48d9-89a0-9f067bcc0c55 0xc002eb9fb7 0xc002eb9fb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  8 16:46:17.853: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-1952,SelfLink:/apis/apps/v1/namespaces/deployment-1952/replicasets/test-rollover-deployment-9b8b997cf,UID:fc7f5ed1-49b2-44d6-84e3-56af7c24e0dc,ResourceVersion:26538,Generation:2,CreationTimestamp:2019-08-08 16:46:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 554aef0a-3b85-48d9-89a0-9f067bcc0c55 0xc003946530 0xc003946531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug  8 16:46:17.873: INFO: Pod "test-rollover-deployment-854595fc44-25fdw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-25fdw,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-1952,SelfLink:/api/v1/namespaces/deployment-1952/pods/test-rollover-deployment-854595fc44-25fdw,UID:228aadbd-a325-4f1c-a201-fbf6b53116dd,ResourceVersion:26551,Generation:0,CreationTimestamp:2019-08-08 16:46:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.228/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 f106a21c-e866-4f9d-a688-144e01f3c716 0xc002b1a377 0xc002b1a378}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f7mmd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f7mmd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-f7mmd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b1a3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b1a400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 16:46:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 16:46:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 16:46:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-08 16:46:03 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.228,StartTime:2019-08-08 16:46:03 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-08 16:46:06 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://a27944136e22f50a7187fb72551af5693577116d58d3624fe7fc036057300d92}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:46:17.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1952" for this suite.
Aug  8 16:46:25.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:46:26.700: INFO: namespace deployment-1952 deletion completed in 8.789182143s
•SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:46:26.700: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug  8 16:46:31.564: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-2310 pod-service-account-e5e7a540-7ed7-4520-a7ba-ecec5781fbf0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug  8 16:46:32.259: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-2310 pod-service-account-e5e7a540-7ed7-4520-a7ba-ecec5781fbf0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug  8 16:46:32.900: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-2310 pod-service-account-e5e7a540-7ed7-4520-a7ba-ecec5781fbf0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:46:33.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2310" for this suite.
Aug  8 16:46:39.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:46:40.389: INFO: namespace svcaccounts-2310 deletion completed in 6.780350593s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:46:40.390: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug  8 16:46:40.821: INFO: Number of nodes with available pods: 0
Aug  8 16:46:40.821: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:41.878: INFO: Number of nodes with available pods: 0
Aug  8 16:46:41.878: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:42.878: INFO: Number of nodes with available pods: 0
Aug  8 16:46:42.878: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:43.880: INFO: Number of nodes with available pods: 1
Aug  8 16:46:43.880: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:46:44.878: INFO: Number of nodes with available pods: 1
Aug  8 16:46:44.878: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk is running more than one daemon pod
Aug  8 16:46:45.878: INFO: Number of nodes with available pods: 2
Aug  8 16:46:45.878: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug  8 16:46:45.977: INFO: Number of nodes with available pods: 1
Aug  8 16:46:45.977: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:47.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:47.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:48.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:48.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:49.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:49.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:50.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:50.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:51.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:51.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:52.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:52.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:53.035: INFO: Number of nodes with available pods: 1
Aug  8 16:46:53.035: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:54.037: INFO: Number of nodes with available pods: 1
Aug  8 16:46:54.037: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:55.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:55.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:56.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:56.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:57.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:57.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:58.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:58.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:46:59.034: INFO: Number of nodes with available pods: 1
Aug  8 16:46:59.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:47:00.034: INFO: Number of nodes with available pods: 1
Aug  8 16:47:00.034: INFO: Node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-j8dtr is running more than one daemon pod
Aug  8 16:47:01.043: INFO: Number of nodes with available pods: 2
Aug  8 16:47:01.043: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5107, will wait for the garbage collector to delete the pods
Aug  8 16:47:01.158: INFO: Deleting DaemonSet.extensions daemon-set took: 26.434955ms
Aug  8 16:47:01.559: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.412057ms
Aug  8 16:47:17.178: INFO: Number of nodes with available pods: 0
Aug  8 16:47:17.178: INFO: Number of running nodes: 0, number of available pods: 0
Aug  8 16:47:17.197: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5107/daemonsets","resourceVersion":"26827"},"items":null}

Aug  8 16:47:17.216: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5107/pods","resourceVersion":"26827"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:47:17.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5107" for this suite.
Aug  8 16:47:25.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:47:26.104: INFO: namespace daemonsets-5107 deletion completed in 8.793667321s
•SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:47:26.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5077
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug  8 16:47:26.379: INFO: Waiting up to 5m0s for pod "pod-e45aa4e5-c4d0-4070-acae-b3e65b978e2d" in namespace "emptydir-5077" to be "success or failure"
Aug  8 16:47:26.398: INFO: Pod "pod-e45aa4e5-c4d0-4070-acae-b3e65b978e2d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.13941ms
Aug  8 16:47:28.418: INFO: Pod "pod-e45aa4e5-c4d0-4070-acae-b3e65b978e2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039306808s
Aug  8 16:47:30.439: INFO: Pod "pod-e45aa4e5-c4d0-4070-acae-b3e65b978e2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059818313s
STEP: Saw pod success
Aug  8 16:47:30.439: INFO: Pod "pod-e45aa4e5-c4d0-4070-acae-b3e65b978e2d" satisfied condition "success or failure"
Aug  8 16:47:30.458: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-e45aa4e5-c4d0-4070-acae-b3e65b978e2d container test-container: <nil>
STEP: delete the pod
Aug  8 16:47:30.556: INFO: Waiting for pod pod-e45aa4e5-c4d0-4070-acae-b3e65b978e2d to disappear
Aug  8 16:47:30.575: INFO: Pod pod-e45aa4e5-c4d0-4070-acae-b3e65b978e2d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:47:30.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5077" for this suite.
Aug  8 16:47:36.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:47:37.418: INFO: namespace emptydir-5077 deletion completed in 6.806623908s
•SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:47:37.418: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3373
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-3373/secret-test-36731160-7153-4285-b365-5a2c9e27a690
STEP: Creating a pod to test consume secrets
Aug  8 16:47:37.787: INFO: Waiting up to 5m0s for pod "pod-configmaps-f627fe4f-6c89-4d8a-84a5-960e287b2266" in namespace "secrets-3373" to be "success or failure"
Aug  8 16:47:37.808: INFO: Pod "pod-configmaps-f627fe4f-6c89-4d8a-84a5-960e287b2266": Phase="Pending", Reason="", readiness=false. Elapsed: 20.735927ms
Aug  8 16:47:39.828: INFO: Pod "pod-configmaps-f627fe4f-6c89-4d8a-84a5-960e287b2266": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040594808s
Aug  8 16:47:41.851: INFO: Pod "pod-configmaps-f627fe4f-6c89-4d8a-84a5-960e287b2266": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063623156s
STEP: Saw pod success
Aug  8 16:47:41.851: INFO: Pod "pod-configmaps-f627fe4f-6c89-4d8a-84a5-960e287b2266" satisfied condition "success or failure"
Aug  8 16:47:41.870: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-f627fe4f-6c89-4d8a-84a5-960e287b2266 container env-test: <nil>
STEP: delete the pod
Aug  8 16:47:41.933: INFO: Waiting for pod pod-configmaps-f627fe4f-6c89-4d8a-84a5-960e287b2266 to disappear
Aug  8 16:47:41.952: INFO: Pod pod-configmaps-f627fe4f-6c89-4d8a-84a5-960e287b2266 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:47:41.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3373" for this suite.
Aug  8 16:47:48.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:47:48.775: INFO: namespace secrets-3373 deletion completed in 6.787599955s
•SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:47:48.776: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:47:49.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7845" for this suite.
Aug  8 16:47:55.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:47:55.935: INFO: namespace kubelet-test-7845 deletion completed in 6.824788447s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:47:55.935: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug  8 16:47:56.252: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6006'
Aug  8 16:47:56.655: INFO: stderr: ""
Aug  8 16:47:56.655: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  8 16:47:56.656: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6006'
Aug  8 16:47:56.856: INFO: stderr: ""
Aug  8 16:47:56.856: INFO: stdout: "update-demo-nautilus-jzkzh update-demo-nautilus-l4wrz "
Aug  8 16:47:56.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-jzkzh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:47:57.044: INFO: stderr: ""
Aug  8 16:47:57.044: INFO: stdout: ""
Aug  8 16:47:57.044: INFO: update-demo-nautilus-jzkzh is created but not running
Aug  8 16:48:02.045: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6006'
Aug  8 16:48:02.229: INFO: stderr: ""
Aug  8 16:48:02.229: INFO: stdout: "update-demo-nautilus-jzkzh update-demo-nautilus-l4wrz "
Aug  8 16:48:02.229: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-jzkzh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:02.402: INFO: stderr: ""
Aug  8 16:48:02.402: INFO: stdout: "true"
Aug  8 16:48:02.402: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-jzkzh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:02.582: INFO: stderr: ""
Aug  8 16:48:02.582: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  8 16:48:02.582: INFO: validating pod update-demo-nautilus-jzkzh
Aug  8 16:48:02.688: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  8 16:48:02.688: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  8 16:48:02.688: INFO: update-demo-nautilus-jzkzh is verified up and running
Aug  8 16:48:02.688: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l4wrz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:02.878: INFO: stderr: ""
Aug  8 16:48:02.878: INFO: stdout: "true"
Aug  8 16:48:02.878: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l4wrz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:03.053: INFO: stderr: ""
Aug  8 16:48:03.053: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  8 16:48:03.053: INFO: validating pod update-demo-nautilus-l4wrz
Aug  8 16:48:03.158: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  8 16:48:03.158: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  8 16:48:03.158: INFO: update-demo-nautilus-l4wrz is verified up and running
STEP: scaling down the replication controller
Aug  8 16:48:03.161: INFO: scanned /root for discovery docs: <nil>
Aug  8 16:48:03.161: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6006'
Aug  8 16:48:03.379: INFO: stderr: ""
Aug  8 16:48:03.379: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  8 16:48:03.379: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6006'
Aug  8 16:48:03.559: INFO: stderr: ""
Aug  8 16:48:03.559: INFO: stdout: "update-demo-nautilus-jzkzh update-demo-nautilus-l4wrz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug  8 16:48:08.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6006'
Aug  8 16:48:08.723: INFO: stderr: ""
Aug  8 16:48:08.723: INFO: stdout: "update-demo-nautilus-l4wrz "
Aug  8 16:48:08.723: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l4wrz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:08.954: INFO: stderr: ""
Aug  8 16:48:08.954: INFO: stdout: "true"
Aug  8 16:48:08.954: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l4wrz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:14.147: INFO: stderr: ""
Aug  8 16:48:14.147: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  8 16:48:14.147: INFO: validating pod update-demo-nautilus-l4wrz
Aug  8 16:48:14.170: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  8 16:48:14.170: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  8 16:48:14.170: INFO: update-demo-nautilus-l4wrz is verified up and running
STEP: scaling up the replication controller
Aug  8 16:48:14.172: INFO: scanned /root for discovery docs: <nil>
Aug  8 16:48:14.172: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6006'
Aug  8 16:48:14.411: INFO: stderr: ""
Aug  8 16:48:14.411: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  8 16:48:14.411: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6006'
Aug  8 16:48:14.590: INFO: stderr: ""
Aug  8 16:48:14.590: INFO: stdout: "update-demo-nautilus-6z8qr update-demo-nautilus-l4wrz "
Aug  8 16:48:14.590: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6z8qr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:14.761: INFO: stderr: ""
Aug  8 16:48:14.761: INFO: stdout: ""
Aug  8 16:48:14.761: INFO: update-demo-nautilus-6z8qr is created but not running
Aug  8 16:48:19.761: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6006'
Aug  8 16:48:19.919: INFO: stderr: ""
Aug  8 16:48:19.919: INFO: stdout: "update-demo-nautilus-6z8qr update-demo-nautilus-l4wrz "
Aug  8 16:48:19.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6z8qr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:20.093: INFO: stderr: ""
Aug  8 16:48:20.093: INFO: stdout: "true"
Aug  8 16:48:20.093: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6z8qr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:20.266: INFO: stderr: ""
Aug  8 16:48:20.266: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  8 16:48:20.266: INFO: validating pod update-demo-nautilus-6z8qr
Aug  8 16:48:20.372: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  8 16:48:20.372: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  8 16:48:20.372: INFO: update-demo-nautilus-6z8qr is verified up and running
Aug  8 16:48:20.372: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l4wrz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:20.539: INFO: stderr: ""
Aug  8 16:48:20.539: INFO: stdout: "true"
Aug  8 16:48:20.539: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l4wrz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6006'
Aug  8 16:48:20.697: INFO: stderr: ""
Aug  8 16:48:20.697: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  8 16:48:20.697: INFO: validating pod update-demo-nautilus-l4wrz
Aug  8 16:48:20.719: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  8 16:48:20.719: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  8 16:48:20.719: INFO: update-demo-nautilus-l4wrz is verified up and running
STEP: using delete to clean up resources
Aug  8 16:48:20.719: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-6006'
Aug  8 16:48:20.943: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 16:48:20.943: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug  8 16:48:20.943: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6006'
Aug  8 16:48:21.130: INFO: stderr: "No resources found.\n"
Aug  8 16:48:21.130: INFO: stdout: ""
Aug  8 16:48:21.130: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-6006 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug  8 16:48:21.356: INFO: stderr: ""
Aug  8 16:48:21.356: INFO: stdout: "update-demo-nautilus-6z8qr\nupdate-demo-nautilus-l4wrz\n"
Aug  8 16:48:21.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6006'
Aug  8 16:48:22.028: INFO: stderr: "No resources found.\n"
Aug  8 16:48:22.028: INFO: stdout: ""
Aug  8 16:48:22.028: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-6006 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug  8 16:48:22.247: INFO: stderr: ""
Aug  8 16:48:22.247: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:48:22.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6006" for this suite.
Aug  8 16:48:44.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:48:45.112: INFO: namespace kubectl-6006 deletion completed in 22.817856396s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:48:45.112: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug  8 16:48:45.345: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1584'
Aug  8 16:48:45.745: INFO: stderr: ""
Aug  8 16:48:45.745: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug  8 16:48:45.745: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1584'
Aug  8 16:48:45.911: INFO: stderr: ""
Aug  8 16:48:45.911: INFO: stdout: "update-demo-nautilus-66wnl update-demo-nautilus-c8dpg "
Aug  8 16:48:45.911: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-66wnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1584'
Aug  8 16:48:46.071: INFO: stderr: ""
Aug  8 16:48:46.071: INFO: stdout: ""
Aug  8 16:48:46.071: INFO: update-demo-nautilus-66wnl is created but not running
Aug  8 16:48:51.071: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1584'
Aug  8 16:48:51.287: INFO: stderr: ""
Aug  8 16:48:51.287: INFO: stdout: "update-demo-nautilus-66wnl update-demo-nautilus-c8dpg "
Aug  8 16:48:51.287: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-66wnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1584'
Aug  8 16:48:51.489: INFO: stderr: ""
Aug  8 16:48:51.489: INFO: stdout: "true"
Aug  8 16:48:51.489: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-66wnl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1584'
Aug  8 16:48:51.693: INFO: stderr: ""
Aug  8 16:48:51.693: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  8 16:48:51.693: INFO: validating pod update-demo-nautilus-66wnl
Aug  8 16:48:51.799: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  8 16:48:51.799: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  8 16:48:51.799: INFO: update-demo-nautilus-66wnl is verified up and running
Aug  8 16:48:51.799: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-c8dpg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1584'
Aug  8 16:48:52.006: INFO: stderr: ""
Aug  8 16:48:52.006: INFO: stdout: "true"
Aug  8 16:48:52.006: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-c8dpg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1584'
Aug  8 16:48:52.211: INFO: stderr: ""
Aug  8 16:48:52.211: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug  8 16:48:52.211: INFO: validating pod update-demo-nautilus-c8dpg
Aug  8 16:48:52.319: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug  8 16:48:52.319: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug  8 16:48:52.319: INFO: update-demo-nautilus-c8dpg is verified up and running
STEP: using delete to clean up resources
Aug  8 16:48:52.319: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1584'
Aug  8 16:48:52.520: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug  8 16:48:52.520: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug  8 16:48:52.520: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1584'
Aug  8 16:48:52.711: INFO: stderr: "No resources found.\n"
Aug  8 16:48:52.711: INFO: stdout: ""
Aug  8 16:48:52.711: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-1584 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug  8 16:48:52.894: INFO: stderr: ""
Aug  8 16:48:52.894: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:48:52.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1584" for this suite.
Aug  8 16:49:16.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:49:17.853: INFO: namespace kubectl-1584 deletion completed in 24.921346817s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:49:17.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-9590
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Aug  8 16:49:18.168: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9590" to be "success or failure"
Aug  8 16:49:18.187: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 19.13501ms
Aug  8 16:49:20.208: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039366886s
Aug  8 16:49:22.237: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068310128s
Aug  8 16:49:24.257: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088417002s
STEP: Saw pod success
Aug  8 16:49:24.257: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug  8 16:49:24.276: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug  8 16:49:24.404: INFO: Waiting for pod pod-host-path-test to disappear
Aug  8 16:49:24.423: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:49:24.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9590" for this suite.
Aug  8 16:49:30.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:49:31.248: INFO: namespace hostpath-9590 deletion completed in 6.787778705s
•
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:49:31.250: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4603
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Aug  8 16:49:31.571: INFO: Waiting up to 5m0s for pod "var-expansion-74ddf24b-0221-46ca-86a4-da7f15094db2" in namespace "var-expansion-4603" to be "success or failure"
Aug  8 16:49:31.591: INFO: Pod "var-expansion-74ddf24b-0221-46ca-86a4-da7f15094db2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.00877ms
Aug  8 16:49:33.611: INFO: Pod "var-expansion-74ddf24b-0221-46ca-86a4-da7f15094db2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039007808s
Aug  8 16:49:35.630: INFO: Pod "var-expansion-74ddf24b-0221-46ca-86a4-da7f15094db2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058968262s
STEP: Saw pod success
Aug  8 16:49:35.631: INFO: Pod "var-expansion-74ddf24b-0221-46ca-86a4-da7f15094db2" satisfied condition "success or failure"
Aug  8 16:49:35.652: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod var-expansion-74ddf24b-0221-46ca-86a4-da7f15094db2 container dapi-container: <nil>
STEP: delete the pod
Aug  8 16:49:35.703: INFO: Waiting for pod var-expansion-74ddf24b-0221-46ca-86a4-da7f15094db2 to disappear
Aug  8 16:49:35.722: INFO: Pod var-expansion-74ddf24b-0221-46ca-86a4-da7f15094db2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:49:35.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4603" for this suite.
Aug  8 16:49:41.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:49:42.544: INFO: namespace var-expansion-4603 deletion completed in 6.785175476s
•SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:49:42.545: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 16:49:42.851: INFO: Creating ReplicaSet my-hostname-basic-844370ba-c93e-4f7a-9cc5-04812ca0f521
Aug  8 16:49:42.894: INFO: Pod name my-hostname-basic-844370ba-c93e-4f7a-9cc5-04812ca0f521: Found 1 pods out of 1
Aug  8 16:49:42.894: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-844370ba-c93e-4f7a-9cc5-04812ca0f521" is running
Aug  8 16:49:46.935: INFO: Pod "my-hostname-basic-844370ba-c93e-4f7a-9cc5-04812ca0f521-4zwvf" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-08 16:49:42 +0000 UTC Reason: Message:}])
Aug  8 16:49:46.935: INFO: Trying to dial the pod
Aug  8 16:49:52.081: INFO: Controller my-hostname-basic-844370ba-c93e-4f7a-9cc5-04812ca0f521: Got expected result from replica 1 [my-hostname-basic-844370ba-c93e-4f7a-9cc5-04812ca0f521-4zwvf]: "my-hostname-basic-844370ba-c93e-4f7a-9cc5-04812ca0f521-4zwvf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:49:52.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5091" for this suite.
Aug  8 16:49:58.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:49:58.908: INFO: namespace replicaset-5091 deletion completed in 6.790547369s
•SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:49:58.909: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug  8 16:49:59.179: INFO: Waiting up to 5m0s for pod "downward-api-34cd8305-1d47-44ac-b9d7-4a6d1634de67" in namespace "downward-api-4825" to be "success or failure"
Aug  8 16:49:59.198: INFO: Pod "downward-api-34cd8305-1d47-44ac-b9d7-4a6d1634de67": Phase="Pending", Reason="", readiness=false. Elapsed: 18.953569ms
Aug  8 16:50:01.218: INFO: Pod "downward-api-34cd8305-1d47-44ac-b9d7-4a6d1634de67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039322387s
Aug  8 16:50:03.244: INFO: Pod "downward-api-34cd8305-1d47-44ac-b9d7-4a6d1634de67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065060745s
Aug  8 16:50:05.264: INFO: Pod "downward-api-34cd8305-1d47-44ac-b9d7-4a6d1634de67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08576077s
STEP: Saw pod success
Aug  8 16:50:05.264: INFO: Pod "downward-api-34cd8305-1d47-44ac-b9d7-4a6d1634de67" satisfied condition "success or failure"
Aug  8 16:50:05.284: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downward-api-34cd8305-1d47-44ac-b9d7-4a6d1634de67 container dapi-container: <nil>
STEP: delete the pod
Aug  8 16:50:05.355: INFO: Waiting for pod downward-api-34cd8305-1d47-44ac-b9d7-4a6d1634de67 to disappear
Aug  8 16:50:05.374: INFO: Pod downward-api-34cd8305-1d47-44ac-b9d7-4a6d1634de67 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:50:05.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4825" for this suite.
Aug  8 16:50:11.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:50:12.203: INFO: namespace downward-api-4825 deletion completed in 6.793098582s
•SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:50:12.203: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4748
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:50:16.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4748" for this suite.
Aug  8 16:50:22.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:50:23.479: INFO: namespace kubelet-test-4748 deletion completed in 6.921255713s
•SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:50:23.479: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-519
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 16:50:23.844: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"caffbc9e-3e56-4909-8655-3e5ea56e372f", Controller:(*bool)(0xc001081d76), BlockOwnerDeletion:(*bool)(0xc001081d77)}}
Aug  8 16:50:23.864: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3d519ceb-ce80-4802-ae62-6e7dba6e67d9", Controller:(*bool)(0xc001116a16), BlockOwnerDeletion:(*bool)(0xc001116a17)}}
Aug  8 16:50:23.885: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dae42838-835d-4a97-adf0-6ea082895a92", Controller:(*bool)(0xc000c0cd76), BlockOwnerDeletion:(*bool)(0xc000c0cd77)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:50:28.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-519" for this suite.
Aug  8 16:50:35.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:50:35.808: INFO: namespace gc-519 deletion completed in 6.83377791s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:50:35.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug  8 16:50:40.738: INFO: Successfully updated pod "annotationupdate48fd92ac-2836-4560-8698-607d321e5766"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:50:42.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5963" for this suite.
Aug  8 16:51:04.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:51:05.632: INFO: namespace projected-5963 deletion completed in 22.802776412s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:51:05.632: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Aug  8 16:51:05.951: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix247567687/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:51:06.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2612" for this suite.
Aug  8 16:51:12.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:51:12.810: INFO: namespace kubectl-2612 deletion completed in 6.777871477s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:51:12.811: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7571
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-d80d3bed-0520-4b42-b285-bbc8e76aa65f
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:51:13.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7571" for this suite.
Aug  8 16:51:19.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:51:19.872: INFO: namespace secrets-7571 deletion completed in 6.782983436s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:51:19.872: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1277
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug  8 16:51:20.242: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-1277'
Aug  8 16:51:20.413: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug  8 16:51:20.413: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Aug  8 16:51:22.461: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-lf7ue.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-1277'
Aug  8 16:51:22.632: INFO: stderr: ""
Aug  8 16:51:22.632: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:51:22.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1277" for this suite.
Aug  8 16:53:26.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:53:27.481: INFO: namespace kubectl-1277 deletion completed in 2m4.810226226s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:53:27.482: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug  8 16:53:27.784: INFO: Waiting up to 5m0s for pod "pod-c562f6ae-7ffa-4ab1-845e-9c9cbcd0c788" in namespace "emptydir-4609" to be "success or failure"
Aug  8 16:53:27.803: INFO: Pod "pod-c562f6ae-7ffa-4ab1-845e-9c9cbcd0c788": Phase="Pending", Reason="", readiness=false. Elapsed: 18.904392ms
Aug  8 16:53:29.822: INFO: Pod "pod-c562f6ae-7ffa-4ab1-845e-9c9cbcd0c788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038607897s
Aug  8 16:53:31.843: INFO: Pod "pod-c562f6ae-7ffa-4ab1-845e-9c9cbcd0c788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059218293s
STEP: Saw pod success
Aug  8 16:53:31.843: INFO: Pod "pod-c562f6ae-7ffa-4ab1-845e-9c9cbcd0c788" satisfied condition "success or failure"
Aug  8 16:53:31.862: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-c562f6ae-7ffa-4ab1-845e-9c9cbcd0c788 container test-container: <nil>
STEP: delete the pod
Aug  8 16:53:31.920: INFO: Waiting for pod pod-c562f6ae-7ffa-4ab1-845e-9c9cbcd0c788 to disappear
Aug  8 16:53:31.947: INFO: Pod pod-c562f6ae-7ffa-4ab1-845e-9c9cbcd0c788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:53:31.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4609" for this suite.
Aug  8 16:53:38.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:53:38.763: INFO: namespace emptydir-4609 deletion completed in 6.779886403s
•S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:53:38.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-548
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-e9dcd7cb-3837-41af-bdde-e4627bce60cb
STEP: Creating a pod to test consume configMaps
Aug  8 16:53:39.100: INFO: Waiting up to 5m0s for pod "pod-configmaps-9a0decf5-9857-4e5a-a8e3-278efb7e6e53" in namespace "configmap-548" to be "success or failure"
Aug  8 16:53:39.121: INFO: Pod "pod-configmaps-9a0decf5-9857-4e5a-a8e3-278efb7e6e53": Phase="Pending", Reason="", readiness=false. Elapsed: 21.54804ms
Aug  8 16:53:41.141: INFO: Pod "pod-configmaps-9a0decf5-9857-4e5a-a8e3-278efb7e6e53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04161008s
Aug  8 16:53:43.162: INFO: Pod "pod-configmaps-9a0decf5-9857-4e5a-a8e3-278efb7e6e53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062022935s
STEP: Saw pod success
Aug  8 16:53:43.162: INFO: Pod "pod-configmaps-9a0decf5-9857-4e5a-a8e3-278efb7e6e53" satisfied condition "success or failure"
Aug  8 16:53:43.181: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-configmaps-9a0decf5-9857-4e5a-a8e3-278efb7e6e53 container configmap-volume-test: <nil>
STEP: delete the pod
Aug  8 16:53:43.232: INFO: Waiting for pod pod-configmaps-9a0decf5-9857-4e5a-a8e3-278efb7e6e53 to disappear
Aug  8 16:53:43.251: INFO: Pod pod-configmaps-9a0decf5-9857-4e5a-a8e3-278efb7e6e53 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:53:43.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-548" for this suite.
Aug  8 16:53:49.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:53:50.078: INFO: namespace configmap-548 deletion completed in 6.789655505s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:53:50.079: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug  8 16:53:50.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:53:59.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7147" for this suite.
Aug  8 16:54:47.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:54:48.451: INFO: namespace pods-7147 deletion completed in 48.799562654s
•SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:54:48.451: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug  8 16:54:57.024: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:54:57.062: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:54:59.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:54:59.082: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:55:01.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:55:01.082: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:55:03.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:55:03.082: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:55:05.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:55:05.083: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:55:07.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:55:07.082: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:55:09.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:55:09.082: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:55:11.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:55:11.083: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:55:13.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:55:13.083: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:55:15.063: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:55:15.082: INFO: Pod pod-with-prestop-exec-hook still exists
Aug  8 16:55:17.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug  8 16:55:17.082: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:55:17.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2400" for this suite.
Aug  8 16:55:41.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:55:41.928: INFO: namespace container-lifecycle-hook-2400 deletion completed in 24.778942119s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:55:41.930: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug  8 16:55:42.278: INFO: Waiting up to 5m0s for pod "downward-api-feba1371-7982-4648-b838-938ad21c571a" in namespace "downward-api-535" to be "success or failure"
Aug  8 16:55:42.297: INFO: Pod "downward-api-feba1371-7982-4648-b838-938ad21c571a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.909255ms
Aug  8 16:55:44.317: INFO: Pod "downward-api-feba1371-7982-4648-b838-938ad21c571a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038881471s
Aug  8 16:55:46.338: INFO: Pod "downward-api-feba1371-7982-4648-b838-938ad21c571a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059206411s
STEP: Saw pod success
Aug  8 16:55:46.338: INFO: Pod "downward-api-feba1371-7982-4648-b838-938ad21c571a" satisfied condition "success or failure"
Aug  8 16:55:46.357: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod downward-api-feba1371-7982-4648-b838-938ad21c571a container dapi-container: <nil>
STEP: delete the pod
Aug  8 16:55:46.656: INFO: Waiting for pod downward-api-feba1371-7982-4648-b838-938ad21c571a to disappear
Aug  8 16:55:46.675: INFO: Pod downward-api-feba1371-7982-4648-b838-938ad21c571a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:55:46.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-535" for this suite.
Aug  8 16:55:52.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:55:53.531: INFO: namespace downward-api-535 deletion completed in 6.812828507s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:55:53.531: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-2f6d78bc-3a28-44fe-ab82-542fb4f10a9a in namespace container-probe-4596
Aug  8 16:55:57.923: INFO: Started pod liveness-2f6d78bc-3a28-44fe-ab82-542fb4f10a9a in namespace container-probe-4596
STEP: checking the pod's current state and verifying that restartCount is present
Aug  8 16:55:57.943: INFO: Initial restart count of pod liveness-2f6d78bc-3a28-44fe-ab82-542fb4f10a9a is 0
Aug  8 16:56:14.127: INFO: Restart count of pod container-probe-4596/liveness-2f6d78bc-3a28-44fe-ab82-542fb4f10a9a is now 1 (16.183819662s elapsed)
Aug  8 16:56:32.325: INFO: Restart count of pod container-probe-4596/liveness-2f6d78bc-3a28-44fe-ab82-542fb4f10a9a is now 2 (34.382731128s elapsed)
Aug  8 16:56:52.526: INFO: Restart count of pod container-probe-4596/liveness-2f6d78bc-3a28-44fe-ab82-542fb4f10a9a is now 3 (54.583731032s elapsed)
Aug  8 16:57:12.734: INFO: Restart count of pod container-probe-4596/liveness-2f6d78bc-3a28-44fe-ab82-542fb4f10a9a is now 4 (1m14.791251848s elapsed)
Aug  8 16:58:15.362: INFO: Restart count of pod container-probe-4596/liveness-2f6d78bc-3a28-44fe-ab82-542fb4f10a9a is now 5 (2m17.419153073s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:58:15.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4596" for this suite.
Aug  8 16:58:21.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:58:22.215: INFO: namespace container-probe-4596 deletion completed in 6.781295137s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug  8 16:58:22.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-d5fd0db1-7ace-485a-9e7b-71d12ad56c4d
STEP: Creating a pod to test consume secrets
Aug  8 16:58:22.495: INFO: Waiting up to 5m0s for pod "pod-secrets-7c7089ff-dd58-4cf1-9244-ee9292b078e1" in namespace "secrets-3349" to be "success or failure"
Aug  8 16:58:22.514: INFO: Pod "pod-secrets-7c7089ff-dd58-4cf1-9244-ee9292b078e1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.902646ms
Aug  8 16:58:24.533: INFO: Pod "pod-secrets-7c7089ff-dd58-4cf1-9244-ee9292b078e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038474688s
Aug  8 16:58:26.553: INFO: Pod "pod-secrets-7c7089ff-dd58-4cf1-9244-ee9292b078e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058379089s
STEP: Saw pod success
Aug  8 16:58:26.553: INFO: Pod "pod-secrets-7c7089ff-dd58-4cf1-9244-ee9292b078e1" satisfied condition "success or failure"
Aug  8 16:58:26.572: INFO: Trying to get logs from node shoot--it--tm-lf7ue-cpu-worker-7fd44fc866-pnzwk pod pod-secrets-7c7089ff-dd58-4cf1-9244-ee9292b078e1 container secret-volume-test: <nil>
STEP: delete the pod
Aug  8 16:58:26.634: INFO: Waiting for pod pod-secrets-7c7089ff-dd58-4cf1-9244-ee9292b078e1 to disappear
Aug  8 16:58:26.653: INFO: Pod pod-secrets-7c7089ff-dd58-4cf1-9244-ee9292b078e1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug  8 16:58:26.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3349" for this suite.
Aug  8 16:58:32.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug  8 16:58:33.497: INFO: namespace secrets-3349 deletion completed in 6.808524095s
•SSSSSSSSSSSSSSSSSSSSSSSSAug  8 16:58:33.498: INFO: Running AfterSuite actions on all nodes
Aug  8 16:58:33.498: INFO: Running AfterSuite actions on node 1
Aug  8 16:58:33.498: INFO: Skipping dumping logs from cluster

Ran 212 of 4413 Specs in 6827.480 seconds
SUCCESS! -- 212 Passed | 0 Failed | 0 Flaked | 0 Pending | 4201 Skipped
PASS

Ginkgo ran 1 suite in 1h54m23.895609235s
Test Suite Passed
